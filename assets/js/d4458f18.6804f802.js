"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[32238],{3905:(e,t,o)=>{o.d(t,{Zo:()=>c,kt:()=>h});var a=o(67294);function n(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function l(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,a)}return o}function r(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?l(Object(o),!0).forEach((function(t){n(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):l(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function i(e,t){if(null==e)return{};var o,a,n=function(e,t){if(null==e)return{};var o,a,n={},l=Object.keys(e);for(a=0;a<l.length;a++)o=l[a],t.indexOf(o)>=0||(n[o]=e[o]);return n}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)o=l[a],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(n[o]=e[o])}return n}var s=a.createContext({}),p=function(e){var t=a.useContext(s),o=t;return e&&(o="function"==typeof e?e(t):r(r({},t),e)),o},c=function(e){var t=p(e.components);return a.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var o=e.components,n=e.mdxType,l=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),d=p(o),h=n,k=d["".concat(s,".").concat(h)]||d[h]||u[h]||l;return o?a.createElement(k,r(r({ref:t},c),{},{components:o})):a.createElement(k,r({ref:t},c))}));function h(e,t){var o=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var l=o.length,r=new Array(l);r[0]=d;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:n,r[1]=i;for(var p=2;p<l;p++)r[p]=o[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,o)}d.displayName="MDXCreateElement"},90972:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>p});var a=o(87462),n=(o(67294),o(3905));const l={},r="Lab: Running Apache Spark jobs on Cloud Dataproc",i={unversionedId:"processing/lab-gcp-dataproc/README",id:"processing/lab-gcp-dataproc/README",title:"Lab: Running Apache Spark jobs on Cloud Dataproc",description:"Objective",source:"@site/docs/03-processing/lab-gcp-dataproc/README.md",sourceDirName:"03-processing/lab-gcp-dataproc",slug:"/processing/lab-gcp-dataproc/",permalink:"/docs/processing/lab-gcp-dataproc/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681732641,formattedLastUpdatedAt:"Apr 17, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Dataproc",permalink:"/docs/processing/gcp-dataproc"},next:{title:"Lab: Simple Data Pipeline with HDInsight",permalink:"/docs/processing/lab-azure-hdinsight-simple-data-processing/"}},s={},p=[{value:"Objective",id:"objective",level:2},{value:"Task 1. Lift and shift",id:"task-1-lift-and-shift",level:2},{value:"Migrate existing Spark jobs to Cloud Dataproc",id:"migrate-existing-spark-jobs-to-cloud-dataproc",level:3},{value:"Configure and start a Cloud Dataproc cluster",id:"configure-and-start-a-cloud-dataproc-cluster",level:3},{value:"Clone the source repository for the lab",id:"clone-the-source-repository-for-the-lab",level:3},{value:"Log in to the Jupyter Notebook",id:"log-in-to-the-jupyter-notebook",level:3},{value:"Task 2. Separate compute and storage",id:"task-2-separate-compute-and-storage",level:2},{value:"Modify Spark jobs to use Cloud Storage instead of HDFS",id:"modify-spark-jobs-to-use-cloud-storage-instead-of-hdfs",level:3},{value:"Task 3. Deploy Spark jobs",id:"task-3-deploy-spark-jobs",level:2},{value:"Optimize Spark jobs to run on Job specific clusters",id:"optimize-spark-jobs-to-run-on-job-specific-clusters",level:3},{value:"Run the Analysis Job from Cloud Shell.",id:"run-the-analysis-job-from-cloud-shell",level:3}],c={toc:p};function u(e){let{components:t,...o}=e;return(0,n.kt)("wrapper",(0,a.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"lab-running-apache-spark-jobs-on-cloud-dataproc"},"Lab: Running Apache Spark jobs on Cloud Dataproc"),(0,n.kt)("h2",{id:"objective"},"Objective"),(0,n.kt)("p",null,"Running Apache Spark jobs on Cloud Dataproc"),(0,n.kt)("p",null,"In this lab you will learn how to migrate Apache Spark code to Cloud Dataproc. You will follow a sequence of steps progressively moving more of the job components over to GCP services:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Run original Spark code on Cloud Dataproc (Lift and Shift)"),(0,n.kt)("li",{parentName:"ul"},"Replace HDFS with Cloud Storage (cloud-native)"),(0,n.kt)("li",{parentName:"ul"},"Automate everything so it runs on job-specific clusters (cloud-optimized)")),(0,n.kt)("p",null,"In this lab you will learn how to:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Migrate existing Spark jobs to Cloud Dataproc"),(0,n.kt)("li",{parentName:"ul"},"Modify Spark jobs to use Cloud Storage instead of HDFS"),(0,n.kt)("li",{parentName:"ul"},"Optimize Spark jobs to run on Job specific clusters")),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Scenario")),(0,n.kt)("p",null,"You are migrating an existing Spark workload to Cloud Dataproc and then progressively modifying the Spark code to make use of GCP native features and services."),(0,n.kt)("h2",{id:"task-1-lift-and-shift"},"Task 1. Lift and shift"),(0,n.kt)("h3",{id:"migrate-existing-spark-jobs-to-cloud-dataproc"},"Migrate existing Spark jobs to Cloud Dataproc"),(0,n.kt)("p",null,"You will create a new Cloud Dataproc cluster and then run an imported Jupyter notebook that uses the cluster's default local Hadoop Distributed File system (HDFS) to store source data and then process that data just as you would on any Hadoop cluster using Spark. This demonstrates how many existing analytics workloads such as Jupyter notebooks containing Spark code require no changes when they are migrated to a Cloud Dataproc environment."),(0,n.kt)("h3",{id:"configure-and-start-a-cloud-dataproc-cluster"},"Configure and start a Cloud Dataproc cluster"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"In the GCP Console, on the\xa0Navigation menu, in the\xa0Analytics\xa0section, click\xa0Dataproc."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Create Cluster."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Create\xa0for the item\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"Cluster on Compute Engine"),"."),(0,n.kt)("li",{parentName:"ol"},"Enter\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"sparktodp"),"\xa0for\xa0Cluster Name."),(0,n.kt)("li",{parentName:"ol"},"In the\xa0Versioning\xa0section, click\xa0Change\xa0and select\xa02.0 (Debian 10, Hadoop 3.2, Spark 3.1). This version includes Python3, which is required for the sample code used in this lab."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Select."),(0,n.kt)("li",{parentName:"ol"},"In the\xa0Components\xa0>\xa0Component gateway\xa0section, select\xa0Enable component gateway."),(0,n.kt)("li",{parentName:"ol"},"Under\xa0Optional components, Select\xa0Jupyter Notebook."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Create.")),(0,n.kt)("p",null,(0,n.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003314-ad107306-8093-496d-a75d-a80ed9ce4582.png",alt:null})),(0,n.kt)("p",null,"The cluster should start in a few minutes. You can proceed to the next step without waiting for the Cloud Dataproc Cluster to fully deploy."),(0,n.kt)("h3",{id:"clone-the-source-repository-for-the-lab"},"Clone the source repository for the lab"),(0,n.kt)("p",null,"In the Cloud Shell you clone the Git repository for the lab and copy the required notebook files to the Cloud Storage bucket used by Cloud Dataproc as the home directory for Jupyter notebooks."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"To clone the Git repository for the lab enter the following command in Cloud Shell:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"git -C ~ clone https://github.com/GoogleCloudPlatform/training-data-analyst\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"To locate the default Cloud Storage bucket used by Cloud Dataproc enter the following command in Cloud Shell:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"export DP_STORAGE=\"gs://$(gcloud dataproc clusters describe sparktodp --region=us-central1 --format=json | jq -r '.config.configBucket')\"\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"To copy the sample notebooks into the Jupyter working folder enter the following command in Cloud Shell:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"gsutil -m cp ~/training-data-analyst/quests/sparktobq/*.ipynb $DP_STORAGE/notebooks/jupyter\n")),(0,n.kt)("h3",{id:"log-in-to-the-jupyter-notebook"},"Log in to the Jupyter Notebook"),(0,n.kt)("p",null,"As soon as the cluster has fully started up you can connect to the Web interfaces. Click the refresh button to check as it may be deployed fully by the time you reach this stage."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"On the Dataproc Clusters page wait for the cluster to finish starting and then click the name of your cluster to open the\xa0Cluster details\xa0page."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Web Interfaces."),(0,n.kt)("li",{parentName:"ol"},"Click the\xa0Jupyter\xa0link to open a new Jupyter tab in your browser. This opens the Jupyter home page. Here you can see the contents of the\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"/notebooks/jupyter"),"\xa0directory in Cloud Storage that now includes the sample Jupyter notebooks used in this lab."),(0,n.kt)("li",{parentName:"ol"},"Under the\xa0Files\xa0tab, click the\xa0GCS\xa0folder and then click\xa001_spark.ipynb\xa0notebook to open it."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Cell\xa0and then\xa0Run All\xa0to run all of the cells in the notebook."),(0,n.kt)("li",{parentName:"ol"},"Page back up to the top of the notebook and follow as the notebook completes runs each cell and outputs the results below them.")),(0,n.kt)("p",null,(0,n.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003305-e80b09a4-9d58-4b5f-a42f-d366f90987e3.png",alt:null})),(0,n.kt)("p",null,(0,n.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003309-5d1e41c8-29e1-49c8-9307-ae826d41e8f1.png",alt:null})),(0,n.kt)("p",null,"You can now step down through the cells and examine the code as it is processed so that you can see what the notebook is doing. In particular pay attention to where the data is saved and processed from."),(0,n.kt)("h2",{id:"task-2-separate-compute-and-storage"},"Task 2. Separate compute and storage"),(0,n.kt)("h3",{id:"modify-spark-jobs-to-use-cloud-storage-instead-of-hdfs"},"Modify Spark jobs to use Cloud Storage instead of HDFS"),(0,n.kt)("p",null,"Taking this original 'Lift & Shift' sample notebook you will now create a copy that decouples the storage requirements for the job from the compute requirements. In this case, all you have to do is replace the Hadoop file system calls with Cloud Storage calls by replacing\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"hdfs://"),"\xa0storage references with\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"gs://"),"\xa0references in the code and adjusting folder names as necessary."),(0,n.kt)("p",null,"You start by using the cloud shell to place a copy of the source data in a new Cloud Storage bucket."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"In the Cloud Shell create a new storage bucket for your source data:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"export PROJECT_ID=$(gcloud info --format='value(config.project)')\ngsutil mb gs://$PROJECT_ID\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"In the Cloud Shell copy the source data into the bucket:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"wget https://storage.googleapis.com/cloud-training/dataengineering/lab_assets/sparklab/kddcup.data_10_percent.gz\ngsutil cp kddcup.data_10_percent.gz gs://$PROJECT_ID/\n")),(0,n.kt)("p",null,"Make sure that the last command completes and the file has been copied to your new storage bucket."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"Switch back to the\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"01_spark"),"\xa0Jupyter Notebook tab in your browser."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0File\xa0and then select\xa0Make a Copy."),(0,n.kt)("li",{parentName:"ol"},"When the copy opens, click the\xa001_spark-Copy1\xa0title and rename it to\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"De-couple-storage"),"."),(0,n.kt)("li",{parentName:"ol"},"Open the Jupyter tab for\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"01_spark"),"."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0File\xa0and then\xa0Save and checkpoint\xa0to save the notebook."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0File\xa0and then\xa0Close and Halt\xa0to shutdown the notebook."),(0,n.kt)("li",{parentName:"ol"},"If you are prompted to confirm that you want to close the notebook click\xa0Leave\xa0or\xa0Cancel."),(0,n.kt)("li",{parentName:"ol"},"Switch back to the\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"De-couple-storage"),"\xa0Jupyter Notebook tab in your browser, if necessary.")),(0,n.kt)("p",null,"You no longer need the cells that download and copy the data onto the cluster's internal HDFS file system so you will remove those first. Delete the initial comment cells and the first three code cells (\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"In [1]"),",\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"In [2]"),", and\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"In [3]"),") so that the notebook now starts with the section\xa0Reading in Data."),(0,n.kt)("p",null,"You will now change the code in the first cell ( still called\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"In[4]"),"\xa0unless you have rerun the notebook ) that defines the data file source location and reads in the source data. The cell currently contains the following code:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-py"},'from pyspark.sql import SparkSession, SQLContext, Row\nspark = SparkSession.builder.appName("kdd").getOrCreate()\nsc = spark.sparkContext\ndata_file = "hdfs:///kddcup.data_10_percent.gz"\nraw_rdd = sc.textFile(data_file).cache()\nraw_rdd.take(5)\n')),(0,n.kt)("p",null,"Replace the contents of cell\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"In [4]"),"\xa0with the following code. The only change here is create a variable to store a Cloud Storage bucket name and then to point the\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"data_file"),"\xa0to the bucket we used to store the source data on Cloud Storage:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-py"},'from pyspark.sql import SparkSession, SQLContext, Row\ngcs_bucket=\'[Your-Bucket-Name]\'\nspark = SparkSession.builder.appName("kdd").getOrCreate()\nsc = spark.sparkContext\ndata_file = "gs://"+gcs_bucket+"//kddcup.data_10_percent.gz"\nraw_rdd = sc.textFile(data_file).cache()\nraw_rdd.take(5)\n')),(0,n.kt)("p",null,"In the cell you just updated, replace the placeholder\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"[Your-Bucket-Name]"),"\xa0with the name of the storage bucket you created in the first step of this section. You created that bucket using the Project ID as the name. Replace all of the placeholder text, including the brackets\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"[]"),"."),(0,n.kt)("p",null,"Click\xa0Cell\xa0and then\xa0Run All\xa0to run all of the cells in the notebook."),(0,n.kt)("p",null,"You will see exactly the same output as you did when the file was loaded and run from internal cluster storage. Moving the source data files to Cloud Storage only requires that you repoint your storage source reference from\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"hdfs://"),"\xa0to\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"gs://"),"."),(0,n.kt)("h2",{id:"task-3-deploy-spark-jobs"},"Task 3. Deploy Spark jobs"),(0,n.kt)("h3",{id:"optimize-spark-jobs-to-run-on-job-specific-clusters"},"Optimize Spark jobs to run on Job specific clusters"),(0,n.kt)("p",null,"You now create a standalone Python file, that can be deployed as a Cloud Dataproc Job, that will perform the same functions as this notebook. To do this you add magic commands to the Python cells in a copy of this notebook to write the cell contents out to a file. You will also add an input parameter handler to set the storage bucket location when the Python script is called to make the code more portable."),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"In the\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"De-couple-storage"),"\xa0Jupyter Notebook menu, click\xa0File\xa0and select\xa0Make a Copy."),(0,n.kt)("li",{parentName:"ol"},"When the copy opens, click the\xa0De-couple-storage-Copy1\xa0and rename it to\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"PySpark-analysis-file"),"."),(0,n.kt)("li",{parentName:"ol"},"Open the Jupyter tab for\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"De-couple-storage"),"."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0File\xa0and then\xa0Save and checkpoint\xa0to save the notebook."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0File\xa0and then\xa0Close and Halt\xa0to shutdown the notebook."),(0,n.kt)("li",{parentName:"ol"},"If you are prompted to confirm that you want to close the notebook click\xa0Leave\xa0or\xa0Cancel."),(0,n.kt)("li",{parentName:"ol"},"Switch back to the\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"PySpark-analysis-file"),"\xa0Jupyter Notebook tab in your browser, if necessary."),(0,n.kt)("li",{parentName:"ol"},"Click the first cell at the top of the notebook."),(0,n.kt)("li",{parentName:"ol"},"Click\xa0Insert\xa0and select\xa0Insert Cell Above."),(0,n.kt)("li",{parentName:"ol"},"Paste the following library import and parameter handling code into this new first code cell:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'%%writefile spark_analysis.py\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument("--bucket", help="bucket for input and output")\nargs = parser.parse_args()\nBUCKET = args.bucket\n')),(0,n.kt)("p",null,"The\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"%%writefile spark_analysis.py"),"\xa0Jupyter magic command creates a new output file to contain your standalone python script. You will add a variation of this to the remaining cells to append the contents of each cell to the standalone script file."),(0,n.kt)("p",null,"This code also imports the\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"matplotlib"),"\xa0module and explicitly sets the default plotting backend via\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"matplotlib.use('agg')"),"\xa0so that the plotting code runs outside of a Jupyter notebook."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"For the remaining cells insert\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"%%writefile -a spark_analysis.py"),"\xa0at the start of each Python code cell. These are the five cells labelled\xa0In ","[x]",".")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"In the last cell, where the Pandas bar chart is plotted, remove the\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"%matplotlib inline"),"\xa0magic command."))),(0,n.kt)("p",null,"Note:\xa0You must remove this inline matplotlib Jupyter magic directive or your script will fail when you run it."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Make sure you have selected the last code cell in the notebook then, in the menu bar, click\xa0Insert\xa0and select\xa0Insert Cell Below.")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"Paste the following code into the new cell:"))),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"%%writefile -a spark_analysis.py\nax[0].get_figure().savefig('report.png');\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Add another new cell at the end of the notebook and paste in the following:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"%%writefile -a spark_analysis.py\nimport google.cloud.storage as gcs\nbucket = gcs.Client().get_bucket(BUCKET)\nfor blob in bucket.list_blobs(prefix='sparktodp/'):\n    blob.delete()\nbucket.blob('sparktodp/report.png').upload_from_filename('report.png')\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Add a new cell at the end of the notebook and paste in the following:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'%%writefile -a spark_analysis.py\nconnections_by_protocol.write.format("csv").mode("overwrite").save(\n    "gs://{}/sparktodp/connections_by_protocol".format(BUCKET))\n')),(0,n.kt)("p",null,"You now test that the PySpark code runs successfully as a file by calling the local copy from inside the notebook, passing in a parameter to identify the storage bucket you created earlier that stores the input data for this job. The same bucket will be used to store the report data files produced by the script."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"In the\xa0",(0,n.kt)("inlineCode",{parentName:"li"},"PySpark-analysis-file"),"\xa0notebook add a new cell at the end of the notebook and paste in the following:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"BUCKET_list = !gcloud info --format='value(config.project)'\nBUCKET=BUCKET_list[0]\nprint('Writing to {}'.format(BUCKET))\n!/opt/conda/miniconda3/bin/python spark_analysis.py --bucket=$BUCKET\n")),(0,n.kt)("p",null,"This code assumes that you have followed the earlier instructions and created a Cloud Storage Bucket using your lab Project ID as the Storage Bucket name. If you used a different name modify this code to set the\xa0",(0,n.kt)("inlineCode",{parentName:"p"},"BUCKET"),"\xa0variable to the name you used."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Add a new cell at the end of the notebook and paste in the following:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"!gsutil ls gs://$BUCKET/sparktodp/**\n")),(0,n.kt)("p",null,"This lists the script output files that have been saved to your Cloud Storage bucket."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"To save a copy of the Python file to persistent storage, add a new cell and paste in the following:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"!gsutil cp spark_analysis.py gs://$BUCKET/sparktodp/spark_analysis.py\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Click\xa0Cell\xa0and then\xa0Run All\xa0to run all of the cells in the notebook.")),(0,n.kt)("h3",{id:"run-the-analysis-job-from-cloud-shell"},"Run the Analysis Job from Cloud Shell."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Switch back to your Cloud Shell and copy the Python script from Cloud Storage so you can run it as a Cloud Dataproc Job:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"gsutil cp gs://$PROJECT_ID/sparktodp/spark_analysis.py spark_analysis.py\n")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Create a launch script:")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"nano submit_onejob.sh\n```\n\n- Paste the following into the script:\n\n```\n#!/bin/bash\ngcloud dataproc jobs submit pyspark\\\n       --cluster sparktodp\\\n       --region us-central1\\\n       spark_analysis.py\\\n       -- --bucket=$1\n```\n\n- Press\xa0`CTRL+X`\xa0then\xa0`Y`\xa0and\xa0`Enter`\xa0key to exit and save.\n\n- Make the script executable:\n\n```\nchmod +x submit_onejob.sh\n```\n\n- Launch the PySpark Analysis job:\n\n```\n./submit_onejob.sh $PROJECT_ID\n```\n\n1.  In the Cloud Console tab navigate to the\xa0Dataproc\xa0>\xa0Clusters\xa0page if it is not already open.\n2.  Click\xa0Jobs.\n3.  Click the name of the job that is listed. You can monitor progress here as well as from the Cloud shell. Wait for the Job to complete successfully.\n4.  Navigate to your storage bucket and note that the output report,\xa0`/sparktodp/report.png`\xa0has an updated time-stamp indicating that the stand-alone job has completed successfully.\n\n![](https://user-images.githubusercontent.com/62965911/214003301-9c41aed5-7119-493c-996d-34ee3e7710ed.png)\n\nThe storage bucket used by this Job for input and output data storage is the bucket that is used just the Project ID as the name.\n\n1.  Navigate back to the\xa0Dataproc\xa0>\xa0Clusters\xa0page.\n2.  Select the\xa0sparktodp\xa0cluster and click\xa0Delete. You don't need it any more.\n3.  Click\xa0CONFIRM.\n4.  Close the\xa0Jupyter\xa0tabs in your browser.\n\nCongratulations!\n\n## Notebooks\n\n[![nbviewer](https://img.shields.io/badge/jupyter-notebook-informational?logo=jupyter)](https://nbviewer.org/github/sparsh-ai/recohut/blob/main/docs/03-processing/lab-gcp-dataproc/nbs/)\n")))}u.isMDXComponent=!0}}]);