"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[46186],{3905:(e,a,t)=>{t.d(a,{Zo:()=>u,kt:()=>g});var r=t(67294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),c=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},u=function(e){var a=c(e.components);return r.createElement(l.Provider,{value:a},e.children)},p={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},d=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(t),g=n,m=d["".concat(l,".").concat(g)]||d[g]||p[g]||o;return t?r.createElement(m,i(i({ref:a},u),{},{components:t})):r.createElement(m,i({ref:a},u))}));function g(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,i=new Array(o);i[0]=d;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:n,i[1]=s;for(var c=2;c<o;c++)i[c]=t[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},86319:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var r=t(87462),n=(t(67294),t(3905));const o={},i="Azure Data Factory",s={unversionedId:"orchestration/azure-data-factory/README",id:"orchestration/azure-data-factory/README",title:"Azure Data Factory",description:"Azure Data Factory is the data orchestration service in Azure. Using Azure Data Factory, you can build pipelines that are capable of reading data from multiple sources, transforming the data, and loading the data into data stores to be consumed by reporting applications such as Power BI. Azure Data Factory much like SQL Server Integration Services (SSIS) in an on-premises world, provides a code-free UI for developing, managing, and maintaining data engineering pipelines.",source:"@site/docs/06-orchestration/azure-data-factory/README.md",sourceDirName:"06-orchestration/azure-data-factory",slug:"/orchestration/azure-data-factory/",permalink:"/docs/orchestration/azure-data-factory/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Spotify-Data-Engineering-and-Analysis",permalink:"/docs/orchestration/airflow/labdev-spotify/"},next:{title:"Incremental Data Loading in Azure Data Factory",permalink:"/docs/orchestration/azure-data-factory/lab-adf-incremental-loading/"}},l={},c=[{value:"Labs",id:"labs",level:2},{value:"Building Data Ingestion Pipelines using Azure Data Factory [Source code]",id:"building-data-ingestion-pipelines-using-azure-data-factory-source-code",level:3},{value:"Incremental Data Loading using Azure Data Factory [Source code]",id:"incremental-data-loading-using-azure-data-factory-source-code",level:3},{value:"Develop Batch Processing Solution using Azure Data Factory [Source code]",id:"develop-batch-processing-solution-using-azure-data-factory-source-code",level:3}],u={toc:c};function p(e){let{components:a,...t}=e;return(0,n.kt)("wrapper",(0,r.Z)({},u,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"azure-data-factory"},"Azure Data Factory"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Azure Data Factory")," is the data orchestration service in Azure. Using Azure Data Factory, you can build pipelines that are capable of reading data from multiple sources, transforming the data, and loading the data into data stores to be consumed by reporting applications such as Power BI. Azure Data Factory much like ",(0,n.kt)("strong",{parentName:"p"},"SQL Server Integration Services")," (",(0,n.kt)("strong",{parentName:"p"},"SSIS"),") in an on-premises world, provides a code-free UI for developing, managing, and maintaining data engineering pipelines."),(0,n.kt)("p",null,"ADF is a managed cloud service that can be used to coordinate and orchestrate complex cloud- or hybrid- (on-premises)-based\xa0pipelines. ADF provides the ability to build ETL and\xa0",(0,n.kt)("strong",{parentName:"p"},"extract, load, transform"),"\xa0(",(0,n.kt)("strong",{parentName:"p"},"ELT"),") pipelines. With ADF, you can do the following:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Ingest data from a\xa0wide variety of sources such as databases, file shares,\xa0",(0,n.kt)("strong",{parentName:"li"},"Internet of Things"),"\xa0(",(0,n.kt)("strong",{parentName:"li"},"IoT"),") hubs,\xa0",(0,n.kt)("strong",{parentName:"li"},"Amazon Web Services"),"\xa0(",(0,n.kt)("strong",{parentName:"li"},"AWS"),"),\xa0",(0,n.kt)("strong",{parentName:"li"},"Google Cloud Platform"),"\xa0(",(0,n.kt)("strong",{parentName:"li"},"GCP"),"), and more."),(0,n.kt)("li",{parentName:"ul"},"Build complex\xa0pipelines using variables, parameters, branches, and so on."),(0,n.kt)("li",{parentName:"ul"},"Transform\xa0data by using compute services such as Synapse, HDInsight, Cosmos DB, and so on."),(0,n.kt)("li",{parentName:"ul"},"Schedule and\xa0monitor ingestions, control flow, and data flow operations.")),(0,n.kt)("p",null,"ADF is built of some basic set of components. The important ones are listed here:"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Pipelines"),"---A pipeline is a collection\xa0of activities that are linked together to perform some control\xa0flow or data transformation."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Activities"),"---Activities in ADF refer\xa0to the steps in the pipeline such\xa0as copying data, running a Spark job, and so on."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Datasets"),"---This is the data\xa0that your pipelines or activities operate\xa0on."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Linked Services"),"---Linked services are\xa0connections that ADF uses to\xa0connect to a variety of data stores and computes in Azure. They are like connection strings that let you access data from external sources."),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("strong",{parentName:"li"},"Triggers"),"---Triggers\xa0are events that are used\xa0to start pipelines or start an activity.")),(0,n.kt)("p",null,"NOTE"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"All ADF transformations happen on datasets. So, before we can do any transformation, we will have to create datasets of the source data."),(0,n.kt)("p",{parentName:"blockquote"},(0,n.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218307653-c4af60b8-fba1-4a6b-a7c1-e2e8fd4f5fa2.jpeg",alt:"B17525_08_003"}))),(0,n.kt)("h2",{id:"labs"},"Labs"),(0,n.kt)("h3",{id:"building-data-ingestion-pipelines-using-azure-data-factory-source-code"},"Building Data Ingestion Pipelines using Azure Data Factory [",(0,n.kt)("a",{parentName:"h3",href:"06-orchestration/azure-data-factory/lab-data-ingestion-pipeline/"},"Source code"),"]"),(0,n.kt)("p",null,"This lab covers ingesting data using Azure Data Factory and copying data between Azure SQL Database and Azure Data Lake."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Recipe 1 - Provisioning Azure Data Factory"),(0,n.kt)("li",{parentName:"ul"},"Recipe 2 - Copying files to a database from a data lake using a control flow and copy activity"),(0,n.kt)("li",{parentName:"ul"},"Recipe 3 - Triggering a pipeline in Azure Data Factory"),(0,n.kt)("li",{parentName:"ul"},"Recipe 4 - Copying data from a SQL Server virtual machine to a data lake using the Copy data wizard")),(0,n.kt)("h3",{id:"incremental-data-loading-using-azure-data-factory-source-code"},"Incremental Data Loading using Azure Data Factory [",(0,n.kt)("a",{parentName:"h3",href:"06-orchestration/azure-data-factory/lab-adf-incremental-loading/"},"Source code"),"]"),(0,n.kt)("p",null,"This lab covers various methods to perform data loading in incremental fashion."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Recipe 1 - Using Watermarking"),(0,n.kt)("li",{parentName:"ul"},"Recipe 2 - Using File Timestamps"),(0,n.kt)("li",{parentName:"ul"},"Recipe 3 - Using File partitions and folder structures")),(0,n.kt)("h3",{id:"develop-batch-processing-solution-using-azure-data-factory-source-code"},"Develop Batch Processing Solution using Azure Data Factory [",(0,n.kt)("a",{parentName:"h3",href:"06-orchestration/azure-data-factory/lab-batch-processing-solution/"},"Source code"),"]"),(0,n.kt)("p",null,"In this lab, we design an end-to-end batch processing solution by using Data Factory, Data Lake, Spark, Azure Synapse Pipelines, PolyBase, and Azure Databricks"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Recipe 1 - Data Ingestion using Data Flow"),(0,n.kt)("li",{parentName:"ul"},"Recipe 2 - Data Transformation using Azure Databricks"),(0,n.kt)("li",{parentName:"ul"},"Recipe 3 - Data Serving using PolyBase"),(0,n.kt)("li",{parentName:"ul"},"Recipe 4 - Data Pipeline using Azure Data Factory Pipeline"),(0,n.kt)("li",{parentName:"ul"},"Recipe 5 - End to end data processing with Azure Batch")))}p.isMDXComponent=!0}}]);