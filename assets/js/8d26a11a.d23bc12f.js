"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[49391],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>p});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),u=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=u(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),h=u(a),p=r,f=h["".concat(l,".").concat(p)]||h[p]||c[p]||o;return a?n.createElement(f,s(s({ref:t},d),{},{components:a})):n.createElement(f,s({ref:t},d))}));function p(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,s=new Array(o);s[0]=h;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,s[1]=i;for(var u=2;u<o;u++)s[u]=a[u];return n.createElement.apply(null,s)}return n.createElement.apply(null,a)}h.displayName="MDXCreateElement"},70522:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>u});var n=a(87462),r=(a(67294),a(3905));const o={},s="Outfit7",i={unversionedId:"casestudies/outfit7",id:"casestudies/outfit7",title:"Outfit7",description:"Multinational video game developer",source:"@site/docs/casestudies/outfit7.md",sourceDirName:"casestudies",slug:"/casestudies/outfit7",permalink:"/docs/casestudies/outfit7",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681732641,formattedLastUpdatedAt:"Apr 17, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Myntra",permalink:"/docs/casestudies/myntra"},next:{title:"Panoramic",permalink:"/docs/casestudies/panoramic"}},l={},u=[{value:"The Billion Events Infrastructure",id:"the-billion-events-infrastructure",level:2},{value:"API endpoints",id:"api-endpoints",level:3},{value:"Data delivery",id:"data-delivery",level:3},{value:"Data processing",id:"data-processing",level:3},{value:"Data storage",id:"data-storage",level:3},{value:"Resolving Big Data Challenges: Why We Use BigQuery at Outfit7",id:"resolving-big-data-challenges-why-we-use-bigquery-at-outfit7",level:2},{value:"BigQuery\u2019s Capabilities",id:"bigquerys-capabilities",level:3}],d={toc:u};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"outfit7"},"Outfit7"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Multinational video game developer")),(0,r.kt)("h2",{id:"the-billion-events-infrastructure"},"The Billion Events Infrastructure"),(0,r.kt)("p",null,"With 430 million monthly active users, it can be a challenge to get a clear picture of how our games and company are performing. The data we gather helps us understand how we perform in the fast-paced mobile gaming market. Getting fast feedback is key. Because of this, we need infrastructure that is scalable and reliable, and which, ideally, doesn\u2019t hurt the company\u2019s wallet too much. Over the past decade we\u2019ve continuously improved our infrastructure to accommodate our growing user base. Today, we handle half a million row inserts per second \u2014 let me show you how. Welcome to the Billion Events Infrastructure."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://miro.medium.com/max/4800/0*Zek8d75_0P4uhYkI",alt:null})),(0,r.kt)("p",null,"Let\u2019s start with an overview of our infrastructure. We receive data from various data sources, like game usage data, third party services, etc., which we save in persistent storage. Here, we\u2019ll focus only on data sent by our games, because this is what forms the backbone of our analyses. From data gathering endpoints to persistent storage, all is hosted on the Google Cloud Platform. Let\u2019s start with the first layer, at our API endpoints:"),(0,r.kt)("h3",{id:"api-endpoints"},"API endpoints"),(0,r.kt)("p",null,"We run our data-gathering endpoints on different managed services (App Engine, Cloud Run and Kubernetes Engine). Our main endpoint, where we gather usage data, runs on Kubernetes Engine. We chose Kubernetes because it offers a nice balance between performance and cost. Because we have a global player base, we run three clusters, each in different regions (US, EU and Asia). Our main cluster resides in the US, where we get the most traffic, while the Asia cluster serves as the endpoint for our users in China. We also set up one cluster in Europe which is our backfall endpoint in the event of endpoint failures in US and Asia clusters. All data is validated and pushed to our data delivery layer."),(0,r.kt)("h3",{id:"data-delivery"},"Data delivery"),(0,r.kt)("p",null,"This second layer is a middleware between our API endpoints and data processing workloads. It provides asynchronous delivery of messages with the benefit of keeping our data safe until it\u2019s written to persistent storage. We use Pub/Sub (similar to Apache Kafka) which provides a simple, reliable and scalable message delivery service."),(0,r.kt)("h3",{id:"data-processing"},"Data processing"),(0,r.kt)("p",null,"In this layer, we do some light data transformation (unbatching the client data) and streaming data into persistent storage. Our main framework to do this is Apache Beam which runs on managed cloud infrastructure called Dataflow. This is a tried and tested way to do ETL pipelines because it provides a reliable way of inserting data into persistent storage."),(0,r.kt)("h3",{id:"data-storage"},"Data storage"),(0,r.kt)("p",null,"After all this, the data is finally written into BigQuery. It\u2019s our main data warehouse, which powers all of our analytics workload. It\u2019s fast, reliable and just what we need to store and analyze our data."),(0,r.kt)("p",null,"All these layers combined create a scalable and reliable infrastructure that can receive, process and ingest more than half a million rows of data per second, or five terabytes of new data per day. Our data teams query 60 petabytes per year with zero downtime and zero maintenance from the core engineering team, just by using BigQuery. By using a simple approach and managed services a single person can maintain this infrastructure. This enables us to focus on developing new solutions that add value to our company."),(0,r.kt)("p",null,"Read more ",(0,r.kt)("a",{parentName:"p",href:"https://medium.com/outfit7/the-billion-events-infrastructure-c5fa1610d786"},"here"),"."),(0,r.kt)("h2",{id:"resolving-big-data-challenges-why-we-use-bigquery-at-outfit7"},"Resolving Big Data Challenges: Why We Use BigQuery at Outfit7"),(0,r.kt)("p",null,"In just a few years, Outfit7\u2019s portfolio has grown to include 19 games with over 8 billion downloads. But big numbers bring big challenges. One of them is how to deal with such huge amounts of data in a timely and cost-effective manner. Our games hit the backend infrastructure with around 8 billion rows of data per day that take up 2.5 TB of space \u2013 and those numbers grow daily. This data is in a raw form and, to be useful, it needs to be processed, aggregated, extracted, and transformed. Above all, it has to be treated securely."),(0,r.kt)("p",null,"If we were to use normal relational database solutions, building a system that would be able to ingest and operationally function with such amount of data \u2013 not to mention handling the explosion of hardware storage space and related costs \u2013 would be a daunting task. It would require an army of sysops/devops/DB engineers."),(0,r.kt)("p",null,"So, to find a way around this, we decided to look at cloud platforms. Google\u2019s solution, the Google Cloud Platform (\u201cGCP\u201d) is steadily becoming one of the best. Outfit7 Group heavily leverages one of the GCP\u2019s flagship services, BigQuery; a serverless, highly scalable, low cost enterprise data warehouse. The service has no problems ingesting the data Outfit7 deals with. In fact, the querying power is just astonishing."),(0,r.kt)("p",null,"BigQuery Across Different Departments\nWe use BigQuery in numerous ways across the company. Our backend department  has a special data team that \u2013 complying with all laws including GDPR \u2013 takes care of data ingestion, transformation, and delivery to appropriate stakeholders. To give you an example, we generate around 50 aggregates that form the basis for further analysis in the Analytics and Controlling departments. The main aggregates we prepare for the Analytics team are the retention and user segments, while the Controlling team is more focused on the daily and monthly revenue aggregates, user live time values, and daily active users aggregates."),(0,r.kt)("p",null,"We also closely collaborate with the Ad Ops and App Sales teams. In this part of the company, daily ad sales, ad mediation and paid user acquisition reports and aggregates need to be calculated. The data is then consumed and evaluated with the preferred tool of choice for each department. The Analytics department relies heavily on iPython and R, Controlling is mainly focused on Tableau reports and the visual representation of data, and the Ad Ops and Ad Sales departments rely on custom made dashboards, Google DataStudio reports, and ad-hoc analysis with Excel/Sheets."),(0,r.kt)("h3",{id:"bigquerys-capabilities"},"BigQuery\u2019s Capabilities"),(0,r.kt)("p",null,"To illustrate the point further, consider the following query. Anyone can run it on the publicly available BQ datasets that come bundled out of the box with a free GCP account."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://cdn-o7.outfit7.com/wp-content/uploads/2018/06/Screen_Shot_2018-06-27_at_12.31.40.png",alt:"Query example on BQ"})),(0,r.kt)("p",null,"The underlying \u201ctrips\u201d table contains 130GB and more than 1.1 billion rows of data that represent yellow taxi fares in New York City from 2009 to 2015. The query included a yearly breakdown of vendors operating in the NYC area, their revenue, average fare cost, the distance traveled, and the total number of fares. It took 4.5 seconds to produce 16 rows of a high level report, which could then be downloaded as a csv or JSON file, saved to a new table, or a Google Sheets document, etc."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://cdn-o7.outfit7.com/wp-content/uploads/2018/06/bigQuery_example.png",alt:"High level aggregation report from a table containing more than 1.1 billion rows of data, which took 4.5 seconds to produce"})),(0,r.kt)("p",null,"Imagine that: it took BigQuery less time to scan 130GB of data and spit out a condensed report than it would take you to say \u201cWhere\u2019s the star schema and other data warehousing buzzword shenanigans?\u201d If that isn\u2019t impressive for a data guy, I don\u2019t know what is. All the upsides aside, however, there\u2019s no such thing as free lunch. If that wasn\u2019t a test, the above query would cost $0.15, excluding the cost of storage, which would bring the amount up to $2.60 per month, decreasing to $1.30 per month after three months."),(0,r.kt)("p",null,"But the days of a data backend engineer aren\u2019t just filled with writing and running BigQuery jobs in one of the various language flavours we\u2019re using, like Bash, Python, Java, etc. They\u2019re also filled with other engineering tasks that are necessary to hold the whole infrastructure together. Nevertheless, if, at the end of the day, we had to choose and point to one of the tools in the GCP arsenal that currently saves the Data and Backend teams the most amount of time, we\u2019d probably say it\u2019s BigQuery. In a competitive industry such as gaming, you need to have a reliable support system that continues to pave the way forward and, for us, that\u2019s BigQuery."))}c.isMDXComponent=!0}}]);