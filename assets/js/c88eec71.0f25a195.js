"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[20850],{3905:(e,t,a)=>{a.d(t,{Zo:()=>h,kt:()=>m});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},h=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),p=d(a),m=o,c=p["".concat(l,".").concat(m)]||p[m]||u[m]||r;return a?n.createElement(c,i(i({ref:t},h),{},{components:a})):n.createElement(c,i({ref:t},h))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=a.length,i=new Array(r);i[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var d=2;d<r;d++)i[d]=a[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},65886:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>s,toc:()=>d});var n=a(87462),o=(a(67294),a(3905));const r={},i="Glue Studio Custom Transforms",s={unversionedId:"processing/lab-glue-studio-custom-transforms/README",id:"processing/lab-glue-studio-custom-transforms/README",title:"Glue Studio Custom Transforms",description:"Objective: Create your own reusable visual transforms for AWS Glue Studio",source:"@site/docs/03-processing/lab-glue-studio-custom-transforms/README.md",sourceDirName:"03-processing/lab-glue-studio-custom-transforms",slug:"/processing/lab-glue-studio-custom-transforms/",permalink:"/docs/processing/lab-glue-studio-custom-transforms/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Handle UPSERT data operations using open-source Delta Lake and AWS Glue",permalink:"/docs/processing/lab-glue-deltalake-cdc-upsert/"},next:{title:"Tickets ETL with Glue Studio",permalink:"/docs/processing/lab-glue-studio-tickets/"}},l={},d=[{value:"Use case: Generate synthetic data on the fly",id:"use-case-generate-synthetic-data-on-the-fly",level:2},{value:"Define the generator component",id:"define-the-generator-component",level:3},{value:"Implement the generator logic",id:"implement-the-generator-logic",level:3},{value:"Deploy and using the generator transform",id:"deploy-and-using-the-generator-transform",level:3},{value:"Use case: Improve the data partitioning",id:"use-case-improve-the-data-partitioning",level:2},{value:"Define the repartitioner transform",id:"define-the-repartitioner-transform",level:3},{value:"Implement the transform logic",id:"implement-the-transform-logic",level:3},{value:"Deploy and use the generator transform",id:"deploy-and-use-the-generator-transform",level:3},{value:"How this feature works under the hood",id:"how-this-feature-works-under-the-hood",level:2},{value:"Conclusion",id:"conclusion",level:2}],h={toc:d};function u(e){let{components:t,...a}=e;return(0,o.kt)("wrapper",(0,n.Z)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"glue-studio-custom-transforms"},"Glue Studio Custom Transforms"),(0,o.kt)("p",null,"Objective: Create your own reusable visual transforms for AWS Glue Studio"),(0,o.kt)("p",null,"Watch this video: ",(0,o.kt)("a",{parentName:"p",href:"https://www.youtube.com/watch?v=dABnsYw1D4I"},"https://www.youtube.com/watch?v=dABnsYw1D4I")),(0,o.kt)("p",null,"Custom visual transform lets you define, reuse, and share business-specific ETL logic among your teams. With this new feature, data engineers can write reusable transforms for the AWS Glue visual job editor. Reusable transforms increase consistency between teams and help keep jobs up-to-date by minimizing duplicate effort and code."),(0,o.kt)("p",null,"In this lab, we will create two custom transforms to illustrate what you can accomplish with this feature. One component will generate synthetic data on the fly for testing purposes, and the other will prepare the data to store it partitioned."),(0,o.kt)("h2",{id:"use-case-generate-synthetic-data-on-the-fly"},"Use case: Generate synthetic data on the fly"),(0,o.kt)("p",null,"There are multiple reasons why you would want to have a component that generates synthetic data. Maybe the real data is heavily restricted or not yet available, or there is not enough quantity or variety at the moment to test performance. Or maybe using the real data imposes some cost or load to the real system, and we want to reduce its usage during development."),(0,o.kt)("p",null,"Using the new custom visual transforms framework, let\u2019s create a component that builds synthetic data for fictional sales during a natural year."),(0,o.kt)("h3",{id:"define-the-generator-component"},"Define the generator component"),(0,o.kt)("p",null,"First, define the component by giving it a name, description, and parameters. In this case, use salesdata_generator for both the name and the function, with two parameters: how many rows to generate and for which year."),(0,o.kt)("p",null,"For the parameters, we define them both as int, and you can add a regex validation to make sure the parameters provided by the user are in the correct format."),(0,o.kt)("p",null,"This is how the component definition would look like. Save it as ",(0,o.kt)("inlineCode",{parentName:"p"},"salesdata_generator.json"),". For convenience, we\u2019ll match the name of the Python file, so it\u2019s important to choose a name that doesn\u2019t conflict with an existing Python module.\nIf the year is not specified, the script will default to last year."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="./src/salesdata_generator.json"',title:'"./src/salesdata_generator.json"'},'{\n  "name": "salesdata_generator",\n  "displayName": "Synthetic Sales Data Generator",\n  "description": "Generate synthetic order datasets for testing purposes.",\n  "functionName": "salesdata_generator",\n  "parameters": [\n    {\n      "name": "numSamples",\n      "displayName": "Number of samples",\n      "type": "int",\n      "description": "Number of samples to generate"\n    },\n    {\n      "name": "year",\n      "displayName": "Year",\n      "isOptional": true,\n      "type": "int",\n      "description": "Year for which generate data distributed randomly, by default last year",\n      "validationRule": "^\\\\d{4}$",\n      "validationMessage": "Please enter a valid year number"\n    }\n  ]\n}\n')),(0,o.kt)("h3",{id:"implement-the-generator-logic"},"Implement the generator logic"),(0,o.kt)("p",null,"Now, you need to create a Python script file with the implementation logic."),(0,o.kt)("p",null,"Save the following script as ",(0,o.kt)("inlineCode",{parentName:"p"},"salesdata_generator.py"),". Notice the name is the same as the JSON, just with a different extension."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="./src/salesdata_generator.py"',title:'"./src/salesdata_generator.py"'},'from awsglue import DynamicFrame\nimport pyspark.sql.functions as F\nimport datetime\nimport time\n\ndef salesdata_generator(self, numSamples, year=None):\n    if not year:\n        # Use last year\n        year = datetime.datetime.now().year - 1\n    \n    year_start_ts = int(time.mktime((year,1,1,0,0,0,0,0,0)))\n    year_end_ts = int(time.mktime((year + 1,1,1,0,0,0,0,0,0)))\n    ts_range = year_end_ts - year_start_ts\n    \n    departments = ["bargain", "checkout", "food hall", "sports", "menswear", "womenwear", "health and beauty", "home"]\n    dep_array = F.array(*[F.lit(x) for x in departments])\n    dep_randomizer = (F.round(F.rand() * (len(departments) -1))).cast("int")\n\n    df = self.glue_ctx.sparkSession.range(numSamples) \\\n      .withColumn("sale_date", F.from_unixtime(F.lit(year_start_ts) + F.rand() * ts_range)) \\\n      .withColumn("amount_dollars", F.round(F.rand() * 1000, 2)) \\\n      .withColumn("department", dep_array.getItem(dep_randomizer))  \n    return DynamicFrame.fromDF(df, self.glue_ctx, "sales_synthetic_data")\n\nDynamicFrame.salesdata_generator = salesdata_generator\n')),(0,o.kt)("p",null,"The function salesdata_generator in the script receives the source DynamicFrame as \u201cself\u201d, and the parameters must match the definition in the JSON file. Notice the \u201cyear\u201d is an optional parameter, so it has assigned a default function on call, which the function detects and replaces with the previous year. The function returns the transformed DynamicFrame. In this case, it\u2019s not derived from the source one, which is the common case, but replaced by a new one."),(0,o.kt)("p",null,"The transform leverages Spark functions as well as Python libraries in order to implement this generator.\nTo keep things simple, this example only generates four columns, but we could do the same for many more by either hardcoding values, assigning them from a list, looking for some other input, or doing whatever makes sense to make the data realistic."),(0,o.kt)("h3",{id:"deploy-and-using-the-generator-transform"},"Deploy and using the generator transform"),(0,o.kt)("p",null,"Now that we have both files ready, all we have to do is upload them on Amazon S3 under the following path."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"s3://aws-glue-assets-<account id>-<region name>/transforms/\n")),(0,o.kt)("p",null,"If AWS Glue has never been used in the account and Region, then that bucket might not exist and needs to be created. AWS Glue will automatically create this bucket when you create your first job."),(0,o.kt)("p",null,"Once you have uploaded both files, the next time we open (or refresh) the page on AWS Glue Studio visual editor, the transform should be listed among the other transforms. You can search for it by name or description."),(0,o.kt)("p",null,"Because this is a transform and not a source, when we try to use the component, the UI will demand a parent node. You can use as a parent the real data source (so you can easily remove the generator and use the real data) or just use a placeholder:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Go to the AWS Glue, and in the left menu, select\xa0Jobs\xa0under\xa0AWS Glue Studio."),(0,o.kt)("li",{parentName:"ol"},"Leave the default options (Visual with a source and target\xa0and S3 source and destination), and choose\xa0Create."),(0,o.kt)("li",{parentName:"ol"},"Give the job a name by editing\xa0Untitled job\xa0at the top left; for example,\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"CustomTransformsDemo")),(0,o.kt)("li",{parentName:"ol"},"Go to the\xa0Job details\xa0tab and select a role with AWS Glue permissions as the\xa0IAM role. If no role is listed on the dropdown, then follow\xa0",(0,o.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/glue/latest/dg/create-an-iam-role.html"},"these instructions"),"\xa0to create one.\\\nFor this lab, you can also reduce\xa0Requested number of workers\xa0to 2 and\xa0Number of retries\xa0to 0 to minimize costs."),(0,o.kt)("li",{parentName:"ol"},"Delete the\xa0Data target\xa0node\xa0S3 bucket\xa0at the bottom of the graph by selecting it and choosing\xa0Remove.\xa0We will restore it later when we need it."),(0,o.kt)("li",{parentName:"ol"},"Edit the S3 source node by selecting it in the\xa0Data source properties\xa0tab and selecting source type\xa0S3 location.\\\nIn the\xa0S3 URL\xa0box, enter a path that doesn't exist on a bucket the role selected can access, for instance:\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"s3://aws-glue-assets-<account id>-<region name>/file_that_doesnt_exist"),". Notice there is no trailing slash.\\\nChoose JSON as the data format with default settings; it doesn't matter.\\\nYou might get a warning that it cannot infer schema because the file doesn't exist; that's OK, we don't need it.!"),(0,o.kt)("li",{parentName:"ol"},'Now search for the transform by typing "synthetic" in the search box of transforms. Once the result appears (or you scroll and search it on the list), choose it so it is added to the job.'),(0,o.kt)("li",{parentName:"ol"},"Set the parent of the transform just added to be\xa0S3 bucket\xa0source in the\xa0Node properties\xa0tab. Then for the\xa0ApplyMapping\xa0node, replace the parent\xa0S3 bucket\xa0with transforms\xa0Synthetic Sales Data Generator. Notice this long name is coming from the\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"displayName"),"\xa0defined in the JSON file uploaded before."),(0,o.kt)("li",{parentName:"ol"},"Select the\xa0Synthetic Sales\xa0node and go to the\xa0Transform\xa0tab. Enter 10000 as the number of samples and leave the year by default, so it uses last year."),(0,o.kt)("li",{parentName:"ol"},"Now we need the generated schema to be applied. This would be needed if we had a source that matches the generator schema.\\\nIn the same node, select the tab\xa0Data preview\xa0and start a session. Once it is running, you should see sample synthetic data. Notice the sale dates are randomly distributed across the year."),(0,o.kt)("li",{parentName:"ol"},"Now select the tab\xa0Output schema\xa0and choose\xa0Use datapreview schema\xa0That way, the four fields generated by the node will be propagated, and we can do the mapping based on this schema."),(0,o.kt)("li",{parentName:"ol"},"Now we want to convert the generated\xa0sale_date\xa0timestamp into a date column, so we can use it to partition the output by day. Select the node\xa0ApplyMapping\xa0in the\xa0Transform\xa0tab. For the\xa0sale_date\xa0field, select\xa0date\xa0as the target type. This will truncate the timestamp to just the date."),(0,o.kt)("li",{parentName:"ol"},"Now it's a good time to save the job. It should let you save successfully.")),(0,o.kt)("p",null,"Finally, we need to configure the sink. Follow these steps:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"With the\xa0ApplyMapping\xa0node selected, go to the\xa0Target\xa0dropdown and choose\xa0Amazon S3. The sink will be added to the\xa0ApplyMapping\xa0node. If you didn't select the parent node before adding the sink, you can still set it in the\xa0Node details\xa0tab of the sink."),(0,o.kt)("li",{parentName:"ol"},"Create an S3 bucket in the same Region as where the job will run. We'll use it to store the output data, so we can clean up easily at the end. If you create it via the console, the default bucket config is OK."),(0,o.kt)("li",{parentName:"ol"},"In the\xa0Data target properties\xa0tab, enter in\xa0S3 Target Location\xa0the URL of the bucket and some path and a trailing slash, for instance:\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"*s3://<your output bucket here>/output/*"),"\\\nLeave the rest with the default values provided."),(0,o.kt)("li",{parentName:"ol"},"Choose\xa0Add partition key\xa0at the bottom and select the field\xa0sale_date.")),(0,o.kt)("p",null,"We could create a partitioned table at the same time just by selecting the corresponding catalog update option. For simplicity, generate the partitioned files at this time without updating the catalog, which is the default option."),(0,o.kt)("p",null,"You can now save and then run the job."),(0,o.kt)("p",null,"Once the job has completed, after a couple of minutes (you can verify this in the\xa0Runs\xa0tab), explore the S3 target location entered above. You can use the Amazon S3 console or the AWS CLI. You will see files named like this:\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"*s3://<your output bucket here>/output/sale_date=<some date yyyy-mm-dd>/<filename>*"),"."),(0,o.kt)("p",null,"If you count the files, there should be close to but not more than 1,460 (depending on the year used and assuming you are using 2 G.1X workers and AWS Glue version 3.0)"),(0,o.kt)("h2",{id:"use-case-improve-the-data-partitioning"},"Use case: Improve the data partitioning"),(0,o.kt)("p",null,"In the previous section, you created a job using a custom visual component that produced synthetic data, did a small transformation on the date, and saved it partitioned on S3 by day."),(0,o.kt)("p",null,"You might be wondering why this job generated so many files for the synthetic data. This is not ideal, especially when they are as small as in this case. If this data was saved as a table with years of history, generating small files has a detrimental impact on tools that consume it, like Amazon Athena."),(0,o.kt)("p",null,'The reason for this is that when the generator calls the "range" function in Apache Spark without specifying a number of memory partitions (notice they are a different kind from the output partitions saved to S3), it defaults to the number of cores in the cluster, which in this example is just 4.'),(0,o.kt)("p",null,"Because the dates are random, each memory partition is likely to contain rows representing all days of the year, so when the sink needs to split the dates into output directories to group the files, each memory partition needs to create one file for each day present, so you can have 4 * 365 (not in a leap year) is 1,460."),(0,o.kt)("p",null,"This example is a bit extreme, and normally data read from the source is not so spread over time. The issue can often be found when you add other dimensions, such as output partition columns."),(0,o.kt)("p",null,"Now you are going to build a component that optimizes this, trying to reduce the number of output files as much as possible: one per output directory.\\\nAlso, let's imagine that on your team, you have the policy of generating S3 date partition separated by year, month, and day as strings, so the files can be selected efficiently whether using a table on top or not."),(0,o.kt)("p",null,"We don't want individual users to have to deal with these optimizations and conventions individually but instead have a component they can just add to their jobs."),(0,o.kt)("h3",{id:"define-the-repartitioner-transform"},"Define the repartitioner transform"),(0,o.kt)("p",null,"For this new transform, create a separate JSON file, let's call it\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"repartition_date.json"),", where we define the new transform and the parameters it needs."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="./src/repartition_date.json"',title:'"./src/repartition_date.json"'},'{\n  "name": "repartition_date",\n  "displayName": "Repartition by date",\n  "description": "Split a date into partition columns and reorganize the data to save them as partitions.",\n  "functionName": "repartition_date",\n  "parameters": [\n    {\n      "name": "dateCol",\n      "displayName": "Date column",\n      "type": "str",\n      "description": "Column with the date to split into year, month and day partitions. The column won\'t be removed"\n    },\n    {\n      "name": "partitionCols",\n      "displayName": "Partition columns",\n      "type": "str",\n      "isOptional": true,\n      "description": "In addition to the year, month and day, you can specify additional columns to partition by, separated by commas"\n    },\n    {\n      "name": "numPartitionsExpected",\n      "displayName": "Number partitions expected",\n      "isOptional": true,\n      "type": "int",\n      "description": "The number of partition column value combinations expected, if not specified the system will calculate it."\n    }\n  ]\n}\n')),(0,o.kt)("h3",{id:"implement-the-transform-logic"},"Implement the transform logic"),(0,o.kt)("p",null,"The script splits the date into multiple columns with leading zeros and then reorganizes the data in memory according to the output partitions. Save the code in a file named\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"repartition_date.py"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py",metastring:'title="./src/repartition_date.py"',title:'"./src/repartition_date.py"'},'from awsglue import DynamicFrame\nimport pyspark.sql.functions as F\n\ndef repartition_date(self, dateCol, partitionCols="", numPartitionsExpected=None):\n    partition_list = partitionCols.split(",") if partitionCols else []\n    partition_list += ["year", "month", "day"]\n\n    date_col = F.col(dateCol)\n    df = self.toDF()\\\n      .withColumn("year", F.year(date_col).cast("string"))\\\n      .withColumn("month", F.format_string("%02d", F.month(date_col)))\\\n      .withColumn("day", F.format_string("%02d", F.dayofmonth(date_col)))\n\n    if not numPartitionsExpected:\n        numPartitionsExpected = df.selectExpr(f"COUNT(DISTINCT {\',\'.join(partition_list)})").collect()[0][0]\n\n    # Reorganize the data so the partitions in memory are aligned when the file partitioning on s3\n    # So each partition has the data for a combination of partition column values\n    df = df.repartition(numPartitionsExpected, partition_list)\n    return DynamicFrame.fromDF(df, self.glue_ctx, self.name)\n\nDynamicFrame.repartition_date = repartition_date\n')),(0,o.kt)("p",null,"Upload the two new files onto the S3 transforms folder like you did for the previous transform."),(0,o.kt)("h3",{id:"deploy-and-use-the-generator-transform"},"Deploy and use the generator transform"),(0,o.kt)("p",null,"Now edit the job to make use of the new component to generate a different output.\\\nRefresh the page in the browser if the new transform is not listed."),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Select the generator transform and from the transforms dropdown, find\xa0Repartition by date\xa0and choose it; it should be added as a child of the generator.\\\nNow change the parent of the\xa0Data target\xa0node to the new node added and remove the\xa0ApplyMapping; we no longer need it."),(0,o.kt)("li",{parentName:"ol"},"Repartition by date\xa0needs you to enter the column that contains the timestamp.\\\nEnter\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"sale_date"),"\xa0(the framework doesn't yet allow field selection using a dropdown) and leave the other two as defaults."),(0,o.kt)("li",{parentName:"ol"},"Now we need to update the output schema with the new date split fields. To do so, use the\xa0Data preview\xa0tab to check it's working correctly (or start a session if the previous one has expired). Then in the\xa0Output schema, choose\xa0Use datapreview schema\xa0so the new fields get added. Notice the transform doesn't remove the original column, but it could if you change it to do so."),(0,o.kt)("li",{parentName:"ol"},"Finally, edit the S3 target to enter a different location so the folders don't mix with the previous run, and it's easier to compare and use. Change the path to\xa0",(0,o.kt)("inlineCode",{parentName:"li"},"/output2/"),".\\\nRemove the existing partition column and instead add year, month, and day.")),(0,o.kt)("p",null,"Save and run the job. After one or two minutes, once it completes, examine the output files. They should be much closer to the optimal number of one per day, maybe two. Consider that in this example, we only have four partitions. In a real dataset, the number of files without this repartitioning would explode very easily.\\\nAlso, now the path follows the traditional date partition structure, for instance:\xa0",(0,o.kt)("inlineCode",{parentName:"p"},"output2/year=2021/month=09/day=01/run-AmazonS3_node1669816624410-4-part-r-00292")),(0,o.kt)("p",null,"Notice that at the end of the file name is the partition number. While we now have more partitions, we have fewer output files because the data is organized in memory more aligned with the desired output."),(0,o.kt)("p",null,'The repartition transform has additional configuration options that we have left empty. You can now go ahead and try different values and see how they affect the output.\\\nFor instance, you can specify "department " as "Partition columns" in the transform and then add it in the sink partition column list. Or you can enter a "Number of partitions expected" and see how it affects the runtime (it no longer needs to determine this at runtime) and the number of files produced as you enter a higher number, for instance, 3,000.'),(0,o.kt)("h2",{id:"how-this-feature-works-under-the-hood"},"How this feature works under the hood"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214525884-6231c016-f617-453b-89e0-58fa0bd34807.png",alt:"flow"})),(0,o.kt)("h2",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"In this lab, you have seen how you can create your own reusable visual transforms and then use them in AWS Glue Studio to enhance your jobs and your team's productivity."),(0,o.kt)("p",null,"You first created a component to use synthetically generated data on demand and then another transform to optimize the data for partitioning on Amazon S3."))}u.isMDXComponent=!0}}]);