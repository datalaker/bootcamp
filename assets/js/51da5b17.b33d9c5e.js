"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[79666],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>d});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=p(a),d=r,k=u["".concat(l,".").concat(d)]||u[d]||c[d]||i;return a?n.createElement(k,o(o({ref:t},m),{},{components:a})):n.createElement(k,o({ref:t},m))}));function d(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},31523:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var n=a(87462),r=(a(67294),a(3905));const i={},o="Machine Learning with Big Data",s={unversionedId:"capstones/other/topic-modeling-pipeline/README",id:"capstones/other/topic-modeling-pipeline/README",title:"Machine Learning with Big Data",description:"Use big-data tools (PySpark) to run topic modeling (unsupervised machine learning) on Twitter data streamed using AWS Kinesis Firehose.",source:"@site/docs/12-capstones/other/topic-modeling-pipeline/README.md",sourceDirName:"12-capstones/other/topic-modeling-pipeline",slug:"/capstones/other/topic-modeling-pipeline/",permalink:"/docs/capstones/other/topic-modeling-pipeline/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{}},l={},p=[{value:"Pre-Requisites",id:"pre-requisites",level:2},{value:"Usage",id:"usage",level:2},{value:"Notebooks",id:"notebooks",level:2},{value:"Notes",id:"notes",level:2},{value:"Project Organization",id:"project-organization",level:2}],m={toc:p};function c(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"machine-learning-with-big-data"},"Machine Learning with Big Data"),(0,r.kt)("p",null,"Use big-data tools (",(0,r.kt)("a",{parentName:"p",href:"https://spark.apache.org/docs/latest/api/python/index.html"},"PySpark"),") to run topic modeling (unsupervised machine learning) on ",(0,r.kt)("a",{parentName:"p",href:"https://developer.twitter.com/en/docs/tutorials/stream-tweets-in-real-time"},"Twitter data streamed")," using ",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/kinesis/data-firehose/"},"AWS Kinesis Firehose"),"."),(0,r.kt)("h2",{id:"pre-requisites"},(0,r.kt)("a",{parentName:"h2",href:"#pre-requisites"},"Pre-Requisites")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"The following AWS (",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials_environment.html"},"1"),", ",(0,r.kt)("a",{parentName:"p",href:"https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_credentials_profiles.html"},"2"),") and ",(0,r.kt)("a",{parentName:"p",href:"https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api"},"Twitter Developer API")," credentials"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_ACCESS_KEY_ID")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_SECRET_ACCESS_KEY")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_REGION")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"AWS_S3_BUCKET_NAME")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"TWITTER_API_KEY")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"TWITTER_API_KEY_SECRET")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"TWITTER_ACCESS_TOKEN")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"TWITTER_ACCESS_TOKEN_SECRET"))),(0,r.kt)("p",{parentName:"li"},"must be stored in a ",(0,r.kt)("a",{parentName:"p",href:"https://help.pythonanywhere.com/pages/environment-variables-for-web-apps/"},(0,r.kt)("inlineCode",{parentName:"a"},".env")," file")," stored one level up from the root directory of this project.i.e. one level up from the directory containing this ",(0,r.kt)("inlineCode",{parentName:"p"},"README.md")," file."))),(0,r.kt)("h2",{id:"usage"},(0,r.kt)("a",{parentName:"h2",href:"#usage"},"Usage")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create AWS resources"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make aws-create\n")),(0,r.kt)("p",{parentName:"li"},"In this step, if the code in the ",(0,r.kt)("a",{parentName:"p",href:"https://nbviewer.org/github/elsdes3/machine-learning-with-big-data/blob/main/1_create_aws_resources.ipynb#set-ec2-public-ip-address-in-ansible-inventory"},"section ",(0,r.kt)("strong",{parentName:"a"},"See EC2 Public IP Address in Ansible Inventory"))," has ",(0,r.kt)("strong",{parentName:"p"},"not")," been manually executed, then edit"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"inventories/production/host_vars/ec2host\n")),(0,r.kt)("p",{parentName:"li"},"and replace ",(0,r.kt)("inlineCode",{parentName:"p"},"...")," in ",(0,r.kt)("inlineCode",{parentName:"p"},"ansible_host: ...")," by the public IP address of the newly created EC2 instance from the AWS Console in the EC2 section.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Provision the EC2 host, excluding Python package installation"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make provision-pre-python\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Install Python packages on the EC2 host"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make provision-post-python\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Start the Twitter streaming script locally"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make stream-local-start\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Stop the Twitter streaming script running locally"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make stream-local-stop\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Start the Twitter streaming script on the EC2 instance"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make stream-start\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Stop the Twitter streaming script running on the EC2 instance"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make stream-stop\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Run the Twitter streaming script locally, saving to a local CSV file but not to S3"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make stream-check\n")),(0,r.kt)("p",{parentName:"li"},"Notes"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"there is no functionality to stop this script (it has to be stopped manually using Ctrl + C, or wait until the specified number of tweets, in ",(0,r.kt)("inlineCode",{parentName:"li"},"max_num_tweets_wanted")," on line 217 of ",(0,r.kt)("inlineCode",{parentName:"li"},"twitter_s3.py"),", have been retrieved)")),(0,r.kt)("p",{parentName:"li"},"Pre-Requisites"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"the following environment variables must be manually set, before running this script, using",(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"export AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_REGION=...\nexport AWS_S3_BUCKET_NAME=...\nexport TWITTER_API_KEY=...\nexport TWITTER_API_KEY_SECRET=...\nexport TWITTER_ACCESS_TOKEN=...\nexport TWITTER_ACCESS_TOKEN_SECRET=...\n"))))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Create AWS SageMaker instance and related resources"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make sagemaker-create\n")),(0,r.kt)("p",{parentName:"li"},"Pre-Requisites"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"an AWS IAM Role granting SageMaker full access to ",(0,r.kt)("strong",{parentName:"li"},"one")," pre-existing S3 bucket (the same bucket created in step 1.) must be created using the AWS console ",(0,r.kt)("strong",{parentName:"li"},"before")," running this step."))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Run quantitative analysis"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make build\n")),(0,r.kt)("p",{parentName:"li"},"and run the following two notebooks in order of the numbered prefix in their name"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"3_combine_raw_data.ipynb")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"4_data_processing.ipynb")))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Destroy AWS SageMaker resources"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make sagemaker-destroy\n"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Destroy AWS resources"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"make aws-destroy\n")))),(0,r.kt)("h2",{id:"notebooks"},(0,r.kt)("a",{parentName:"h2",href:"#notebooks"},"Notebooks")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"1_create_aws_resources.ipynb")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/1_create_aws_resources.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"use the AWS Python SDK (",(0,r.kt)("inlineCode",{parentName:"li"},"boto3")," ",(0,r.kt)("a",{parentName:"li",href:"https://pypi.org/project/boto3/"},"link"),") to create AWS resources",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html"},"S3 storage bucket")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CloudWatchLogsConcepts.html"},"CloudWatch Log Group and Log Stream")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html"},"IAM Role")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html"},"Kinesis Firehose Delivery Stream")))))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"2_create_sagemaker_resources")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/2_create_sagemaker_resources.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"use ",(0,r.kt)("inlineCode",{parentName:"li"},"boto3")," to create an ",(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html"},"AWS SageMaker instance")),(0,r.kt)("li",{parentName:"ul"},"PySpark version 2.4.0 will be used on a ",(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html"},"SageMaker notebook")))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"3_combine_raw_data.ipynb")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/3_combine_raw_data.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"combines raw data in the S3 bucket into hourly CSVs",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"since each hour of data files were small enough to read into a single data object (DataFrame), in-memory tools were used to combine each hourly folder of streamed data into a single CSV"))),(0,r.kt)("li",{parentName:"ul"},"filters out unwanted tweets based on a list of words that are not relevant to the subject of this project"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"3_1_combine_raw_data_pyspark.ipynb")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/3_1_combine_raw_data_pyspark.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"combines all raw data in the S3 bucket using PySpark and Databricks",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"files in all hourly folders were loaded into a single Spark DataFrame"))),(0,r.kt)("li",{parentName:"ul"},"uses only Spark DataFrame methods to filter out unwanted tweets"),(0,r.kt)("li",{parentName:"ul"},"this notebook ",(0,r.kt)("strong",{parentName:"li"},"must be run on Databricks")))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"4_data_processing.ipynb")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/4_data_processing.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"perform topic modeling (unsupervised machine learning) on combined hourly CSVs using PySpark and PySparkML"),(0,r.kt)("li",{parentName:"ul"},"this notebook ",(0,r.kt)("strong",{parentName:"li"},"must run on the AWS SageMaker instance")," ",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/2_create_sagemaker_resources.ipynb"},"created in the ",(0,r.kt)("inlineCode",{parentName:"a"},"2_create_sagemaker_resources.ipynb")," notebook")))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"5_delete_sagemaker_resources.ipynb")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/5_delete_sagemaker_resources.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"use ",(0,r.kt)("inlineCode",{parentName:"li"},"boto3")," to delete all AWS resources created to support creation of a SageMaker instance"))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"6_delete_aws_resources.ipynb")," (",(0,r.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/elsdes3/big-data-ml/blob/main/6_delete_aws_resources.ipynb"},"view"),")",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"use ",(0,r.kt)("inlineCode",{parentName:"li"},"boto3")," to delete all AWS resources")))),(0,r.kt)("h2",{id:"notes"},(0,r.kt)("a",{parentName:"h2",href:"#notes"},"Notes")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Running the notebooks to create and destroy AWS resources in a non-interactive approach has not been verified. It is not currently known if this is possible."),(0,r.kt)("li",{parentName:"ol"},"AWS resources are created and destroyed using the ",(0,r.kt)("inlineCode",{parentName:"li"},"boto3")," AWS Python SDK. The AWS EC2 instance that is used to host the Twitter streaming (Python) code is ",(0,r.kt)("a",{parentName:"li",href:"https://www.ansible.com/use-cases/provisioning"},"provisioned using Ansible playbooks"),"."),(0,r.kt)("li",{parentName:"ol"},"The AWS credentials must be associated to a user group whose users have been granted programmatic access to AWS resources. In order to configure this for the IAM user group from the AWS console, see the documentation ",(0,r.kt)("a",{parentName:"li",href:"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console"},"here"),". For this project, this was done before creating any AWS resources using the AWS Python SDK."),(0,r.kt)("li",{parentName:"ol"},"The Twitter credentials must be for a user account with ",(0,r.kt)("a",{parentName:"li",href:"https://developer.twitter.com/en/support/twitter-api/v2"},"elevated access")," to the Twitter Developer API.")),(0,r.kt)("h2",{id:"project-organization"},(0,r.kt)("a",{parentName:"h2",href:"#project-organization"},"Project Organization")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 .gitignore                          <- files and folders to be ignored by version control system\n\u251c\u2500\u2500 .pre-commit-config.yaml             <- configuration file for pre-commit hooks\n\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 workflows\n\u2502       \u2514\u2500\u2500 main.yml                    <- configuration file for CI build on Github Actions\n\u251c\u2500\u2500 Makefile                            <- Makefile with commands like`make lint` or `make build`\n\u251c\u2500\u2500 README.md                           <- The top-level README for developers using this project.\n\u251c\u2500\u2500 ansible.cfg                         <- configuration file for Ansible\n\u251c\u2500\u2500 environment.yml                     <- configuration file to create environment to run project on Binder\n\u251c\u2500\u2500 manage_host.yml                     <- manage provisioning of EC2 host\n\u251c\u2500\u2500 read_data.py                        <- Python script to read streamed Twitter data that has been saved locally\n\u251c\u2500\u2500 streamer.py                         <- Wrapper script to control local or remote Twitter streaming\n\u251c\u2500\u2500 stream_twitter.yml                  <- stream Twitter data on EC2 instance\n\u251c\u2500\u2500 twitter_s3.py                       <- Python script to stream Twitter data locally or on EC2 instance\n\u251c\u2500\u2500 variables_run.yaml                  <- Ansible playbook variables\n\u251c\u2500\u2500 executed_notebooks\n|   \u2514\u2500\u2500 *.ipynb                         <- executed notebooks, with output and execution datetime suffix in filename\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 raw                             <- The original, immutable data dump.\n|   \u2514\u2500\u2500 processed                       <- Intermediate (transformed) data and final, canonical data sets for modeling.\n\u251c\u2500\u2500 1_create_aws_resources.ipynb        <- create cloud resources on AWS\n\u251c\u2500\u2500 2_create_sagemaker_resources.ipynb  <- create AWS SageMaker resources\n\u251c\u2500\u2500 3_combine_raw_data.ipynb            <- combine raw tweets data in stored in S3 into CSV files\n\u251c\u2500\u2500 4_data_processing.ipynb             <- unsupervised machine learning\n\u251c\u2500\u2500 5_delete_sagemaker_resources.ipynb  <- destroy AWS SageMaker resources\n\u251c\u2500\u2500 6_delete_aws_resources.ipynb        <- destroy AWS cloud resources\n\u251c\u2500\u2500 requirements.txt                    <- base packages required to execute all Jupyter notebooks (incl. jupyter)\n\u251c\u2500\u2500 inventories\n\u2502   \u251c\u2500\u2500 production\n\u2502       \u251c\u2500\u2500 host_vars                   <- variables to inject into Ansible playbooks, per target host\n\u2502           \u2514\u2500\u2500 ec2_host\n|       \u2514\u2500\u2500 hosts                       <- Ansible inventory\n\u251c\u2500\u2500 src                                 <- Source code for use in this project.\n\u2502   \u251c\u2500\u2500 __init__.py                     <- Makes src a Python module\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ansible                         <- Utilities to support Ansible orchestration playbooks\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.ansible a Python module\n\u2502       \u2514\u2500\u2500 playbook_utils.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 cw                              <- Scripts to manage AWS CloudWatch Log Groups and Streams\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.cw a Python module\n\u2502       \u2514\u2500\u2500 cloudwatch_logs.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 data                            <- Scripts to combine raw tweets data pre hour into a CSV file\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.data a Python module\n\u2502       \u2514\u2500\u2500 combine_data.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ec2                             <- Scripts to manage AWS EC2 instances and security groups\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.ec2 a Python module\n\u2502       \u2514\u2500\u2500 ec2_instances_sec_groups.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 firehose                        <- Scripts to manage AWS Kinesis firehose data streams\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.firehose a Python module\n\u2502       \u2514\u2500\u2500 kinesis_firehose.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 iam                             <- Scripts to manage AWS IAM\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.iam a Python module\n\u2502       \u2514\u2500\u2500 iam_roles.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 keypairs                        <- Scripts to manage AWS EC2 SSH key pairs\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.keypairs a Python module\n\u2502       \u2514\u2500\u2500 ssh_keypairs.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 model_interpretation            <- Scripts to manage ML models\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.model_interpretation a Python module\n\u2502       \u251c\u2500\u2500 import_export_models.py\n\u2502       \u2514\u2500\u2500 interpret_models.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 nlp                             <- Scripts to preform NLP tasks on text data\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.nlp a Python module\n\u2502       \u251c\u2500\u2500 clean_text.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 s3                              <- Scripts to manage AWS S3 buckets\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.s3 a Python module\n\u2502       \u251c\u2500\u2500 bucket_contents.py\n\u2502       \u2514\u2500\u2500 buckets.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 visualization                   <- Scripts to preform data visualization tasks\n|       \u251c\u2500\u2500 __init__.py                 <- Makes src.visualization a Python module\n\u2502       \u251c\u2500\u2500 visualize.py\n\u2502\n\u251c\u2500\u2500 papermill_runner.py                 <- Python functions that execute system shell commands.\n\u2514\u2500\u2500 tox.ini                             <- tox file with settings for running tox; see https://tox.readthedocs.io/en/latest/\n")),(0,r.kt)("hr",null),(0,r.kt)("p",null,(0,r.kt)("small",null,"Project based on the ",(0,r.kt)("a",{target:"_blank",href:"https://drivendata.github.io/cookiecutter-data-science/"},"cookiecutter data science project template"),". #cookiecutterdatascience")))}c.isMDXComponent=!0}}]);