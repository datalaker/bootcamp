"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[11309],{3905:(e,t,n)=>{n.d(t,{Zo:()=>m,kt:()=>d});var r=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},u=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,c=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=l(n),d=o,f=u["".concat(c,".").concat(d)]||u[d]||p[d]||a;return n?r.createElement(f,i(i({ref:t},m),{},{components:n})):r.createElement(f,i({ref:t},m))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var l=2;l<a;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}u.displayName="MDXCreateElement"},27723:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>l});var r=n(87462),o=(n(67294),n(3905));const a={},i="Lab: CSV to Parquet Transformation with Glue Studio",s={unversionedId:"processing/lab-csv-to-parquet-conversion/README",id:"processing/lab-csv-to-parquet-conversion/README",title:"Lab: CSV to Parquet Transformation with Glue Studio",description:"Task: Process raw (CSV or JSON) data from the Landing S3 bucket and save it into another S3 bucket in a Columnar format with partitioning",source:"@site/docs/03-processing/lab-csv-to-parquet-conversion/README.md",sourceDirName:"03-processing/lab-csv-to-parquet-conversion",slug:"/processing/lab-csv-to-parquet-conversion/",permalink:"/docs/processing/lab-csv-to-parquet-conversion/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681732641,formattedLastUpdatedAt:"Apr 17, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Lab: Tickets ETL with Glue Studio",permalink:"/docs/processing/lab-glue-studio-tickets/"},next:{title:"AWS Lambda Function",permalink:"/docs/processing/aws-lambda"}},c={},l=[],m={toc:l};function p(e){let{components:t,...n}=e;return(0,o.kt)("wrapper",(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"lab-csv-to-parquet-transformation-with-glue-studio"},"Lab: CSV to Parquet Transformation with Glue Studio"),(0,o.kt)("p",null,"Task: Process raw (CSV or JSON) data from the Landing S3 bucket and save it into another S3 bucket in a Columnar format with partitioning"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Create Glue job using PySpark and enable bookmarking."),(0,o.kt)("li",{parentName:"ol"},"Make Glue job process raw (CSV or JSON) data from landing S3 bucket and save it into another S3 bucket in a columnar format with partitioning: partition on stock name and bucket on year."),(0,o.kt)("li",{parentName:"ol"},"Create Glue trigger to automatically start the PySpark job when new files are created in the S3 bucket."),(0,o.kt)("li",{parentName:"ol"},"Make sure the components are deployable using CloudFormation.")),(0,o.kt)("p",null,"The data from the landing S3 bucket looks like this:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"Date,Open,High,Low,Close,Volume,ticker_id,ticker_name\n2017-11-22 00:00:00-05:00,28.5271320343017,28.5271320343017,28.5271320343017,28.5271320343017,0,0,VASGX\n2017-11-24 00:00:00-05:00,28.5954589843749,28.5954589843749,28.5954589843749,28.595458984375,0,0,VASGX\n2017-11-27 00:00:00-05:00,28.5356674194335,28.5356674194335,28.5356674194335,28.5356674194335,0,0,VASGX\n2017-11-28 00:00:00-05:00,28.7150325775146,28.7150325775146,28.7150325775146,28.7150325775146,0,0,VASGX\n2017-11-29 00:00:00-05:00,28.6637859344482,28.6637859344482,28.6637859344482,28.6637859344482,0,0,VASGX\n2017-11-30 00:00:00-05:00,28.757734298706,28.757734298706,28.757734298706,28.757734298706,0,0,VASGX\n2017-12-01 00:00:00-05:00,28.7150325775146,28.7150325775146,28.7150325775146,28.7150325775146,0,0,VASGX\n2017-12-04 00:00:00-05:00,28.6637859344482,28.6637859344482,28.6637859344482,28.6637859344482,0,0,VASGX\n2017-12-05 00:00:00-05:00,28.60400390625,28.60400390625,28.60400390625,28.60400390625,0,0,VASGX\n")),(0,o.kt)("p",null,"The following code is our Glue job which processes the CSV file from the landing s3 bucket, splits the date column up, partitions on stock name and buckets on year. No major transformations were needed. All we had to do was convert the date column from a string to a date and then split the column to create \u2018year\u2019, \u2018month\u2019, \u2018and day\u2019 columns. The partitioned data is then written to the output s3 bucket in parquet format."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},'import sys\nimport boto3\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.functions import to_date, split\n\n## @params: [JOB_NAME]\nargs = getResolvedOptions(sys.argv, [\'JOB_NAME\'])\n\n# Create spark cluster\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args[\'JOB_NAME\'], args)\n\n# Create dynamic frame of CSV from S3 bucket\ndynamicFrame = glueContext.create_dynamic_frame.from_options(\n    connection_type="s3",\n    connection_options={"paths": ["s3://myonrampbucket/tickers_fct_name.csv"]},\n    format="csv",\n    format_options={\n        "withHeader": True,\n        # "optimizePerformance": True,\n    },\n    )\n\n# Convert dynamic frame to a DataFrame\ndf = dynamicFrame.toDF()\ndf.show()\nprint("Dataframe converted")\n\n# Create \'Year\' column from \'date\' column\ndf = df.withColumn("dateAdded", to_date(split(df["Date"], " ").getItem(0)\\\n    .cast("string"), \'yyyy-MM-dd\')) \\\n    .withColumn("year", split(col("dateAdded"), "-").getItem(0)) \\\n    .withColumn("month", split(col("dateAdded"), "-").getItem(1)) \\\n    .withColumn("day", split(col("dateAdded"), "-").getItem(2)) \\\n    .withColumn("tickerName",col("ticker_name").cast("string")) \\\n    .orderBy(\'year\')\n  \nprint("Dataframe columns added and sorted by year.")\nprint(df.show())\n\n# Partition dataframe by year and ticker name\npartitioned_dataframe = df.repartition(col("year"), col("tickerName"))\nprint("Dataframe repartitioned")\n\n# Convert back to dynamic frame\ndynamic_frame_write = DynamicFrame.fromDF(partitioned_dataframe, glueContext, "dynamic_frame_write")\nprint("Dataframe converted to dynamic frame")\n\n\n# Save dynamic frame into S3 bucket\nglueContext.write_dynamic_frame.from_options(frame=dynamic_frame_write, \n    connection_type="s3", \n    connection_options=dict(path="s3://stocks-partitioned/", \n                            partitionKeys=["year", "ticker_name"]), \n                            format="parquet",\n                            transformation_ctx="datasink2")\n\nprint("Dynamic frame saved in s3")\n\n\n\n# Commit file read to Job Bookmark\njob.commit()\nprint("Job completed!!!")\n\n\njob.commit()\n')),(0,o.kt)("p",null,"Finally, a Lambda trigger that invokes the Glue job using boto3 every time a new item is created in the landing S3 bucket. All components in this story are deployable using CloudFormation."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"import json\nimport urllib.parse\nimport boto3\nfrom botocore.exceptions import ClientError\n\nprint('Loading function')\n\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n    print(\"Received event: \" + json.dumps(event, indent=2))\n\n    # Get the object from the event and show its content type\n    bucket = event['Records'][0]['s3']['bucket']['name']\n    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')\n    try:\n        response = s3.get_object(Bucket=bucket, Key=key)\n        print(\"CONTENT TYPE: \" + response['ContentType'])\n        #return response['ContentType']\n    except Exception as e:\n        print(e)\n        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))\n        raise e\n      \n    def run_glue_job(job_name, arguments = {}):\n        session = boto3.session.Session()\n        glue_client = session.client('glue')\n        try:\n            job_run_id = glue_client.start_job_run(JobName=job_name, Arguments=arguments)\n            return job_run_id\n        except ClientError as e:\n            raise Exception( \"boto3 client error in run_glue_job: \" + e.__str__())\n        except Exception as e:\n            raise Exception( \"Unexpected error in run_glue_job: \" + e.__str__())\n  \n    print(run_glue_job(\"Stocks\"))\n")))}p.isMDXComponent=!0}}]);