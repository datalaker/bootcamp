"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[75082],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(a),h=r,g=d["".concat(l,".").concat(h)]||d[h]||u[h]||o;return a?n.createElement(g,i(i({ref:t},p),{},{components:a})):n.createElement(g,i({ref:t},p))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},1125:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const o={},i="Developing batch processing solutions by using Data Factory, Data Lake, Spark, Azure Synapse Pipelines, PolyBase, and Azure Databricks",s={unversionedId:"orchestration/azure-data-factory/lab-batch-processing-solution/README",id:"orchestration/azure-data-factory/lab-batch-processing-solution/README",title:"Developing batch processing solutions by using Data Factory, Data Lake, Spark, Azure Synapse Pipelines, PolyBase, and Azure Databricks",description:"B1752509001",source:"@site/docs/06-orchestration/azure-data-factory/lab-batch-processing-solution/README.md",sourceDirName:"06-orchestration/azure-data-factory/lab-batch-processing-solution",slug:"/orchestration/azure-data-factory/lab-batch-processing-solution/",permalink:"/docs/orchestration/azure-data-factory/lab-batch-processing-solution/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Incremental Data Loading in Azure Data Factory",permalink:"/docs/orchestration/azure-data-factory/lab-adf-incremental-loading/"},next:{title:"Cloud Data Fusion",permalink:"/docs/orchestration/datafusion/"}},l={},c=[{value:"Storage",id:"storage",level:2},{value:"Transform",id:"transform",level:2},{value:"Creating data pipelines",id:"creating-data-pipelines",level:2}],p={toc:c};function u(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"developing-batch-processing-solutions-by-using-data-factory-data-lake-spark-azure-synapse-pipelines-polybase-and-azure-databricks"},"Developing batch processing solutions by using Data Factory, Data Lake, Spark, Azure Synapse Pipelines, PolyBase, and Azure Databricks"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218307988-ac4d682d-4250-48a7-9a05-67d1173d1b0e.jpeg",alt:"B17525_09_001"})),(0,r.kt)("p",null,"A batch processing solution\xa0typically consists of five major components:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Storage systems such as Azure Blob storage, ADLS Gen2, HDFS, or similar"),(0,r.kt)("li",{parentName:"ul"},"Transformation/batch processing systems such as Spark, SQL, or Hive (via Azure HDInsight)"),(0,r.kt)("li",{parentName:"ul"},"Analytical data stores such as Synapse Dedicated SQL pool, Cosmos DB, and HBase (via Azure HDInsight)"),(0,r.kt)("li",{parentName:"ul"},"Orchestration systems such as ADF and Oozie (via Azure HDInsight)"),(0,r.kt)("li",{parentName:"ul"},"Business Intelligence\xa0(BI) reporting systems\xa0such as Power BI")),(0,r.kt)("p",null,"Let's assume\xa0that we are continuously\xa0getting trip data\xa0from different regions (zip codes), which\xa0is stored in Azure Blob storage, and the trip\xa0fares are stored in\xa0an Azure SQL Server. We have a requirement to merge these two datasets and generate daily revenue reports for each region."),(0,r.kt)("p",null,"In order to take care of this requirement, we can build a pipeline as shown in the following diagram:"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218308144-c76f7039-a1b0-4e72-abbd-2b21f008e0a7.jpeg",alt:"B17525_09_002"})),(0,r.kt)("p",null,"The preceding pipeline, when translated into an ADF pipeline, would look like the following figure:"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218308190-7f6b7abc-c83b-40c4-8adb-e8d137441504.jpeg",alt:"B17525_09_003"})),(0,r.kt)("p",null,"As you can see, the pipeline has four stages:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Data ingestion"),": The first two stages,\xa0",(0,r.kt)("strong",{parentName:"li"},"FetchTripsFrmBlob"),"\xa0and\xa0",(0,r.kt)("strong",{parentName:"li"},"FetchFaresFrmSQL"),"\xa0get the data\xa0into the data lake."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Data cleansing"),": The\xa0",(0,r.kt)("strong",{parentName:"li"},"DataCleansing"),"\xa0stage in the diagram\xa0cleans up the data."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Transformation"),": The Spark Notebook\xa0",(0,r.kt)("strong",{parentName:"li"},"Transform"),"\xa0stage\xa0in the diagram transforms the data."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Loading into an analytical database"),": The\xa0",(0,r.kt)("strong",{parentName:"li"},"PolyBaseCopySQLDW"),"\xa0stage to copies the data\xa0into a Synapse SQL pool.")),(0,r.kt)("p",null,"The last stage would be BI tools reading from the analytical database and generating reports (which is not shown in the diagram as that is not an ADF activity)."),(0,r.kt)("h2",{id:"storage"},"Storage"),(0,r.kt)("p",null,"Let's consider ADLS Gen2 as our data lake storage. We can\xa0create the following folder structure to handle our batch pipeline:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The\xa0",(0,r.kt)("strong",{parentName:"li"},"raw trips"),"\xa0data can be stored here: ",(0,r.kt)("inlineCode",{parentName:"li"},"iac/raw/trips/2022/01/01")),(0,r.kt)("li",{parentName:"ul"},"The cleaned-up data can be copied over to the\xa0",(0,r.kt)("strong",{parentName:"li"},"transform/in"),"\xa0folder: ",(0,r.kt)("inlineCode",{parentName:"li"},"iac/transform/in/2022/01/01")),(0,r.kt)("li",{parentName:"ul"},"The output of the transformed data can move into the\xa0",(0,r.kt)("strong",{parentName:"li"},"transform/out"),"\xa0folder: ",(0,r.kt)("inlineCode",{parentName:"li"},"iac/transform/out/2022/01/01")),(0,r.kt)("li",{parentName:"ul"},"Finally, we can import the data from\xa0",(0,r.kt)("strong",{parentName:"li"},"transform/out"),"\xa0into Synapse\xa0SQL Dedicated pool using\xa0",(0,r.kt)("strong",{parentName:"li"},"PolyBase"),".")),(0,r.kt)("p",null,"Note that tools such as\xa0",(0,r.kt)("strong",{parentName:"p"},"ADF"),"\xa0and\xa0",(0,r.kt)("strong",{parentName:"p"},"PolyBase"),"\xa0also provide the ability to directly move data between Spark\xa0and Synapse SQL Dedicated pool. You can choose this direct approach instead of storing the intermediate data in the data lake if that works better for you in terms of performance and cost. But in most data lakes, more than one tool might access the intermediate data from the data lake and it will be useful to keep historical datasets for future analysis. Hence it might make sense to keep a copy in the data lake also."),(0,r.kt)("h2",{id:"transform"},"Transform"),(0,r.kt)("p",null,"You can find the code (scala script as well as dbc file, that can be imported in databricks) in the assets folder. That data that is produced by this notebook can also be found in the data folder. I used AzCopy to download it from the ADLS2 container."),(0,r.kt)("p",null,"AzCopy command:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"AzCopy copy 'https://sparshstorage1.blob.core.windows.net/databricks/dailytrips' './data/' --recursive\n")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Configuring an ADB notebook activity in ADF:")),(0,r.kt)("p",null,"From the Azure Data Factory Activities tab, choose Notebook under Databricks and add it to the pipeline by dragging the icon into the worksheet area as shown in the following screenshot."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218311436-8cc30531-9258-4907-be8a-4a491fb7eca6.jpeg",alt:"B17525_09_015"})),(0,r.kt)("p",null,"You have to link this Notebook activity to the notebook that you created in the previous step. In order to link the notebook, you will have to first get the access token from Azure Databricks. You can generate the access token from the Azure Databricks portal from the User Settings tab. Click on the Generate New Token button to create a new access token."),(0,r.kt)("p",null,"Now, link the previously created ADB notebook using a linked service. You will have to fill in Databricks Workspace URL, the Access token field \u2013 with the access token, and select whether you want to spin up a New job cluster or point to an Existing interactive cluster, and so on."),(0,r.kt)("p",null,"Once you have created the linked service and entered those details into the ADF notebook activity, your sample transformation stage will be complete."),(0,r.kt)("h2",{id:"creating-data-pipelines"},"Creating data pipelines"),(0,r.kt)("p",null,"You can create a pipeline from the Pipeline tab of Azure Data Factory. All you need to do is to select the activities for your pipeline from the Activities tab and click and drag it into the canvas. You can link the activities using the green box (on the right side of each activity) and chain the blocks together either sequentially or parallelly to derive the required output. The following screenshot shows an example."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218311781-14233984-08bc-45e0-be4f-d7d7999488ce.jpeg",alt:"B17525_09_025"})),(0,r.kt)("p",null,"Once you have the pipeline stitched together, you can trigger it using the Add trigger button. The trigger could be one-time, event-based, or recurring. I hope you now have an understanding of how to create and publish an end-to-end batch pipeline."))}u.isMDXComponent=!0}}]);