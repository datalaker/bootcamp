"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[61271],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},s=Object.keys(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(r=0;r<s.length;r++)n=s[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),p=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=p(e.components);return r.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),c=p(n),m=a,f=c["".concat(l,".").concat(m)]||c[m]||u[m]||s;return n?r.createElement(f,i(i({ref:t},d),{},{components:n})):r.createElement(f,i({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var s=n.length,i=new Array(s);i[0]=c;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:a,i[1]=o;for(var p=2;p<s;p++)i[p]=n[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},43763:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>p});var r=n(87462),a=(n(67294),n(3905));const s={},i="Reddit Submissions, Authors and Subreddits analysis",o={unversionedId:"capstones/reddit/README",id:"capstones/reddit/README",title:"Reddit Submissions, Authors and Subreddits analysis",description:"Problem Statement",source:"@site/docs/12-capstones/reddit/README.md",sourceDirName:"12-capstones/reddit",slug:"/capstones/reddit/",permalink:"/docs/capstones/reddit/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{}},l={},p=[{value:"Problem Statement",id:"problem-statement",level:2},{value:"Data Sources",id:"data-sources",level:2},{value:"Airflow",id:"airflow",level:2},{value:"Pipeline 1",id:"pipeline-1",level:3},{value:"Pipeline 2",id:"pipeline-2",level:3},{value:"Pipeline 3",id:"pipeline-3",level:3},{value:"Project Structure",id:"project-structure",level:2}],d={toc:p};function u(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"reddit-submissions-authors-and-subreddits-analysis"},"Reddit Submissions, Authors and Subreddits analysis"),(0,a.kt)("h2",{id:"problem-statement"},"Problem Statement"),(0,a.kt)("p",null,'Reddit is "the frontpage of the internet" and has a broad range of discussions about nearly every topic one can think of. This makes it a perfect candidate for analytics. This project will use a sample of publicly available dump of Reddit and load it into a AWS Redshift warehouse so that Data Scientists can make use of the content and for example develop a recommender system that finds the most suitable subreddit for your purposes.'),(0,a.kt)("p",null,"The goal of this project is to create a Data Warehouse to analyze trending and new subreddits using Airflow."),(0,a.kt)("p",null,"The project uses the Reddit API to get subreddits and stores them on AWS S3 in JSON format. Data processing happens on an EMR cluster on AWS using PySpark and processed data gets stored on AWS S3 in parquet format. Finally, the data gets inserted into AWS Redshift, gets denormalized to create fact and dimension tables."),(0,a.kt)("h2",{id:"data-sources"},"Data Sources"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"http://files.pushshift.io/reddit/authors/"},"Authors")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"http://files.pushshift.io/reddit/submissions/"},"Submissions")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("a",{parentName:"li",href:"http://files.pushshift.io/reddit/subreddits/"},"Subreddits"))),(0,a.kt)("h2",{id:"airflow"},"Airflow"),(0,a.kt)("h3",{id:"pipeline-1"},"Pipeline 1"),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/215309302-da77564a-f1cc-44c4-ab49-5313f0845063.png",alt:null})),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Setting up the airflow environment")),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/215309290-0b99e64f-20bb-47eb-9fbf-4e9e00ce70d7.png",alt:null})),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/215309292-d2eab5b0-0d25-4ab0-8579-6748d6f5b788.png",alt:null})),(0,a.kt)("p",null,(0,a.kt)("strong",{parentName:"p"},"Output")),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/215309315-2cef7bf0-ab34-4281-a191-867c762b666a.png",alt:null})),(0,a.kt)("h3",{id:"pipeline-2"},"Pipeline 2"),(0,a.kt)("p",null,"#TODO: This pipeline is yet to be tested."),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/215309305-15c2d3d0-5add-4fc2-bf15-ad0a0f02f206.png",alt:null})),(0,a.kt)("h3",{id:"pipeline-3"},"Pipeline 3"),(0,a.kt)("p",null,"Architecture:"),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://recohut-images.s3.amazonaws.com/labs/lab-117-reddit/datawarehouse_architecture.png",alt:null})),(0,a.kt)("h2",{id:"project-structure"},"Project Structure"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"\u251c\u2500\u2500 [ 18K]  01-sa.ipynb\n\u251c\u2500\u2500 [4.6K]  README.md\n\u251c\u2500\u2500 [ 16K]  REPORT.md\n\u251c\u2500\u2500 [ 768]  airflow\n\u2502   \u251c\u2500\u2500 [ 288]  dags\n\u2502   \u251c\u2500\u2500 [ 160]  plugins\n\u2502   \u2514\u2500\u2500 [ 160]  plugins_2\n\u251c\u2500\u2500 [ 148]  data\n\u2502   \u2514\u2500\u2500 [  52]  download.sh\n\u251c\u2500\u2500 [134K]  nbs\n\u2502   \u251c\u2500\u2500 [ 55K]  create_dates_and_holiday_dataset.ipynb\n\u2502   \u2514\u2500\u2500 [ 79K]  data-model-explained.ipynb\n\u2514\u2500\u2500 [ 14K]  src\n    \u251c\u2500\u2500 [1.8K]  data_quality_queries.py\n    \u251c\u2500\u2500 [1.4K]  download_datasets.sh\n    \u251c\u2500\u2500 [1.5K]  make_chunks.py\n    \u251c\u2500\u2500 [1.4K]  preprocess_authors.py\n    \u251c\u2500\u2500 [ 922]  sample_dataset.sh\n    \u251c\u2500\u2500 [ 192]  sql\n    \u2514\u2500\u2500 [6.4K]  sql_queries.py\n\n 187K used in 8 directories, 12 files\n")))}u.isMDXComponent=!0}}]);