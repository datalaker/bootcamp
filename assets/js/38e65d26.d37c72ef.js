"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[94074],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>h});var o=a(67294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},i=Object.keys(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=o.createContext({}),c=function(e){var t=o.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},d=function(e){var t=c(e.components);return o.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},p=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,l=e.parentName,d=r(e,["components","mdxType","originalType","parentName"]),p=c(a),h=n,m=p["".concat(l,".").concat(h)]||p[h]||u[h]||i;return a?o.createElement(m,s(s({ref:t},d),{},{components:a})):o.createElement(m,s({ref:t},d))}));function h(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,s=new Array(i);s[0]=p;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:n,s[1]=r;for(var c=2;c<i;c++)s[c]=a[c];return o.createElement.apply(null,s)}return o.createElement.apply(null,a)}p.displayName="MDXCreateElement"},44614:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var o=a(87462),n=(a(67294),a(3905));const i={},s="Work around the world Job vacancies analysis",r={unversionedId:"capstones/other/us-job-vacancies/README",id:"capstones/other/us-job-vacancies/README",title:"Work around the world Job vacancies analysis",description:"Problem Statement",source:"@site/docs/12-capstones/other/us-job-vacancies/README.md",sourceDirName:"12-capstones/other/us-job-vacancies",slug:"/capstones/other/us-job-vacancies/",permalink:"/docs/capstones/other/us-job-vacancies/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{}},l={},c=[{value:"Problem Statement",id:"problem-statement",level:2},{value:"What you&#39;ll build",id:"what-youll-build",level:2},{value:"Data Sources",id:"data-sources",level:2},{value:"US Jobs on Dice.com DAG",id:"us-jobs-on-dicecom-dag",level:3},{value:"JobTechDev.se Historical Jobs DAG",id:"jobtechdevse-historical-jobs-dag",level:3},{value:"Landing Jobs API DAG",id:"landing-jobs-api-dag",level:3},{value:"GitHub Jobs API DAG",id:"github-jobs-api-dag",level:3},{value:"Stackoverflow RSS Feed DAG",id:"stackoverflow-rss-feed-dag",level:3},{value:"Angel.co jobs DAG",id:"angelco-jobs-dag",level:3},{value:"Algolia Search Index Jobs DAG",id:"algolia-search-index-jobs-dag",level:3}],d={toc:c};function u(e){let{components:t,...a}=e;return(0,n.kt)("wrapper",(0,o.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"work-around-the-world-job-vacancies-analysis"},"Work around the world Job vacancies analysis"),(0,n.kt)("h2",{id:"problem-statement"},"Problem Statement"),(0,n.kt)("p",null,"Have you ever tried to look for a visa sponsor job vacancy? Or looking for a job overall nowadays is a little bit\noverwhelming because of the number of different places you have to go to find a list of vacancies."),(0,n.kt)("p",null,"This project goals is to unify and provide a simple normalized and unified database of job vacancies from several data\nsources (datasets for historical purposes and APIs to more up-to-date jobs). The final data schema is a star-schema\nmodel to ease querying through the jobs list, whereas the job listing is the fact and the company, skills and provider\nare the dimensions of our data model."),(0,n.kt)("h2",{id:"what-youll-build"},"What you'll build"),(0,n.kt)("p",null,"The purpose of this project is to assemble a dataset to ease query for jobs of a determined set of skills or company.\nFor that we don't need normalized informations. Instead we adopted a star schema because of it's scalability in terms of\nreading and querying. "),(0,n.kt)("p",null,"To simplify our job listing we'll have only a single fact table with the job vacancies and two other tables to aggregate\ndata on companies and tags."),(0,n.kt)("h2",{id:"data-sources"},"Data Sources"),(0,n.kt)("h3",{id:"us-jobs-on-dicecom-dag"},"US Jobs on Dice.com DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"dice_com_jobs_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: CSV Dataset"),(0,n.kt)("li",{parentName:"ul"},"Data Source: ",(0,n.kt)("a",{parentName:"li",href:"https://data.world/promptcloud/us-jobs-on-dice-com"},"https://data.world/promptcloud/us-jobs-on-dice-com"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A dataset of 22.000 USA tech job posts crawled from the dice.com jobs site. "),(0,n.kt)("li",{parentName:"ul"},"It contains the job title, description, company and skills (tags). ")))),(0,n.kt)("p",null,"Basically this DAG takes care of parsing the CSV from a S3 Bucket and staging it on our Redshift cluster to then upsert the dimensions and facts data.\nIt runs only once as this dataset will not be updated anymore, it's for historical comparison purpose only."),(0,n.kt)("h3",{id:"jobtechdevse-historical-jobs-dag"},"JobTechDev.se Historical Jobs DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"jobtechdev_se_historical_jobs_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: JSON Dataset"),(0,n.kt)("li",{parentName:"ul"},"Data Source: ",(0,n.kt)("a",{parentName:"li",href:"https://jobtechdev.se/api/jobs/historical/"},"https://jobtechdev.se/api/jobs/historical/"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"A historical dataset of nordic job vacancies with anonymized data. It has 4.4 million job posts starting from 2006 until 2017. "),(0,n.kt)("li",{parentName:"ul"},"Altought this is a very large dataset it only contains informations about the job title, description and company. No skills were provided.")))),(0,n.kt)("p",null,"Basically this DAG takes care of parsing the JSON from a S3 Bucket and staging it on our Redshift cluster.\nAs this historical dataset may be updated we parse it yearly, starting from 2006 and ending in 2017."),(0,n.kt)("h3",{id:"landing-jobs-api-dag"},"Landing Jobs API DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"landing_jobs_api_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: JSON API"),(0,n.kt)("li",{parentName:"ul"},"Source: ",(0,n.kt)("a",{parentName:"li",href:"https://landing.jobs/api/v1"},"https://landing.jobs/api/v1"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Landing.jobs is one of the most used jobs websites for jobs across the Europe (the majority of the jobs posts is from Europe). At this time they have around 420 jobs available to fetch from their API."),(0,n.kt)("li",{parentName:"ul"},"The Landing Jobs API is very straight forward, as it has a well structured data model the only thing that is missing in the job endpoint was the company name inside the job vacancy payload."),(0,n.kt)("li",{parentName:"ul"},"So, for this source we have the job title, description, salary, relocation flag, skills (tags). Missing only the company name."),(0,n.kt)("li",{parentName:"ul"},"Almost forgot to mention that the results are paginated. So we will limit the number of pages we'll download.")))),(0,n.kt)("p",null,"Basically this DAG takes care of fetching the API results within a range of pages available.\nAs this is a very dynamic source, this DAG runs everyday at midnight to fetch new jobs from Landing.jobs."),(0,n.kt)("h3",{id:"github-jobs-api-dag"},"GitHub Jobs API DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"github_jobs_api_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: JSON API"),(0,n.kt)("li",{parentName:"ul"},"Source: ",(0,n.kt)("a",{parentName:"li",href:"https://jobs.github.com/api"},"https://jobs.github.com/api"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The Github Jobs API is very simple, it was just get the page you want to fetch and appends a ",(0,n.kt)("inlineCode",{parentName:"li"},".json")," suffix, then it will returns only data formatted in JSON."),(0,n.kt)("li",{parentName:"ul"},"Altought GitHub is the largest developer community, this source is the one with less jobs to fetch. Around 280 jobs found in their API."),(0,n.kt)("li",{parentName:"ul"},"It provides the job title, description and company name. But there is no normalized data regarding the skills required in the job.")))),(0,n.kt)("p",null,"This DAG takes care of requesting the landing.jobs API and fetch a limited range of jobs.\nIt is ran everyday at midnight so we have fresh jobs posting every day."),(0,n.kt)("h3",{id:"stackoverflow-rss-feed-dag"},"Stackoverflow RSS Feed DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"stackoverflow_jobs_rss_feed_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: RSS Feed (XML)"),(0,n.kt)("li",{parentName:"ul"},"Source: ",(0,n.kt)("a",{parentName:"li",href:"https://stackoverflow.com/jobs/feed"},"https://stackoverflow.com/jobs/feed"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"The Stackoverflow RSS feed is the one that has more jobs available to fetch (1000 jobs to be more precise)."),(0,n.kt)("li",{parentName:"ul"},"It is also the most complete: has the job title, description, the company information and also the skills required in the job.")))),(0,n.kt)("p",null,"This DAG takes of requesting the Stackoverflow Jobs RSS Feed. This is a single page request, but it returns 1000 jobs per time.\nSo I configured to run this DAG daily too."),(0,n.kt)("h3",{id:"angelco-jobs-dag"},"Angel.co jobs DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"angel_co_jobs_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: HTML Crawling (selenium)")),(0,n.kt)("p",null,"This DAG uses selenium to crawl the angel.co website and store all the HTML that contains job vacancies."),(0,n.kt)("h3",{id:"algolia-search-index-jobs-dag"},"Algolia Search Index Jobs DAG"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Id: ",(0,n.kt)("inlineCode",{parentName:"li"},"algoliasearch_index_jobs_dag")),(0,n.kt)("li",{parentName:"ul"},"Source Type: Warehouse (Database)")),(0,n.kt)("p",null,"This DAG takes care of fetching all inserted jobs within the ",(0,n.kt)("inlineCode",{parentName:"p"},"job_vacancies")," table and index it on the instant search engine\ncalled Algolia."))}u.isMDXComponent=!0}}]);