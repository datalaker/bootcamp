"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[80757],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>u});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=c(a),u=r,h=m["".concat(l,".").concat(u)]||m[u]||p[u]||i;return a?n.createElement(h,o(o({ref:t},d),{},{components:a})):n.createElement(h,o({ref:t},d))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},1073:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const i={},o="Caching",s={unversionedId:"foundations/programming-languages/pyspark/caching",id:"foundations/programming-languages/pyspark/caching",title:"Caching",description:"What is Caching?",source:"@site/docs/01-foundations/04-programming-languages/pyspark/caching.md",sourceDirName:"01-foundations/04-programming-languages/pyspark",slug:"/foundations/programming-languages/pyspark/caching",permalink:"/docs/foundations/programming-languages/pyspark/caching",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Broadcasting",permalink:"/docs/foundations/programming-languages/pyspark/broadcasting"},next:{title:"cheat-sheet",permalink:"/docs/foundations/programming-languages/pyspark/cheat-sheet"}},l={},c=[{value:"What is Caching?",id:"what-is-caching",level:2},{value:"Managing Memory and Disk Resources in PySpark with Cache and Persist",id:"managing-memory-and-disk-resources-in-pyspark-with-cache-and-persist",level:2},{value:"Advantages of using Cache and Persist in PySpark",id:"advantages-of-using-cache-and-persist-in-pyspark",level:3},{value:"Different Levels of Caching and Persistence in PySpark",id:"different-levels-of-caching-and-persistence-in-pyspark",level:3},{value:"What is uncache() and unpersist() in PySpark",id:"what-is-uncache-and-unpersist-in-pyspark",level:3}],d={toc:c};function p(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"caching"},"Caching"),(0,r.kt)("h2",{id:"what-is-caching"},"What is Caching?"),(0,r.kt)("p",null,"Caching is the process of storing the results of an operation in memory so that they can be reused later. This can significantly improve the performance of Spark jobs by reducing the amount of data that needs to be read and processed."),(0,r.kt)("p",null,"PySpark provides the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},".persist()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},".cache()"),"\xa0methods to cache DataFrames and RDDs in memory. The difference between the two methods is that\xa0",(0,r.kt)("inlineCode",{parentName:"p"},".persist()"),"\xa0allows for specifying the storage level, such as MEMORY_ONLY, MEMORY_AND_DISK, etc."),(0,r.kt)("p",null,"Here is an example of how to cache a DataFrame in memory:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# Cache a DataFrame in memory\ndf.persist(StorageLevel.MEMORY_ONLY)\n")),(0,r.kt)("p",null,"It is important to note that caching can consume a large amount of memory and should be used with care. It\u2019s also important to check the storage level Spark is using and remove the cache if you run out of memory."),(0,r.kt)("h2",{id:"managing-memory-and-disk-resources-in-pyspark-with-cache-and-persist"},"Managing Memory and Disk Resources in PySpark with Cache and Persist"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"An overview of PySpark's cache and persist methods and how to optimize performance and scalability in PySpark applications")),(0,r.kt)("p",null,"In PySpark,\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0are methods used to improve the performance of Spark jobs by storing intermediate results in memory or on disk. Here's a brief description of each:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"cache()"),":\xa0This method is used to cache the RDD (Resilient Distributed Dataset) in memory. When an RDD is cached, the data is stored in memory so that it can be quickly accessed the next time it is needed. This can greatly improve the performance of Spark jobs by reducing the amount of time spent reading data from disk.")),(0,r.kt)("p",null,"For example, consider the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"rdd = sc.parallelize(range(1000000))\nrdd.cache()\nresult = rdd.reduce(lambda x, y: x + y)\n")),(0,r.kt)("p",null,"(OR)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df = spark.range(1000000)\ndf.cache()\ndf_filtered = df.filter("id % 2 == 0")\ndf_sum = df_filtered.selectExpr("sum(id)").collect()\n')),(0,r.kt)("p",null,"In this code, the RDD is cached before the reduce operation. This means that the data will be stored in memory and can be quickly accessed during the reduce operation, which should improve performance."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Note that caching a DataFrame can be especially useful if you plan to reuse it multiple times in your PySpark application. However, it's important to use caching judiciously, as it can consume a lot of memory if not used correctly. In some cases, persisting a DataFrame with a more suitable storage level (e.g. disk storage) may be a better option.")),(0,r.kt)("p",null,"2",".","\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),":\xa0This method is similar to\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),", but it allows you to specify where the RDD should be stored (in memory, on disk, or both). By default,\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0caches the RDD in memory, but you can use the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"StorageLevel"),"\xa0parameter to specify a different storage level."),(0,r.kt)("p",null,"For example, consider the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"rdd = sc.parallelize(range(1000000))\nrdd.persist(storageLevel=StorageLevel.DISK_ONLY)\nresult = rdd.reduce(lambda x, y: x + y)\n")),(0,r.kt)("p",null,"(OR)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df = spark.range(1000000)\ndf.persist(storageLevel="DISK_ONLY")\ndf_filtered = df.filter("id % 2 == 0")\ndf_sum = df_filtered.selectExpr("sum(id)").collect()\n')),(0,r.kt)("p",null,"In this code, the RDD is persisted on disk instead of being cached in memory. This means that the data will be stored on disk and can be accessed from there during the reduce operation, which should improve performance compared to reading the data from disk."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Also note that, like caching, persisting a DataFrame can be useful if you plan to reuse it multiple times in your PySpark application. However, it's important to use persistence judiciously, as it can consume a lot of disk space if not used correctly.")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Note that Dataset\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0is an alias for\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist(StorageLevel.MEMORY_AND_DISK)"))),(0,r.kt)("h3",{id:"advantages-of-using-cache-and-persist-in-pyspark"},"Advantages of using Cache and Persist in PySpark"),(0,r.kt)("p",null,"There are several advantages to using\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0in PySpark:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Faster Execution:\xa0By caching or persisting an RDD or DataFrame, subsequent computations that use the same RDD or DataFrame can avoid the overhead of reading the data from a disk. This can result in much faster execution times, especially for iterative or interactive workloads."),(0,r.kt)("li",{parentName:"ol"},"Reduced Data Movement:\xa0When an RDD or DataFrame is cached or persisted, it stays on the nodes where it was computed, which can reduce data movement across the network. This can be particularly beneficial in distributed environments where network bandwidth is limited."),(0,r.kt)("li",{parentName:"ol"},"Improved Resource Utilization:\xa0Caching or persisting an RDD or DataFrame can reduce the need for recomputing the same data multiple times, which can improve the utilization of compute resources. This can be particularly useful when working with large datasets or complex computations."),(0,r.kt)("li",{parentName:"ol"},"Improved Debugging:\xa0Caching or persisting an RDD or DataFrame can help with debugging by allowing you to examine the data that is stored in memory or on a disk. This can be particularly useful when working with complex or iterative algorithms."),(0,r.kt)("li",{parentName:"ol"},"Custom Storage Levels:\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"persist()"),"\xa0allows you to specify custom storage levels for an RDD or DataFrame, which can be useful when working with different types of data or hardware configurations. For example, you might want to store some data in memory but persist other data on disk.")),(0,r.kt)("p",null,"Overall, using\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0can help improve the performance, scalability, and usability of Spark applications. However, it's important to use these methods judiciously, as caching or persisting too much data can lead to memory issues or inefficient use of resources."),(0,r.kt)("h3",{id:"different-levels-of-caching-and-persistence-in-pyspark"},"Different Levels of Caching and Persistence in PySpark"),(0,r.kt)("p",null,"PySpark provides different levels of caching and persistence for RDDs, which determines where the data is stored and how it is partitioned across the cluster. Here are the different storage levels that can be used with\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0methods:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"MEMORY_ONLY:\xa0This level stores the RDD in memory as deserialized Java objects. This is the default level used by\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"cache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"persist()"),". It provides fast access to the data, but if the RDD does not fit entirely in memory, it may need to be recomputed from the original data source."),(0,r.kt)("li",{parentName:"ol"},"MEMORY_ONLY_SER:\xa0This level stores the RDD or DataFrame in memory as serialized Java objects. This can reduce memory usage compared to MEMORY_ONLY, but accessing the data requires deserialization, which can be slower than using deserialized objects."),(0,r.kt)("li",{parentName:"ol"},"MEMORY_AND_DISK:\xa0This level stores the RDD or DataFrame in memory as deserialized Java objects, but if the RDD or DataFrame does not fit entirely in memory, it spills the excess data to disk. This provides better performance than recomputing the data, but accessing data from disk can be slower than accessing it from memory."),(0,r.kt)("li",{parentName:"ol"},"MEMORY_AND_DISK_SER:\xa0This level stores the RDD or DataFrame in memory as serialized Java objects, and spills excess data to disk if needed. This can be useful when memory usage is a concern, but accessing the data requires deserialization, which can be slower than using deserialized objects."),(0,r.kt)("li",{parentName:"ol"},"DISK_ONLY:\xa0This level stores the RDD or DataFrame on disk only, and not in memory. This can be useful when memory usage is a concern and the data does not fit entirely in memory, but accessing the data from disk can be slower than accessing it from memory.")),(0,r.kt)("p",null,"In addition to these basic storage levels, PySpark also provides options for controlling how the data is partitioned and cached, such as\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"MEMORY_ONLY_2"),", which replicates the data on two nodes for fault tolerance, or\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"MEMORY_ONLY_SER_10"),", which serializes the data and splits it into ten partitions."),(0,r.kt)("h3",{id:"what-is-uncache-and-unpersist-in-pyspark"},"What is uncache() and unpersist() in PySpark"),(0,r.kt)("p",null,"In PySpark,\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"uncache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"unpersist()"),"\xa0are methods used to remove RDDs from memory or disk, respectively, after they have been cached or persisted using\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0or\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0methods. Here's a brief description of each:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("inlineCode",{parentName:"li"},"uncache()"),":\xa0This method is used to remove an RDD from memory that was previously cached using the\xa0",(0,r.kt)("inlineCode",{parentName:"li"},"cache()"),"\xa0method. Once an RDD has been uncached, its data is no longer stored in memory, and it must be recomputed from its original source if it is needed again.")),(0,r.kt)("p",null,"For example, consider the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"rdd = sc.parallelize(range(1000000))\nrdd.cache()\nresult = rdd.reduce(lambda x, y: x + y)\nrdd.unpersist()\n")),(0,r.kt)("p",null,"(OR)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df = spark.range(1000000)\ndf.cache()\ndf_filtered = df.filter("id % 2 == 0")\ndf_sum = df_filtered.selectExpr("sum(id)").collect()\ndf.unpersist()\n')),(0,r.kt)("p",null,"In this code, the RDD is uncached after the reduce operation has been completed. This frees up the memory used by the RDD, which can be beneficial in cases where memory usage is a concern."),(0,r.kt)("p",null,"2.\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"unpersist()"),":\xa0This method is used to remove an RDD from the disk that was previously persisted using the\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0method. Once an RDD has been unpersisted, its data is no longer stored on disk, and it must be recomputed from its original source if it is needed again."),(0,r.kt)("p",null,"For example, consider the following code:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"rdd = sc.parallelize(range(1000000))\nrdd.persist(storageLevel=StorageLevel.DISK_ONLY)\nresult = rdd.reduce(lambda x, y: x + y)\nrdd.unpersist()\n")),(0,r.kt)("p",null,"(OR)"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'df = spark.range(1000000)\ndf.persist(storageLevel="DISK_ONLY")\ndf_filtered = df.filter("id % 2 == 0")\ndf_sum = df_filtered.selectExpr("sum(id)").collect()\ndf.unpersist()\n')),(0,r.kt)("p",null,"In this code, the RDD is unpersisted after the reduce operation has been completed. This frees up the disk space used by the RDD, which can be beneficial in cases where disk usage is a concern."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Note that it's important to use\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"uncache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"unpersist()"),"\xa0methods carefully to avoid unnecessary recomputations or data movements. It's generally a good practice to remove RDDs from memory or disk when they are no longer needed, especially if the RDDs are large or memory or disk resources are limited.")),(0,r.kt)("p",null,"In conclusion,\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"uncache()"),"\xa0and\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"unpersist()"),"\xa0methods in PySpark are used to remove RDDs from memory or disk, respectively after they have been cached or persisted using\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"cache()"),"\xa0or\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"persist()"),"\xa0methods. These methods are important to manage memory and disk resources efficiently and avoid unnecessary recomputations or data movements. When using these methods, it's important to carefully consider when an RDD is no longer needed and remove it from memory or disk accordingly. By doing so, we can optimize performance and scalability in PySpark applications."))}p.isMDXComponent=!0}}]);