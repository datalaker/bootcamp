"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[52767],{3905:(e,a,t)=>{t.d(a,{Zo:()=>d,kt:()=>h});var n=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function i(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),c=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},d=function(e){var a=c(e.components);return n.createElement(l.Provider,{value:a},e.children)},u={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},p=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),p=c(t),h=r,m=p["".concat(l,".").concat(h)]||p[h]||u[h]||o;return t?n.createElement(m,s(s({ref:a},d),{},{components:t})):n.createElement(m,s({ref:a},d))}));function h(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,s=new Array(o);s[0]=p;var i={};for(var l in a)hasOwnProperty.call(a,l)&&(i[l]=a[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,s[1]=i;for(var c=2;c<o;c++)s[c]=t[c];return n.createElement.apply(null,s)}return n.createElement.apply(null,t)}p.displayName="MDXCreateElement"},30168:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var n=t(87462),r=(t(67294),t(3905));const o={},s="Lab: Build an Iceberg Lakehouse",i={unversionedId:"storage/lab-glue-emr-iceberg-serverless-lakehouse/README",id:"storage/lab-glue-emr-iceberg-serverless-lakehouse/README",title:"Lab: Build an Iceberg Lakehouse",description:"Build a serverless transactional data lake with Apache Iceberg, Amazon EMR Serverless, and Amazon Athena",source:"@site/docs/02-storage/lab-glue-emr-iceberg-serverless-lakehouse/README.md",sourceDirName:"02-storage/lab-glue-emr-iceberg-serverless-lakehouse",slug:"/storage/lab-glue-emr-iceberg-serverless-lakehouse/",permalink:"/docs/storage/lab-glue-emr-iceberg-serverless-lakehouse/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681583434,formattedLastUpdatedAt:"Apr 15, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Lab: Read Delta Tables stored in Amazon S3 with Python",permalink:"/docs/storage/lab-read-s3-delta-in-python/"},next:{title:"Lab: The Easy Ways to Clean Up Production Messes",permalink:"/docs/storage/lab-production-cleaning-deltalake/"}},l={},c=[{value:"Build your transactional data lake on AWS",id:"build-your-transactional-data-lake-on-aws",level:2},{value:"Serverless architecture overview",id:"serverless-architecture-overview",level:2},{value:"Sales data model",id:"sales-data-model",level:2},{value:"Code",id:"code",level:2},{value:"Conclusion",id:"conclusion",level:2}],d={toc:c};function u(e){let{components:a,...t}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"lab-build-an-iceberg-lakehouse"},"Lab: Build an Iceberg Lakehouse"),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Build a serverless transactional data lake with Apache Iceberg, Amazon EMR Serverless, and Amazon Athena")),(0,r.kt)("p",null,"Since the deluge of big data over a decade ago, many organizations have learned to build applications to process and analyze petabytes of data. Data lakes have served as a central repository to store structured and unstructured data at any scale and in various formats. However, as data processing at scale solutions grow, organizations need to build more and more features on top of their data lakes. One important feature is to run different workloads such as business intelligence (BI), Machine Learning (ML), Data Science and data exploration, and Change Data Capture (CDC) of transactional data, without having to maintain multiple copies of data. Additionally, the task of maintaining and managing files in the data lake can be tedious and sometimes complex."),(0,r.kt)("p",null,"Table formats like Apache Iceberg provide solutions to these issues. They enable transactions on top of data lakes and can simplify data storage, management, ingestion, and processing. These transactional data lakes combine features from both the data lake and the data warehouse. You can simplify your data strategy by running multiple workloads and applications on the same data in the same location. However, using these formats requires building, maintaining, and scaling infrastructure and integration connectors that can be time-consuming, challenging, and costly."),(0,r.kt)("p",null,"In this lab, we show how you can build a serverless transactional data lake with Apache Iceberg on ",(0,r.kt)("a",{parentName:"p",href:"http://aws.amazon.com/s3"},"Amazon Simple Storage Service")," (Amazon S3) using ",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/emr/serverless/"},"Amazon EMR Serverless")," and ",(0,r.kt)("a",{parentName:"p",href:"http://aws.amazon.com/athena"},"Amazon Athena"),". We provide an example for data ingestion and querying using an ecommerce sales data lake."),(0,r.kt)("h2",{id:"build-your-transactional-data-lake-on-aws"},"Build your transactional data lake on AWS"),(0,r.kt)("p",null,"You can build your modern data architecture with a scalable data lake that integrates seamlessly with an ",(0,r.kt)("a",{parentName:"p",href:"http://aws.amazon.com/redshift"},"Amazon Redshift")," powered cloud warehouse. Moreover, many customers are looking for an architecture where they can combine the benefits of a data lake and a data warehouse in the same storage location. In the following figure, we show a comprehensive architecture that uses the modern data architecture strategy on AWS to build a fully featured transactional data lake. AWS provides flexibility and a wide breadth of features to ingest data, build AI and ML applications, and run analytics workloads without having to focus on the undifferentiated heavy lifting."),(0,r.kt)("p",null,"Data can be organized into three different zones, as shown in the following figure. The first zone is the raw zone, where data can be captured from the source as is. The transformed zone is an enterprise-wide zone to host cleaned and transformed data in order to serve multiple teams and use cases. Iceberg provides a table format on top of Amazon S3 in this zone to provide ACID transactions, but also to allow seamless file management and provide time travel and rollback capabilities. The business zone stores data specific to business cases and applications aggregated and computed from data in the transformed zone."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224493008-fbe32641-6c75-41cd-8977-37c28a373822.jpg",alt:"bdb-2850-image001"})),(0,r.kt)("p",null,"One important aspect to a successful data strategy for any organization is data governance. On AWS, you can implement a thorough governance strategy with fine-grained access control to the data lake with ",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/lake-formation/"},"AWS Lake Formation"),"."),(0,r.kt)("h2",{id:"serverless-architecture-overview"},"Serverless architecture overview"),(0,r.kt)("p",null,"In this section, we show you how to ingest and query data in your transactional data lake in a few steps. EMR Serverless is a serverless option that makes it easy for data analysts and engineers to run Spark-based analytics without configuring, managing, and scaling clusters or servers. You can run your Spark applications without having to plan capacity or provision infrastructure, while paying only for your usage. EMR Serverless supports Iceberg natively to create tables and query, merge, and insert data with Spark. In the following architecture diagram, Spark transformation jobs can load data from the raw zone or source, apply the cleaning and transformation logic, and ingest data in the transformed zone on Iceberg tables. Spark code can run instantaneously on an EMR Serverless application, which we demonstrate later in this lab."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224493010-5d573dee-e20f-4fbb-978e-5c309cb87e75.jpg",alt:"bdb-2850-image002"})),(0,r.kt)("p",null,"The Iceberg table is synced with the ",(0,r.kt)("a",{parentName:"p",href:"https://aws.amazon.com/glue"},"AWS Glue")," Data Catalog. The Data Catalog provides a central location to govern and keep track of the schema and metadata. With Iceberg, ingestion, update, and querying processes can benefit from atomicity, snapshot isolation, and managing concurrency to keep a consistent view of data."),(0,r.kt)("p",null,"Athena is a serverless, interactive analytics service built on open-source frameworks, supporting open-table and file formats. Athena provides a simplified, flexible way to analyze petabytes of data where it lives. To serve BI and reporting analysis, it allows you to build and run queries on Iceberg tables natively and integrates with a variety of BI tools."),(0,r.kt)("h2",{id:"sales-data-model"},"Sales data model"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://aws-samples.github.io/aws-dbs-refarch-edw/src/star-schema/"},"Star schema")," and its variants are very popular for modeling data in data warehouses. They implement one or more fact tables and dimension tables. The fact table stores the main transactional data from the business logic with foreign keys to dimensional tables. Dimension tables hold additional complementary data to enrich the fact table."),(0,r.kt)("p",null,"In this lab, we take the example of sales data from the ",(0,r.kt)("a",{parentName:"p",href:"https://www.tpc.org/tpc_documents_current_versions/pdf/tpc-ds_v2.13.0.pdf"},"TPC-DS benchmark"),". We zoom in on a subset of the schema with the ",(0,r.kt)("inlineCode",{parentName:"p"},"web_sales")," fact table, as shown in the following figure. It stores numeric values about sales cost, ship cost, tax, and net profit. Additionally, it has foreign keys to dimensional tables like ",(0,r.kt)("inlineCode",{parentName:"p"},"date_dim"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"time_dim"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"customer"),", and ",(0,r.kt)("inlineCode",{parentName:"p"},"item"),". These dimensional tables store records that give more details. For instance, you can show when a sale took place by which customer for which item."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224493011-b3da90b0-96d2-4e77-b66e-19e32c13c824.png",alt:"bdb-2850-image003"})),(0,r.kt)("p",null,"Dimension-based models have been used extensively to build data warehouses. In the following sections, we show how to implement such a model on top of Iceberg, providing data warehousing features on top of your data lake, and run different workloads in the same location. We provide a complete example of building a serverless architecture with data ingestion using EMR Serverless and Athena using TPC-DS queries."),(0,r.kt)("h2",{id:"code"},"Code"),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://nbviewer.org/github/sparsh-ai/recohut/blob/main/02-storage/lab-glue-emr-iceberg-serverless-lakehouse/main.ipynb"},(0,r.kt)("img",{parentName:"a",src:"https://img.shields.io/badge/jupyter-notebook-informational?logo=jupyter",alt:null}))),(0,r.kt)("h2",{id:"conclusion"},"Conclusion"),(0,r.kt)("p",null,"In this lab, we created a serverless transactional data lake with Iceberg tables, EMR Serverless, and Athena. We used TPC-DS sales data with 10 GB data and more than 7 million records in the fact table. We demonstrated how straightforward it is to rely on SQL and Spark to run serverless jobs for data ingestion and upserts. Moreover, we showed how to run complex BI queries directly on Iceberg tables from Athena for reporting."),(0,r.kt)("p",null,"You can start building your serverless transactional data lake on AWS today, and dive deep into the features and optimizations Iceberg provides to build analytics applications more easily. Iceberg can also help you in the future to improve performance and reduce costs."))}u.isMDXComponent=!0}}]);