"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[63812],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>h});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function s(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?s(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},s=Object.keys(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(n=0;n<s.length;n++)a=s[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,s=e.originalType,l=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=p(a),h=r,d=u["".concat(l,".").concat(h)]||u[h]||m[h]||s;return a?n.createElement(d,o(o({ref:t},c),{},{components:a})):n.createElement(d,o({ref:t},c))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var s=a.length,o=new Array(s);o[0]=u;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:r,o[1]=i;for(var p=2;p<s;p++)o[p]=a[p];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},15868:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>p});var n=a(87462),r=(a(67294),a(3905));const s={},o="Lab: Implementing the Serving Layer Star Schema",i={unversionedId:"processing/lab-azure-synapse-implementing-star-schema/README",id:"processing/lab-azure-synapse-implementing-star-schema/README",title:"Lab: Implementing the Serving Layer Star Schema",description:"In this lab, we will learn about implementing the serving layer, which involves implementing star schemas, techniques to read and write different data formats, sharing data between services such as SQL and Spark, and more. Once you complete this lab, you should be able to understand the differences between a Synapse dedicated SQL pool versus traditional SQL systems for implementing the Star schema, the various ways of accessing Parquet data using technologies such as Spark and SQL, and the details involved in storing metadata across services. All this knowledge should help you build a practical and maintainable serving layer in a data lake.",source:"@site/docs/03-processing/lab-azure-synapse-implementing-star-schema/README.md",sourceDirName:"03-processing/lab-azure-synapse-implementing-star-schema",slug:"/processing/lab-azure-synapse-implementing-star-schema/",permalink:"/docs/processing/lab-azure-synapse-implementing-star-schema/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681732641,formattedLastUpdatedAt:"Apr 17, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Lab: Processing Data Using Azure Synapse Analytics",permalink:"/docs/processing/lab-azure-synapse-data-processing/"},next:{title:"Lab: GCP Dataprep",permalink:"/docs/processing/lab-gcp-dataprep"}},l={},p=[{value:"Technical requirements",id:"technical-requirements",level:2},{value:"Delivering data in a relational star schema",id:"delivering-data-in-a-relational-star-schema",level:2},{value:"Implementing a dimensional hierarchy",id:"implementing-a-dimensional-hierarchy",level:2},{value:"Maintaining metadata",id:"maintaining-metadata",level:2},{value:"Metadata using Synapse SQL and Spark pools",id:"metadata-using-synapse-sql-and-spark-pools",level:3},{value:"Metadata using Azure Databricks",id:"metadata-using-azure-databricks",level:3}],c={toc:p};function m(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"lab-implementing-the-serving-layer-star-schema"},"Lab: Implementing the Serving Layer Star Schema"),(0,r.kt)("p",null,"In this lab, we will learn about implementing the serving layer, which involves implementing star schemas, techniques to read and write different data formats, sharing data between services such as SQL and Spark, and more. Once you complete this lab, you should be able to understand the differences between a Synapse dedicated SQL pool versus traditional SQL systems for implementing the Star schema, the various ways of accessing Parquet data using technologies such as Spark and SQL, and the details involved in storing metadata across services. All this knowledge should help you build a practical and maintainable serving layer in a data lake."),(0,r.kt)("p",null,"We will cover the following topics in this lab:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Delivering data in a relational star schema"),(0,r.kt)("li",{parentName:"ul"},"Implementing a dimensional hierarchy"),(0,r.kt)("li",{parentName:"ul"},"Delivering data in Parquet files"),(0,r.kt)("li",{parentName:"ul"},"Maintaining metadata")),(0,r.kt)("h2",{id:"technical-requirements"},"Technical requirements"),(0,r.kt)("p",null,"For this lab, you will need the following:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"An Azure account (free or paid)"),(0,r.kt)("li",{parentName:"ul"},"An active Synapse workspace")),(0,r.kt)("p",null,"Let's get started."),(0,r.kt)("h2",{id:"delivering-data-in-a-relational-star-schema"},"Delivering data in a relational star schema"),(0,r.kt)("p",null,"In this recipe, we\xa0will learn how to implement a star schema in Synapse SQL and deliver data from it."),(0,r.kt)("p",null,"Star schemas have two types of tables,\xa0",(0,r.kt)("strong",{parentName:"p"},"fact tables"),"\xa0and\xa0",(0,r.kt)("strong",{parentName:"p"},"dimensional tables"),". Fact tables are usually much higher in volume than the dimension tables and hence would benefit from\xa0using a\xa0",(0,r.kt)("strong",{parentName:"p"},"hash distribution"),"\xa0with\xa0",(0,r.kt)("strong",{parentName:"p"},"clustered columnstore indexing"),". On\xa0the other hand, dimension tables are smaller and can benefit from using\xa0",(0,r.kt)("strong",{parentName:"p"},"replicated tables"),"."),(0,r.kt)("p",null,"IMPORTANT NOTE"),(0,r.kt)("p",null,"Synapse dedicated SQL pools didn't support\xa0",(0,r.kt)("strong",{parentName:"p"},"foreign key"),"\xa0constraints at the time of development of this lab. Hence, the responsibility of maintaining data integrity falls on the applications."),(0,r.kt)("p",null,"Let's consider the\xa0",(0,r.kt)("strong",{parentName:"p"},"Imaginary Airport Cabs"),"\xa0(",(0,r.kt)("strong",{parentName:"p"},"IAC"),") cab rides example for our star schema. We have the following tables in this design:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"FactTrips")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"DimDriver")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"DimCustomer")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"DimCab")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"DimDate"))),(0,r.kt)("p",null,"Let's see how to implement these tables."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"In the Synapse screen, create a new SQL pool from the\xa0",(0,r.kt)("strong",{parentName:"li"},"Manage"),"\xa0tab, as shown in the following screenshot. Click on the\xa0",(0,r.kt)("strong",{parentName:"li"},"+New"),"\xa0symbol and fill in the details to create a new dedicated SQL pool.")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294867-6f115513-ea35-4c12-bf96-90644d5e3a7b.jpg",alt:"B17525_07_01"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Next, create\xa0a new SQL script from the\xa0",(0,r.kt)("strong",{parentName:"li"},"Editor"),"\xa0tab\xa0by clicking on the\xa0",(0,r.kt)("strong",{parentName:"li"},"+")," sign, as shown in the next screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294869-4bacd6c1-3987-49b9-90cc-5597c8214cb0.jpg",alt:"B17525_07_02"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"In the SQL editor, you can enter SQL commands to create tables representing the star schema. You can use the\xa0",(0,r.kt)("strong",{parentName:"li"},"COPY INTO"),"\xa0statement to populate the tables. The fact and dimension tables will have the same syntax for loading information. And, finally, query\xa0from the star schema\xa0tables. Here is a sample query to get the list of all customers whose end location was\xa0",(0,r.kt)("strong",{parentName:"li"},"'San Jose'"),":")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"-- Example for creating a Star schema \n\n-- Create the Fact Table. In our case it would be the TripTable\nDROP TABLE dbo.FactTrips;\n\nCREATE TABLE dbo.FactTrips\n(\n    [tripId] INT NOT NULL,\n    [driverId] INT NOT NULL,\n    [customerId] INT NOT NULL,\n    [tripdate] INT,\n    [startLocation] VARCHAR (40),\n    [endLocation] VARCHAR (40)\n )\n WITH\n (\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = HASH ([tripId])\n )\nGO\n\n-- Insert some sample values. In reality the Fact tables will have Millions of rows.\n\n INSERT INTO dbo.FactTrips VALUES (101, 201, 301, 20220101, 'New York', 'New Jersey');\n INSERT INTO dbo.FactTrips VALUES (102, 202, 302, 20220101, 'Miami', 'Dallas');\n INSERT INTO dbo.FactTrips VALUES (103, 203, 303, 20220102, 'Phoenix', 'Tempe');\n INSERT INTO dbo.FactTrips VALUES (104, 204, 304, 20220204, 'LA', 'San Jose');\n INSERT INTO dbo.FactTrips VALUES (105, 205, 305, 20220205, 'Seattle', 'Redmond');\n INSERT INTO dbo.FactTrips VALUES (106, 206, 306, 20220301, 'Atlanta', 'Chicago');\n\n\n-- Create the Customer Dimension table\nDROP TABLE dbo.DimCustomer;\n\nCREATE TABLE dbo.DimCustomer\n(\n    [customerId] INT NOT NULL,\n    [name] VARCHAR(40) NOT NULL,\n    [emailId] VARCHAR(40),\n    [phoneNum] VARCHAR(40),\n    [city] VARCHAR(40)\n)\nWITH\n(\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = REPLICATE\n)\nGO\n\n-- Another way of inserting data using COPY INTO\n-- You will have to use the https format here instead of the abfss format\n-- Copy the customer.csv file in this directory to the ADLS Gen2 location and use that path here.\n\nCOPY INTO dbo.DimCustomer\nFROM 'https://sparshstorage1.blob.core.windows.net/dataloading/customer.csv'\nWITH (\n    FILE_TYPE='CSV',\n    FIELDTERMINATOR=',',\n    FIELDQUOTE='',\n    ROWTERMINATOR='\\n',\n    ENCODING = 'UTF8',\n    FIRSTROW = 2\n);\n\nSELECT * from dbo.DimCustomer;\n\n-- Create a Driver Dimension table\nCREATE TABLE dbo.DimDriver\n(\n    [driverId] INT NOT NULL,\n    [firstName] VARCHAR(40),\n    [middleName] VARCHAR(40),\n    [lastName] VARCHAR(40),\n    [city] VARCHAR(40),\n    [gender] VARCHAR(40),\n    [salary] INT\n)\nWITH\n(\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = REPLICATE\n)\nGO\n\n-- Insert some sample values\n\nINSERT INTO dbo.DimDriver VALUES (210, 'Alicia','','Yang','New York', 'Female', 2000);\nINSERT INTO dbo.DimDriver VALUES (211, 'Brandon','','Rhodes','New York','Male', 3000);\nINSERT INTO dbo.DimDriver VALUES (212, 'Cathy','','Mayor','California','Female', 3000);\nINSERT INTO dbo.DimDriver VALUES (213, 'Dennis','','Brown','Florida','Male', 2500);\nINSERT INTO dbo.DimDriver VALUES (214, 'Jeremey','','Stilton','Arizona','Male', 2500);\nINSERT INTO dbo.DimDriver VALUES (215, 'Maile','','Green','Florida','Female', 4000);\n\nSELECT * from dbo.DimDriver;\n\nDROP TABLE dbo.DimDate\n-- Create the date dimension table\nCREATE TABLE dbo.DimDate\n(\n    [dateId] INT NOT NULL,\n    [date] DATETIME NOT NULL,\n    [dayOfWeek] VARCHAR(40),\n    [fiscalQuarter] VARCHAR(40)\n)\nWITH\n(\n    CLUSTERED COLUMNSTORE INDEX,\n    DISTRIBUTION = REPLICATE\n)\nGO\n\nINSERT INTO dbo.DimDate VALUES (20210101, '20210101','Saturday','Q3');\nINSERT INTO dbo.DimDate VALUES (20210102, '20210102','Sunday','Q3');\nINSERT INTO dbo.DimDate VALUES (20210103, '20210103','Monday','Q3');\nINSERT INTO dbo.DimDate VALUES (20210104, '20210104','Tuesday','Q3');\nINSERT INTO dbo.DimDate VALUES (20210105, '20210105','Wednesday','Q3');\n")),(0,r.kt)("p",null,"Now run some sample queries"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT trip.[tripId], customer.[name] from \ndbo.FactTrips AS trip\nJOIN dbo.DimCustomer AS customer\nON trip.[customerId] = customer.[customerId] \nWHERE trip.[endLocation] = 'San Jose';\n")),(0,r.kt)("p",null,"As you can see, once we understand the concept of star schemas, creating them is as simple as creating tables in Synapse SQL."),(0,r.kt)("p",null,"You can learn more about\xa0Synapse SQL and schemas here:\xa0",(0,r.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview"},"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview"),"."),(0,r.kt)("h2",{id:"implementing-a-dimensional-hierarchy"},"Implementing a dimensional hierarchy"),(0,r.kt)("p",null,"Let's look at the techniques available for reading and writing data in Parquet files."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218298780-34ee67c4-ec50-42d8-a000-edbecc62c4e1.png",alt:null})),(0,r.kt)("p",null,"The code is in the ",(0,r.kt)("inlineCode",{parentName:"p"},"./assets")," folder."),(0,r.kt)("h2",{id:"maintaining-metadata"},"Maintaining metadata"),(0,r.kt)("p",null,"Metastores are like data catalogs that contain information about all the tables you have, the table schemas, the relationships among them, where they are stored, and so on."),(0,r.kt)("h3",{id:"metadata-using-synapse-sql-and-spark-pools"},"Metadata using Synapse SQL and Spark pools"),(0,r.kt)("p",null,"Synapse\xa0supports\xa0a shared\xa0metadata\xa0model. The databases and tables that use Parquet or CSV storage formats are automatically shared between the compute pools, such as SQL and Spark."),(0,r.kt)("p",null,"IMPORTANT NOTE"),(0,r.kt)("p",null,"Data created from Spark can only be read and queried by SQL pools but cannot be modified as of now."),(0,r.kt)("p",null,"Let's look at an example of creating a database and a table using Spark and accessing it via SQL:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"In the Synapse Spark notebook, create a sample table, as shown in the following screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294882-b336953a-0691-4d66-99ff-73484bcaacd6.jpg",alt:"B17525_07_08"})),(0,r.kt)("p",null,"Now, let's query the contents of the table from the SQL serverless pool."),(0,r.kt)("p",null,"IMPORTANT NOTE"),(0,r.kt)("p",null,"This database will be synced asynchronously, so there might be a slight delay before you see the databases and tables in the SQL pool."),(0,r.kt)("p",null,"SQL serverless pool is an on-demand service, so all you need to do is just click on the\xa0",(0,r.kt)("strong",{parentName:"p"},"+"),"\xa0sign in the Synapse workspace page and select\xa0",(0,r.kt)("strong",{parentName:"p"},"SQL script"),"\xa0to create a new\xa0SQL\xa0editor, as\xa0shown\xa0in the following screenshot:"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294885-22c12346-1733-4fa9-91ad-d1da6e7d30c8.jpg",alt:"B17525_07_09"})),(0,r.kt)("p",null,"Then, in the\xa0",(0,r.kt)("strong",{parentName:"p"},"Connect to"),"\xa0field, select the\xa0",(0,r.kt)("strong",{parentName:"p"},"Built-in"),"\xa0pool, as shown in the following screenshot:"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294886-fe417706-deea-4e58-868e-291285c6f8fd.jpg",alt:"B17525_07_10"})),(0,r.kt)("p",null,"Now, just run a simple script to query the shared table; in the example, the shared table would be the\xa0",(0,r.kt)("strong",{parentName:"p"},"tripID"),"\xa0table:"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294888-51c67fb8-5470-4896-b5ee-11bcfa310178.jpg",alt:"B17525_07_11"})),(0,r.kt)("p",null,"As you just noticed, the shared data model of Synapse makes it very easy to share data between SQL\xa0and Spark pools. Everything is already taken care of by\xa0Synapse and the data\xa0is\xa0readily made available to us."),(0,r.kt)("p",null,"You can learn\xa0more about maintaining metadata in Synapse here:\xa0",(0,r.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/synapse-analytics/metadata/overview"},"https://docs.microsoft.com/en-us/azure/synapse-analytics/metadata/overview"),"."),(0,r.kt)("p",null,"Let's next explore how to work with metastores in Azure Databricks."),(0,r.kt)("h3",{id:"metadata-using-azure-databricks"},"Metadata using Azure Databricks"),(0,r.kt)("p",null,"In order to\xa0share data between Spark and other services outside of\xa0Synapse, we have to make use of the Hive metastore. Spark uses the Hive metastore to share information with other services. Let's look at an example of sharing data between Azure Databricks Spark and Azure HDInsight Hive. The logic and steps for using an external Hive metastore would be similar for Synapse Spark too. Here are the steps:"),(0,r.kt)("p",null,"We will need a standalone database that can be used by Spark to store the metadata. Let's use Azure SQL for this purpose. Create an Azure SQL database from the Azure portal. Search for\xa0",(0,r.kt)("strong",{parentName:"p"},"Azure SQL"),"\xa0from the portal search box and select it. Click on the\xa0",(0,r.kt)("strong",{parentName:"p"},"+ Create"),"\xa0option. You will see a screen, as shown in the following\xa0screenshot:"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294890-029b290a-3962-4a18-a6df-647eb83a6185.jpg",alt:"B17525_07_12"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Select the\xa0",(0,r.kt)("strong",{parentName:"li"},"SQL databases"),"\xa0box and click the\xa0",(0,r.kt)("strong",{parentName:"li"},"Single database"),"\xa0option from the dropdown."),(0,r.kt)("li",{parentName:"ul"},"You can create the database with pre-populated sample data so that we have ready-made data for experimentation. Select the\xa0",(0,r.kt)("strong",{parentName:"li"},"Sample"),"\xa0option for the\xa0",(0,r.kt)("strong",{parentName:"li"},"Use existing data"),"\xa0field, as shown in the following screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294893-69e754ee-2282-4bb9-a6cf-4f9b57094e8f.jpg",alt:"B17525_07_13"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Fill up\xa0the\xa0rest of the tabs and click on\xa0",(0,r.kt)("strong",{parentName:"li"},"Review + create"),"\xa0to create the Azure SQL database."),(0,r.kt)("li",{parentName:"ul"},"Retrieve the JDBC connection string from the\xa0",(0,r.kt)("strong",{parentName:"li"},"Connection Strings"),"\xa0tab, as shown in the following screenshot, and save it in Notepad. We will need this information later:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294895-759c3a11-d86b-41b9-a008-a14da27a14a7.jpg",alt:"B17525_07_14"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Next, we\xa0have\xa0to create an\xa0",(0,r.kt)("strong",{parentName:"li"},"HDInsight"),"\xa0Hive cluster. By now, you might already know the process to instantiate any Azure service. Just search for\xa0",(0,r.kt)("strong",{parentName:"li"},"HDInsight"),"\xa0in the portal search bar and click on it. On the HDInsight portal home page, click on the\xa0",(0,r.kt)("strong",{parentName:"li"},"+ Create"),"\xa0link to open the create form and fill in the details."),(0,r.kt)("li",{parentName:"ul"},"On the\xa0",(0,r.kt)("strong",{parentName:"li"},"Create HDInsight cluster"),"\xa0screen, you can choose either the\xa0",(0,r.kt)("strong",{parentName:"li"},"Hadoop"),"\xa0option or the\xa0",(0,r.kt)("strong",{parentName:"li"},"Interactive Query"),"\xa0option for\xa0",(0,r.kt)("strong",{parentName:"li"},"cluster type"),", as both will install Hive. Refer to the next screenshot for the options available:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294896-d6591494-c745-4055-bc1b-56e9fd27d06f.jpg",alt:"B17525_07_15"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Once\xa0you\xa0have selected the type of cluster, fill up the rest of the fields on the\xa0",(0,r.kt)("strong",{parentName:"li"},"Basics"),"\xa0screen."),(0,r.kt)("li",{parentName:"ul"},"In the\xa0",(0,r.kt)("strong",{parentName:"li"},"Storage"),"\xa0tab, under the\xa0",(0,r.kt)("strong",{parentName:"li"},"External metadata stores"),"\xa0section, provide the Azure SQL database that we created in the earlier steps as\xa0",(0,r.kt)("strong",{parentName:"li"},"SQL database for Hive"),". The following screenshot shows the location:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294897-e4c67860-47d2-4b03-98f1-b5bec500cb90.jpg",alt:"B17525_07_16"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Complete\xa0the\xa0rest of the fields and then click on\xa0",(0,r.kt)("strong",{parentName:"li"},"Review + Create"),"\xa0to create the HDInsight cluster."),(0,r.kt)("li",{parentName:"ul"},"Once the cluster is created, go to the Ambari home page from the HDInsight portal by clicking on the\xa0",(0,r.kt)("strong",{parentName:"li"},"Ambari home"),"\xa0link, as shown in the following screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294899-ceebc268-a35e-45bf-ac84-43e572caf4c4.jpg",alt:"B17525_07_17"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"From the\xa0Ambari dashboard, click on\xa0",(0,r.kt)("strong",{parentName:"li"},"Hive view 2.0"),", as\xa0shown in the following screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294900-121dabcb-d1e5-4954-b237-6854e578dba8.jpg",alt:"B17525_07_18"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Now, you should be able to see the\xa0",(0,r.kt)("strong",{parentName:"li"},"Hivesampletable"),"\xa0database in the Hive dashboard, as shown in the following screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294902-ba49163d-1400-439f-869a-28724c7823bf.jpg",alt:"B17525_07_19"})),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Now\xa0that\xa0we have the HDInsight cluster, we have to next create an Azure Databricks cluster. We have to create a new cluster with the following configurations. Let's see how to enter these configurations in the Spark create screen in the next step:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"spark.sql.hive.metastore.version 2.1.1 // For HDInsight Interactive 2.1 version\n\nspark.hadoop.javax.jdo.option.ConnectionUserName\xa0**`<Your Azure SQL Database Username>`**\n\nspark.hadoop.javax.jdo.option.ConnectionURL\xa0**`<Your Azure SQL Database JDBC connection string>`**\n\nspark.hadoop.javax.jdo.option.ConnectionPassword\xa0**`<Your Azure SQL Database Password>`**\n\nspark.hadoop.javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver\n\nspark.sql.hive.metastore.jars\xa0**`<Location where you have copied the Hive Metastore Jars>`**\n\ndatanucleus.autoCreateSchema true\n\ndatanucleus.fixedDatastore false\n")),(0,r.kt)("p",null,"Note\xa0that\xa0you will have to use the JDBC link that you had saved earlier, for the config that says\xa0",(0,r.kt)("strong",{parentName:"p"},"spark.hadoop.javax.jdo.option.ConnectionURL"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"You will have to enter the configs in the\xa0",(0,r.kt)("strong",{parentName:"li"},"Spark Config"),"\xa0field on the\xa0",(0,r.kt)("strong",{parentName:"li"},"Create Cluster"),"\xa0page, as shown in the following screenshot:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294905-15ab62ea-514e-4b40-9f89-657078c2059d.jpg",alt:"B17525_07_20"})),(0,r.kt)("p",null,"IMPORTANT NOTE"),(0,r.kt)("p",null,"Apart\xa0from\xa0the config fields, you will also have to download the Hive metastore JAR files and provide them a location where the Azure Databricks clusters can access them. Azure provides step-by-step instructions along with readymade scripts to easily download the JAR files here:\xa0",(0,r.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/setup-metastore-jars.html"},"https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/setup-metastore-jars.html"),"."),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Once you have created the Spark cluster, you will be able to access the Hive Metastore tables directly from a Spark notebook. In the following screenshot, you can see how the Databricks Spark cluster is able to access the\xa0",(0,r.kt)("strong",{parentName:"li"},"HiveSampletable"),"\xa0table\xa0that we saw earlier\xa0using the Hive query view:")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218294907-c79685ae-f97c-4663-b37a-e673f92accba.jpg",alt:"B17525_07_21"})),(0,r.kt)("p",null,"Hurray! Now you know how to access metadata between Spark and Hive clusters using an external Hive metastore."),(0,r.kt)("p",null,"You can learn\xa0more about Azure Databricks metastores here:\xa0",(0,r.kt)("a",{parentName:"p",href:"https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore"},"https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore"),". With that, we have come to the end of this section and the lab. You should now be familiar with the different ways in which metadata can be shared across the SQL, Spark, and Hive services in Azure."))}m.isMDXComponent=!0}}]);