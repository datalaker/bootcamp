"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[79180],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>m});var r=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,o=function(e,t){if(null==e)return{};var a,r,o={},n=Object.keys(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)a=n[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=r.createContext({}),u=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},d=function(e){var t=u(e.components);return r.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var a=e.components,o=e.mdxType,n=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),p=u(a),m=o,k=p["".concat(s,".").concat(m)]||p[m]||c[m]||n;return a?r.createElement(k,i(i({ref:t},d),{},{components:a})):r.createElement(k,i({ref:t},d))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var n=a.length,i=new Array(n);i[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,i[1]=l;for(var u=2;u<n;u++)i[u]=a[u];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}p.displayName="MDXCreateElement"},17490:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>n,metadata:()=>l,toc:()=>u});var r=a(87462),o=(a(67294),a(3905));const n={},i="reddit-data-engineering of r/FedEx",l={unversionedId:"capstones/other/reddit-kafka-sparkstream-bigquery/README",id:"capstones/other/reddit-kafka-sparkstream-bigquery/README",title:"reddit-data-engineering of r/FedEx",description:"- Create Kafka cluster using docker-compose hosted on GCP VM",source:"@site/docs/12-capstones/other/reddit-kafka-sparkstream-bigquery/README.md",sourceDirName:"12-capstones/other/reddit-kafka-sparkstream-bigquery",slug:"/capstones/other/reddit-kafka-sparkstream-bigquery/",permalink:"/docs/capstones/other/reddit-kafka-sparkstream-bigquery/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{}},s={},u=[{value:"Purpose + Goal:",id:"purpose--goal",level:3},{value:"Pipeline Diagram",id:"pipeline-diagram",level:3},{value:"Confluent Kafka UI:",id:"confluent-kafka-ui",level:4},{value:"Sample Data:",id:"sample-data",level:4},{value:"Dashboard UI:",id:"dashboard-ui",level:4},{value:"Dashboard Link: https://datastudio.google.com/reporting/afbed74b-8f18-4012-bc91-ddbc66e4574c",id:"dashboard-link-httpsdatastudiogooglecomreportingafbed74b-8f18-4012-bc91-ddbc66e4574c",level:4},{value:"Tools/Framework Used:",id:"toolsframework-used",level:3},{value:"Procedure/General Setup",id:"proceduregeneral-setup",level:3},{value:"Wordcloud",id:"wordcloud",level:2}],d={toc:u};function c(e){let{components:t,...a}=e;return(0,o.kt)("wrapper",(0,r.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"reddit-data-engineering-of-rfedex"},"reddit-data-engineering of r/FedEx"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Create Kafka cluster using docker-compose hosted on GCP VM"),(0,o.kt)("li",{parentName:"ul"},"Used PRAW API to stream both comments and posts data from r/FedEx into Kafka"),(0,o.kt)("li",{parentName:"ul"},"Consumes streaming data from Kafka using Spark Streaming"),(0,o.kt)("li",{parentName:"ul"},"Saves data into Google Cloud Storage Bucket (Data Lake) -> Batch load data into BigQuery (Data Warehouse) using Cloud Functions"),(0,o.kt)("li",{parentName:"ul"},"Visualization of Data in Google Data Studio")),(0,o.kt)("h3",{id:"purpose--goal"},"Purpose + Goal:"),(0,o.kt)("p",null,"Learn technologies like (Kafka, Spark Streaming, Cloud Functions, Data Studio), NOT producing the best/optimal architecture"),(0,o.kt)("h3",{id:"pipeline-diagram"},"Pipeline Diagram"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531394-c37e0fcf-11fc-42e0-987b-0f3428f2cffb.png",alt:"pipeline_diagram"})),(0,o.kt)("p",null,"Currently, Spark Stream writes a new file every 4 hours -> which triggers the Cloud Function to load to BigQuery "),(0,o.kt)("h4",{id:"confluent-kafka-ui"},"Confluent Kafka UI:"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531392-5a93ce58-6483-4ab8-af9d-c534a5a17a7d.png",alt:"kafka_topics"})),(0,o.kt)("h4",{id:"sample-data"},"Sample Data:"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531397-650fc6ec-c25f-4626-a630-126409258680.png",alt:"submissions_data"})),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531380-8288bfec-7679-4d6c-9de6-d9c0c4432df7.png",alt:"comments_data"})),(0,o.kt)("h4",{id:"dashboard-ui"},"Dashboard UI:"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531387-5e334e2d-f4ad-4bce-a8ab-30be1367ce37.png",alt:"dashboard_1"})),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531391-d49da467-f6b2-491e-96d5-abec387c1418.png",alt:"dashboard_2"})),(0,o.kt)("h4",{id:"dashboard-link-httpsdatastudiogooglecomreportingafbed74b-8f18-4012-bc91-ddbc66e4574c"},"Dashboard Link: ",(0,o.kt)("a",{parentName:"h4",href:"https://datastudio.google.com/reporting/afbed74b-8f18-4012-bc91-ddbc66e4574c"},"https://datastudio.google.com/reporting/afbed74b-8f18-4012-bc91-ddbc66e4574c")),(0,o.kt)("h3",{id:"toolsframework-used"},"Tools/Framework Used:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Terraform: To create and standardize GCP resources (ex: storage bucket, VM, dataproc cluster, etc.)"),(0,o.kt)("li",{parentName:"ul"},"Docker: Used docker compose to create Kafka cluster hosted on GCP VM"),(0,o.kt)("li",{parentName:"ul"},"Python + PRAW: Extract data from reddit"),(0,o.kt)("li",{parentName:"ul"},"Kafka: Message broker to handle stream data"),(0,o.kt)("li",{parentName:"ul"},"Spark Streaming: Consumer of Streaming data from Kafka"),(0,o.kt)("li",{parentName:"ul"},"Google Cloud Storage Bucket: Data Lake to store files produced by Spark Streaming"),(0,o.kt)("li",{parentName:"ul"},"Google Cloud Function: To batch load data from GCS to BigQuery because streaming insets are expensive!"),(0,o.kt)("li",{parentName:"ul"},"Google BigQuery: Data Warehouse to store data so that it can be queried"),(0,o.kt)("li",{parentName:"ul"},"Google Data Studio: Visualization Tool to create dashboard")),(0,o.kt)("h3",{id:"proceduregeneral-setup"},"Procedure/General Setup"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Create resources with Terraform"),(0,o.kt)("li",{parentName:"ol"},"Kafka",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Reserve Static External IP address for Kafka VM"),(0,o.kt)("li",{parentName:"ul"},"Install Dependencies in Kafka VM (Docker, PRAW python library)"),(0,o.kt)("li",{parentName:"ul"},'Set "KAFKA_ADDRESS" env variable to exeternal IP of Kafka VM'),(0,o.kt)("li",{parentName:"ul"},'Utilize "screen" command to run docker compose command to start Kafka cluster'),(0,o.kt)("li",{parentName:"ul"},'Utilize "screen" command to run both python producer programs'))),(0,o.kt)("li",{parentName:"ol"},"Spark Streaming",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Use SparkStreaming.py file to submit PySpark job to cluster"))),(0,o.kt)("li",{parentName:"ol"},"Cloud Functions",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},'Use "Create/Finializing" Trigger event in GCS bucket to execute Cloud Function to upload new Parquet file to BigQuery'))),(0,o.kt)("li",{parentName:"ol"},"Data Studio",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Connect to BigQuery")))),(0,o.kt)("h2",{id:"wordcloud"},"Wordcloud"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531408-98822cba-f206-493c-9741-58adfa070842.png",alt:"fedex-reddit-wordcloud"})),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/224531413-9c26a318-f8cd-4b79-bfe6-d66f0c979a5d.png",alt:"submission-wordcloud"})))}c.isMDXComponent=!0}}]);