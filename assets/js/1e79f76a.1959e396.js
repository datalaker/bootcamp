"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[86560],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>h});var n=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var s=n.createContext({}),u=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},p=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,p=r(e,["components","mdxType","originalType","parentName"]),c=u(a),h=o,k=c["".concat(s,".").concat(h)]||c[h]||d[h]||i;return a?n.createElement(k,l(l({ref:t},p),{},{components:a})):n.createElement(k,l({ref:t},p))}));function h(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,l=new Array(i);l[0]=c;var r={};for(var s in t)hasOwnProperty.call(t,s)&&(r[s]=t[s]);r.originalType=e,r.mdxType="string"==typeof e?e:o,l[1]=r;for(var u=2;u<i;u++)l[u]=a[u];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},59748:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>u});var n=a(87462),o=(a(67294),a(3905));const i={},l="Azure Data Engineering",r={unversionedId:"interviewprep/azure-data-engineering",id:"interviewprep/azure-data-engineering",title:"Azure Data Engineering",description:"Case study - data lake",source:"@site/docs/interviewprep/azure-data-engineering.md",sourceDirName:"interviewprep",slug:"/interviewprep/azure-data-engineering",permalink:"/docs/interviewprep/azure-data-engineering",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1682175507,formattedLastUpdatedAt:"Apr 22, 2023",frontMatter:{}},s={},u=[{value:"Case study - data lake",id:"case-study---data-lake",level:2},{value:"Background",id:"background",level:3},{value:"Technical details",id:"technical-details",level:3},{value:"Question 1",id:"question-1",level:4},{value:"Solution",id:"solution",level:4},{value:"Explanation",id:"explanation",level:4},{value:"Question 2",id:"question-2",level:4},{value:"Solution",id:"solution-1",level:4},{value:"Explanation",id:"explanation-1",level:4},{value:"Question 3",id:"question-3",level:4},{value:"Solution",id:"solution-2",level:4},{value:"Explanation",id:"explanation-2",level:4},{value:"Question 4",id:"question-4",level:4},{value:"Solution",id:"solution-3",level:4},{value:"Explanation",id:"explanation-3",level:4},{value:"Case study - data lake",id:"case-study---data-lake-1",level:2},{value:"Background",id:"background-1",level:3},{value:"Technical details",id:"technical-details-1",level:3},{value:"Question 1",id:"question-1-1",level:4},{value:"Solution",id:"solution-4",level:4},{value:"Explanation",id:"explanation-4",level:4},{value:"Question 2",id:"question-2-1",level:4},{value:"Solution",id:"solution-5",level:4},{value:"Explanation",id:"explanation-5",level:4},{value:"Question 3",id:"question-3-1",level:4},{value:"Solution",id:"solution-6",level:4},{value:"Explanation",id:"explanation-6",level:4},{value:"Data visualization",id:"data-visualization",level:2},{value:"Solution",id:"solution-7",level:3},{value:"Explanation",id:"explanation-7",level:3},{value:"Data partitioning",id:"data-partitioning",level:2},{value:"Solution",id:"solution-8",level:3},{value:"Explanation",id:"explanation-8",level:3},{value:"Synapse SQL pool table design - 1",id:"synapse-sql-pool-table-design---1",level:2},{value:"Solution",id:"solution-9",level:3},{value:"Explanation",id:"explanation-9",level:3},{value:"Synapse SQL pool table design - 2",id:"synapse-sql-pool-table-design---2",level:2},{value:"Solution",id:"solution-10",level:3},{value:"Explanation",id:"explanation-10",level:3},{value:"Slowly changing dimensions",id:"slowly-changing-dimensions",level:2},{value:"Solution",id:"solution-11",level:3},{value:"Explanation",id:"explanation-11",level:3},{value:"Storage tiers",id:"storage-tiers",level:2},{value:"Solution",id:"solution-12",level:3},{value:"Explanation",id:"explanation-12",level:3},{value:"Disaster recovery",id:"disaster-recovery",level:2},{value:"Solution",id:"solution-13",level:3},{value:"Explanation",id:"explanation-13",level:3},{value:"Synapse SQL external tables",id:"synapse-sql-external-tables",level:2},{value:"Solution",id:"solution-14",level:3},{value:"Explanation",id:"explanation-14",level:3},{value:"Data lake design",id:"data-lake-design",level:2},{value:"Solution",id:"solution-15",level:3},{value:"Explanation",id:"explanation-15",level:3},{value:"ASA windows",id:"asa-windows",level:2},{value:"Solution",id:"solution-16",level:3},{value:"Explanation",id:"explanation-16",level:3},{value:"Spark transformation",id:"spark-transformation",level:2},{value:"Solution",id:"solution-17",level:3},{value:"Explanation",id:"explanation-17",level:3},{value:"ADF - integration runtimes",id:"adf---integration-runtimes",level:2},{value:"Solution",id:"solution-18",level:3},{value:"Explanation",id:"explanation-18",level:3},{value:"ADF triggers",id:"adf-triggers",level:2},{value:"Solution",id:"solution-19",level:3},{value:"Explanation",id:"explanation-19",level:3},{value:"TDE/Always Encrypted",id:"tdealways-encrypted",level:2},{value:"Solution",id:"solution-20",level:3},{value:"Explanation",id:"explanation-20",level:3},{value:"Auditing Azure SQL/Synapse SQL",id:"auditing-azure-sqlsynapse-sql",level:2},{value:"Solution",id:"solution-21",level:3},{value:"Explanation",id:"explanation-21",level:3},{value:"Dynamic data masking",id:"dynamic-data-masking",level:2},{value:"Solution",id:"solution-22",level:3},{value:"Explanation",id:"explanation-22",level:3},{value:"RBAC - POSIX",id:"rbac---posix",level:2},{value:"Solution",id:"solution-23",level:3},{value:"Explanation",id:"explanation-23",level:3},{value:"Row-level security",id:"row-level-security",level:2},{value:"Solution",id:"solution-24",level:3},{value:"Explanation",id:"explanation-24",level:3},{value:"Blob storage monitoring",id:"blob-storage-monitoring",level:2},{value:"Solution",id:"solution-25",level:3},{value:"Explanation",id:"explanation-25",level:3},{value:"T-SQL optimization",id:"t-sql-optimization",level:2},{value:"Solution",id:"solution-26",level:3},{value:"Explanation",id:"explanation-26",level:3},{value:"ADF monitoring",id:"adf-monitoring",level:2},{value:"Solution",id:"solution-27",level:3},{value:"Explanation",id:"explanation-27",level:3},{value:"Setting up alerts in ASA",id:"setting-up-alerts-in-asa",level:2},{value:"Solution",id:"solution-28",level:3},{value:"Explanation",id:"explanation-28",level:3}],p={toc:u};function d(e){let{components:t,...a}=e;return(0,o.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"azure-data-engineering"},"Azure Data Engineering"),(0,o.kt)("h2",{id:"case-study---data-lake"},"Case study - data lake"),(0,o.kt)("p",null,"In a case\xa0study question, a use case will be described in detail with multiple inputs such as business requirements and technical requirements. You will have to carefully read the question and understand the requirements before answering the question."),(0,o.kt)("h3",{id:"background"},"Background"),(0,o.kt)("p",null,"Let's assume\xa0you are a data architect in a retail company that has both online and bricks and mortar outlets all over the world. You have been asked to design their data processing solution. The leadership team wants to see a unified dashboard of daily, monthly, and yearly revenue reports in a graphical format, from across all their geographic locations and the online store."),(0,o.kt)("p",null,"The company\xa0has analysts who are SQL experts."),(0,o.kt)("p",null,"For simplicity, let's assume that the retail outlets are in friendly countries, so there is no limitation in terms of moving data across the countries."),(0,o.kt)("h3",{id:"technical-details"},"Technical details"),(0,o.kt)("p",null,"The online transaction data gets collected into Azure SQL instances that are geographically\xa0spread out. The overall size is about 10 GB per day."),(0,o.kt)("p",null,"The bricks and mortar point of sale transactions are getting collected in local country-specific SQL Server databases with different schemas. The size is about 20 GB per day."),(0,o.kt)("p",null,"The store details are stored as JSON files in Azure Data Lake Gen2."),(0,o.kt)("p",null,"The inventory data is available as CSV files in Azure Data Lake Gen2. The size is about 500 MB per day."),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"TIP: The trick is to identify key terminologies such as file formats, streaming, or batching (based on the frequency of reports), the size of the data, security restrictions - if any, and technologies to be used, such as SQL in this case (as the analysts are SQL experts). Once we have all this data, the decision-making process becomes a bit simpler.")),(0,o.kt)("h4",{id:"question-1"},"Question 1"),(0,o.kt)("p",null,"Choose the right storage solution to collect and store all the different types of data."),(0,o.kt)("p",null,"[Options: Azure Synapse SQL pool, Azure Data Lake Gen2, Azure Files, Event Hubs]"),(0,o.kt)("h4",{id:"solution"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Azure Data Lake Gen2")),(0,o.kt)("h4",{id:"explanation"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"ADLS Gen2 can\xa0handle multiple different types of formats and can store petabytes of data. Hence, it would suit our use case."),(0,o.kt)("li",{parentName:"ul"},"Azure Synapse SQL pool is for storing processed data in SQL tables."),(0,o.kt)("li",{parentName:"ul"},"Azure Files\xa0are file sharing storage services that\xa0can be accessed via\xa0",(0,o.kt)("strong",{parentName:"li"},"Server Message Block"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"SMB"),") or\xa0",(0,o.kt)("strong",{parentName:"li"},"Network File System"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"NFS"),") protocols. They are used to share application settings, as extended on-premises file servers, and so on."),(0,o.kt)("li",{parentName:"ul"},"Event Hubs is used for streaming real-time events and not an actual analytical data store.")),(0,o.kt)("h4",{id:"question-2"},"Question 2"),(0,o.kt)("p",null,"Choose the mechanism to copy data over into your common storage."),(0,o.kt)("p",null,"[Choices: PolyBase, Azure Data Factory, Azure Databricks, Azure Stream Analytics]"),(0,o.kt)("h4",{id:"solution-1"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Azure Data Factory")),(0,o.kt)("h4",{id:"explanation-1"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"ADF provides\xa0connectors to read data from a huge variety of sources, both on the cloud and on-premises. Hence, it will be a good fit for this situation."),(0,o.kt)("li",{parentName:"ul"},"PolyBase is mostly used for converting data from different formats to standard SQL table formats and copying them into Synapse SQL pool."),(0,o.kt)("li",{parentName:"ul"},"Azure Databricks can be used for batch and Spark stream processing, not for storing large volumes of data."),(0,o.kt)("li",{parentName:"ul"},"Azure Stream Analytics is used for stream processing, not for storing large volumes of data.")),(0,o.kt)("h4",{id:"question-3"},"Question 3"),(0,o.kt)("p",null,"Choose storage to store the daily, monthly, and yearly data for the analysts to query and generate reports using SQL."),(0,o.kt)("p",null,"[Choices: Azure Databricks, Azure Queues, Synapse SQL pool, Azure Data Factory]"),(0,o.kt)("h4",{id:"solution-2"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Synapse SQL pool")),(0,o.kt)("h4",{id:"explanation-2"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Synapse SQL pool is\xa0a data warehouse solution that perfectly fits the requirements for storing data to generate reports and find insights. The daily, monthly, and yearly data is usually the data that is cleaned, filtered, joined, aggregated from various sources, and stored in pre-defined schemas for easy analysis. Since Synapse SQL pools are natively SQL-based, it works well for analysts of the company who are SQL experts.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Databricks is used for batch and stream processing, not for storing large volumes of data. Hence, it wouldn't fit the bill for our use case.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Queues storage is a messaging service that can hold millions of messages and that can be processed asynchronously. Hence, it wouldn't fit the bill for our use case.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Data Factory is used to copy/move data, do basic transformations, and orchestrate pipelines. It cannot be used for storing data."),(0,o.kt)("blockquote",{parentName:"li"},(0,o.kt)("p",{parentName:"blockquote"},"TIP: If you find terminologies that you are not aware of, use the principle of negation to find the most suitable answer. In this case, if you didn't know what Azure Queues does, you can try to establish whether any of the other options is a good solution and then go with it. Or, if you are not sure, try to eliminate the obviously wrong answers and take an educated guess.")))),(0,o.kt)("h4",{id:"question-4"},"Question 4"),(0,o.kt)("p",null,"Visualize the insights generated in a graphical format."),(0,o.kt)("p",null,"[Choices: Azure Data Factory, Synapse Serverless SQL pool, Power BI, Azure Databricks]"),(0,o.kt)("h4",{id:"solution-3"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Power BI")),(0,o.kt)("h4",{id:"explanation-3"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Power BI can\xa0generate insights from various sources of data, such as Synapse SQL pools, Azure Stream Analytics, Azure SQL, and Cosmos DB. It provides a very rich set of tools to graphically display the data."),(0,o.kt)("li",{parentName:"ul"},"ADF provides connectors to read data from a huge variety of sources and orchestration support. Hence, it will not be a good fit for this situation."),(0,o.kt)("li",{parentName:"ul"},"Synapse SQL pool is a data warehouse solution that can be used to process data and store it, to be used by business intelligence tools such as Power BI."),(0,o.kt)("li",{parentName:"ul"},"Azure Databricks can be used for visualizing data patterns, but not usually for generating and visualizing graphical insights.")),(0,o.kt)("h2",{id:"case-study---data-lake-1"},"Case study - data lake"),(0,o.kt)("p",null,"The case\xa0study questions will have a detailed description of the case followed by the questions."),(0,o.kt)("h3",{id:"background-1"},"Background"),(0,o.kt)("p",null,"You have\xa0been hired to build a ticket scanning system for a country's railways department. Millions of passengers will be traveling on the trains every day. It has been observed that some passengers misuse their tickets by sharing them with others or using them for more rides than allowed. The railway officers want a real-time system to track such fraud occurrences."),(0,o.kt)("h3",{id:"technical-details-1"},"Technical details"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A ticket\xa0is considered fraudulent it if is used more than 10 times a day."),(0,o.kt)("li",{parentName:"ul"},"Build a real-time alerting system to generate alerts whenever such fraud happens."),(0,o.kt)("li",{parentName:"ul"},"Generate a monthly fraud report of the number of incidents and the train stations where it happens.")),(0,o.kt)("p",null,"You need\xa0to build a data pipeline. Recommend the services that can be used to build such a fraud detection system."),(0,o.kt)("h4",{id:"question-1-1"},"Question 1"),(0,o.kt)("p",null,"You recommend the following components to be used:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Azure Blob storage to consume the data"),(0,o.kt)("li",{parentName:"ul"},"Azure Stream Analytics to process the fraud alerts"),(0,o.kt)("li",{parentName:"ul"},"Power BI to display the monthly report")),(0,o.kt)("p",null,"[Options: Correct/ Incorrect]"),(0,o.kt)("h4",{id:"solution-4"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Incorrect")),(0,o.kt)("h4",{id:"explanation-4"},"Explanation"),(0,o.kt)("p",null,"We cannot use Azure Blob storage to consume real-time data. It is used to store different formats of data for analytical processing or long-term storage."),(0,o.kt)("h4",{id:"question-2-1"},"Question 2"),(0,o.kt)("p",null,"You recommend a system to use:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"IOT Hub to consume the data"),(0,o.kt)("li",{parentName:"ul"},"Azure Stream Analytics to process the fraud alerts"),(0,o.kt)("li",{parentName:"ul"},"Azure Databricks to store the monthly data and generate the reports"),(0,o.kt)("li",{parentName:"ul"},"Power BI to display the monthly report")),(0,o.kt)("p",null,"[Options: Correct/ Incorrect]"),(0,o.kt)("h4",{id:"solution-5"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Incorrect")),(0,o.kt)("h4",{id:"explanation-5"},"Explanation"),(0,o.kt)("p",null,"We cannot\xa0use Azure Databricks to store the monthly data. It is not a storage service; it is a compute service."),(0,o.kt)("h4",{id:"question-3-1"},"Question 3"),(0,o.kt)("p",null,"You recommend a system to use:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"IOT Hub to consume the data"),(0,o.kt)("li",{parentName:"ul"},"Azure Stream Analytics to process the fraud alerts"),(0,o.kt)("li",{parentName:"ul"},"Azure Synapse SQL pool to store the monthly data and generate the reports"),(0,o.kt)("li",{parentName:"ul"},"Power BI to display the monthly report")),(0,o.kt)("p",null,"[Options: Correct/ Incorrect]"),(0,o.kt)("h4",{id:"solution-6"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Correct")),(0,o.kt)("h4",{id:"explanation-6"},"Explanation"),(0,o.kt)("p",null,"IOT Hub can be used to consume real-time data and feed it to Azure Stream Analytics. Stream Analytics can perform real-time fraud detection and store the aggregated results in Synapse SQL pool. Synapse SQL pool can store petabytes of data for longer durations to generate reports. Power BI can graphically display both the real-time alerts and monthly reports. So, this is the right set of options."),(0,o.kt)("p",null,"Let's look at a data visualization question next."),(0,o.kt)("h2",{id:"data-visualization"},"Data visualization"),(0,o.kt)("p",null,"You have\xa0data from various data sources in JSON and CSV formats\xa0that has been copied over into Azure Data Lake Gen2. You need\xa0to graphically visualize the data. What tool would you use?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Power BI"),(0,o.kt)("li",{parentName:"ul"},"Azure Databricks/Synapse Spark"),(0,o.kt)("li",{parentName:"ul"},"Azure Data Factory"),(0,o.kt)("li",{parentName:"ul"},"Azure Storage Explorer")),(0,o.kt)("h3",{id:"solution-7"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Azure Databricks/Synapse Spark")),(0,o.kt)("h3",{id:"explanation-7"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Databricks\xa0Spark or Synapse Spark provides graphing\xa0options that can be used to sample and visualize data.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Power BI is not used to visualize raw data. It is used to visualize insights derived from processed data.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Azure Data Factory provides options to preview the data, but not many options for graphically visualizing it.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Storage Explorer helps explore the filesystem but doesn't have the ability to visualize the data graphically."),(0,o.kt)("blockquote",{parentName:"li"},(0,o.kt)("p",{parentName:"blockquote"},"TIP: Look for the nuances in the question. The moment we see\xa0",(0,o.kt)("em",{parentName:"p"},"graphically visualize"),", we tend to select Power BI. But Azure Databricks Spark has built-in graphing tools that can help visualize the data. Power BI is used to build and display insights from processed data.")))),(0,o.kt)("p",null,"Let's look at a data partition question next."),(0,o.kt)("h2",{id:"data-partitioning"},"Data partitioning"),(0,o.kt)("p",null,"You\xa0have a table as follows in Azure SQL:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE Books {\n\xa0\xa0\xa0BookID VARCHAR(20) NOT NULL,\n\xa0\xa0\xa0CategoryID VARCHAR (20) NOT NULL,\n\xa0\xa0\xa0BookName VARCHAR (100),\n\xa0\xa0\xa0AuthorID VARCHAR (20),\n\xa0\xa0\xa0ISBN VARCHAR (40)\n}\n")),(0,o.kt)("p",null,"Assume there\xa0are 100 million entries in this table.\xa0",(0,o.kt)("strong",{parentName:"p"},"CategoryID"),"\xa0has about 25 entries and 60% of the books align to about 20 categories. You need to optimize the performance of this table for queries that aggregate on\xa0",(0,o.kt)("strong",{parentName:"p"},"CategoryID"),". What partitioning technique\xa0would you use and what key would you choose?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Vertical partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"CategoryID")),(0,o.kt)("li",{parentName:"ul"},"Horizontal partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"BookID")),(0,o.kt)("li",{parentName:"ul"},"Vertical partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"BookID")),(0,o.kt)("li",{parentName:"ul"},"Horizontal partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"CategoryID"))),(0,o.kt)("h3",{id:"solution-8"},"Solution"),(0,o.kt)("p",null,"Horizontal partitioning with\xa0",(0,o.kt)("strong",{parentName:"p"},"CategoryID")),(0,o.kt)("h3",{id:"explanation-8"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Horizontal partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"CategoryID"),"\xa0is the right choice as we need to horizontally partition (shard) the data based on\xa0",(0,o.kt)("strong",{parentName:"li"},"categoryID"),", which has a fairly good distribution. This can speed up the processing by distributing the data evenly across the partitions."),(0,o.kt)("li",{parentName:"ul"},"Vertical partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"CategoryID"),"\xa0- Splitting the table vertically will not optimize as\xa0we will have to scan through the entire database to aggregate the categories. Vertical partitioning is effective when we need to speed up queries only based on a few columns."),(0,o.kt)("li",{parentName:"ul"},"Horizontal partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"BookID"),"\xa0- Horizontal partitioning (sharding) is fine, but the key we are looking to optimize is the categories. So\xa0",(0,o.kt)("strong",{parentName:"li"},"BookID"),"\xa0will not create the optimal partitions."),(0,o.kt)("li",{parentName:"ul"},"Vertical partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"BookID"),"\xa0- For the same reason as vertical partitioning with\xa0",(0,o.kt)("strong",{parentName:"li"},"CategoryID"),", vertical partitions will not be efficient as we need to access all the rows.")),(0,o.kt)("p",null,"Let's look at a Synapse SQL pool design question next."),(0,o.kt)("h2",{id:"synapse-sql-pool-table-design---1"},"Synapse SQL pool table design - 1"),(0,o.kt)("p",null,"You are\xa0the architect of a cab company. You are designing the schema to store trip information. You have a large fact table that has a billion rows. You have dimension tables in the range of 500--600 MB and you have\xa0daily car health data in the range of 50 GB. The car health data needs to be loaded into a staging table as quickly as possible. What distributions would you choose for each of these types of data?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Fact table"),(0,o.kt)("li",{parentName:"ul"},"B - Dimension tables"),(0,o.kt)("li",{parentName:"ul"},"C - Staging table")),(0,o.kt)("p",null,"[Options: Round Robin, Hash, Replicated]"),(0,o.kt)("h3",{id:"solution-9"},"Solution"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Fact table - Hash"),(0,o.kt)("li",{parentName:"ul"},"B - Dimension tables - Replicated"),(0,o.kt)("li",{parentName:"ul"},"C - Staging table - Round Robin")),(0,o.kt)("h3",{id:"explanation-9"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Replicated"),"\xa0- Use replication\xa0to copy small tables to all the nodes so that the processing is much faster without much network traffic."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Hash"),"\xa0- Use hash\xa0distribution for fact tables that contain millions or billions of rows/are several GBs in size. For small tables, hash distribution will not be very performant."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Round Robin"),"\xa0- Use round robin\xa0for staging tables where you want to quickly load the data.")),(0,o.kt)("p",null,"Let's look at another Synapse SQL pool design question next."),(0,o.kt)("h2",{id:"synapse-sql-pool-table-design---2"},"Synapse SQL pool table design - 2"),(0,o.kt)("p",null,"You are\xa0a data engineer for an online bookstore. The\xa0bookstore processes hundreds of millions of transactions every month. It has a Catalog table of about 100 MB. Choose the optimal distribution for the Catalog table and complete the following script:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE Catalogue (\n\xa0\xa0\xa0BookID VARCHAR 50,\n\xa0\xa0\xa0BookName VARCHAR 100,\n\xa0\xa0\xa0ISBN: VARCHAR 100,\n\xa0\xa0\xa0FORMAT: VARCHAR 20\n) WITH\n\xa0\xa0\xa0CLUSTERED COLUMNSTORE INDEX,\n\xa0\xa0\xa0DISTRIBUTION = ___________\n)\n")),(0,o.kt)("p",null,"[Options:\xa0",(0,o.kt)("strong",{parentName:"p"},"ROUND-ROBIN"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"REPLICATE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"HASH"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"PARTITION"),"]"),(0,o.kt)("h3",{id:"solution-10"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Replicate")),(0,o.kt)("h3",{id:"explanation-10"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Replicate\xa0distribution copies the data to all the compute nodes. Hence, the processing will be much faster in the case of smaller tables."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Hash"),"\xa0- Use hash\xa0distribution for fact tables that contain millions of rows or are several GBs in size. For small tables, hash distribution will not be very performant."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Round Robin"),"\xa0- Use\xa0round robin for staging tables where you want to quickly load the data."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Partition"),"\xa0- This is\xa0used for data partitioning, which is not our use case.")),(0,o.kt)("p",null,"Let's look at a slowly changing dimension question next."),(0,o.kt)("h2",{id:"slowly-changing-dimensions"},"Slowly changing dimensions"),(0,o.kt)("p",null,"Identify\xa0the type\xa0of SCD by looking at this table definition:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE DimCustomer (\n\xa0\xa0\xa0SurrogateID IDENTITY,\n\xa0\xa0\xa0CustomerID VARCHAR(20),\n\xa0\xa0\xa0Name VARCHAR(100),\n\xa0\xa0\xa0Email VARCHAR(100),\n\xa0\xa0\xa0StartDate DATE,\n\xa0\xa0\xa0EndDate DATE,\n\xa0\xa0\xa0IsActive INT\n)\n")),(0,o.kt)("p",null,"[Options: SCD Type 1, SCD Type 2, SCD Type 3]"),(0,o.kt)("h3",{id:"solution-11"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"SCD Type 2")),(0,o.kt)("h3",{id:"explanation-11"},"Explanation"),(0,o.kt)("p",null,"SCD Type 2\xa0keeps track of all the previous records using the\xa0",(0,o.kt)("strong",{parentName:"p"},"StartDate"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"EndDate"),", and, optionally, an\xa0",(0,o.kt)("strong",{parentName:"p"},"IsActive"),"\xa0or a\xa0",(0,o.kt)("strong",{parentName:"p"},"VersionNumber"),"\xa0field."),(0,o.kt)("p",null,"Let's look at a storage tier-based question next."),(0,o.kt)("h2",{id:"storage-tiers"},"Storage tiers"),(0,o.kt)("p",null,"You are a data\xa0engineer working with an ad serving company. There are three types of data the company wants to store in Azure Blob storage. Select the\xa0storage tiers that you should recommend for each of the following scenarios."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Auditing data for the last 5 years for yearly financial reporting"),(0,o.kt)("li",{parentName:"ul"},"B - Data to generate monthly customer expenditure reports"),(0,o.kt)("li",{parentName:"ul"},"C - Media files to be displayed in online ads")),(0,o.kt)("p",null,"[Options: Hot, Cold, Archive]"),(0,o.kt)("h3",{id:"solution-12"},"Solution"),(0,o.kt)("p",null,"A - Archive"),(0,o.kt)("p",null,"B - Cold"),(0,o.kt)("p",null,"C - Hot"),(0,o.kt)("h3",{id:"explanation-12"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Auditing data is accessed rarely and the use case says yearly financial reporting. So, this is a good candidate for the archive tier. The archive tier requires the data to be stored for at least 180 days."),(0,o.kt)("li",{parentName:"ul"},"Monthly\xa0customer expenditure data is not used frequently, so it is a good candidate for cold storage. Cold storage requires the data to be stored for at least 30 days."),(0,o.kt)("li",{parentName:"ul"},"Media files to be displayed in ads will be used every time the ad is displayed. Hence, this needs to be on the hot tier.")),(0,o.kt)("p",null,"Let's look at a disaster recovery question next."),(0,o.kt)("h2",{id:"disaster-recovery"},"Disaster recovery"),(0,o.kt)("p",null,"You work\xa0in a stock trading company that stores most of its data\xa0on ADLS Gen2 and the company wants to ensure that the business continues uninterrupted even when an entire data center goes down. Select the disaster recovery option(s) that you should choose for such a requirement:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Geo-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"GRS"),")"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Zone-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"ZRS"),")"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Geo-Zone-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"GZRS"),")"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Locally Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"LRS"),")"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Geo-Replication"))),(0,o.kt)("h3",{id:"solution-13"},"Solution"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Geo-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"GRS"),") or\xa0",(0,o.kt)("strong",{parentName:"li"},"Geo-Zone-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"GZRS"),")")),(0,o.kt)("h3",{id:"explanation-13"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Both\xa0",(0,o.kt)("strong",{parentName:"p"},"Geo-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"GRS"),") and\xa0",(0,o.kt)("strong",{parentName:"p"},"Geo-Zone-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"GZRS"),") can\xa0ensure that the data\xa0will be available even if entire data centers or regions go down. The difference between GRS and GZRS is that in GRS, the data is synchronously copied three times within the primary region using the LRS technique, but in GZRS, the data is synchronously copied three times within the primary region using ZRS. With GRS and GZRS, the data in the secondary region will not be available for simultaneous read or write access. If you\xa0need simultaneous read\xa0access in the secondary regions, you could use the\xa0",(0,o.kt)("strong",{parentName:"p"},"Read-Access - Geo-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"RA-GRS"),") or\xa0",(0,o.kt)("strong",{parentName:"p"},"Read-Access Geo-Zone-Redundant Storage"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"RA-GZRS"),") options.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"LRS - LRS provides\xa0only local redundancy, but doesn't guarantee data availability if entire data centers or regions go down.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"ZRS - ZRS provides\xa0zone-level redundancy but doesn't hold up if the entire data center or region goes down.")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("p",{parentName:"li"},"Geo-replication - This is an\xa0Azure SQL replication feature that replicates the entire SQL server to another region and provides read-only access in the secondary region."),(0,o.kt)("blockquote",{parentName:"li"},(0,o.kt)("p",{parentName:"blockquote"},"TIP: If you notice any options that you are not aware of, don't panic. Just look at the ones you are aware of and check whether any of those could be the answer. For example, in the preceding question, if you had not read about geo-replication, it would have still been okay because the answer was among the choices that you already knew.")))),(0,o.kt)("h2",{id:"synapse-sql-external-tables"},"Synapse SQL external tables"),(0,o.kt)("p",null,"Fill in the\xa0missing code segment to read Parquet data\xa0from an ADLS Gen2 location into Synapse Serverless SQL:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"IF NOT EXISTS (SELECT * FROM sys.external_file_formats\nWHERE name = 'SynapseParquetFormat')\n\xa0\xa0\xa0\xa0CREATE ____________ [SynapseParquetFormat]\n\xa0\xa0\xa0\xa0WITH (FORMAT_TYPE = PARQUET)\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources\nWHERE name = 'sample_acct')\n\xa0\xa0\xa0\xa0CREATE _____________ [sample_acct]\n\xa0\xa0\xa0\xa0WITH (\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0LOCATION\xa0\xa0\xa0= 'https://sample_acct.dfs.core.windows.net/users',\n\xa0\xa0\xa0\xa0)\n\nCREATE ______________ TripsExtTable (\n\xa0\xa0\xa0\xa0[TripID] varchar(50),\n\xa0\xa0\xa0\xa0[DriverID] varchar(50),\n\xa0\xa0\xa0\xa0. . .\n\xa0\xa0\xa0\xa0)\n\xa0\xa0\xa0\xa0WITH (\n\xa0\xa0\xa0\xa0LOCATION = 'path/to/*.parquet',\n\xa0\xa0\xa0\xa0DATA_SOURCE = [sample_acct],\n\xa0\xa0\xa0\xa0FILE_FORMAT = [SynapseParquetFormat]\n\xa0\xa0\xa0\xa0)\n\nGO\n")),(0,o.kt)("p",null,"[Options:\xa0",(0,o.kt)("strong",{parentName:"p"},"TABLE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"EXTERNAL TABLE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"EXTERNAL FILE FORMAT"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"EXTERNAL DATA SOURCE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"VIEW"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"FUNCTION"),"]"),(0,o.kt)("p",null,"You can reuse the options provided above for more than one blank if needed."),(0,o.kt)("h3",{id:"solution-14"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"EXTERNAL FILE FORMAT"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"EXTERNAL DATA SOURCE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"EXTERNAL TABLE")),(0,o.kt)("h3",{id:"explanation-14"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The correct keywords are\xa0",(0,o.kt)("strong",{parentName:"li"},"EXTERNAL FILE FORMAT"),",\xa0",(0,o.kt)("strong",{parentName:"li"},"EXTERNAL DATA SOURCE"),", and\xa0",(0,o.kt)("strong",{parentName:"li"},"EXTERNAL TABLE"),"\xa0in the order in which they appear in the question."),(0,o.kt)("li",{parentName:"ul"},"You cannot\xa0use\xa0",(0,o.kt)("strong",{parentName:"li"},"TABLE"),"\xa0as this is not an internal\xa0table. We are reading external Parquet data as an external table."),(0,o.kt)("li",{parentName:"ul"},"You cannot use\xa0",(0,o.kt)("strong",{parentName:"li"},"VIEW"),"\xa0as views are logical projections of existing tables."),(0,o.kt)("li",{parentName:"ul"},"You cannot use\xa0",(0,o.kt)("strong",{parentName:"li"},"FUNCTION"),"\xa0as this is not a UDF.")),(0,o.kt)("p",null,"Let's next look at some sample questions from the data processing section."),(0,o.kt)("h2",{id:"data-lake-design"},"Data lake design"),(0,o.kt)("p",null,"You are\xa0working in a marketing firm. The firm provides social\xa0media sentiment analysis to its customers. It captures data from various social media websites, Twitter feeds, product reviews, and other online forums."),(0,o.kt)("p",null,"Technical requirements:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The input data includes files in CSV, JSON, image, video, and plain text formats."),(0,o.kt)("li",{parentName:"ul"},"The data is expected to have inconsistencies such as duplicate entries and missing fields."),(0,o.kt)("li",{parentName:"ul"},"The overall data size would be about 5 petabytes every month."),(0,o.kt)("li",{parentName:"ul"},"The engineering team are experts in Scala and Python and would like a Notebook experience."),(0,o.kt)("li",{parentName:"ul"},"Engineers must be able to visualize the data for debugging purposes."),(0,o.kt)("li",{parentName:"ul"},"The reports have to be generated on a daily basis."),(0,o.kt)("li",{parentName:"ul"},"The reports should have charts with the ability to filter and sort data directly in the reports.")),(0,o.kt)("p",null,"You need to build a data pipeline to accomplish the preceding requirements. What are the components you would select for the following zones of your data lake?"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Landing zone"),":"),(0,o.kt)("p",null,"[Options: Azure Data Lake Gen2, Azure Blob storage, Azure Synapse SQL, Azure Data Factory]"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Transformation zone"),":"),(0,o.kt)("p",null,"[Options: Synapse SQL pool, Azure Databricks Spark, Azure Stream Analytics]"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Serving zone"),":"),(0,o.kt)("p",null,"[Options: Synapse SQL pool, Azure Data Lake Gen2, Azure Stream Analytics]"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Reporting"),":"),(0,o.kt)("p",null,"[Azure Databricks Spark, Power BI, the Azure portal]"),(0,o.kt)("h3",{id:"solution-15"},"Solution"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Landing zone"),": Azure Blob storage"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Transformation zone"),": Azure Databricks Spark"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Serving zone"),": Synapse SQL pool"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Reporting"),": Power BI")),(0,o.kt)("h3",{id:"explanation-15"},"Explanation"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Landing zone"),":"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Since the\xa0input contains a wide variety of data formats, including images and videos, it is better to store them in Azure Blob storage."),(0,o.kt)("li",{parentName:"ul"},"Azure Data\xa0Lake Gen2 provides a hierarchical namespace and is usually a good storage choice for data lakes. But since this use case includes images and videos, it is not recommended here."),(0,o.kt)("li",{parentName:"ul"},"Synapse SQL pool is a data warehouse solution that can be used to process data and store it to be used by business intelligence tools such as Power BI."),(0,o.kt)("li",{parentName:"ul"},"Azure Data Factory provides connectors to read data from a huge variety of sources and orchestration support. Hence, it will not be a good fit for this situation.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Transformation zone"),":"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Since the\xa0requirement includes cleaning up the incoming data, visualizing the data, and transforming the different formats into a standard\xa0schema that can be consumed by reports, Azure Databricks would fit the bill. Azure Databricks also supports Notebooks with Scala and Python support."),(0,o.kt)("li",{parentName:"ul"},"Synapse SQL pool can be used to store the processed data generated by Azure Databricks, but would not be a good fit for Scala and Python support."),(0,o.kt)("li",{parentName:"ul"},"Azure Stream Analytics is used for real-time processing. Hence, it will not work for our use case.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Serving zone"),":"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Synapse SQL\xa0pool, being a data warehouse\xa0that can support petabytes of data, would be a perfect choice here."),(0,o.kt)("li",{parentName:"ul"},"Azure Data Lake Gen2 provides a hierarchical namespace and is usually a good storage choice for data lake landing zones, but not for the serving zone. Serving zones need to be able to serve the results quickly to BI systems, so usually SQL-based or key-value-based services work the best."),(0,o.kt)("li",{parentName:"ul"},"Azure Stream Analytics is used for real-time data processing. Hence, it will not work for our use case.")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Reporting"),":"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Power BI is a\xa0graphical business intelligence tool that can help visualize\xa0data insights. It provides a rich set of graphs and data filtering, aggregating, and sorting options."),(0,o.kt)("li",{parentName:"ul"},"The Azure portal is the starting page for all Azure services. It is the control center for all services that provides options for creating, deleting, managing, and monitoring the services.")),(0,o.kt)("p",null,"Let's next look at an ASA windowed aggregates question."),(0,o.kt)("h2",{id:"asa-windows"},"ASA windows"),(0,o.kt)("p",null,"You are working for a credit card company. You have been asked to design a system to detect credit\xa0card transaction fraud. One of the scenarios\xa0is to check whether a credit card has been used more than 3 times within the last 10 mins. The system is already configured to use Azure Event Hubs and Azure Stream Analytics. You have decided to use the windowed aggregation feature of ASA. Which of the following solutions would work? (Select one or more)"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Use a tumbling window with a size of 10 mins and check whether the count for the same credit card > 3."),(0,o.kt)("li",{parentName:"ul"},"B - Use a sliding window with a size of 10 mins and check whether the count for the same credit card > 3."),(0,o.kt)("li",{parentName:"ul"},"C - Use a hopping window with a size of 10 mins and a hop of 3 mins and check whether the count for the same credit card > 3."),(0,o.kt)("li",{parentName:"ul"},"D - Use a session window with a size of 10 mins and check whether the count for the same credit card > 3.")),(0,o.kt)("h3",{id:"solution-16"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"B - Sliding Window")),(0,o.kt)("h3",{id:"explanation-16"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A sliding window\xa0has a fixed size, but the window\xa0moves forward only when events are either added or removed. Otherwise, it won't emit any results. This will work perfectly as the window is of a fixed size and is moving after considering each and every event in progressive windows of 10 mins. This is a typical use case for a sliding window:\xa0",(0,o.kt)("em",{parentName:"li"},"For every 10 seconds, alert if an event appears more than 5 times"),"."),(0,o.kt)("li",{parentName:"ul"},"A tumbling\xa0window calculates the number\xa0of events in fixed-size non-overlapping windows, so it might miss out on counting the events across window boundaries. Here's a typical use case:\xa0",(0,o.kt)("em",{parentName:"li"},"Find the number of events grouped by card number, in 10-second-wide tumbling windows"),"."),(0,o.kt)("li",{parentName:"ul"},"A hopping\xa0window calculates the count of events\xa0at every X interval, for the previous Y window width duration. If the overlap window is not big enough, this will also miss counting the events across window boundaries. Here's a typical use case:\xa0",(0,o.kt)("em",{parentName:"li"},"Every 10 seconds, fetch the transaction count for the last 20 seconds"),"."),(0,o.kt)("li",{parentName:"ul"},"Session\xa0windows don't have fixed sizes. We need to specify a\xa0maximum window size and a timeout duration for session windows. The session window tries to grab as many events as possible within the max window size. Since this is not a fixed-size window, it will not work for our use case. Here's a typical use case:\xa0",(0,o.kt)("em",{parentName:"li"},"Find the number of trips that occur within 5 seconds of each other"),".")),(0,o.kt)("p",null,"Let's next look at a Spark transformation question."),(0,o.kt)("h2",{id:"spark-transformation"},"Spark transformation"),(0,o.kt)("p",null,"You work\xa0for a cab company that is storing trip data in Parquet format\xa0and fare data in CSV format. You are required to generate a report to list all the trips aggregated using the\xa0",(0,o.kt)("strong",{parentName:"p"},"City"),"\xa0field. The report should contain all fields from both files."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Trip file format (Parquet)"),":"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"tripId, driverId, City, StartTime, EndTime")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Fare file format (CSV)"),":"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"tripId, Fare")),(0,o.kt)("p",null,"Fill in the blanks of the following code snippet to achieve the preceding objective:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'%%scala\nval fromcsv = spark.read.options(Map("inferSchema"->"true","header"->"true"))\n.csv("abfss://path/to/csv/*")\nval fromparquet = spark.read.options(Map("inferSchema"->"true"))\n.parquet("abfss:// abfss://path/to/parquet/*")\nval joinDF = fromcsv.________(fromparquet,fromcsv("tripId") === fromparquet("tripId"),"inner")._________("City")\n')),(0,o.kt)("p",null,"[Options:\xa0",(0,o.kt)("strong",{parentName:"p"},"join"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"orderBy"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"select"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"groupBy"),"]"),(0,o.kt)("h3",{id:"solution-17"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Join"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"groupBy")),(0,o.kt)("h3",{id:"explanation-17"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"join()"),"\xa0- To join two tables based on the provided conditions"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"groupBy()"),"\xa0- Used to aggregate values based on some column values, such as\xa0",(0,o.kt)("strong",{parentName:"li"},"City"),"\xa0in this case"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"select()"),"\xa0- To select the data from a subset of columns"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"orderBy()"),"\xa0- To sort the rows by a particular column")),(0,o.kt)("p",null,"Let's next look at an ADF integration runtime-based question."),(0,o.kt)("h2",{id:"adf---integration-runtimes"},"ADF - integration runtimes"),(0,o.kt)("p",null,"You are working as a data engineer for a tax consulting firm. The firm processes thousands\xa0of tax forms for its customers every day. Your firm is growing and has decided to move to the cloud, but they want\xa0to be in a hybrid mode as they already have invested in a good set of on-premises servers for data processing. You plan to use ADF to copy data over nightly. Which of the following integration runtimes would you suggest?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Azure integration runtime"),(0,o.kt)("li",{parentName:"ul"},"B - Self-Hosted integration runtime"),(0,o.kt)("li",{parentName:"ul"},"C - Azure - SSIS integration runtime")),(0,o.kt)("h3",{id:"solution-18"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"B - Self-Hosted Integration Runtime"),": Since this is an on-premises to the cloud use case, the\xa0self-hosted integration\xa0would be ideal. Also, since they have their local compute available, it would become much easier to set up the IR on the local servers."),(0,o.kt)("h3",{id:"explanation-18"},"Explanation"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Azure Integration Runtime"),"\xa0- This is\xa0the default option\xa0and supports connecting data stores and compute services across public endpoints. Use this option to copy data between Azure-hosted services."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Self-Hosted Integration Runtime"),"\xa0- Use the self-hosted IR when you need to copy data between on-premises clusters and Azure services. You will need machines or VMs on the\xa0on-premises private network to install a self-hosted integration runtime."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Azure - SSIS Integration Runtime"),"\xa0- The SSIS IRs\xa0are\xa0used for\xa0",(0,o.kt)("strong",{parentName:"p"},"SQL Server Integration Services"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"SSIS"),") lift and shift use cases."),(0,o.kt)("p",null,"Let's next look at a question on ADF triggers."),(0,o.kt)("h2",{id:"adf-triggers"},"ADF triggers"),(0,o.kt)("p",null,"Choose the\xa0right kind of trigger for your ADF pipelines:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Trigger\xa0when a file gets deleted in Azure Blob storage"),(0,o.kt)("li",{parentName:"ul"},"B - To handle custom events in Event Grid"),(0,o.kt)("li",{parentName:"ul"},"C - Trigger a pipeline every Monday and Wednesday at 9:00 A.M. EST"),(0,o.kt)("li",{parentName:"ul"},"D - Trigger a pipeline daily at 9:00 A.M. EST but wait for the previous run to complete")),(0,o.kt)("p",null,"[Options: Storage event trigger, Custom event trigger, Tumbling window trigger, Schedule trigger]"),(0,o.kt)("h3",{id:"solution-19"},"Solution"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Storage event trigger"),(0,o.kt)("li",{parentName:"ul"},"B - Custom event trigger"),(0,o.kt)("li",{parentName:"ul"},"C - Schedule trigger"),(0,o.kt)("li",{parentName:"ul"},"D - Tumbling window trigger")),(0,o.kt)("h3",{id:"explanation-19"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Schedule trigger"),"\xa0- These are triggers that get fired on fixed schedules. You specify\xa0the start date, recurrence, and end date, and ADF\xa0takes care of firing the pipeline at the mentioned date and time."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Tumbling window trigger"),"\xa0- These are stateful scheduled triggers that are aware\xa0of the previous pipeline runs\xa0and offer retry capabilities."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Storage event trigger"),"\xa0- These\xa0are triggers\xa0that get fired on Blob storage events such as creating or deleting a file."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Custom trigger"),"\xa0- These\xa0are triggers that work on\xa0custom events mainly for Event Grid.")),(0,o.kt)("p",null,"Let's next look at a question from the data security section."),(0,o.kt)("h2",{id:"tdealways-encrypted"},"TDE/Always Encrypted"),(0,o.kt)("p",null,"You have\xa0configured active geo-replication\xa0on an Azure Synapse SQL instance. You are worried that the data might be accessible from the replicated instances or backup files and need to safeguard it. Which security solution do you configure?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Enable Always Encrypted"),(0,o.kt)("li",{parentName:"ul"},"Enable\xa0",(0,o.kt)("strong",{parentName:"li"},"Transport Layer Security"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"TLS"),")"),(0,o.kt)("li",{parentName:"ul"},"Enable\xa0",(0,o.kt)("strong",{parentName:"li"},"Transparent Data Encryption"),"\xa0(",(0,o.kt)("strong",{parentName:"li"},"TDE"),")"),(0,o.kt)("li",{parentName:"ul"},"Enable row-level security")),(0,o.kt)("h3",{id:"solution-20"},"Solution"),(0,o.kt)("p",null,"Enable",(0,o.kt)("strong",{parentName:"p"},"\xa0Transparent Data Encryption"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"TDE"),")"),(0,o.kt)("h3",{id:"explanation-20"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"TDE encrypts\xa0the complete database, including offline access files such as backup files and log files."),(0,o.kt)("li",{parentName:"ul"},"Always Encrypted is used to encrypt specific columns of database tables, not the complete database or the offline files."),(0,o.kt)("li",{parentName:"ul"},"TLS is for encrypting data in motion. It doesn't deal with encrypting databases."),(0,o.kt)("li",{parentName:"ul"},"Row-level security is for hiding selected rows from non-privileged database users. It doesn't encrypt the database itself.")),(0,o.kt)("p",null,"Let's next look at an Azure SQL/Synapse SQL auditing question."),(0,o.kt)("h2",{id:"auditing-azure-sqlsynapse-sql"},"Auditing Azure SQL/Synapse SQL"),(0,o.kt)("p",null,"You work\xa0for a financial institution that stores all transactions\xa0in an Azure SQL database. You are\xa0required to keep track of all the delete activities on the SQL server. Which\xa0of the following activities should you perform? (Select one or more correct options)"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Create alerts using Azure SQL Metrics."),(0,o.kt)("li",{parentName:"ul"},"Enable auditing."),(0,o.kt)("li",{parentName:"ul"},"Configure Log Analytics as the destination for the audit logs."),(0,o.kt)("li",{parentName:"ul"},"Build custom metrics for delete events.")),(0,o.kt)("h3",{id:"solution-21"},"Solution"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Enable auditing."),(0,o.kt)("li",{parentName:"ul"},"Configure Log Analytics as the destination for the audit logs.")),(0,o.kt)("h3",{id:"explanation-21"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Enabling auditing will track all the events in the database, including delete activities."),(0,o.kt)("li",{parentName:"ul"},"Configuring\xa0the destination as Log Analytics or Storage (Blob) will\xa0suffice the requirement to keep track of\xa0the activities. Log Analytics provides\xa0the advantage of Kusto queries, which can be run to analyze the audit logs. In the case of Blob storage, we will have to write custom code to analyze the audit logs."),(0,o.kt)("li",{parentName:"ul"},"Building custom metrics is not required as the audit function will automatically keep track of all deletions."),(0,o.kt)("li",{parentName:"ul"},"Creating alerts is not required as the requirement is to only keep track of the delete activities, not to alert.")),(0,o.kt)("p",null,"Let's next look at a\xa0",(0,o.kt)("strong",{parentName:"p"},"Dynamic Data Masking"),"\xa0(",(0,o.kt)("strong",{parentName:"p"},"DDM"),") question."),(0,o.kt)("h2",{id:"dynamic-data-masking"},"Dynamic data masking"),(0,o.kt)("p",null,"You need\xa0to partially mask the numbers of an SSN column. Only\xa0the last four digits should be visible. Which of the following solutions would work?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A --\xa0",(0,o.kt)("strong",{parentName:"li"},"ALTER TABLE ","[dbo]",".","[Customer]"," ALTER COLUMN SSN ADD MASKED WITH FUNCTION ='PARTIAL(4, \"xxx-xx-\", 4)');")),(0,o.kt)("li",{parentName:"ul"},"B --\xa0",(0,o.kt)("strong",{parentName:"li"},"ALTER TABLE ","[dbo]",".","[Customer]"," ALTER COLUMN SSN ADD MASKED WITH FUNCTION = 'PARTIAL(4, \"xxx-xx-\", 0)');")),(0,o.kt)("li",{parentName:"ul"},"C --\xa0",(0,o.kt)("strong",{parentName:"li"},"ALTER TABLE ","[dbo]",".","[Customer]"," ALTER COLUMN SSN ADD MASKED WITH (FUNCTION = 'PARTIAL(0,\"xxx-xx-\", 4)');")),(0,o.kt)("li",{parentName:"ul"},"D --\xa0",(0,o.kt)("strong",{parentName:"li"},"ALTER TABLE ","[dbo]",".","[Customer]"," ALTER COLUMN SSN ADD MASKED WITH FUNCTION = 'PARTIAL(\"xxx-xx-\")');"))),(0,o.kt)("h3",{id:"solution-22"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"C"),"\xa0--\xa0",(0,o.kt)("strong",{parentName:"p"},"ALTER TABLE ","[dbo]",".","[Customer]"," ALTER COLUMN SSN ADD MASKED WITH FUNCTION ='PARTIAL(0, 'xxx-xx-', 4)');")),(0,o.kt)("h3",{id:"explanation-22"},"Explanation"),(0,o.kt)("p",null,"The syntax for partial masking is\xa0",(0,o.kt)("strong",{parentName:"p"},"partial(prefix,","[padding]",",suffix)"),"."),(0,o.kt)("p",null,"Let's next look at an RBAC-based question."),(0,o.kt)("h2",{id:"rbac---posix"},"RBAC - POSIX"),(0,o.kt)("p",null,"Let's assume\xa0that you are part of the engineering AAD security group in your company. The sales team has a directory with the following details:"),(0,o.kt)("table",null,(0,o.kt)("thead",{parentName:"table"},(0,o.kt)("tr",{parentName:"thead"},(0,o.kt)("th",{parentName:"tr",align:null},"Container"),(0,o.kt)("th",{parentName:"tr",align:null},"Owner"),(0,o.kt)("th",{parentName:"tr",align:null},"Permission (POSIX)"))),(0,o.kt)("tbody",{parentName:"table"},(0,o.kt)("tr",{parentName:"tbody"},(0,o.kt)("td",{parentName:"tr",align:null},"/Sales"),(0,o.kt)("td",{parentName:"tr",align:null},"Sales AAD security group"),(0,o.kt)("td",{parentName:"tr",align:null},"740")))),(0,o.kt)("p",null,"Will you be able to read the files under the\xa0",(0,o.kt)("strong",{parentName:"p"},"/Sales"),"\xa0directory?"),(0,o.kt)("h3",{id:"solution-23"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"No")),(0,o.kt)("h3",{id:"explanation-23"},"Explanation"),(0,o.kt)("p",null,"In POSIX representation, there\xa0are numbers to indicate the permissions for\xa0",(0,o.kt)("strong",{parentName:"p"},"Owner"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"Owner Group"),", and\xa0",(0,o.kt)("strong",{parentName:"p"},"Others"),"."),(0,o.kt)("p",null,"In our question, the 740 would expand into:"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Owner"),": 7 (Read - 4, Write - 2, Execute - 1. Total: 4+2+1 = 7) // Can Read, Write, and Execute"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Owner Group"),": 4 (Read - 4, Write - 0, Execute - 0. Total: 4+0+0 = 4) // Can Read, but not Write or Execute"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Others"),": 0 (Read - 0, Write - 0, Execute - 0. Total: 0+0+0 = 0) // Cannot Read, Write, or Execute"),(0,o.kt)("p",null,"So, the answer\xa0to the question would be\xa0",(0,o.kt)("em",{parentName:"p"},"No"),". Since you are part of the engineering security group, you would fall under the\xa0",(0,o.kt)("strong",{parentName:"p"},"Other"),"\xa0category, which doesn't have any permissions."),(0,o.kt)("p",null,"Let's next look at a row-level security question."),(0,o.kt)("h2",{id:"row-level-security"},"Row-level security"),(0,o.kt)("p",null,"You are\xa0building a learning management system and you want\xa0to ensure that a teacher can see only the students in their class with any\xa0",(0,o.kt)("strong",{parentName:"p"},"SELECT"),"\xa0queries. Here is the\xa0",(0,o.kt)("strong",{parentName:"p"},"STUDENT"),"\xa0table:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE StudentTable {\n\xa0\xa0StudentId VARCHAR (20),\n\xa0\xa0StudentName VARCHAR(40),\n\xa0\xa0TeacherName sysname,\n\xa0\xa0Grade VARCHAR (3)\n}\n")),(0,o.kt)("p",null,"Fill in the missing sections of the following row-level security script:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Step 1:"),(0,o.kt)("p",{parentName:"li"},"CREATE ",(0,o.kt)("strong",{parentName:"p"},"__")," Security.TeacherPredicate (@TeacherName AS sysname)"),(0,o.kt)("p",{parentName:"li"},"RETURNS TABLE"),(0,o.kt)("p",{parentName:"li"},"AS RETURN SELECT 1"),(0,o.kt)("p",{parentName:"li"},"WHERE @TeacherName = USER_NAME()"))),(0,o.kt)("p",null,"[Options:\xa0",(0,o.kt)("strong",{parentName:"p"},"FUNCTION"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"TABLE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"VIEW"),"]"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("p",{parentName:"li"},"Step 2:"),(0,o.kt)("p",{parentName:"li"},"CREATE ",(0,o.kt)("strong",{parentName:"p"},(0,o.kt)("strong",{parentName:"strong"},"___"))," PrivFilter"),(0,o.kt)("p",{parentName:"li"},"ADD FILTER PREDICATE Security",(0,o.kt)("strong",{parentName:"p"},".TeacherPredicate\xa0"),"(",(0,o.kt)("strong",{parentName:"p"},"TeacherName"),")"),(0,o.kt)("p",{parentName:"li"},"ON StudentTable WITH (STATE = ON);"))),(0,o.kt)("p",null,"[Options:\xa0",(0,o.kt)("strong",{parentName:"p"},"SECURITY POLICY"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"TABLE"),",\xa0",(0,o.kt)("strong",{parentName:"p"},"VIEW"),"]"),(0,o.kt)("h3",{id:"solution-24"},"Solution"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Step 1:\xa0",(0,o.kt)("strong",{parentName:"li"},"FUNCTION")),(0,o.kt)("li",{parentName:"ul"},"Step 2:\xa0",(0,o.kt)("strong",{parentName:"li"},"SECURITY POLICY"))),(0,o.kt)("h3",{id:"explanation-24"},"Explanation"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Step 1: You must create a\xa0",(0,o.kt)("strong",{parentName:"li"},"FUNCTION"),"\xa0that can be applied as a\xa0",(0,o.kt)("strong",{parentName:"li"},"FILTER PREDICATE"),", not a\xa0",(0,o.kt)("strong",{parentName:"li"},"TABLE"),"\xa0or a\xa0",(0,o.kt)("strong",{parentName:"li"},"VIEW"),"."),(0,o.kt)("li",{parentName:"ul"},"Step 2: You must create a\xa0",(0,o.kt)("strong",{parentName:"li"},"SECURITY POLICY"),"\xa0that can be applied on the table, not a\xa0",(0,o.kt)("strong",{parentName:"li"},"TABLE"),"\xa0or a\xa0",(0,o.kt)("strong",{parentName:"li"},"VIEW"),".")),(0,o.kt)("p",null,"Let's next\xa0look at a few sample questions from the monitoring\xa0and optimization section."),(0,o.kt)("h2",{id:"blob-storage-monitoring"},"Blob storage monitoring"),(0,o.kt)("p",null,"You have been hired as an external consultant to evaluate Azure Blob storage. Your team\xa0has been using Blob storage for a month now. You want to find the usage and availability of the Blob storage."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 1"),":"),(0,o.kt)("p",null,"You can find the Blob storage usage from the\xa0",(0,o.kt)("strong",{parentName:"p"},"Storage Metrics"),"\xa0tab."),(0,o.kt)("p",null,"[Options: Yes/No]"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 2"),":"),(0,o.kt)("p",null,"You can find the Blob availability metrics from the\xa0",(0,o.kt)("strong",{parentName:"p"},"Storage Metrics"),"\xa0tab."),(0,o.kt)("p",null,"[Options: Yes/No]"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 3"),":"),(0,o.kt)("p",null,"You can find the Blob availability metrics from the\xa0",(0,o.kt)("strong",{parentName:"p"},"Azure Monitor"),"\xa0->\xa0",(0,o.kt)("strong",{parentName:"p"},"Storage Accounts"),"\xa0--\x3e\xa0",(0,o.kt)("strong",{parentName:"p"},"Insights\xa0"),"tab."),(0,o.kt)("p",null,"[Options: Yes/No]"),(0,o.kt)("h3",{id:"solution-25"},"Solution"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Question 1"),":\xa0",(0,o.kt)("strong",{parentName:"li"},"Yes")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Question 2"),":\xa0",(0,o.kt)("strong",{parentName:"li"},"No")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("strong",{parentName:"li"},"Question 3"),":\xa0",(0,o.kt)("strong",{parentName:"li"},"Yes"))),(0,o.kt)("h3",{id:"explanation-25"},"Explanation"),(0,o.kt)("p",null,"You can find the Blob storage usage from the\xa0",(0,o.kt)("strong",{parentName:"p"},"Metrics"),"\xa0tab on the Storage portal page. But it doesn't have the store availability metrics. To look at the availability, you will have to go to Azure Monitor and click on the\xa0",(0,o.kt)("strong",{parentName:"p"},"Insights"),"\xa0tab under\xa0",(0,o.kt)("strong",{parentName:"p"},"Storage accounts"),"."),(0,o.kt)("h2",{id:"t-sql-optimization"},"T-SQL optimization"),(0,o.kt)("p",null,"You are\xa0running a few T-SQL queries and realize that the queries are taking much longer than before. You want to analyze why the queries are taking longer. Which of the following solutions will work? (Select one or more)."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Create a diagnostic setting for Synapse SQL pool to send the\xa0",(0,o.kt)("strong",{parentName:"li"},"ExecRequests"),"\xa0and\xa0",(0,o.kt)("strong",{parentName:"li"},"Waits"),"\xa0logs to Log Analytics and analyze the diagnostics table using Kusto to get the details of the running query and the query waits."),(0,o.kt)("li",{parentName:"ul"},"B - Run a T-SQL query against the\xa0",(0,o.kt)("strong",{parentName:"li"},"sys.dm_pdw_exec_requests"),"\xa0and\xa0",(0,o.kt)("strong",{parentName:"li"},"sys.dm_pdw_waits"),"\xa0table to get the details of the running query and the query waits."),(0,o.kt)("li",{parentName:"ul"},"C - Go to the Synapse SQL Metrics dashboard and look at the query execution and query wait metrics.")),(0,o.kt)("h3",{id:"solution-26"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"B")),(0,o.kt)("h3",{id:"explanation-26"},"Explanation"),(0,o.kt)("p",null,"The diagnostic\xa0setting for Synapse SQL pool didn't provide the options for\xa0",(0,o.kt)("strong",{parentName:"p"},"ExecRequests"),"\xa0and\xa0",(0,o.kt)("strong",{parentName:"p"},"Waits"),"\xa0as of writing this book."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"sys.dm_pdw_exec_requests"),"\xa0- Contains all the current and recently active requests in Azure Synapse Analytics."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"sys.dm_pdw_waits"),"\xa0- Contains details of the wait states in a query, including locks and waits on transmission queues."),(0,o.kt)("p",null,"The SQL Metrics dashboard doesn't provide the query performance details."),(0,o.kt)("p",null,"Let's next look at an ADF monitoring question."),(0,o.kt)("h2",{id:"adf-monitoring"},"ADF monitoring"),(0,o.kt)("p",null,"There are\xa0two sub-questions for this question. Select all the statements that apply."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 1"),":"),(0,o.kt)("p",null,"How would you monitor ADF pipeline performance for the last month?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Use the ADF pipeline\xa0",(0,o.kt)("strong",{parentName:"li"},"activity"),"\xa0dashboard."),(0,o.kt)("li",{parentName:"ul"},"B - Create a diagnostic setting, route the pipeline data to Log Analytics, and use Kusto to analyze the performance data.")),(0,o.kt)("p",null,"[Options: A, B]"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 2"),":"),(0,o.kt)("p",null,"How would you\xa0monitor ADF pipeline performance for the last 3 months?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Use the ADF pipeline\xa0",(0,o.kt)("strong",{parentName:"li"},"activity"),"\xa0dashboard."),(0,o.kt)("li",{parentName:"ul"},"B - Create a diagnostic setting, route the pipeline data to Log Analytics, and use Kusto to analyze the performance data.")),(0,o.kt)("p",null,"[Options: A, B]"),(0,o.kt)("h3",{id:"solution-27"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 1"),":"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"A"),"\xa0and\xa0",(0,o.kt)("strong",{parentName:"p"},"B")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Question 2"),":"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"Only A")),(0,o.kt)("h3",{id:"explanation-27"},"Explanation"),(0,o.kt)("p",null,"The ADF activity dashboard only keeps 45 days of data. Beyond that, we need to use Azure Monitoring and Log Analytics."),(0,o.kt)("p",null,"Let's next look at an ASA alert-related question."),(0,o.kt)("h2",{id:"setting-up-alerts-in-asa"},"Setting up alerts in ASA"),(0,o.kt)("p",null,"Select the\xa0four steps required to set up an alert to fire if SU % goes above 80%. Arrange the steps in the right order:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"A - Configure diagnostic settings."),(0,o.kt)("li",{parentName:"ul"},"B - Define the actions to be done when the alert is triggered."),(0,o.kt)("li",{parentName:"ul"},"C - Select the signal as SU % utilization."),(0,o.kt)("li",{parentName:"ul"},"D - Redirect logs to Log Analytics and use Kusto to check for Threshold > 80%"),(0,o.kt)("li",{parentName:"ul"},"E - Select the scope as your Azure Stream Analytics job."),(0,o.kt)("li",{parentName:"ul"},"F - Set the alert logic as\xa0",(0,o.kt)("strong",{parentName:"li"},"Greater Than"),"\xa0Threshold value\xa0",(0,o.kt)("strong",{parentName:"li"},"80"),"%.")),(0,o.kt)("h3",{id:"solution-28"},"Solution"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"E, C, F, B")),(0,o.kt)("h3",{id:"explanation-28"},"Explanation"),(0,o.kt)("p",null,"The steps\xa0involved in setting up an ASA alert for SU % utilization are as follows:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Select the scope as your Azure Stream Analytics job."),(0,o.kt)("li",{parentName:"ul"},"Select the signal as SU % utilization."),(0,o.kt)("li",{parentName:"ul"},"Set the alert logic as\xa0",(0,o.kt)("strong",{parentName:"li"},"Greater Than"),"\xa0Threshold value\xa0",(0,o.kt)("strong",{parentName:"li"},"80"),"%."),(0,o.kt)("li",{parentName:"ul"},"Define the actions to be done when the alert is triggered.")),(0,o.kt)("p",null,"Diagnostic settings and Log Analytics are not required. The required SU % utilization metric is already available as part of ASA metrics."))}d.isMDXComponent=!0}}]);