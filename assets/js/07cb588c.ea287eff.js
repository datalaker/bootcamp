"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[42039],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var o=a(67294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function i(e,t){if(null==e)return{};var a,o,s=function(e,t){if(null==e)return{};var a,o,s={},n=Object.keys(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(o=0;o<n.length;o++)a=n[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=o.createContext({}),d=function(e){var t=o.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=d(e.components);return o.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},u=o.forwardRef((function(e,t){var a=e.components,s=e.mdxType,n=e.originalType,l=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=d(a),m=s,h=u["".concat(l,".").concat(m)]||u[m]||c[m]||n;return a?o.createElement(h,r(r({ref:t},p),{},{components:a})):o.createElement(h,r({ref:t},p))}));function m(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var n=a.length,r=new Array(n);r[0]=u;var i={};for(var l in t)hasOwnProperty.call(t,l)&&(i[l]=t[l]);i.originalType=e,i.mdxType="string"==typeof e?e:s,r[1]=i;for(var d=2;d<n;d++)r[d]=a[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,a)}u.displayName="MDXCreateElement"},7524:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>n,metadata:()=>i,toc:()=>d});var o=a(87462),s=(a(67294),a(3905));const n={},r="Hadoop Basics",i={unversionedId:"foundations/basics/hadoop-basics",id:"foundations/basics/hadoop-basics",title:"Hadoop Basics",description:"Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.",source:"@site/docs/01-foundations/basics/hadoop-basics.md",sourceDirName:"01-foundations/basics",slug:"/foundations/basics/hadoop-basics",permalink:"/docs/foundations/basics/hadoop-basics",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681212836,formattedLastUpdatedAt:"Apr 11, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Origin of Spark",permalink:"/docs/foundations/basics/spark-origin"},next:{title:"Map Reduce",permalink:"/docs/foundations/basics/map-reduce"}},l={},d=[{value:"Hadoop Architecture",id:"hadoop-architecture",level:2},{value:"Hadoop on Amazon EMR",id:"hadoop-on-amazon-emr",level:2}],p=(c="YouTube",function(e){return console.warn("Component "+c+" was not imported, exported, or provided by MDXProvider as global scope"),(0,s.kt)("div",e)});var c;const u={toc:d};function m(e){let{components:t,...n}=e;return(0,s.kt)("wrapper",(0,o.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"hadoop-basics"},"Hadoop Basics"),(0,s.kt)("p",null,"Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly."),(0,s.kt)("p",null,"Hadoop consists of four main modules:"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"Hadoop Distributed File System (HDFS) \u2013 A distributed file system that runs on standard or low-end hardware. HDFS provides better data throughput than traditional file systems, in addition to high fault tolerance and native support of large datasets."),(0,s.kt)("li",{parentName:"ol"},"Yet Another Resource Negotiator (YARN) \u2013 Manages and monitors cluster nodes and resource usage. It schedules jobs and tasks."),(0,s.kt)("li",{parentName:"ol"},"MapReduce \u2013 A framework that helps programs do the parallel computation on data. The map task takes input data and converts it into a dataset that can be computed in key value pairs. The output of the map task is consumed by reduce tasks to aggregate output and provide the desired result."),(0,s.kt)("li",{parentName:"ol"},"Hadoop Common \u2013 Provides common Java libraries that can be used across all modules.")),(0,s.kt)(p,{vid:"aReuLtY0YMI",title:"Hadoop In 5 Minutes | What Is Hadoop? | Introduction To Hadoop | Hadoop Explained |Simplilearn",mdxType:"YouTube"}),(0,s.kt)("p",null,"Hadoop makes it easier to use all the storage and processing capacity in cluster servers, and to execute distributed processes against huge amounts of data. Hadoop provides the building blocks on which other services and applications can be built."),(0,s.kt)("p",null,"Applications that collect data in various formats can place data into the Hadoop cluster by using an API operation to connect to the NameNode. The NameNode tracks the file directory structure and placement of \u201cchunks\u201d for each file, replicated across DataNodes. To run a job to query the data, provide a MapReduce job made up of many map and reduce tasks that run against the data in HDFS spread across the DataNodes. Map tasks run on each node against the input files supplied, and reducers run to aggregate and organize the final output."),(0,s.kt)("p",null,"The Hadoop ecosystem has grown significantly over the years due to its extensibility. Today, the Hadoop ecosystem includes many tools and applications to help collect, store, process, analyze, and manage big data. Some of the most popular applications are:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Spark \u2013 An open source, distributed processing system commonly used for big data workloads. Apache Spark uses in-memory caching and optimized execution for fast performance, and it supports general batch processing, streaming analytics, machine learning, graph databases, and ad hoc queries."),(0,s.kt)("li",{parentName:"ul"},"Presto \u2013 An open source, distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. Presto can process data from multiple data sources including the Hadoop Distributed File System (HDFS) and Amazon S3."),(0,s.kt)("li",{parentName:"ul"},"Hive \u2013 Allows users to leverage Hadoop MapReduce using a SQL interface, enabling analytics at a massive scale, in addition to distributed and fault-tolerant data warehousing."),(0,s.kt)("li",{parentName:"ul"},"HBase \u2013 An open source, non-relational, versioned database that runs on top of Amazon S3 (using EMRFS) or the Hadoop Distributed File System (HDFS). HBase is a massively scalable, distributed big data store built for random, strictly consistent, real-time access for tables with billions of rows and millions of columns."),(0,s.kt)("li",{parentName:"ul"},"Zeppelin \u2013 An interactive notebook that enables interactive data exploration.")),(0,s.kt)("h2",{id:"hadoop-architecture"},"Hadoop Architecture"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"img",src:a(73225).Z,width:"701",height:"362"})),(0,s.kt)(p,{vid:"ohroxsisQ0w",title:"The Hadoop ecosystem",mdxType:"YouTube"}),(0,s.kt)("p",null,"Let's take a quick look at the simplified Hadoop components in this figure, where the starting point is the servers:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/219852824-20a08079-101c-4e86-bd49-1a5a0fbade42.jpg",alt:"Figure_5 1"})),(0,s.kt)("p",null,"Hadoop has\xa0one too many working servers or, in Hadoop terms,\xa0",(0,s.kt)("strong",{parentName:"p"},"worker nodes"),". The\xa0worker nodes are simply computers with a filesystem where\xa0you can store files. HDFS sits on top of these multiple filesystems; when you store data in HDFS, Hadoop seamlessly distributes the files to these filesystems. You can create a table structure on top of the files using Hive, so SQL users can access the data using the SQL language. The other option is to process it using distributed processing frameworks such as Spark or MapReduce. These frameworks can read and write directly to HDFS.\xa0\xa0"),(0,s.kt)("p",null,"There\xa0are tools that are usually used to ingest data to HDFS, for example, Flume, Kafka, and Sqoop.\xa0The above figure\xa0is just a very simplified summary of the Hadoop ecosystem; in the marketplace, there are a lot of other alternatives for each component. If this is the first time you have learned about Hadoop, don't fall into a trap where you think you need to understand all of the products and their alternatives. Instead, focus on understanding how HDFS works and how Spark or MapReduce can process data in HDFS.\xa0The other components will come naturally with time when you understand HDFS and the processing framework."),(0,s.kt)("h2",{id:"hadoop-on-amazon-emr"},"Hadoop on Amazon EMR"),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"Amazon EMR is a managed service that lets you process and analyze large datasets using the latest versions of big data processing frameworks such as Apache Hadoop, Spark, HBase, and Presto on fully customizable clusters.")),(0,s.kt)("p",null,"Elastic MapReduce, or EMR, is Amazon Web Services\u2019 solution for managing prepackaged Hadoop clusters and running jobs on them. You can work with regular MapReduce jobs or Apache Spark jobs, and can use Apache Hive, Apache Pig, Apache HBase, and some third-party applications. Scripting hooks enable the installation of additional services. Data is typically stored in Amazon S3 or Amazon DynamoDB."),(0,s.kt)("p",null,"The normal mode of operation for EMR is to define the parameters for a cluster, such as its size, location, Hadoop version, and variety of services, point to where data should be read from and written to, and define steps to execute such as MapReduce or Spark jobs. EMR launches a cluster, performs the steps to generate the output data, and then tears the cluster down. However, you can leave clusters running for further use, and even resize them for greater capacity or a smaller footprint."),(0,s.kt)("p",null,"EMR provides an API so that you can automate the launching and management of Hadoop clusters."),(0,s.kt)("p",null,"Follow ",(0,s.kt)("a",{parentName:"p",href:"https://aws.amazon.com/emr/features/hadoop/"},"this")," blog for more information."))}m.isMDXComponent=!0},73225:(e,t,a)=>{a.d(t,{Z:()=>o});const o=a.p+"assets/images/hadoop-arch.drawio-cc045a40e9d09475152135e5c7324d92.svg"}}]);