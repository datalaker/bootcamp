"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[30062],{3905:(e,t,i)=>{i.d(t,{Zo:()=>u,kt:()=>h});var n=i(67294);function r(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function o(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function s(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?o(Object(i),!0).forEach((function(t){r(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):o(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function a(e,t){if(null==e)return{};var i,n,r=function(e,t){if(null==e)return{};var i,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||(r[i]=e[i]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var c=n.createContext({}),l=function(e){var t=n.useContext(c),i=t;return e&&(i="function"==typeof e?e(t):s(s({},t),e)),i},u=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},f={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var i=e.components,r=e.mdxType,o=e.originalType,c=e.parentName,u=a(e,["components","mdxType","originalType","parentName"]),p=l(i),h=r,d=p["".concat(c,".").concat(h)]||p[h]||f[h]||o;return i?n.createElement(d,s(s({ref:t},u),{},{components:i})):n.createElement(d,s({ref:t},u))}));function h(e,t){var i=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=i.length,s=new Array(o);s[0]=p;var a={};for(var c in t)hasOwnProperty.call(t,c)&&(a[c]=t[c]);a.originalType=e,a.mdxType="string"==typeof e?e:r,s[1]=a;for(var l=2;l<o;l++)s[l]=i[l];return n.createElement.apply(null,s)}return n.createElement.apply(null,i)}p.displayName="MDXCreateElement"},17027:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>f,frontMatter:()=>o,metadata:()=>a,toc:()=>l});var n=i(87462),r=(i(67294),i(3905));const o={},s="Logistic Regression",a={unversionedId:"datascience/algorithms/logistic-regression",id:"datascience/algorithms/logistic-regression",title:"Logistic Regression",description:"The logistic function first appeared in Pierre Francois Verhulst\u2019s publication \u201cCorrespondance mathmematique et physique\u201d in 1838. Then later in 1845, a more detailed version of the logistic function was published by him. However, the first practical application of such a function wasn\u2019t apparent until 1943 when Wilson and Worcester used the logistic function in bioassay. In the following years, various advances were made toward the function, but the original logistic function is used for Logistic Regression. The Logistic Regression model found its use not only in areas related to biology but also widely in social science.",source:"@site/docs/10-datascience/algorithms/logistic-regression.md",sourceDirName:"10-datascience/algorithms",slug:"/datascience/algorithms/logistic-regression",permalink:"/docs/datascience/algorithms/logistic-regression",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Linear Regression",permalink:"/docs/datascience/algorithms/linear-regression"},next:{title:"Decision Trees",permalink:"/docs/datascience/algorithms/decision-trees"}},c={},l=[],u={toc:l};function f(e){let{components:t,...i}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"logistic-regression"},"Logistic Regression"),(0,r.kt)("p",null,"The logistic function first appeared in Pierre Francois Verhulst\u2019s publication \u201cCorrespondance mathmematique et physique\u201d in 1838. Then later in 1845, a more detailed version of the logistic function was published by him. However, the first practical application of such a function wasn\u2019t apparent until 1943 when Wilson and Worcester used the logistic function in bioassay. In the following years, various advances were made toward the function, but the original logistic function is used for Logistic Regression. The Logistic Regression model found its use not only in areas related to biology but also widely in social science."),(0,r.kt)("p",null,"Among many variants, the general goal of Logistic Regression remains the same: classification based on explanatory variables or features provided. It utilizes the sample principles of Linear Regression."),(0,r.kt)("p",null,"The logistic function is a broad term referring to a function with various adjustable parameters. However, Logistic Regression only uses one set of parameters, turning the function into what\u2019s referred to as a sigmoid function. The sigmoid function is known to have an \u201cS\u201d-shaped curve with two horizontal asymptotes at y = 1 and y = 0. In other words, any value inputted to the function will be \u201csquished\u201d between 1 and 0."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230723774-187d4577-01f3-4974-b4a3-f31e9c27f34b.png",alt:"525591_1_En_1_Fig51_HTML"})),(0,r.kt)("p",null,"Although there are other variants of the sigmoid function that restrain the output at different values, such as the hyperbolic tangent function in which the output values are restrained between \u20131 and 1, binary classification on Logistic Regression uses the sigmoid function."),(0,r.kt)("p",null,"Logistic Regression operates on the same principles as those of Linear Regression, finding an equation that graphs a \u201cline of best fit\u201d of the data. In this case, the \u201cline of best fit\u201d will take the shape of the sigmoid function, while the labels are binary labels sitting either at y = 0 or y = 1."),(0,r.kt)("p",null,"Although the process of gradient descent for Logistic Regression is the same as that of Linear Regression, the cost function would be different. MSE calculates the difference between two values in a regression-like situation. While it may be able to learn under classification situations, there are better cost functions where it suits the problem. The output of this function is probability values ranging from 0 to 1, while our labels are discrete binary values. Instead of MSE, our cost function will be the log loss or sometimes called the binary cross-entropy (BCE)."))}f.isMDXComponent=!0}}]);