"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[98996],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>u});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},c=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(a),u=i,h=d["".concat(l,".").concat(u)]||d[u]||m[u]||o;return a?n.createElement(h,r(r({ref:t},c),{},{components:a})):n.createElement(h,r({ref:t},c))}));function u(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},44373:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var n=a(87462),i=(a(67294),a(3905));const o={},r="Computer Vision",s={unversionedId:"datascience/computer-vision/README",id:"datascience/computer-vision/README",title:"Computer Vision",description:"Categories",source:"@site/docs/10-datascience/computer-vision/README.md",sourceDirName:"10-datascience/computer-vision",slug:"/datascience/computer-vision/",permalink:"/docs/datascience/computer-vision/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Video Sharing Analysis",permalink:"/docs/datascience/challenges/video-sharing-analysis"},next:{title:"Agricultural Satellite Image Segmentation",permalink:"/docs/datascience/computer-vision/lab-agri-setallite-image-segmentation/"}},l={},p=[{value:"Categories",id:"categories",level:2},{value:"Image Similarity",id:"image-similarity",level:3},{value:"Models",id:"models",level:4},{value:"Process flow",id:"process-flow",level:4},{value:"Use Cases",id:"use-cases",level:4},{value:"Object Detection",id:"object-detection",level:3},{value:"Introduction",id:"introduction",level:4},{value:"Models",id:"models-1",level:4},{value:"Process flow",id:"process-flow-1",level:4},{value:"Use Cases",id:"use-cases-1",level:4},{value:"Image Segmentation",id:"image-segmentation",level:3},{value:"Models",id:"models-2",level:4},{value:"Process flow",id:"process-flow-2",level:4},{value:"Use Cases",id:"use-cases-2",level:4},{value:"Face Detection and Recognition",id:"face-detection-and-recognition",level:3},{value:"Models",id:"models-3",level:4},{value:"Process flow",id:"process-flow-3",level:4},{value:"Use Cases",id:"use-cases-3",level:4},{value:"Object Tracking",id:"object-tracking",level:3},{value:"Models",id:"models-4",level:4},{value:"Process flow",id:"process-flow-4",level:4},{value:"Use Cases",id:"use-cases-4",level:4},{value:"Pose Estimation",id:"pose-estimation",level:3},{value:"Models",id:"models-5",level:4},{value:"Process flow",id:"process-flow-5",level:4},{value:"Use Cases",id:"use-cases-5",level:4},{value:"Scene Text Recognition",id:"scene-text-recognition",level:3},{value:"Models",id:"models-6",level:4},{value:"Process flow",id:"process-flow-6",level:4},{value:"Use Cases",id:"use-cases-6",level:4},{value:"Video Action Recognition",id:"video-action-recognition",level:3},{value:"<strong>Models</strong>",id:"models-7",level:4},{value:"Process flow",id:"process-flow-7",level:4},{value:"Use Cases",id:"use-cases-7",level:4},{value:"Labs",id:"labs",level:2}],c={toc:p};function m(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"computer-vision"},"Computer Vision"),(0,i.kt)("h2",{id:"categories"},"Categories"),(0,i.kt)("h3",{id:"image-similarity"},"Image Similarity"),(0,i.kt)("p",null,"Image similarity is the measure of how similar two images are. In other words, it quantifies the degree of similarity between intensity patterns in two images."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822791-efe44b82-976d-46ca-81ee-eeb700ee641d.png",alt:"content-concepts-raw-computer-vision-image-similarity-slide19"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," Duplicate product detection, image clustering, visual search, product recommendations."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," Fine-tuning on classes for greater accuracy"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," TFHub")),(0,i.kt)("h4",{id:"models"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1710.05649"},"DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. arXiv, 2017."))),(0,i.kt)("li",{parentName:"ul"},"ConvNets: Pre-trained models like MobileNet, EfficientNet, BiT-L/BiT-M can be used to convert images into vectors. These models can be found on TFHub. For more accuracy, fine-tuning can be done."),(0,i.kt)("li",{parentName:"ul"},"FAISS: ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1702.08734"},"Billion-scale similarity search with GPUs. arXiv, 2017."))," - Faiss is a library for efficient similarity search and clustering of dense vectors."),(0,i.kt)("li",{parentName:"ul"},"Siamese Network: Siamese network is a neural network that contains two or more identical subnetwork. The purpose of this network is to find the similarity or comparing the relationship between two comparable things. Unlike the classification task that uses cross-entropy as the loss function, the siamese network usually uses contrastive loss or triplet loss."),(0,i.kt)("li",{parentName:"ul"},"Similarity Measures: L1 (Manhattan distance), L2 (Euclidean distance), Hinge Loss for Triplets.")),(0,i.kt)("h4",{id:"process-flow"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect Images - Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended."),(0,i.kt)("li",{parentName:"ul"},"Step 2: Encoder Fine-tuning - Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use."),(0,i.kt)("li",{parentName:"ul"},"Step 3: Image Vectorization - Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save the feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use."),(0,i.kt)("li",{parentName:"ul"},"Step 4: Metadata and Indexing - We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index objects for later use."),(0,i.kt)("li",{parentName:"ul"},"Step 5: UAT Testing - Wrap the model inference engine in API for client testing. We will receive an image from user, encode it with our image encoder, find Top-K similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user."),(0,i.kt)("li",{parentName:"ul"},"Step 6: Deployment - Deploy the model on cloud or edge as per the requirement."),(0,i.kt)("li",{parentName:"ul"},"Step 7: Documentation - Prepare the documentation and transfer all assets to the client.")),(0,i.kt)("h4",{id:"use-cases"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Multi-endpoint API Similarity System")),(0,i.kt)("p",null,"The task was to build an API that will support multiple endpoints. Each endpoint supports a separate similarity system. We built 2 endpoints: endpoint 1 would find tok-K most similar fashion images and endpoint 2 would find top-K most similar food images. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Multi-endpoint-Image-Similarity-System-159b47b635ea42299a0214551630e740"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Beanstalk Image Similarity System")),(0,i.kt)("p",null,"There are 2 endpoints in the API - one for training and the other for inference. During training, the system will receive a zipped file of images. At the time of inference, this trained system would receive an image over inference endpoint and send back top-K most similar images with a confidence score. The API was deployed on AWS beanstalk. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Similarity-AWS-b8f33261750047a69744e91a554eabff"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Image + Text Similarity")),(0,i.kt)("p",null,"Use the textual details and images of products, find the exact similar product among different groups. Around 35 GB of retail product images was scraped and used to build the system. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Text-Similarity-fe5130324ae14ab48a30c93444348f4a"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Siamese Network Image Similarity on MNIST")),(0,i.kt)("p",null,"Siamese networks are incredibly powerful networks, responsible for significant increases in face recognition, signature verification, and prescription pill identification applications. The objective was to build image pairs for the siamese network, train the siamese network with TF Keras, and then compare image similarity with this siamese network."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Visual Recommendation")),(0,i.kt)("p",null,"Use image similarity to recommend users visually similar products based on what they searched. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Similarity-Detection-in-Action-with-Tensorflow-2-0-c2b4421d75dd42a3a1becf9c98251ccb"},"here"),"."),(0,i.kt)("h3",{id:"object-detection"},"Object Detection"),(0,i.kt)("p",null,"Object detection is a computer vision technique that allows us to identify and locate objects in an image or video."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822793-68a55d64-a10e-4ae2-aa2b-53c3c8100117.png",alt:"content-concepts-raw-computer-vision-object-detection-slide29"})),(0,i.kt)("h4",{id:"introduction"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," Crowd counting, Self-driving cars, Video surveillance, Face detection, Anomaly detection"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," Detect objects in images and videos, 2-dimensional bounding boxes, Real-time"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," Detectron2, TF Object Detection API, OpenCV, TFHub, TorchVision")),(0,i.kt)("h4",{id:"models-1"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1506.01497"},"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. arXiv, 2016."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1512.02325"},"SSD: Single Shot MultiBox Detector. CVPR, 2016."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1804.02767"},(0,i.kt)("em",{parentName:"a"},"YOLOv3: An Incremental Improvement. arXiv, 2018."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1911.09070"},"EfficientDet: Scalable and Efficient Object Detection. CVPR, 2020."))," - It achieved 55.1 AP on COCO test-dev with 77M parameters.")),(0,i.kt)("h4",{id:"process-flow-1"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets"),(0,i.kt)("li",{parentName:"ul"},"Step 2: Create Labels - This step is required only if the object category is not available in any pre-trained model or labels are not freely available on the web. To create the labels (bounding boxes) using either open-source tools like Labelme or any other professional tool"),(0,i.kt)("li",{parentName:"ul"},"Step 3: Data Acquisition - Setup the database connection and fetch the data into python environment"),(0,i.kt)("li",{parentName:"ul"},"Step 4: Data Exploration - Explore the data, validate it and create preprocessing strategy"),(0,i.kt)("li",{parentName:"ul"},"Step 5: Data Preparation - Clean the data and make it ready for modeling"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Model Building - Create the model architecture in python and perform a sanity check"),(0,i.kt)("li",{parentName:"ul"},"Step 7: Model Training - Start the training process and track the progress and experiments"),(0,i.kt)("li",{parentName:"ul"},"Step 8: Model Validation - Validate the final set of models and select/assemble the final model"),(0,i.kt)("li",{parentName:"ul"},"Step 9: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 10: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 11: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-1"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Automatic License Plate Recognition")),(0,i.kt)("p",null,"Recognition of vehicle license plate number using various methods including YOLO4 object detector and Tesseract OCR. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Automatic-License-Plate-Recognition-10ec22181b454b1facc99abdeadbf78f"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Object Detection App")),(0,i.kt)("p",null,"This is available as a streamlit app. It detects common objects. 3 models are available for this task - Caffe MobileNet-SSD, Darknet YOLO3-tiny, and Darknet YOLO3. Along with common objects, this app also detects human faces and fire. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Object-Detector-App-c60fddae2fcd426ab763261436fb15d8"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Logo Detector")),(0,i.kt)("p",null,"Build a REST API to detect logos in images. API will receive 2 zip files - 1) a set of images in which we have to find the logo and 2) an image of the logo. Deployed the model in AWS Elastic Beanstalk. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Logo-Detection-91bfe4953dcf4558807b342efe05a9ff"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"TF Object Detection API Experiments")),(0,i.kt)("p",null,"The TensorFlow Object Detection API is an open-source framework built on top of TensorFlow that makes it easy to construct, train, and deploy object detection models. We did inference on pre-trained models, few-shot training on single class, few-shot training on multiple classes and conversion to TFLite model. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Tensorflow-Object-Detection-API-499b017e502d4950a9d448fb35a41d58"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Pre-trained Inference Experiments")),(0,i.kt)("p",null,"Inference on 6 pre-trained models - Inception-ResNet (TFHub), SSD-MobileNet (TFHub), PyTorch YOLO3, PyTorch SSD, PyTorch Mask R-CNN, and EfficientDet. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Object-Detection-Inference-Experiments-568fa092b1d34471b676fd43a42974b2"},"here")," and ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Object-Detection-Inference-with-Pre-trained-models-da9e2e5bfab944bc90f568f6bc4b3e1f"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Object Detection App")),(0,i.kt)("p",null,"TorchVision Mask R-CNN model Gradio App. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/MaskRCNN-TorchVision-Object-Detection-Gradio-App-c22f2a13ab63493b9b38720b20c50051"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Real-time Object Detector in OpenCV")),(0,i.kt)("p",null,"Build a model to detect common objects like scissors, cups, bottles, etc. using the MobileNet SSD model in the OpenCV toolkit. It will task input from the camera and detect objects in real-time. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Object-Detection-with-OpenCV-MobileNet-SSD-38ff496d2f0d427185a9c51cebc1ddf2"},"here"),". Available as a Streamlit app also (this app is not real-time)."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"EfficientDet Fine-tuning")),(0,i.kt)("p",null,"Fine-tune YOLO4 model on new classes. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/EfficientDet-fine-tuning-01a6ffd1e11f4dc1941073aff4b9b486"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"YOLO4 Fine-tuning")),(0,i.kt)("p",null,"Fine-tune YOLO4 model on new classes. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/YOLO-4-b32c2d2a4b8644b59f1c05e6887ffcca"},"here"),"."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Detectron2 Fine-tuning")),(0,i.kt)("p",null,"Fine-tune Detectron2 Mask R-CNN (with PointRend) model on new classes. Checkout the notion ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/YOLO-4-b32c2d2a4b8644b59f1c05e6887ffcca"},"here"),"."),(0,i.kt)("h3",{id:"image-segmentation"},"Image Segmentation"),(0,i.kt)("p",null,"Image segmentation is the task of assigning labels to each pixel of an image."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822790-17e672b8-9485-4189-be5c-38032dc37d11.png",alt:"content-concepts-raw-computer-vision-image-segmentation-slide46"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," Medical imaging, self-driving cars, satellite imaging"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," Semantic and Instance masks, 2D pixel-mask, Real-time"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," Detectron2, TFHub, TorchVision, DeepLab")),(0,i.kt)("h4",{id:"models-2"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1505.04597"},"U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv, 2015."))," - It was originally designed to perform medical image segmentation but it works well on a wide variety of tasks, from segmenting cells on microscope images to detecting ships or houses on photos taken from satellites."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1703.06870"},"Mask R-CNN. arXiv, 2017."))," - The Mask R-CNN framework is built on top of Faster R-CNN. ****So, for a given image, Mask R-CNN, in addition to the class label and bounding box coordinates for each object, will also return the object mask."),(0,i.kt)("li",{parentName:"ul"},"DeepLabV3+: ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1802.02611"},"Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. arXiv, 2018."))," - It achieves a mean IOU score of 89% on the PASCAL VOC 2012 dataset.")),(0,i.kt)("h4",{id:"process-flow-2"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets"),(0,i.kt)("li",{parentName:"ul"},"Step 2: Create Labels - This step is required only if the object category is not available in any pre-trained model or labels are not freely available on the web. To create the labels (pixel masks) using either open-source tools like Labelme or any other professional tool"),(0,i.kt)("li",{parentName:"ul"},"Step 3: Data Acquisition - Setup the database connection and fetch the data into python environment"),(0,i.kt)("li",{parentName:"ul"},"Step 4: Data Exploration - Explore the data, validate it and create preprocessing strategy"),(0,i.kt)("li",{parentName:"ul"},"Step 5: Data Preparation - Clean the data and make it ready for modeling"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Model Building - Create the model architecture in python and perform a sanity check"),(0,i.kt)("li",{parentName:"ul"},"Step 7: Model Training - Start the training process and track the progress and experiments"),(0,i.kt)("li",{parentName:"ul"},"Step 8: Model Validation - Validate the final set of models and select/assemble the final model"),(0,i.kt)("li",{parentName:"ul"},"Step 9: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 10: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 11: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-2"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Satellite Image Segmentation for Agricultural Fields")),(0,i.kt)("p",null,"An image with 1800 x 1135 resolution and 60 channels. Every Month 5 bands images were shot from agricultural land for 12 months. There is 8 type of croplands. Task is to classify all unknown label pixels into one of these 8 categories. U-Net model was trained from scratch on patches. Checkout ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/F991454-Satellite-Image-Segmentation-for-Agricultural-Fields-9914b549617746578c509e0382deb211"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Detectron2 Fine-tuning")),(0,i.kt)("p",null,"Fine-tune Detectron2 Mask R-CNN (with PointRend) model on new classes. It supports semantic, instance, and panoptic segmentation. We fine-tuned on balloons, chipsets, and faces. Checkout ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Industrial Use Cases for Image Segmentation")),(0,i.kt)("p",null,"Experimented with 3 industrial use cases - Carvana Vehicle Image Masking, Airbus Ship Detection, and Severstal Steel Defect Detection. Checkout ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Kaggle-Image-Segmentation-Experiments-770728c2ef9a493da20863789b112d78"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Real-time segmentation on Videos")),(0,i.kt)("p",null,"Real-time tracking and segmentation with SiamMask, semantic segmentation with LightNet++ and instance segmentation with YOLACT. Checkout ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Segmentation-Inference-Experiments-26fac32c220f419a902121129b2924db"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Image Segmentation Exercises")),(0,i.kt)("p",null,"Thresholding with Otsu and Riddler\u2013Calvard, Image segmentation with self-organizing maps, Random Walk segmentation with scikit-image, Skin color segmentation with the GMM\u2013EM algorithm, Medical image segmentation, Deep semantic segmentation, Deep instance segmentation. Checkout ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Segmentation-Exercises-cc3262c55d374fb684362f5d333fb91a"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"TorchVision Inference Experiments")),(0,i.kt)("p",null,"FCN-ResNet and DeepLabV3 (both are available in TorchVision library) inference. Available as a streamlit app. Checkout ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/FCN-ResNet-vs-DeepLab-App-FF841-5168fac2ed0b42b1ad95a0b9e8b26d53"},"this")," notion."),(0,i.kt)("h3",{id:"face-detection-and-recognition"},"Face Detection and Recognition"),(0,i.kt)("p",null,"Analyze the facial features like age, gender, emotion, and identity."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822789-86a34319-c100-48be-9581-ea5671cca059.png",alt:"content-concepts-raw-computer-vision-facial-analytics-img"})),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Applications:")," Identity verification, emotion detection"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Scope:")," Human faces only, Real-time"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Tools:")," OpenCV, dlib"),(0,i.kt)("h4",{id:"models-3"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://openaccess.thecvf.com/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf"},"FaceNet: A Unified Embedding for Face Recognition and Clustering. CVPR, 2015."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://arxiv.org/abs/1905.00641v2"},"RetinaFace: Single-stage Dense Face Localisation in the Wild. arXiv, 2019.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1608.01041v2"},"FER+: Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution. arXiv, 2016.")))),(0,i.kt)("h4",{id:"process-flow-3"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets"),(0,i.kt)("li",{parentName:"ul"},"Step 2: Create Labels - Compile a metadata table containing a unique id (preferably the same as the image name) for each face id."),(0,i.kt)("li",{parentName:"ul"},"Step 3: Data Preparation - Setup the database connection and fetch the data into the environment. Explore the data, validate it, and create a preprocessing strategy. Clean the data and make it ready for modeling"),(0,i.kt)("li",{parentName:"ul"},"Step 4: Model Building - Create the model architecture in python and perform a sanity check. Start the training process and track the progress and experiments. Validate the final set of models and select/assemble the final model"),(0,i.kt)("li",{parentName:"ul"},"Step 5: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 7: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-3"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Automatic Attendance System via Webcam")),(0,i.kt)("p",null,"We use Face Recognition library and OpenCV to create a real-time webcam-based attendance system that will automatically recognizes the face and log an attendance into the excel sheet. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Face-Recognition-based-Automated-Attendance-System-dfb6f70527994ea4be11caf69b054350"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Detectron2 Fine-tuning for face detection")),(0,i.kt)("p",null,"Fine-tuned detectron2 on human face dataset to detect the faces in images and videos. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb"},"this")," notion."),(0,i.kt)("h3",{id:"object-tracking"},"Object Tracking"),(0,i.kt)("p",null,"Object tracking is the process of 1)** Taking an initial set of object detections (such as an input set of bounding box coordinates, 2) Creating a unique ID for each of the initial detections, and then 3) tracking each of the objects as they move around frames in a video, maintaining the assignment of unique IDs."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822794-bbebf98d-269b-46f7-bd51-aa9f417105c8.png",alt:"content-concepts-raw-computer-vision-object-tracking-img"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," In-store consumer behavior tracking, Apply security policies like crowd management, traffic management, vision-based control, human-computer interface, medical imaging, augmented reality, robotics."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," Track objects in images and videos, 2-dimensional tracking, Bounding boxes and pixel masks, Single and Multiple Object Tracking"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," Detectron2, OpenCV")),(0,i.kt)("h4",{id:"models-4"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"FairMOT"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/2004.01888"},"On the Fairness of Detection and Re-Identification in Multiple Object Tracking. arXiv, 2020."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"DeepSORT"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1703.07402"},"Simple Online and Realtime Tracking with a Deep Association Metric. arXiv, 2017."))," - Detect object with models like YOLO or Mask R-CNN and then track using DeepSORT."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"GOTURN"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1604.01802"},"Learning to Track at 100 FPS with Deep Regression Networks. arXiv, 2016."))," - CNN offline learning tracker."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"MDNet"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1808.08834"},"Real-Time MDNet. arXiv, 2018."))," - CNN online learning tracker."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"ROLO"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1607.05781"},"Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking. arXiv, 2016."))," - CNN + LSTM tracker.")),(0,i.kt)("h4",{id:"process-flow-4"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect the data - Capture via camera, scrap from the internet or use public datasets"),(0,i.kt)("li",{parentName:"ul"},"Step 2: Train Object Detection Model - Train an object detector model (or use existing one if available in open-source domain)"),(0,i.kt)("li",{parentName:"ul"},"Step 3: Annotate the data - Apply object detector on the images to create a training set for object tracking"),(0,i.kt)("li",{parentName:"ul"},"Step 4: Data Preparation - Clean the data and make it ready for modeling"),(0,i.kt)("li",{parentName:"ul"},"Step 5: Train the Tracker - Build and train an object tracking model (e.g. DeepSORT, FairMOT) to accurately track the target object in images/videos. Track the progress and experiments"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Model Validation - Validate the final set of models and select/assemble the final model"),(0,i.kt)("li",{parentName:"ul"},"Step 7: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 8: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 9: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-4"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Pedestrian Tracking")),(0,i.kt)("p",null,"Pedestrian Tracking with YOLOv3 and DeepSORT. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Pedestrian-Tracking-with-YOLOv3-and-DeepSORT-a38ea37a2abf4755aacc691bd6b859a1"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Object Tracking")),(0,i.kt)("p",null,"Object tracking with FRCNN and SORT. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Object-tracking-with-FRCNN-and-SORT-e555d6174d2e4c1e993526c89555f96b"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Object Tracking")),(0,i.kt)("p",null,"Tested out 5 algorithms on videos - OpticalFlow, DenseFlow, Camshift, MeanShift and Single Object Tracking with OpenCV. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Object-Tracking-with-OpenCV-and-Python-2bf91e9f6f49405ca40409c392a2d429"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Social Distancing Violation Detection")),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"People and Vehicle Counter Detection")),(0,i.kt)("h3",{id:"pose-estimation"},"Pose Estimation"),(0,i.kt)("p",null,"Pose estimation is a computer vision task that infers the pose of a person or object in an image or video. This is typically done by identifying, locating, and tracking the number of\xa0key points\xa0on a given object or person. For objects, this could be corners or other significant features. And for humans, these key points represent major joints like an elbow or knee."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822795-bdcc650d-08c1-4e8a-9b67-f3a8a0aba2a5.png",alt:"content-concepts-raw-computer-vision-pose-estimation-img"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," Activity recognition, motion capture, fall detection, plank pose corrector, yoga pose identifier, body ration estimation"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," 2D skeleton map, Human Poses, Single and Multi-pose, Real-time"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," Tensorflow PoseNet API")),(0,i.kt)("h4",{id:"models-5"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1812.08008"},"OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv, 2016."))," - A standard bottom-up model that supports real-time multi-person 2D pose estimation. The authors of the paper have shared two models \u2013 one is trained on the Multi-Person Dataset ( MPII ) and the other is trained on the COCO dataset. The COCO model produces 18 points, while the MPII model outputs 15 points."),(0,i.kt)("li",{parentName:"ul"},"PoseNet is a machine learning model that allows for Real-time Human Pose Estimation. PoseNet can be used to estimate either a single pose or multiple poses PoseNet v1 is trained on MobileNet backbone and v2 on ResNet backbone.")),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822796-8d922553-a504-43cf-9718-2ef84a23bc0b.png",alt:"content-concepts-raw-computer-vision-pose-estimation-slide52"})),(0,i.kt)("h4",{id:"process-flow-5"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets"),(0,i.kt)("li",{parentName:"ul"},"Step 2: Create Labels - Use a pre-trained model like PoseNet, OpenPose to identify the key points. These key points are our labels for pose estimation based classification task. If the model is not compatible/available for the required key points (e.g. identify the cap and bottom of a bottle product to measure if manufacturing is correct), we have to first train a pose estimation model using transfer learning in that case (this is out of scope though, as we are only focusing on human poses and pre-trained models are already available for this use case)"),(0,i.kt)("li",{parentName:"ul"},"Step 3: Data Preparation - Setup the database connection and fetch the data into the environment. Explore the data, validate it, and create a preprocessing strategy. Clean the data and make it ready for modeling"),(0,i.kt)("li",{parentName:"ul"},"Step 4: Model Building - Create the model architecture in python and perform a sanity check. Start the training process and track the progress and experiments. Validate the final set of models and select/assemble the final model"),(0,i.kt)("li",{parentName:"ul"},"Step 5: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 7: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-5"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"OpenPose Experiments")),(0,i.kt)("p",null,"Four types of experiments with pre-trained OpenPose model - Single and Multi-Person Pose Estimation with OpenCV, Multi-Person Pose Estimation with PyTorch and Pose Estimation on Videos. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Pose-Estimation-with-OpenPose-2D8F5-7f01bee1534243f3836728d03a419969"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Pose Estimation Inference Experiments")),(0,i.kt)("p",null,"Experimented with pre-trained pose estimation models. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Pose-Estimation-with-OpenPifPaf-7517E-8cb982455e01478e876c52e9324d8e6b"},"this")," notion for experiments with the OpenPifPaf model, ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Pose-Estimation-with-Keypoint-RCNN-in-TorchVision-96e6aad0f36f44d3bff28e60525c6d31"},"this")," one for the TorchVision Keypoint R-CNN model, and ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb"},"this")," notion for the Detectron2 model."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Pose Detection on the Edge")),(0,i.kt)("p",null,"Train the pose detector using Teachable machine, employing the PoseNet model (multi-person real-time pose estimation) as the backbone and serve it to the web browser using ml5.js. This system will infer the end-users pose in real-time via a web browser. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://teachablemachine.withgoogle.com/train/pose"},"this")," link and ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/ml5-js-Pose-Estimation-with-PoseNet-5661cefe46b449998cc31838441dc26a"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Pose Detection on the Edge using OpenVINO")),(0,i.kt)("p",null,"Optimize the pre-trained pose estimation model using the OpenVINO toolkit to make it ready to serve at the edge (e.g. small embedded devices) and create an OpenVINO inference engine for real-time inference. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/OpenVINO-4c4fc4f167cc4601ade5795a241a60da"},"this")," notion."),(0,i.kt)("h3",{id:"scene-text-recognition"},"Scene Text Recognition"),(0,i.kt)("p",null,"Text\u2014as a fundamental tool of communicating information\u2014scatters throughout natural scenes, e.g., street signs, product labels, license plates, etc. Automatically reading text in natural scene images is an important task in machine learning and gains increasing attention due to a variety of applications. For example, accessing text in images can help the visually impaired understand the surrounding environment. To enable autonomous driving, one must accurately detect and recognize every road sign. Indexing text in images would enable image search and retrieval from billions of consumer photos on the internet."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822802-f8a98007-4d46-42bf-9198-0132a6a8a270.png",alt:"content-concepts-raw-computer-vision-scene-text-recognition-img"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," Indexing of multimedia archives, recognizing signs in driver assisted systems, providing scene information to visually impaired people, identifying vehicles by reading their license plates."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," No scope decided yet."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," OpenCV, Tesseract, PaddleOCR")),(0,i.kt)("h4",{id:"models-6"},"Models"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Semantic Reasoning Networks"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/2003.12294v1"},"Towards Accurate Scene Text Recognition with Semantic Reasoning Networks. arXiv, 2020."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Differentiable Binarization"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1911.08947v2"},"Real-time Scene Text Detection with Differentiable Binarization. arXiv, 2019."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"CRAFT"),": ",(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1904.01941v1"},"Character Region Awareness for Text Detection. arXiv, 2019."))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("em",{parentName:"li"},(0,i.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1704.03155v2"},"EAST: An Efficient and Accurate Scene Text Detector. arXiv, 2017.")))),(0,i.kt)("h4",{id:"process-flow-6"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect Images - Fetch from database, scrap from the internet or use public datasets. Setup the database connection and fetch the data into python environment."),(0,i.kt)("li",{parentName:"ul"},"Step 2: Data Preparation - Explore the data, validate it and create preprocessing strategy. Clean the data and make it ready for processing."),(0,i.kt)("li",{parentName:"ul"},"Step 3: Model Building - Apply different kinds of detection, recognition and single-shot models on the images. Track the progress and experiments. Validate the final set of models and select/assemble the final model."),(0,i.kt)("li",{parentName:"ul"},"Step 4: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 5: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-6"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Scene Text Detection with EAST Tesseract")),(0,i.kt)("p",null,"Detect the text in images and videos using EAST model. Read the characters using Tesseract. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Scene-Text-Detection-with-EAST-Tesseract-583f882db70b43b5b3005d89ced8d8fd"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Scene Text Recognition with DeepText")),(0,i.kt)("p",null,"Detect and Recognize text in images with an end-to-end model named DeepText. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Scene-Text-Recognition-with-DeepText-3dbc00e6bdf548a3b8539be1adb8f2d5"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Automatic License Plate Recognition")),(0,i.kt)("p",null,"Read the characters on the license plate image using Tesseract OCR. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Automatic-License-Plate-Recognition-10ec22181b454b1facc99abdeadbf78f"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Keras OCR Toolkit Experiment")),(0,i.kt)("p",null,"Keras OCR is a deep learning based toolkit for text recognition in images. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Keras-OCR-d15bff7629fa4fbf8d8a7fb21d2a69c5"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"OCR Experiments")),(0,i.kt)("p",null,"Experiments with three OCR tools - Tesseract OCR, Easy OCR, and Arabic OCR. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/OCR-Simple-Experiments-a606ff9003b14de589073864c150aa81"},"this")," and ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Optical-Character-Recognition-6eec9092cc70455a91dd92278e4677a8"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"PaddleOCR Experiments")),(0,i.kt)("p",null,"Experiments with state of the art lightweight and multi-lingual OCR. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Paddle-OCR-5ab56a38a594478da92314f246159193"},"this")," notion."),(0,i.kt)("h3",{id:"video-action-recognition"},"Video Action Recognition"),(0,i.kt)("p",null,"This is the task of identifying human activities/actions (e.g. eating, playing) in videos. In other words, this task classifies segments of videos into a set of pre-defined categories."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216822803-052c478d-b961-487b-a68f-702948660afc.png",alt:"content-concepts-raw-computer-vision-video-action-recognition-img"})),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Applications:")," Automated surveillance, elderly behavior monitoring, human-computer interaction, content-based video retrieval, and video summarization."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Scope:")," Human Action only"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Tools:")," OpenCV")),(0,i.kt)("h4",{id:"models-7"},(0,i.kt)("strong",{parentName:"h4"},"Models")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"3D-ResNet"),": ",(0,i.kt)("strong",{parentName:"li"},(0,i.kt)("a",{parentName:"strong",href:"https://arxiv.org/abs/1711.09577"},"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?"))," - the authors explore how existing state-of-the-art 2D architectures (such as ResNet, ResNeXt, DenseNet, etc.) can be extended to video classification via 3D kernels."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"R(2+1)D"),": This model was pre-trained on 65 million social media videos and fine-tuned on Kinetics400.")),(0,i.kt)("h4",{id:"process-flow-7"},"Process flow"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Step 1: Collect videos - Capture via camera, scrap from the internet or use public datasets"),(0,i.kt)("li",{parentName:"ul"},"Step 2: Create Labels - Use open-source tools like VGA Video Annotator for video annotation"),(0,i.kt)("li",{parentName:"ul"},"Step 3: Data Acquisition - Setup the database connection and fetch the data into python environment"),(0,i.kt)("li",{parentName:"ul"},"Step 4: Data Exploration - Explore the data, validate it and create preprocessing strategy"),(0,i.kt)("li",{parentName:"ul"},"Step 5: Data Preparation - Clean the data and make it ready for modeling"),(0,i.kt)("li",{parentName:"ul"},"Step 6: Model Building - Create the model architecture in python and perform a sanity check"),(0,i.kt)("li",{parentName:"ul"},"Step 7: Model Training - Start the training process and track the progress and experiments"),(0,i.kt)("li",{parentName:"ul"},"Step 8: Model Validation - Validate the final set of models and select/assemble the final model"),(0,i.kt)("li",{parentName:"ul"},"Step 9: UAT Testing - Wrap the model inference engine in API for client testing"),(0,i.kt)("li",{parentName:"ul"},"Step 10: Deployment - Deploy the model on cloud or edge as per the requirement"),(0,i.kt)("li",{parentName:"ul"},"Step 11: Documentation - Prepare the documentation and transfer all assets to the client")),(0,i.kt)("h4",{id:"use-cases-7"},"Use Cases"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Kinetics 3D CNN Human Activity Recognition")),(0,i.kt)("p",null,"This dataset consists of 400 human activity recognition classes, at least 400 video clips per class\xa0(downloaded via YouTube) and a total of 300,000 videos. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Kinetics-3D-CNN-Human-Activity-Recognition-fd10fd7b5858459cba65dc4a6cb73630"},"this")," notion."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Action Recognition using R(2+1)D Model")),(0,i.kt)("p",null,"VGA Annotator was used for creating the video annotation for training. Check out ",(0,i.kt)("a",{parentName:"p",href:"https://www.notion.so/Action-Recognition-using-R-2-1-D-Model-4c796f308aed40f29fc230a757af98e8"},"this")," notion."),(0,i.kt)("h2",{id:"labs"},"Labs"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Video Classification Modeling with X3D Model [",(0,i.kt)("a",{parentName:"li",href:"10-datascience/computer-vision/lab-video-classification/"},"source code"),"]")))}m.isMDXComponent=!0}}]);