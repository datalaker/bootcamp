"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[27694],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>u});var n=a(67294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?r(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var m=n.createContext({}),l=function(e){var t=n.useContext(m),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},p=function(e){var t=l(e.components);return n.createElement(m.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,r=e.originalType,m=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),d=l(a),u=s,h=d["".concat(m,".").concat(u)]||d[u]||c[u]||r;return a?n.createElement(h,i(i({ref:t},p),{},{components:a})):n.createElement(h,i({ref:t},p))}));function u(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var r=a.length,i=new Array(r);i[0]=d;var o={};for(var m in t)hasOwnProperty.call(t,m)&&(o[m]=t[m]);o.originalType=e,o.mdxType="string"==typeof e?e:s,i[1]=o;for(var l=2;l<r;l++)i[l]=a[l];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},52303:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>m,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>o,toc:()=>l});var n=a(87462),s=(a(67294),a(3905));const r={},i="Natural Language Processing (NLP)",o={unversionedId:"datascience/nlp/README",id:"datascience/nlp/README",title:"Natural Language Processing (NLP)",description:"Text Classification",source:"@site/docs/10-datascience/nlp/README.md",sourceDirName:"10-datascience/nlp",slug:"/datascience/nlp/",permalink:"/docs/datascience/nlp/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{}},m={},l=[{value:"Text Classification",id:"text-classification",level:2},{value:"Introduction",id:"introduction",level:3},{value:"Models",id:"models",level:3},{value:"FastText",id:"fasttext",level:4},{value:"XLNet",id:"xlnet",level:4},{value:"BERT",id:"bert",level:4},{value:"TextCNN",id:"textcnn",level:4},{value:"Embedding",id:"embedding",level:4},{value:"Bag-of-words",id:"bag-of-words",level:4},{value:"Process flow",id:"process-flow",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Email Classification",id:"email-classification",level:4},{value:"User Sentiment towards Vaccine",id:"user-sentiment-towards-vaccine",level:4},{value:"ServiceNow IT Ticket Classification",id:"servicenow-it-ticket-classification",level:4},{value:"Toxic Comment Classification",id:"toxic-comment-classification",level:4},{value:"Pre-trained Transformer Experiments",id:"pre-trained-transformer-experiments",level:4},{value:"Long Docs Classification",id:"long-docs-classification",level:4},{value:"BERT Sentiment Classification",id:"bert-sentiment-classification",level:4},{value:"Libraries",id:"libraries",level:3},{value:"Common applications",id:"common-applications",level:3},{value:"Links",id:"links",level:3},{value:"Text Embeddings",id:"text-embeddings",level:2},{value:"Utilities",id:"utilities",level:3},{value:"Averaging word vectors to create sentence vector",id:"averaging-word-vectors-to-create-sentence-vector",level:4},{value:"Word2vec",id:"word2vec",level:3},{value:"Training a custom Word2vec CBoW model with Gensim",id:"training-a-custom-word2vec-cbow-model-with-gensim",level:4},{value:"Training a custom Word2vec SkipGram model with Gensim",id:"training-a-custom-word2vec-skipgram-model-with-gensim",level:4},{value:"GloVe",id:"glove",level:3},{value:"TF-IDF",id:"tf-idf",level:3},{value:"BERT",id:"bert-1",level:3},{value:"Sentence BERT",id:"sentence-bert",level:4},{value:"Text Similarity",id:"text-similarity",level:2},{value:"<strong>Introduction</strong>",id:"introduction-1",level:3},{value:"<strong>Models</strong>",id:"models-1",level:3},{value:"BERT",id:"bert-2",level:4},{value:"Bag-of-words",id:"bag-of-words-1",level:4},{value:"DeepRank",id:"deeprank",level:4},{value:"FAISS",id:"faiss",level:4},{value:"Similarity Measures",id:"similarity-measures",level:4},{value:"<strong>Process flow</strong>",id:"process-flow-1",level:3},{value:"Use Cases",id:"use-cases-1",level:3},{value:"Semantic Relation Estimation",id:"semantic-relation-estimation",level:4},{value:"Finding Hardware Parts in Warehouse",id:"finding-hardware-parts-in-warehouse",level:4},{value:"Image + Text Similarity",id:"image--text-similarity",level:4},{value:"Text Recommendation",id:"text-recommendation",level:4},{value:"Topic Modeling",id:"topic-modeling",level:2},{value:"<strong>Introduction</strong>",id:"introduction-2",level:3},{value:"Models",id:"models-2",level:3},{value:"LSA",id:"lsa",level:4},{value:"Process flow",id:"process-flow-2",level:4},{value:"Use Cases",id:"use-cases-2",level:3},{value:"Identify Themes and Emerging Issues in ServiceNow Incident Tickets",id:"identify-themes-and-emerging-issues-in-servicenow-incident-tickets",level:4},{value:"IT Support Ticket Management",id:"it-support-ticket-management",level:4},{value:"Chatbot",id:"chatbot",level:2},{value:"<strong>Introduction</strong>",id:"introduction-3",level:3},{value:"<strong>Models</strong>",id:"models-3",level:3},{value:"RASA Chatbot",id:"rasa-chatbot",level:4},{value:"DialogFlow Chatbot",id:"dialogflow-chatbot",level:4},{value:"Alexa Skill",id:"alexa-skill",level:4},{value:"<strong>Process flow</strong>",id:"process-flow-3",level:3},{value:"<strong>Use Cases</strong>",id:"use-cases-3",level:3},{value:"RASA Chatbot",id:"rasa-chatbot-1",level:4},{value:"Insurance Voicebot",id:"insurance-voicebot",level:4},{value:"Wellness Tracker",id:"wellness-tracker",level:4},{value:"RASA Chatbot Experiments",id:"rasa-chatbot-experiments",level:4},{value:"Named Entity Recognition",id:"named-entity-recognition",level:2},{value:"<strong>Introduction</strong>",id:"introduction-4",level:3},{value:"<strong>Models</strong>",id:"models-4",level:3},{value:"Flair-NER",id:"flair-ner",level:4},{value:"Spacy-NER",id:"spacy-ner",level:4},{value:"Transformer-NER",id:"transformer-ner",level:4},{value:"<strong>Process flow</strong>",id:"process-flow-4",level:3},{value:"<strong>Use Cases</strong>",id:"use-cases-4",level:3},{value:"Name and Address Parsing",id:"name-and-address-parsing",level:4},{value:"NER Methods Experiment",id:"ner-methods-experiment",level:4},{value:"Text Summarization",id:"text-summarization",level:2},{value:"Introduction",id:"introduction-5",level:3},{value:"Scheduled sampling",id:"scheduled-sampling",level:3},{value:"Models",id:"models-5",level:3},{value:"ProphetNet",id:"prophetnet",level:4},{value:"PEGASUS",id:"pegasus",level:4},{value:"BERTSum",id:"bertsum",level:4},{value:"Seq2Seq PointerGenerator",id:"seq2seq-pointergenerator",level:4},{value:"Process flow",id:"process-flow-5",level:3},{value:"Use Cases",id:"use-cases-5",level:3},{value:"Enron Email Summarization",id:"enron-email-summarization",level:4},{value:"PDF Summarization over mail",id:"pdf-summarization-over-mail",level:4},{value:"BART Text Summarization",id:"bart-text-summarization",level:4},{value:"Transformers Summarization Experiment",id:"transformers-summarization-experiment",level:4},{value:"CNN-DailyMail and InShorts News Summarization",id:"cnn-dailymail-and-inshorts-news-summarization",level:4},{value:"Transformers Summarization Experiment",id:"transformers-summarization-experiment-1",level:4},{value:"Covid-19 article summarization",id:"covid-19-article-summarization",level:4},{value:"Common Applications",id:"common-applications-1",level:3},{value:"Variations",id:"variations",level:3},{value:"Summary Variations",id:"summary-variations",level:3},{value:"Seq2Seq",id:"seq2seq",level:3},{value:"Pointer Generator",id:"pointer-generator",level:3},{value:"Experiments",id:"experiments",level:3},{value:"Benchmark datasets",id:"benchmark-datasets",level:3},{value:"BigPatent",id:"bigpatent",level:4},{value:"CNN and DailyMail",id:"cnn-and-dailymail",level:4},{value:"Amazon Fine Food Reviews",id:"amazon-fine-food-reviews",level:4},{value:"DUC-2014",id:"duc-2014",level:4},{value:"Gigaword",id:"gigaword",level:4},{value:"Cornell Newsroom",id:"cornell-newsroom",level:4},{value:"Opinosis",id:"opinosis",level:4},{value:"Evaluation metrics",id:"evaluation-metrics",level:3},{value:"ROGUE",id:"rogue",level:4},{value:"BLEU",id:"bleu",level:4},{value:"Library",id:"library",level:3},{value:"Awesome List",id:"awesome-list",level:3},{value:"References",id:"references",level:3},{value:"Text Generation",id:"text-generation",level:2},{value:"Algorithms",id:"algorithms",level:3},{value:"Text Generation with Markov Chain",id:"text-generation-with-markov-chain",level:4},{value:"Important Papers",id:"important-papers",level:3},{value:"Experiments",id:"experiments-1",level:3},{value:"Text Generation with char-RNNs",id:"text-generation-with-char-rnns",level:3},{value:"References",id:"references-1",level:3},{value:"Decoding techniques - Greedy search, Beam search, Top-K sampling and Top-p sampling with Transformer",id:"decoding-techniques---greedy-search-beam-search-top-k-sampling-and-top-p-sampling-with-transformer",level:4},{value:"Controlling Text Generation with Plug and Play Language Models (PPLM)",id:"controlling-text-generation-with-plug-and-play-language-models-pplm",level:4},{value:"GPT-2 Fine Tuning",id:"gpt-2-fine-tuning",level:4},{value:"Autoregressive Language Generation",id:"autoregressive-language-generation",level:4},{value:"<strong>Word-Level Generation vs Character-Level Generation</strong>",id:"word-level-generation-vs-character-level-generation",level:4},{value:"Transformers",id:"transformers",level:2},{value:"The transformer block",id:"the-transformer-block",level:3},{value:"Additional tricks",id:"additional-tricks",level:4},{value:"In Pytorch: basic self-attention",id:"in-pytorch-basic-self-attention",level:4},{value:"Embedding",id:"embedding-1",level:4},{value:"Giving our words context: The Positional Encodings",id:"giving-our-words-context-the-positional-encodings",level:4},{value:"<strong>Creating Our Masks</strong>",id:"creating-our-masks",level:4},{value:"<strong>Multi-Headed Attention</strong>",id:"multi-headed-attention",level:4},{value:"Calculating Attention",id:"calculating-attention",level:4},{value:"<strong>The Feed-Forward Network</strong>",id:"the-feed-forward-network",level:4},{value:"<strong>One Last Thing : Normalization</strong>",id:"one-last-thing--normalization",level:4},{value:"Putting it all together!",id:"putting-it-all-together",level:4},{value:"Language Modeling",id:"language-modeling",level:2},{value:"<strong>Masked language modeling</strong>",id:"masked-language-modeling",level:3},{value:"Causal language modeling",id:"causal-language-modeling",level:3},{value:"Permutation language modeling",id:"permutation-language-modeling",level:3},{value:"Text Preprocessing and Cleaning",id:"text-preprocessing-and-cleaning",level:2},{value:"TF-IDF Vectorizor",id:"tf-idf-vectorizor",level:2},{value:"Count Vectorizer",id:"count-vectorizer",level:2},{value:"Wordcloud",id:"wordcloud",level:2},{value:"Labs",id:"labs",level:2}],p={toc:l};function c(e){let{components:t,...r}=e;return(0,s.kt)("wrapper",(0,n.Z)({},p,r,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"natural-language-processing-nlp"},"Natural Language Processing (NLP)"),(0,s.kt)("h2",{id:"text-classification"},"Text Classification"),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"/img/content-concepts-raw-text-classification-img.png",src:a(77226).Z,width:"960",height:"720"})),(0,s.kt)("h3",{id:"introduction"},"Introduction"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition"),": Text classification is a supervised learning method for learning and predicting the category or the class of a document given its text content. The state-of-the-art methods are based on neural networks of different architectures as well as pre-trained language models or word embeddings."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications"),": Spam classification, sentiment analysis, email classification, service ticket classification, question and comment classification"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope"),": Muticlass and Multilabel classification"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools"),": TorchText, Spacy, NLTK, FastText, HuggingFace, pyss3")),(0,s.kt)("h3",{id:"models"},"Models"),(0,s.kt)("h4",{id:"fasttext"},"FastText"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1607.01759"},"Bag of Tricks for Efficient Text Classification. arXiv, 2016."))),(0,s.kt)("p",null,"fastText is an open-source library, developed by the Facebook AI Research lab. Its main focus is on achieving scalable solutions for the tasks of text classification and representation while processing large datasets quickly and accurately."),(0,s.kt)("h4",{id:"xlnet"},"XLNet"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1906.08237"},"XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv, 2019."))),(0,s.kt)("h4",{id:"bert"},"BERT"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1810.04805"},"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv, 2018."))),(0,s.kt)("h4",{id:"textcnn"},"TextCNN"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1801.06287"},"What Does a TextCNN Learn?. arXiv, 2018."))),(0,s.kt)("h4",{id:"embedding"},"Embedding"),(0,s.kt)("p",null,"Feature extraction using either any pre-trained embedding models (e.g. Glove, FastText embedding) or custom-trained embedding model (e.g. using Doc2Vec) and then training an ML classifier (e.g. SVM, Logistic regression) on these extracted features."),(0,s.kt)("h4",{id:"bag-of-words"},"Bag-of-words"),(0,s.kt)("p",null,"Feature extraction using methods (CountVectorizer, TF-IDF) and then training an ML classifier (e.g. SVM, Logistic regression) on these extracted features."),(0,s.kt)("h3",{id:"process-flow"},"Process flow"),(0,s.kt)("p",null,"Step 1: Collect text data"),(0,s.kt)("p",null,"Collect via surveys, scrap from the internet or use public datasets"),(0,s.kt)("p",null,"Step 2: Create Labels"),(0,s.kt)("p",null,"In-house labeling or via outsourcing e.g. amazon mechanical turk"),(0,s.kt)("p",null,"Step 3: Data Acquisition"),(0,s.kt)("p",null,"Setup the database connection and fetch the data into python environment"),(0,s.kt)("p",null,"Step 4: Data Exploration"),(0,s.kt)("p",null,"Explore the data, validate it and create preprocessing strategy"),(0,s.kt)("p",null,"Step 5: Data Preparation"),(0,s.kt)("p",null,"Clean the data and make it ready for modeling"),(0,s.kt)("p",null,"Step 6: Model Building"),(0,s.kt)("p",null,"Create the model architecture in python and perform a sanity check"),(0,s.kt)("p",null,"Step 7: Model Training"),(0,s.kt)("p",null,"Start the training process and track the progress and experiments"),(0,s.kt)("p",null,"Step 8: Model Validation"),(0,s.kt)("p",null,"Validate the final set of models and select/assemble the final model"),(0,s.kt)("p",null,"Step 9: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing"),(0,s.kt)("p",null,"Step 10: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement"),(0,s.kt)("p",null,"Step 11: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client  "),(0,s.kt)("h3",{id:"use-cases"},"Use Cases"),(0,s.kt)("h4",{id:"email-classification"},"Email Classification"),(0,s.kt)("p",null,"The objective is to build an email classifier, trained on 700K emails and 300+ categories. Preprocessing pipeline to handle HTML and template-based content. Ensemble of FastText and BERT classifier. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/MOFSL-Email-Classification-309EC-7a451afa54d446b7b8f2f656450c6167"},"this")," notion."),(0,s.kt)("h4",{id:"user-sentiment-towards-vaccine"},"User Sentiment towards Vaccine"),(0,s.kt)("p",null,"Based on the tweets of the users, and manually annotated labels (label 0 means against vaccines and label 1 means in-favor of vaccine), build a binary text classifier. 1D-CNN was trained on the training dataset. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Twitter-Sentiment-Analysis-18d2d4ca41314b88a18db5c93f9eb2b2"},"this")," notion"),(0,s.kt)("h4",{id:"servicenow-it-ticket-classification"},"ServiceNow IT Ticket Classification"),(0,s.kt)("p",null,'Based on the short description, along with a long description if available for that particular ticket, identify the subject of the incident ticket in order to automatically classify it into a set of pre-defined categories. e.g. If custom wrote "Oracle connection giving error", this ticket type should be labeled as "Database". Check out ',(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/ESMCafe-IT-Support-Ticket-Management-69965830d39d486194f9a2f1222a81d8"},"this")," notion."),(0,s.kt)("h4",{id:"toxic-comment-classification"},"Toxic Comment Classification"),(0,s.kt)("p",null,"Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Toxic-Comment-Classification-Challenge-E5207-affac70cc5614f6dad38ab11ac15e1ab"},"this")," notion."),(0,s.kt)("h4",{id:"pre-trained-transformer-experiments"},"Pre-trained Transformer Experiments"),(0,s.kt)("p",null,"Experiment with different types of text classification models that are available in the HuggingFace Transformer library. Wrapped experiment based inference as a streamlit app."),(0,s.kt)("h4",{id:"long-docs-classification"},"Long Docs Classification"),(0,s.kt)("p",null,"Check out this ",(0,s.kt)("a",{parentName:"p",href:"https://colab.research.google.com/github/ArmandDS/bert_for_long_text/blob/master/final_bert_long_docs.ipynb"},"colab"),"."),(0,s.kt)("h4",{id:"bert-sentiment-classification"},"BERT Sentiment Classification"),(0,s.kt)("p",null,"Scrap App reviews data from Android playstore. Fine-tune a BERT model to classify the review as positive, neutral or negative. And then deploy the model as an API using FastAPI. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/BERT-Sentiment-Analysis-and-FastAPI-Deployment-43175-d0d19b234561445a84517538ad211405"},"this")," notion."),(0,s.kt)("h3",{id:"libraries"},"Libraries"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"pySS3"),(0,s.kt)("li",{parentName:"ul"},"FastText"),(0,s.kt)("li",{parentName:"ul"},"TextBrewer"),(0,s.kt)("li",{parentName:"ul"},"HuggingFace"),(0,s.kt)("li",{parentName:"ul"},"QNLP"),(0,s.kt)("li",{parentName:"ul"},"RMDL"),(0,s.kt)("li",{parentName:"ul"},"Spacy")),(0,s.kt)("h3",{id:"common-applications"},"Common applications"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Sentiment analysis."),(0,s.kt)("li",{parentName:"ul"},"Hate speech detection."),(0,s.kt)("li",{parentName:"ul"},"Document indexing in digital libraries."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Forum data"),": Find out how people feel about various products and features."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Restaurant and movie reviews"),": What are people raving about? What do people hate?"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Social media"),": What is the sentiment about a hashtag, e.g. for a company, politician, etc?"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Call center transcripts"),": Are callers praising or complaining about particular topics?"),(0,s.kt)("li",{parentName:"ul"},"General-purpose categorization in medical, academic, legal, and many other domains.")),(0,s.kt)("h3",{id:"links"},"Links"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"},"Text Classification - PyTorch Official")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://youtu.be/bEOiYF1a6Ak"},"Building a News Classifier ML App with Streamlit and Python")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/brightmart/text_classification"},"https://github.com/brightmart/text_classification")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.tensorflow.org/tutorials/keras/text_classification"},"IMDB Sentiment. Tensorflow.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/multi-label-text-classification-with-xlnet-b5f5755302df"},"XLNet Fine-tuning. Toxic Comment Multilabel.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://mccormickml.com/2019/09/19/XLNet-fine-tuning/"},"XLNet Fine-tuning. CoLA.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://notebooks.quantumstat.com/"},"Classification Demo Notebooks")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/microsoft/nlp-recipes/tree/master/examples/text_classification"},"Microsoft NLP Recipes")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f"},"Report on Text Classification using CNN, RNN & HAN")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"},"Implementing a CNN for Text Classification in TensorFlow")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/prakashpandey9/Text-Classification-Pytorch"},"prakashpandey9/Text-Classification-Pytorch")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/github/georgianpartners/Multimodal-Toolkit/blob/master/notebooks/text_w_tabular_classification.ipynb#scrollTo=QZR8kqmfRssU"},"Google Colaboratory")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.tensorflow.org/tutorials/text/classify_text_with_bert"},"Classify text with BERT | TensorFlow Core")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/2004.03705v1.pdf"},"Deep Learning Based Text Classification: A Comprehensive Review")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/"},"https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb"},"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://blog.valohai.com/machine-learning-pipeline-classifying-reddit-posts"},"https://blog.valohai.com/machine-learning-pipeline-classifying-reddit-posts")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.kdnuggets.com/2018/03/simple-text-classifier-google-colaboratory.html"},"https://www.kdnuggets.com/2018/03/simple-text-classifier-google-colaboratory.html")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/AbeerAbuZayed/Quora-Insincere-Questions-Classification"},"https://github.com/AbeerAbuZayed/Quora-Insincere-Questions-Classification")),(0,s.kt)("li",{parentName:"ul"},"Document Classification: 7 pragmatic approaches for small datasets | Neptune's Blog"),(0,s.kt)("li",{parentName:"ul"},"Multi-label Text Classification using BERT - The Mighty Transformer"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.6-classifying-newswires.ipynb"},"https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.6-classifying-newswires.ipynb")),(0,s.kt)("li",{parentName:"ul"},"XLNet Fine-Tuning Tutorial with PyTorch"),(0,s.kt)("li",{parentName:"ul"},"Text Classification with XLNet in Action"),(0,s.kt)("li",{parentName:"ul"},"AchintyaX/XLNet_Classification_tuning"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/Dkreitzer/Text_ML_Classification_UMN"},"https://github.com/Dkreitzer/Text_ML_Classification_UMN")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/github/markdaoust/models/blob/basic-text-classification/samples/core/get_started/basic_text_classification.ipynb"},"https://colab.research.google.com/github/markdaoust/models/blob/basic-text-classification/samples/core/get_started/basic_text_classification.ipynb")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/how-to-do-text-binary-classification-with-bert-f1348a25d905"},"https://towardsdatascience.com/how-to-do-text-binary-classification-with-bert-f1348a25d905")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/github/tensorflow/hub/blob/master/docs/tutorials/text_classification_with_tf_hub.ipynb"},"https://colab.research.google.com/github/tensorflow/hub/blob/master/docs/tutorials/text_classification_with_tf_hub.ipynb")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/thomas-chauvet/kaggle_toxic_comment_classification"},"https://github.com/thomas-chauvet/kaggle_toxic_comment_classification")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/scionoftech/TextClassification-Vectorization-DL"},"https://github.com/scionoftech/TextClassification-Vectorization-DL")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/netik1020/Concise-iPython-Notebooks-for-Deep-learning/blob/master/Text_Classification/classification_imdb.ipynb"},"https://github.com/netik1020/Concise-iPython-Notebooks-for-Deep-learning/blob/master/Text_Classification/classification_imdb.ipynb")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/getmrinal/ML-Notebook/tree/master/17.%20textClassificationProject"},"https://github.com/getmrinal/ML-Notebook/tree/master/17. textClassificationProject")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/mpuig/textclassification"},"https://github.com/mpuig/textclassification")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/netik1020/Concise-iPython-Notebooks-for-Deep-learning/blob/master/Text_Classification/self_Attn_on_seperate_fets_of_2embds.ipynb"},"https://github.com/netik1020/Concise-iPython-Notebooks-for-Deep-learning/blob/master/Text_Classification/self_Attn_on_seperate_fets_of_2embds.ipynb")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb"},"https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb")),(0,s.kt)("li",{parentName:"ul"},"sgrvinod/a-PyTorch-Tutorial-to-Text-Classification")),(0,s.kt)("h2",{id:"text-embeddings"},"Text Embeddings"),(0,s.kt)("h3",{id:"utilities"},"Utilities"),(0,s.kt)("h4",{id:"averaging-word-vectors-to-create-sentence-vector"},"Averaging word vectors to create sentence vector"),(0,s.kt)("p",null,"In case we already have the vectors for the words in the text, it makes sense to aggregate the word embeddings into a single vector representing the whole text."),(0,s.kt)("p",null,(0,s.kt)("img",{src:a(75026).Z,width:"597",height:"231"})),(0,s.kt)("p",null,"This is a great baseline approach chosen by many practitioners, and probably the one we should take first if we already have the word vectors or can easily obtain them."),(0,s.kt)("p",null,"The most frequent operations for aggregation are:"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"averaging"),(0,s.kt)("li",{parentName:"ol"},"max-pooling")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"def get_mean_vector(words, word2vec_model):\n    ## remove out-of-vocabulary words\n    words = [word for word in words if word in word2vec_model.wv.vocab]\n    if len(words) >= 1:\n        return np.mean(word2vec_model.wv[words], axis=0)\n    else:\n        return []\n")),(0,s.kt)("h3",{id:"word2vec"},"Word2vec"),(0,s.kt)("h4",{id:"training-a-custom-word2vec-cbow-model-with-gensim"},"Training a custom Word2vec CBoW model with Gensim"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"model_cbow = gensim.models.Word2Vec(window=10, min_count=10, workers=2, size=100)\nmodel_cbow.build_vocab(df.tokens.tolist())\nmodel_cbow.train(df.tokens.tolist(), total_examples=model_cbow.corpus_count, epochs=20)\ndf['vecs_w2v_cbow'] = df['tokens'].apply(get_mean_vector, word2vec_model=model_cbow)\ndf.head()\n")),(0,s.kt)("h4",{id:"training-a-custom-word2vec-skipgram-model-with-gensim"},"Training a custom Word2vec SkipGram model with Gensim"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"model_sg = gensim.models.Word2Vec(window=10, min_count=10, workers=2, size=100, sg=1)\nmodel_sg.build_vocab(df.tokens.tolist())\nmodel_sg.train(df.tokens.tolist(), total_examples=model_sg.corpus_count, epochs=20)\ndf['vecs_w2v_sg'] = df['tokens'].apply(get_mean_vector, word2vec_model=model_sg)\ndf.head()\n")),(0,s.kt)("h3",{id:"glove"},"GloVe"),(0,s.kt)("p",null,"Files with the pre-trained vectors Glove can be found in many sites like Kaggle or in the previous link of the Stanford University. We will use the glove.6B.100d.txt file containing the glove vectors trained on the Wikipedia and GigaWord dataset."),(0,s.kt)("p",null,"First we convert the GloVe file containing the word embeddings to the word2vec format for convenience of use. We can do it using the gensim library, a function called glove2word2vec."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-sh"},"wget -O glove.6B.zip -q --show-progress https://nlp.stanford.edu/data/glove.6B.zip\nunzip glove.6B.zip\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"## We just need to run this code once, the function glove2word2vec saves the Glove embeddings in the word2vec format \n## that will be loaded in the next section\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n## glove_input_file = glove_filename\nword2vec_output_file = 'glove.word2vec'\nglove2word2vec('glove.6B.100d.txt', word2vec_output_file)\n")),(0,s.kt)("p",null,"So our vocabulary contains 400K words represented by a feature vector of shape 100. Now we can load the Glove embeddings in word2vec format and then analyze some analogies. In this way if we want to use a pre-trained word2vec embeddings we can simply change the filename and reuse all the code below."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"## load the Stanford GloVe model\nmodel_glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n\ndf['vecs_w2v_glove'] = df['tokens'].apply(get_mean_vector, word2vec_model=model_glove)\ndf.head()\n")),(0,s.kt)("h3",{id:"tf-idf"},"TF-IDF"),(0,s.kt)("p",null,"The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents."),(0,s.kt)("p",null,"Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large."),(0,s.kt)("p",null,"This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms."),(0,s.kt)("p",null,"A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks)."),(0,s.kt)("p",null,"The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.feature_extraction.text import HashingVectorizer\n\n## create the transform\nvectorizer = HashingVectorizer(n_features=100)\n\n## encode document\nvector = vectorizer.transform(df.clean_text.tolist())\n\n## summarize encoded vector\nprint(vector.shape)\n\ndf['vecs_tfidf'] = list(vector.toarray())\ndf.head()\n")),(0,s.kt)("h3",{id:"bert-1"},"BERT"),(0,s.kt)("h4",{id:"sentence-bert"},"Sentence BERT"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"from sentence_transformers import SentenceTransformer\n\nsbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n\ndf['vecs_bert'] = df['clean_text'].progress_apply(sbert_model.encode)\n")),(0,s.kt)("h2",{id:"text-similarity"},"Text Similarity"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823098-80abfe58-bb80-40a1-bced-7e144a61b042.png",alt:"content-concepts-raw-nlp-text-similarity-img"})),(0,s.kt)("h3",{id:"introduction-1"},(0,s.kt)("strong",{parentName:"h3"},"Introduction")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition:")," Text similarity has to determine how 'close' two pieces of text are both in surface closeness (lexical similarity) and meaning (semantic similarity)"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications:")," Duplicate document detection, text clustering, product recommendations"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope:")," No scope decided yet"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools:")," Sentence Transformer Library, Universal Sentence Encoder Model (TFHub), Scikit-learn")),(0,s.kt)("h3",{id:"models-1"},(0,s.kt)("strong",{parentName:"h3"},"Models")),(0,s.kt)("h4",{id:"bert-2"},"BERT"),(0,s.kt)("p",null,"Use transfer learning to fine-tune a BERT encoder. This encoder will work as a feature extractor. e.g.  the most common version of BERT convert any given text into a numeric vector of length 768 (this vector is also known as contextual embedding)."),(0,s.kt)("h4",{id:"bag-of-words-1"},"Bag-of-words"),(0,s.kt)("p",null,"Extract features using models like TF-IDF, CountVectorizer."),(0,s.kt)("h4",{id:"deeprank"},"DeepRank"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1710.05649"},"DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. arXiv, 2017."))),(0,s.kt)("h4",{id:"faiss"},"FAISS"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1702.08734"},"Billion-scale similarity search with GPUs. arXiv, 2017."))),(0,s.kt)("p",null,"Faiss is a library for efficient similarity search and clustering of dense vectors."),(0,s.kt)("h4",{id:"similarity-measures"},"Similarity Measures"),(0,s.kt)("p",null,"L1 (Manhatten distance), L2 (Euclidean distance), Hinge Loss for Triplets."),(0,s.kt)("h3",{id:"process-flow-1"},(0,s.kt)("strong",{parentName:"h3"},"Process flow")),(0,s.kt)("p",null,"Step 1: Collect Text Data"),(0,s.kt)("p",null,"Fetch the raw text dataset into a directory."),(0,s.kt)("p",null,"Step 2: Encoder Training/Fine-tuning"),(0,s.kt)("p",null,"Download the pre-trained models if available (e.g. BERT model) or train the model from scratch (e.g. TF-IDF model). After training/fine-tuning the model, we will save it as a feature extractor for later use."),(0,s.kt)("p",null,"Step 3: Text Vectorization"),(0,s.kt)("p",null,"Now, we will use the encoder (prepared in step 2) to encode the text (prepared in step 1). We will save the feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use."),(0,s.kt)("p",null,"Step 4: Metadata and Indexing"),(0,s.kt)("p",null,"We will assign a unique id to each text document and create dictionaries to locate information of these documents: 1) Document id to document name dictionary, 2) Document id to document feature vector dictionary, and 3) (optional) Document id to metadata product id dictionary. We will also create a Document id to document feature vector indexing. Then we will save these dictionaries and index objects for later use."),(0,s.kt)("p",null,"Step 5: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing. We will receive a text document from the user, encode it with our text encoder, find TopK similar vectors using Indexing object, and retrieve the text documents (and metadata) using dictionaries. We send these documents (and metadata) back to the user."),(0,s.kt)("p",null,"Step 6: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement."),(0,s.kt)("p",null,"Step 7: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client."),(0,s.kt)("h3",{id:"use-cases-1"},"Use Cases"),(0,s.kt)("h4",{id:"semantic-relation-estimation"},"Semantic Relation Estimation"),(0,s.kt)("p",null,"To maintain a level of coherence and similarity among various letters and speeches, a model was built that will help in assessing this document similarity. In approach 1, TF-IDF with Latent semantic indexing was used to extract features and cosine similarity as the distance metric. In approach 2, BERT with PCA was used for feature extraction and 3 distance measures - L1, L2, and cosine, for similarity calculation. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Semantic-Similarity-085a0be7e70f4dec99a06c07cca3b12c"},"this")," notion."),(0,s.kt)("h4",{id:"finding-hardware-parts-in-warehouse"},"Finding Hardware Parts in Warehouse"),(0,s.kt)("p",null,"There are millions of hardware items (e.g. 0.5mm steel wire grade q195) in the warehouse and customer generally asks for items in natural language (e.g. grade195 steel wire with 0.5mm thickness). A text similarity system was built using an ensemble of 3 Bag-of-words based Count vectorizer model with different types of tokenization process and n-gram range. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Finding-Hardware-Parts-in-Warehouse-922EE-fa28cf6931a94fa89ad2d4d2183f6bcc"},"this")," notion."),(0,s.kt)("h4",{id:"image--text-similarity"},"Image + Text Similarity"),(0,s.kt)("p",null,"Use the textual details and images of products, find the exact similar product among different groups. Around 35 GB of retail product images was scraped and used to build the system. Checkout the notion ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Image-Text-Similarity-fe5130324ae14ab48a30c93444348f4a"},"here"),"."),(0,s.kt)("h4",{id:"text-recommendation"},"Text Recommendation"),(0,s.kt)("p",null,"For the given BRM text, recommend top-5 GAO text. We used universal sentence encoder to encode the text and calculated cosine similarity within group. Then an item-based recommender model was used to find most suitable top-K candidates in GAO based on the interaction history. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Text-Recommendation-System-351c57bdb60e40da8531bf19c867314a"},"this")," notion."),(0,s.kt)("h2",{id:"topic-modeling"},"Topic Modeling"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823108-f57ec710-01e0-4f24-9696-d9c2dcb32cc8.png",alt:"content-concepts-raw-nlp-topic-modeling-img"})),(0,s.kt)("h3",{id:"introduction-2"},(0,s.kt)("strong",{parentName:"h3"},"Introduction")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition:")," Topic modeling is an unsupervised machine learning technique that\u2019s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications:")," Identify emerging themes and topics"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope:")," No scope decided yet"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools:")," Gensim")),(0,s.kt)("h3",{id:"models-2"},"Models"),(0,s.kt)("h4",{id:"lsa"},"LSA"),(0,s.kt)("p",null,"The core idea is to take a matrix of what we have \u2014 documents and terms \u2014 and decompose it into a separate document-topic matrix and a topic-term matrix."),(0,s.kt)("h4",{id:"process-flow-2"},"Process flow"),(0,s.kt)("p",null,"Step 1: Collect Text Data"),(0,s.kt)("p",null,"Fetch from database, scrap from the internet or use public datasets. Setup the database connection and fetch the data into python environment."),(0,s.kt)("p",null,"Step 2: Data Preparation"),(0,s.kt)("p",null,"Explore the data, validate it and create preprocessing strategy. Clean the data and make it ready for modeling."),(0,s.kt)("p",null,"Step 3: Model Building"),(0,s.kt)("p",null,"Start the training process and track the progress and experiments. Validate the final set of models and select/assemble the final model."),(0,s.kt)("p",null,"Step 4: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing"),(0,s.kt)("p",null,"Step 5: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement"),(0,s.kt)("p",null,"Step 6: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client"),(0,s.kt)("h3",{id:"use-cases-2"},"Use Cases"),(0,s.kt)("h4",{id:"identify-themes-and-emerging-issues-in-servicenow-incident-tickets"},"Identify Themes and Emerging Issues in ServiceNow Incident Tickets"),(0,s.kt)("p",null,"Extracted key phrases from the incident ticket descriptions and trained an LSA topic model on these phrases to identify emerging themes and incident topics. This enabled a proactive approach to manage and contain the issues and thus increasing CSAT. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/ServiceNow-Advanced-Analytics-1D5F3-f35e7f17377544c8b11cdf624e5da800"},"this")," notion."),(0,s.kt)("h4",{id:"it-support-ticket-management"},"IT Support Ticket Management"),(0,s.kt)("p",null,"In Helpdesk, almost 30\u201340% of incident tickets are not routed to the right team and the tickets keep roaming around and around and by the time it reaches the right team, the issue might have widespread and reached the top management inviting a lot of trouble. To solve this issue, we built a system with 6 deliverables: Key Phrase Analysis, Topic Modeling, Ticket Classification, Trend, Seasonality and Outlier Analysis, PowerBI Dashboard to visually represent the KPIs and dividing tickets into standard vs. non-standard template responses. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/ESMCafe-IT-Support-Ticket-Management-69965830d39d486194f9a2f1222a81d8"},"this")," notion."),(0,s.kt)("h2",{id:"chatbot"},"Chatbot"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823087-2286659a-d2fd-4b8c-a6a2-f4bf512f2e74.png",alt:"content-concepts-raw-nlp-chatbot-img"})),(0,s.kt)("h3",{id:"introduction-3"},(0,s.kt)("strong",{parentName:"h3"},"Introduction")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition:")," This product will automate tasks and handle conversations with the user."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications:")," Customer support, Product suggestion, Interactive FAQ, Form filling, Question Answering"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope:")," Chat and Voice Support, FAQ, Knowledge-based and Contextual bot"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools:")," DialogFlow, RASA, DeepPavlov, Alexa Skill, HuggingFace, ParlAI")),(0,s.kt)("h3",{id:"models-3"},(0,s.kt)("strong",{parentName:"h3"},"Models")),(0,s.kt)("h4",{id:"rasa-chatbot"},"RASA Chatbot"),(0,s.kt)("p",null,"RASA supports contextual conversational AI. It provided an integrated framework for Natural language understanding, dialogue generation and management. It also supprots multiple endpoints (e.g. Facebook messenger, WhatsApp) for easy deployment."),(0,s.kt)("h4",{id:"dialogflow-chatbot"},"DialogFlow Chatbot"),(0,s.kt)("p",null,"It is an API to easily create and deploy chatbots. It also supports Voice interaction via Google cloud Voice API."),(0,s.kt)("h4",{id:"alexa-skill"},"Alexa Skill"),(0,s.kt)("p",null,"This API enable us to create an alexa skill that can be used via alexa services. This also supports voice interaction via Alexa Voice API."),(0,s.kt)("h3",{id:"process-flow-3"},(0,s.kt)("strong",{parentName:"h3"},"Process flow")),(0,s.kt)("p",null,"Step 1: Create As-Is Process"),(0,s.kt)("p",null,"Create the current process"),(0,s.kt)("p",null,"Step 2: Propose To-Be Process"),(0,s.kt)("p",null,"Creeate the to-be process in which chatbot will handle the conversations in collaboration with human in the loop or on fully automated basis"),(0,s.kt)("p",null,"Step 3: Collect the training data"),(0,s.kt)("p",null,"Collect or create the training data and example conversations for training the chatbot"),(0,s.kt)("p",null,"Step 6: Chatbot Training"),(0,s.kt)("p",null,"Train the chatbot model"),(0,s.kt)("p",null,"Step 9: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing"),(0,s.kt)("p",null,"Step 10: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement"),(0,s.kt)("p",null,"Step 11: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client"),(0,s.kt)("h3",{id:"use-cases-3"},(0,s.kt)("strong",{parentName:"h3"},"Use Cases")),(0,s.kt)("h4",{id:"rasa-chatbot-1"},"RASA Chatbot"),(0,s.kt)("p",null,"Categorization of services and selected 4 most usable services for automation process. Development of a text-based chatbot application for this automation. RASA framework (python) was selected for implementation of this chatbot. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Insurance-Chatbot-d39fb9b575f5470d8be08cb7f2a12994"},"this")," notion."),(0,s.kt)("h4",{id:"insurance-voicebot"},"Insurance Voicebot"),(0,s.kt)("p",null,"Automate the low-skill contact center services with the help of Voicebot AI technology. Context - Insurance Contact Centre, Role - A virtual customer care representative, Skills \u2013 Claim status, Language \u2013 English (US), Technology \u2013 Voice-enabled Goal-oriented Conversational AI Agents (Voicebots). Modules - Dialogflow Voicebot, Alexa Skill Voicebot, Rasa with 3rd-party Voice API, Rasa powered Alexa skill, Rasa powered Google assistant, Rasa voicebot with Mozilla tools, and DeepPavlov Voicebot. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Insurance-Voicebot-0F2A3-b8eaf980a04840a6945a076d878c107a"},"this")," notion."),(0,s.kt)("h4",{id:"wellness-tracker"},"Wellness Tracker"),(0,s.kt)("p",null,"A bot that logs daily wellness data to a spreadsheet (using the Airtable API), to help the user keep track of their health goals. Connect the assistant to a messaging channel\u2014Twilio\u2014so users can talk to the assistant via text message and Whatsapp. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Wellness-Tracker-Chatbot-c867f589d0f14ec98a7c0cd9e58fec2a"},"this")," notion."),(0,s.kt)("h4",{id:"rasa-chatbot-experiments"},"RASA Chatbot Experiments"),(0,s.kt)("p",null,"Experiment with 4 chatbots in RASA: Financial Bot - A chatbot demonstrating how to build AI assistants for financial services and banking, Movie Bot - A bot to book movie tickets, Cricket Bot - A bot that will bring the live info about IPL cricket match as per user query, and Pokedex - This is a demonstration of a digital assistant that can answer questions about pokemon. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/RASA-a698cb020e9e451f9200437586f1444e"},"this")," notion."),(0,s.kt)("h2",{id:"named-entity-recognition"},"Named Entity Recognition"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823095-cb1f05c0-8d3e-49f5-888c-b3972bd741de.png",alt:"content-concepts-raw-nlp-named-entity-recognition-img"})),(0,s.kt)("h3",{id:"introduction-4"},(0,s.kt)("strong",{parentName:"h3"},"Introduction")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition:")," NER models classify each word/phrase in the document into a pre-defined category. In other words, these models identify named entities (classes/labels) in the given text document"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications:")," Opinion mining, Affinity towards brands"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope:")," No scope decided yet"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools:")," Doccano, Flair, Spacy, HuggingFace Transformer Library")),(0,s.kt)("h3",{id:"models-4"},(0,s.kt)("strong",{parentName:"h3"},"Models")),(0,s.kt)("h4",{id:"flair-ner"},"Flair-NER"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://www.aclweb.org/anthology/N19-1078/"},"Pooled Contextualized Embeddings for Named Entity Recognition. ACL, 2019."))),(0,s.kt)("p",null,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. This model achieves an F1 score of 93.2 on the CoNLL-03 dataset."),(0,s.kt)("h4",{id:"spacy-ner"},"Spacy-NER"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://spacy.io/universe/project/video-spacys-ner-model"},"Incremental parsing with bloom embeddings and residual CNNs."))),(0,s.kt)("p",null,'spaCy v2.0\'s Named Entity Recognition system features a sophisticated word embedding strategy using subword features and "Bloom" embeddings, a deep convolutional neural network with residual connections, and a novel transition-based approach to named entity parsing.'),(0,s.kt)("h4",{id:"transformer-ner"},"Transformer-NER"),(0,s.kt)("p",null,"Fine-tuning of transformer based models like BERT, Roberta and Electra."),(0,s.kt)("h3",{id:"process-flow-4"},(0,s.kt)("strong",{parentName:"h3"},"Process flow")),(0,s.kt)("p",null,"Step 1: Collect Text Data"),(0,s.kt)("p",null,"Fetch the raw text dataset into a directory."),(0,s.kt)("p",null,"Step 2: Create Labels"),(0,s.kt)("p",null,"Use open-source tools like Doccano or paid tools like Prodigy to annotate the entities."),(0,s.kt)("p",null,"Step 3: Model Training & Validation"),(0,s.kt)("p",null,"Train the NER model and validate it."),(0,s.kt)("p",null,"Step 4: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing. We will receive a text document from the user, encode it with our text encoder, find TopK similar vectors using Indexing object, and retrieve the text documents (and metadata) using dictionaries. We send these documents (and metadata) back to the user."),(0,s.kt)("p",null,"Step 5: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement."),(0,s.kt)("p",null,"Step 6: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client."),(0,s.kt)("h3",{id:"use-cases-4"},(0,s.kt)("strong",{parentName:"h3"},"Use Cases")),(0,s.kt)("h4",{id:"name-and-address-parsing"},"Name and Address Parsing"),(0,s.kt)("p",null,"Parse names (person ","[first, middle and last name]",", household or corporation) and address (street, city, state, country, zip) from the given text. We used Doccano for annotation and trained a Flair NER model on GPU. Check out this ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Name-Address-Parsing-209653cc37d2413f9b6e902712338ed4"},"notion"),"."),(0,s.kt)("h4",{id:"ner-methods-experiment"},"NER Methods Experiment"),(0,s.kt)("p",null,"Data is extracted from GMB(Groningen Meaning Bank) corpus and annotated using BIO scheme. 10 different NER models were trained and compared on this dataset. Frequency based tagging model was taken as the baseline. Classification, CRF, LSTM, LSTM-CRF, Char-LSTM, Residual-LSTM-ELMo, BERT tagger, Spacy tagger and an interpretable tagger with keras and LIME were trained. Checkout ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Multiple-methods-NER-319c0e2cc2b74008a931b849377557d1"},"this")," notion."),(0,s.kt)("h2",{id:"text-summarization"},"Text Summarization"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823104-f5413836-8502-4cc1-9924-9b391e1677c6.png",alt:"content-concepts-raw-nlp-text-summarization-untitled"})),(0,s.kt)("p",null,"Automatic Text Summarization gained attention as early as the 1950\u2019s. A\xa0",(0,s.kt)("a",{parentName:"p",href:"http://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf"},"research paper"),", published by Hans Peter Luhn in the late 1950s, titled \u201cThe automatic creation of literature abstracts\u201d, used features such as word frequency and phrase frequency to extract important sentences from the text for summarization purposes."),(0,s.kt)("p",null,"Another important\xa0",(0,s.kt)("a",{parentName:"p",href:"http://courses.ischool.berkeley.edu/i256/f06/papers/edmonson69.pdf"},"research"),", done by Harold P Edmundson in the late 1960\u2019s, used methods like the presence of cue words, words used in the title appearing in the text, and the location of sentences, to extract significant sentences for text summarization. Since then, many important and exciting studies have been published to address the challenge of automatic text summarization."),(0,s.kt)("p",null,"Text summarization can broadly be divided into two categories \u2014\xa0",(0,s.kt)("strong",{parentName:"p"},"Extractive Summarization"),"\xa0and\xa0",(0,s.kt)("strong",{parentName:"p"},"Abstractive Summarization"),"."),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("strong",{parentName:"li"},"Extractive Summarization:"),"\xa0These methods rely on extracting several parts, such as phrases and sentences, from a piece of text and stack them together to create a summary. Therefore, identifying the right sentences for summarization is of utmost importance in an extractive method."),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("strong",{parentName:"li"},"Abstractive Summarization:"),"\xa0These methods use advanced NLP techniques to generate an entirely new summary. Some parts of this summary may not even appear in the original text.")),(0,s.kt)("h3",{id:"introduction-5"},"Introduction"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition:")," ",(0,s.kt)("em",{parentName:"li"},"To take the appropriate action, we need the latest information, but on the contrary, the amount of information is more and more growing. Making an automatic & accurate summaries feature will helps us to understand the topics and shorten the time to do it.")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications:")," News Summarization, Social media Summarization, Entity timelines, Storylines of event, Domain specific summaries, Sentence Compression, Event understanding, Summarization of user-generated content"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope:")," Extractive and Abstractive summary",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"Single document summarization: summary = summarize(document)"),(0,s.kt)("li",{parentName:"ul"},"Multi-document summarization: summary = summarize(document_1, document_2, ...)"),(0,s.kt)("li",{parentName:"ul"},"Query focused summarization: summary = summarize(document, query)"),(0,s.kt)("li",{parentName:"ul"},"Update summarization: summary = summarize(document, previous_document_or_summary)"))),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools:")," HuggingFace Transformer Library")),(0,s.kt)("h3",{id:"scheduled-sampling"},"Scheduled sampling"),(0,s.kt)("p",null,"In the inference (testing) phase, the model only depends on the previous step, which means that it totally depends on itself. The problem actually arises when the model results in a bad output in (t-1) (i.e. the previous time step results in a bad output). This would actually affect all the coming sequences. It would lead the model to an entirely different state space from where it has seen and trained on in the training phase, so it simply won\u2019t be able to know what to do. A solution to this problem that has been suggested by\xa0",(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1506.03099"},"bengio et ai"),"\xa0from google research, was to gradually change the reliance of the model from being totally dependent on the ground truth being supplied to it to depending on itself (i.e. depend on only its previous tokens generated from previous time steps in the decoder). The concept of making the learning path difficult through time (i.e. making the model depends on only itself) is called curriculum learning. Their technique to implement this was truly genius. They call it \u2018",(0,s.kt)("strong",{parentName:"p"},"scheduled sampling\u2019.")),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823101-094f1ba6-cb4f-477d-b968-ccaa83c2cb0a.png",alt:"content-concepts-raw-nlp-text-summarization-image_(4)"})),(0,s.kt)("p",null,"Landscape of seq2seq models for neural abstractive text summarization"),(0,s.kt)("h3",{id:"models-5"},"Models"),(0,s.kt)("h4",{id:"prophetnet"},"ProphetNet"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/2001.04063"},"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training. arXiv, 2020."))),(0,s.kt)("h4",{id:"pegasus"},"PEGASUS"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1912.08777"},"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. arXiv, 2019."))),(0,s.kt)("h4",{id:"bertsum"},"BERTSum"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/pdf/1903.10318.pdf"},"Fine-tune BERT for Extractive Summarization. arXiv, 2019."))),(0,s.kt)("h4",{id:"seq2seq-pointergenerator"},"Seq2Seq PointerGenerator"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1704.04368v2"},"Get To The Point: Summarization with Pointer-Generator Networks. arXiv, 2017."))),(0,s.kt)("h3",{id:"process-flow-5"},"Process flow"),(0,s.kt)("p",null,"Step 1: Collect Text Data"),(0,s.kt)("p",null,"Fetch the raw text dataset into a directory."),(0,s.kt)("p",null,"Step 2: Create Labels"),(0,s.kt)("p",null,"Step 3: Model Training & Validation"),(0,s.kt)("p",null,"Step 4: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing. We will receive a text document from the user, encode it with our text encoder, find TopK similar vectors using Indexing object, and retrieve the text documents (and metadata) using dictionaries. We send these documents (and metadata) back to the user."),(0,s.kt)("p",null,"Step 5: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement."),(0,s.kt)("p",null,"Step 6: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client."),(0,s.kt)("h3",{id:"use-cases-5"},"Use Cases"),(0,s.kt)("h4",{id:"enron-email-summarization"},"Enron Email Summarization"),(0,s.kt)("p",null,"Email overload can be a difficult problem to manage for both work and personal email inboxes. With the average office worker receiving between 40 to 90 emails a day, it has become difficult to extract the most important information in an optimal amount of time. A system that can create concise and coherent summaries of all emails received within a timeframe can reclaim a large amount of time. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Enron-Email-Summarization-d137f618b4c5445fb595714fdc30c68d"},"this")," notion."),(0,s.kt)("h4",{id:"pdf-summarization-over-mail"},"PDF Summarization over mail"),(0,s.kt)("p",null,"Built a system that will receive a pdf over outlook mail and create a word cloud for this pdf. Then, send this word cloud back as an attachment to that email. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/PDF-to-Wordcloud-via-Mail-b7ae38d0e95e439eb68194b66bdcb889"},"this")," notion."),(0,s.kt)("h4",{id:"bart-text-summarization"},"BART Text Summarization"),(0,s.kt)("p",null,"Document text summarization using the BART transformer model and visual API using the Plotly Dash app. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/BART-Text-Summarization-on-Plotly-Dash-f023de73b80c43bf9856a5ecbf24398b"},"this")," notion."),(0,s.kt)("h4",{id:"transformers-summarization-experiment"},"Transformers Summarization Experiment"),(0,s.kt)("p",null,"Experiment with various transformers for text summarization using HuggingFace library. Summarization of 4 books. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/HuggingFace-Transformers-based-Summarization-082d7af9c97944188f735c761cd27b9b"},"this")," notion."),(0,s.kt)("h4",{id:"cnn-dailymail-and-inshorts-news-summarization"},"CNN-DailyMail and InShorts News Summarization"),(0,s.kt)("p",null,"Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/CNN-DailyMail-News-Summarization-f831bc810195478985aa6356bd43465a"},"this")," notion for CNN-DailyMail and ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/InShorts-News-Summarization-daeb6fe3aa924a24a88657937ab43145"},"this")," one for InShorts."),(0,s.kt)("h4",{id:"transformers-summarization-experiment-1"},"Transformers Summarization Experiment"),(0,s.kt)("p",null,"Experiment with various transformers for text summarization using HuggingFace library. Summarization of 4 books. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/HuggingFace-Transformers-based-Summarization-082d7af9c97944188f735c761cd27b9b"},"this")," notion."),(0,s.kt)("h4",{id:"covid-19-article-summarization"},"Covid-19 article summarization"),(0,s.kt)("p",null,"Used BERT and GPT-2 for article summarization related to covid-19. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Text-Summarization-of-Articles-ec756ff596614f29b6896927e87609b1"},"this")," notion."),(0,s.kt)("h3",{id:"common-applications-1"},"Common Applications"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"News Summarization: The summarization systems leverage the power of multi-document\nsummarization techniques in order to summarize the news coming from various sources.\nIt generates a compact summary that is informative and non-redundant. Methods\nused can be extractive or abstractive. For example, the \u2019Inshort\u2019 application generates a sixty-word summary of news articles."),(0,s.kt)("li",{parentName:"ul"},"Social media Summarization: deals with the summarization of social media text such as:\ntweets, blogs, community forums, etc. These summarization systems are built keeping in\nmind the needs of the user and are dependent on the genre of social media text. Tweets\nsummarization system will be different from blog summarization systems as tweets have\na short text (140 characters) and are often noisy. While the blog has considerably longer\nlength text with a different writing style."),(0,s.kt)("li",{parentName:"ul"},"Entity timelines: The system generates the summary of most salient events related to\nan entity within a given timeline. The news collections from various news sources are\ncollected over a period of time. Each of these news collections define a main event. For\na given entity, these news collection are identified and ranked in order of importance.\nMost salient sentences with respect to the given entity is selected from each of these news\ncluster to finally generate an entity timeline."),(0,s.kt)("li",{parentName:"ul"},"Storylines of event: deals with identifying and summarization of events that leads\nto event of interest. It helps in providing background information about an event and\nstructured timeline for an entity. The news collections from various news sources are\ncollected over a period of time. Each of these news collections define a main even. A\ngraph of events (news collection) is defined using similarity, then heaviest path ending in\na given event is identified and finally, the events on this path related to salient entities in\ntarget event are summarized to obtain storylines of the event."),(0,s.kt)("li",{parentName:"ul"},"Domain specific summaries: Summarization systems are often used in generating\ndomain specific summaries. These systems are designed in accordance with the needs of\nthe user for a specific domain. For example: legal document summarization deals with\ngenerating summary out of a legal/law documents, medical report summarization has aim\nof generating a summary form a patient report history such that it includes all important\nclinical events in order of timeline."),(0,s.kt)("li",{parentName:"ul"},"Sentence Compression: generates a short version of a longer sentence. The system\nis trained using a parallel corpus containing headlines of news articles and first sentence\nof the same article. The headline is assumed to be shorter version of the first sentence.\nRecent works based on abstractive neural network approaches has proven to generate high\nquality compressed sentences."),(0,s.kt)("li",{parentName:"ul"},"Event understanding: is understanding the way events are referred to in the text and\nrepresenting these event mentioning text in predicate argument structure. For example:\nMichael marries Sara is represented as ","[actor]"," marries ","[actor]",". It is very helpful for semiautomatically updation of knowledge graphs and also for the task of generating headlines\n(abstractive summarization)."),(0,s.kt)("li",{parentName:"ul"},"Summarization of usergenerated content: deals with summarizing user generated\ncontents like youtube comments, reviews of products, opinions etc. compact version of\nreviews and comment are very helpful in identifying overall sentiment of mass towards\na particular product or topic. Which are often used by the consumers as well as the\nplatform itself in recommending products.")),(0,s.kt)("h3",{id:"variations"},"Variations"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Single document summarization: ",(0,s.kt)("em",{parentName:"li"},"summary = summarize(document)")),(0,s.kt)("li",{parentName:"ul"},"Multi-document summarization: ",(0,s.kt)("em",{parentName:"li"},"summary = summarize(document_1, document_2, ...)")),(0,s.kt)("li",{parentName:"ul"},"Query focused summarization: ",(0,s.kt)("em",{parentName:"li"},"summary = summarize(document, query)")),(0,s.kt)("li",{parentName:"ul"},"Update summarization: ",(0,s.kt)("em",{parentName:"li"},"summary = summarize(document, previous_document_or_summary)"))),(0,s.kt)("p",null,'Basically, we can regard the "summarization" as the "function" its input is document and output is summary. And its input & output type helps us to categorize the multiple summarization tasks.'),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Single document summarization",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("em",{parentName:"li"},"summary = summarize(document)")))),(0,s.kt)("li",{parentName:"ul"},"Multi-document summarization",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("em",{parentName:"li"},"summary = summarize(document_1, document_2, ...)"))))),(0,s.kt)("p",null,"We can take the query to add the viewpoint of summarization."),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Query focused summarization",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("em",{parentName:"li"},"summary = summarize(document, query)"))))),(0,s.kt)("p",null,'This type of summarization is called "Query focused summarization" on the contrary to the "Generic summarization". Especially, a type that set the viewpoint to the "difference" (update) is called "Update summarization".'),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Update summarization",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("em",{parentName:"li"},"summary = summarize(document, previous_document_or_summary)"))))),(0,s.kt)("p",null,"And the\xa0",(0,s.kt)("em",{parentName:"p"},'"summary"'),"\xa0itself has some variety."),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Indicative summary",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"It looks like a summary of the book. This summary describes what kinds of the story, but not tell all of the stories especially its ends (so indicative summary has only partial information)."))),(0,s.kt)("li",{parentName:"ul"},"Informative summary",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"In contrast to the indicative summary, the informative summary includes full information of the document."))),(0,s.kt)("li",{parentName:"ul"},"Keyword summary",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"Not the text, but the words or phrases from the input document."))),(0,s.kt)("li",{parentName:"ul"},"Headline summary",(0,s.kt)("ul",{parentName:"li"},(0,s.kt)("li",{parentName:"ul"},"Only one line summary.")))),(0,s.kt)("h3",{id:"summary-variations"},"Summary Variations"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Indicative summary: It looks like a summary of the book. This summary describes what kinds of the story, but not tell all of the stories especially its ends (so indicative summary has only partial information)."),(0,s.kt)("li",{parentName:"ul"},"Informative summary: In contrast to the indicative summary, the informative summary includes full information of the document."),(0,s.kt)("li",{parentName:"ul"},"Keyword summary: Not the text, but the words or phrases from the input document."),(0,s.kt)("li",{parentName:"ul"},"Headline summary: Only one line summary.")),(0,s.kt)("h3",{id:"seq2seq"},"Seq2Seq"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823103-14b70635-485d-43fa-b08c-b2dbb99dff04.png",alt:"content-concepts-raw-nlp-text-summarization-untitled-1"})),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Seq2seq - mainly bi-LSTM for encoder and attention mechanism for decoder network")),(0,s.kt)("h3",{id:"pointer-generator"},"Pointer Generator"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823099-408a7f2f-f929-402d-9110-4e90f545b665.png",alt:"content-concepts-raw-nlp-text-summarization-image_(3)"})),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://miro.medium.com/max/1400/0*vJgFcKRJpdN1sO2Z.png",alt:"https://miro.medium.com/max/1400/0*vJgFcKRJpdN1sO2Z.png"})),(0,s.kt)("p",null,"Researchers found 2 main problems with the seq2seq model, as discussed in this truly ",(0,s.kt)("a",{parentName:"p",href:"http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html"},"amazing blog"),", which is 1) the inability of the network to copy facts and 2) repetition of words. The pointer generator network (with coverage mechanism) tried to address these problems."),(0,s.kt)("h3",{id:"experiments"},"Experiments"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/theamrzaki/text_summurization_abstractive_methods"},"theamrzaki/text_summurization_abstractive_methods")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/dongjun-Lee/text-summarization-tensorflow"},"dongjun-Lee/text-summarization-tensorflow")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/nikhilcss97/Text-Summarization"},"nikhilcss97/Text-Summarization")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/glopasso/text-summarization"},"glopasso/text-summarization")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://www.dlology.com/blog/tutorial-summarizing-text-with-amazon-reviews/"},"How to Summarize Amazon Reviews with Tensorflow")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/pytorch/fairseq"},"pytorch/fairseq")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/tshi04/LeafNATS"},"tshi04/LeafNATS")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1iAIFX1QQiFm1F01vMmnAgFh4oH1H-K8W"},"Text summarisation with BART ","&"," T5 using HuggingFace")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/1PlWjOQ9IV-MAtoZuAFfgccLKg_Uu-ufo#scrollTo=m85bgroRzV3_"},"TextRank, Summy and BERT Summarizer")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/755dfc1a-4752-44a5-bc21-49aa5c80adb7/TED_Talk_Tag_Generator_and_Summarizer___by_Qi_Haodi___Jul_2020___Medium.html?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAT73L2G45O3KS52Y5%2F20201014%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20201014T060921Z&X-Amz-Expires=86400&X-Amz-Signature=82cab92ef127ef9a6122b243ef2f050e3db4282f524c9eaea9f8307af8699624&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22TED%2520Talk%2520Tag%2520Generator%2520and%2520Summarizer%2520_%2520by%2520Qi%2520Haodi%2520_%2520Jul%252C%25202020%2520_%2520Medium.html%22"},"TED Talk Tag Generator and Summarizer"))),(0,s.kt)("h3",{id:"benchmark-datasets"},"Benchmark datasets"),(0,s.kt)("h4",{id:"bigpatent"},"BigPatent"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1906.03741.pdf"})),(0,s.kt)("p",null,"Most existing text summarization datasets are compiled from the news domain, where summaries have a flattened discourse structure. In such datasets, summary-worthy content often appears in the beginning of input articles. Moreover, large segments from input articles are present verbatim in their respective summaries. These issues impede the learning and evaluation of systems that can understand an article\u2019s global content structure as well as produce abstractive summaries with high compression ratio. In this work, we present a novel dataset, BIGPATENT, consisting of 1.3 million records of U.S. patent documents along with human written abstractive summaries."),(0,s.kt)("p",null,"Compared to existing summarization datasets, BigPatent has the following properties:"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"summaries contain richer discourse structure with more recurring entities,"),(0,s.kt)("li",{parentName:"ol"},"salient content is evenly distributed in the input, and"),(0,s.kt)("li",{parentName:"ol"},"lesser and shorter extractive fragments present in the summaries.")),(0,s.kt)("p",null,"We train and evaluate baselines and popular learning models on BIGPATENT to shed light on new challenges and motivate future directions for summarization research."),(0,s.kt)("h4",{id:"cnn-and-dailymail"},"CNN and DailyMail"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://cs.nyu.edu/~kcho/DMQA/"},"DMQA")),(0,s.kt)("p",null,"CNN daily mail dataset consists of long news articles(an average of ~800 words). It consists of both articles and summaries of those articles. Some of the articles have multi line summaries also. We have used this dataset in our Pointer Generator model."),(0,s.kt)("p",null,"CNN dataset contains the documents from the news articles of CNN. There are approximately 90k documents/stories. DM dataset contains the documents from the news articles of Daily Mail. There are approximately 197k documents/stories. OTHR contains a few thousands stories scraped from 4 news websites."),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"CNN/DM - the news body is used as the input for our model , while the header would be used as the summary target output."),(0,s.kt)("li",{parentName:"ul"},"Data can be acquired from this ",(0,s.kt)("a",{parentName:"li",href:"https://drive.google.com/drive/folders/1Izsbg_p1s52dFNh8NmSG5jmDtRgHcLUN"},"gdrive"),".")),(0,s.kt)("h4",{id:"amazon-fine-food-reviews"},"Amazon Fine Food Reviews"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://www.kaggle.com/snap/amazon-fine-food-reviews"},"https://www.kaggle.com/snap/amazon-fine-food-reviews")),(0,s.kt)("h4",{id:"duc-2014"},"DUC-2014"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"http://duc.nist.gov/data.html"},"DUC-2014 dataset")," that involves generating approximately 14-word summaries for 500 news articles. The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes.")),(0,s.kt)("h4",{id:"gigaword"},"Gigaword"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://catalog.ldc.upenn.edu/LDC2012T21"},"Annotated English Gigaword")),(0,s.kt)("p",null,"It is popularly known as GIGAWORLD dataset and contains nearly ten million documents (over four billion words) of the original English Gigaword Fifth Edition. It consists of articles and their headlines. We have used this dataset to train our Abstractive summarization model."),(0,s.kt)("h4",{id:"cornell-newsroom"},"Cornell Newsroom"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"http://lil.nlp.cornell.edu/newsroom/"},"Cornell Newsroom Summarization Dataset")),(0,s.kt)("h4",{id:"opinosis"},"Opinosis"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"http://kavita-ganesan.com/opinosis-opinion-dataset/#.Xs0trf8za01"},"Opinosis Dataset - Topic related review sentences | Kavita Ganesan")),(0,s.kt)("p",null,"This dataset contains sentences extracted from user reviews on a given topic. Example topics are \u201cperformance of Toyota Camry\u201d and \u201csound quality of ipod nano\u201d, etc. The reviews were obtained from various sources \u2014 Tripadvisor (hotels), Edmunds.com (cars) and amazon.com (various electronics).Each article in the dataset has 5 manually written \u201cgold\u201d summaries. This dataset was used to score the results of the abstractive summarization model."),(0,s.kt)("h3",{id:"evaluation-metrics"},"Evaluation metrics"),(0,s.kt)("h4",{id:"rogue"},"ROGUE"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/07/was2004.pdf"})),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1803.01937v1.pdf"})),(0,s.kt)("p",null,'Rouge-N is a word N-gram count that matche between the model and the gold summary. It is similart to the "recall" because it evaluates the covering rate of gold summary, and not consider the not included n-gram in it.'),(0,s.kt)("p",null,'ROUGE-1 and ROUGE-2 is usually used. The ROUGE-1 means word base, so its order is not regarded. So "apple pen" and "pen apple" is same ROUGE-1 score. But if ROUGE-2, "apple pen" becomes single entity so "apple pen" and "pen apple" does not match. If you increase the ROUGE-"N" count, finally evaluates completely match or not.'),(0,s.kt)("h4",{id:"bleu"},"BLEU"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://www.aclweb.org/anthology/P02-1040.pdf"})),(0,s.kt)("p",null,'BLEU is a modified form of "precision", that used in machine translation evaluation usually. BLEU is basically calculated on the n-gram co-occerance between the generated summary and the gold (You don\'t need to specify the "n" unlike ROUGE).'),(0,s.kt)("h3",{id:"library"},"Library"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/chakki-works/sumeval"},"chakki-works/sumeval")),(0,s.kt)("h3",{id:"awesome-list"},"Awesome List"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/mathsyouth/awesome-text-summarization"},"mathsyouth/awesome-text-summarization")),(0,s.kt)("h3",{id:"references"},"References"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1812.02303v3"},"Neural Abstractive Text Summarization with Sequence-to-Sequence Models: A Survey")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1804.04589"},"A Survey on Neural Network-Based Summarization Methods")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/a-neural-attention-model-for-abstractive"},"Papers with Code - A Neural Attention Model for Abstractive Sentence Summarization")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://ai.googleblog.com/2016/08/text-summarization-with-tensorflow.html"},"Text summarization with TensorFlow")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/get-to-the-point-summarization-with-pointer"},"Papers with Code - Get To The Point: Summarization with Pointer-Generator Networks")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1602.06023"},"Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1705.04304"},"A Deep Reinforced Model for Abstractive Summarization")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1803.11070v1"},"Actor-Critic based Training Framework for Abstractive Summarization")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/unified-language-model-pre-training-for"},"Papers with Code - Unified Language Model Pre-training for Natural Language Understanding and Generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/mass-masked-sequence-to-sequence-pre-training"},"Papers with Code - MASS: Masked Sequence to Sequence Pre-training for Language Generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/deep-reinforcement-learning-for-sequence-to"},"Papers with Code - Deep Reinforcement Learning For Sequence to Sequence Models")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/text-summarization-with-pretrained-encoders"},"Papers with Code - Text Summarization with Pretrained Encoders")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/prophetnet-predicting-future-n-gram-for"},"Papers with Code - ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1911.02247v2.pdf"})),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html"},"Taming Recurrent Neural Networks for Better Summarization")," ","[\u2b50]"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/mathsyouth/awesome-text-summarization"},"Awesome Text Summarization")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://ai.googleblog.com/2016/08/text-summarization-with-tensorflow.html"},"Text summarization with TensorFlow, Google AI Blog 2016"))),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/sourcecode369/deep-natural-language-processing/blob/master/text%20summarization/summarization.md"},"sourcecode369/deep-natural-language-processing")),(0,s.kt)("h2",{id:"text-generation"},"Text Generation"),(0,s.kt)("p",null,"Natural language generation (NLG) can actually tell a story \u2013 exactly like that of a human analyst \u2013 by writing the sentences and paragraphs for you. It can also summarize reports. "),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},'\u201cConversations with systems that have access to data about our world will allow us to understand the status of our jobs, our businesses, our health, our homes, our families, our devices, and our neighborhoods \u2014 all through the power of NLG. It will be the difference between getting a report and having a conversation. The information is the same but the interaction will be more natural"'),".  **"),(0,s.kt)("h3",{id:"algorithms"},"Algorithms"),(0,s.kt)("h4",{id:"text-generation-with-markov-chain"},"Text Generation with Markov Chain"),(0,s.kt)("p",null,"Markov chains are a stochastic process that are used to describe the next event in a sequence given the previous event only. In our case the state will be the previous word (unigram) or 2 words (bigram) or 3 (trigram). These are more generally known as ngrams since we will be using the last n words to generate the next possible word in the sequence. A Markov chain usually picks the next state via a probabilistic weighting but in our case that would just create text that would be too deterministic in structure and word choice. You could play with the weighting of the probabilities, but really having a random choice helps make the generated text feel original."),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Corpus"),": The dog jumped over the moon. The dog is funny."),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Language model:")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"(The, dog)     -> [jumped, is]\n(dog, jumped)  -> [over]\n(jumped, over) -> [the]\n(over, the)    -> [moon]\n(the, moon)    -> [#END#]\n(dog, is)      -> [funny]\n(is, funny)    -> [#END#]\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import random\nimport string\n\nclass MarkovModel:\n\n    def __init__(self):\n        self.model = None\n\n    def learn(self,tokens,n=2):\n        model = {}\n\n        for i in range(0,len(tokens)-n):\n            gram = tuple(tokens[i:i+n])\n            token = tokens[i+n]\n\n            if gram in model:\n                model[gram].append(token)\n            else:\n                model[gram] = [token]\n\n        final_gram = tuple(tokens[len(tokens) - n:])\n        if final_gram in model:\n            model[final_gram].append(None)\n        else:\n            model[final_gram] = [None]\n        self.model = model\n        return model\n\n    def generate(self,n=2,seed=None, max_tokens=100):\n        if seed == None:\n            seed = random.choice(self.model.keys())\n\n        output = list(seed)\n        output[0] = output[0].capitalize()\n        current = seed\n\n        for i in range(n, max_tokens):\n            ## get next possible set of words from the seed word\n            if current in self.model:\n                possible_transitions = self.model[current]\n                choice = random.choice(possible_transitions)\n                if choice is None: break\n\n                ## check if choice is period and if so append to previous element\n                if choice == '.':\n                    output[-1] = output[-1] + choice\n                else:\n                    output.append(choice)\n                current = tuple(output[-n:])\n            else:\n                ## should return ending punctuation of some sort\n                if current not in string.punctuation:\n                    output.append('.')\n        return output\n")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/natural-language-generation-part-1-back-to-basics-2f0b2654624f"},"Natural Language Generation Part 1: Back to Basics")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://paperswithcode.com/paper/maskgan-better-text-generation-via-filling-in"},"MaskGAN")," - Fill in the blank technique"),(0,s.kt)("li",{parentName:"ul"},"RankGAN"),(0,s.kt)("li",{parentName:"ul"},"LeakGAN"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://paperswithcode.com/paper/bart-denoising-sequence-to-sequence-pre"},"BART")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://paperswithcode.com/paper/ctrl-a-conditional-transformer-language-model"},"CTRL: A Conditional Transformer Language Model for Controllable Generation")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://paperswithcode.com/paper/neural-assistant-joint-action-prediction"},"Neural Assistant: Joint Action Prediction, Response Generation, and Latent Knowledge Reasoning"))),(0,s.kt)("h3",{id:"important-papers"},"Important Papers"),(0,s.kt)("p",null,"The survey: Text generation models in deep learning [",(0,s.kt)("a",{parentName:"p",href:"https://reader.elsevier.com/reader/sd/pii/S1319157820303360?token=AF608DFD534B46032B10D2F3DFD1B74CE0D3DC436E536660C856B15C296B929524171B9064D8772BA4762DCC57DAA69E"},"2020"),"]"),(0,s.kt)("p",null,"Survey of the State of the Art in Natural Language Generation: Core tasks, applications [",(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1703.09902.pdf"},"2017"),"]"),(0,s.kt)("p",null,"Neural Text Generation: A Practical Guide [",(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1711.09534.pdf"},"2017"),"]"),(0,s.kt)("p",null,"Neural Text Generation: Past, Present and Beyond [",(0,s.kt)("a",{parentName:"p",href:"https://arxiv.org/pdf/1803.07133.pdf"},"2018"),"]"),(0,s.kt)("h3",{id:"experiments-1"},"Experiments"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/abisee/story-generation-eval"},"Do Massively Pretrained Language Models Make Better Storytellers?")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://mc.ai/build-your-own-whatsapp-text-generator-and-learn-all-about-language-models/"},"Build your own WhatsApp text generator")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a"},"A beginner\u2019s guide to training and generating text using GPT2")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://minimaxir.com/2020/01/twitter-gpt2-bot/"},"How to Build a Twitter Text-Generating AI Bot With GPT-2")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.tensorflow.org/tutorials/text/text_generation"},"Tensorflow guide on Text generation with an RNN")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/generating-text-with-tensorflow-2-0-6a65c7bdc568"},"Generating Text with TensorFlow 2.0")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/tokenmill/accelerated-text"},"Accelerated Text")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/geek-ai/Texygen"},"Texygen: A text generation benchmarking platform")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://www.analyticsvidhya.com/blog/2018/03/text-generation-using-python-nlp/"},"How to create a poet / writer using Deep Learning (Text Generation using Python)")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb"},"Text generation with LSTM")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/AngusTheMack/title-generator"},"Title Generation using Recurrent Neural Networks")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://arxiv.org/pdf/1904.06828.pdf"},"Pun Generation with Surprise")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://medium.com/phrasee/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b"},"Neural text generation: How to generate text using conditional language models")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html"},"Encode, Tag and Realize: A Controllable and Efficient Approach for Text Generation"))),(0,s.kt)("h3",{id:"text-generation-with-char-rnns"},"Text Generation with char-RNNs"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://karpathy.github.io/2015/05/21/rnn-effectiveness/"},"The Unreasonable Effectiveness of Recurrent Neural Networks")),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://minimaxir.com/2018/05/text-neural-networks/"},"How to Quickly Train a Text-Generating Neural Network for Free")),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://github.com/shashank-bhatt-07/Natural-Language-Generation-using-LSTM-Keras"},"Natural Language Generation using LSTM-Keras"))),(0,s.kt)("h3",{id:"references-1"},"References"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://boredhumans.com/"},"BoredHumans.com - Fun AI Programs You Can Use Online")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/eaglenlp/Text-Generation"},"eaglenlp/Text-Generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/tokenmill/awesome-nlg"},"tokenmill/awesome-nlg")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/ChenChengKuan/awesome-text-generation"},"ChenChengKuan/awesome-text-generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/Tianwei-She/awesome-natural-language-generation"},"Tianwei-She/awesome-natural-language-generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/task/text-generation"},"Papers with Code - Text Generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/Eulring/Text-Generation-Papers"},"Eulring/Text-Generation-Papers")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"http://www.arxiv-sanity.com/search?q=natural+language+generation"},"Arxiv Sanity Preserver")),(0,s.kt)("hr",null),(0,s.kt)("h4",{id:"decoding-techniques---greedy-search-beam-search-top-k-sampling-and-top-p-sampling-with-transformer"},"Decoding techniques - Greedy search, Beam search, Top-K sampling and Top-p sampling with Transformer"),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://huggingface.co/blog/how-to-generate"},"How to generate text: using different decoding methods for language generation with Transformers")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://colab.research.google.com/drive/1Wf9HT2JjW-g_7UKLk2jmlN_K3SVkL9-i#scrollTo=lzaVvvIXMGOx"},"Google Colaboratory")),(0,s.kt)("h4",{id:"controlling-text-generation-with-plug-and-play-language-models-pplm"},"Controlling Text Generation with Plug and Play Language Models (PPLM)"),(0,s.kt)("p",null,"PPLM lets users combine small attribute models with an LM to steer its generation. Attribute model scan be 100,000 times smaller than the LM and still be effective insteering it, like a mouse sitting atop our wooly mammoth friend and telling it where to go.The mouse tells the mammoth where to go using gradients."),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://eng.uber.com/pplm/"},"Controlling Text Generation with Plug and Play Language Models")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://github.com/uber-research/PPLM"},"uber-research/PPLM")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://paperswithcode.com/paper/plug-and-play-language-models-a-simple"},"Plug and Play Language Models: A Simple Approach to Controlled Text Generation")),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"https://colab.research.google.com/drive/1DXBs1w_yyBbXsfRnFW98k_iZ3AYHhITS#scrollTo=fN-oIcZNo4xD"},"Google Colaboratory")),(0,s.kt)("h4",{id:"gpt-2-fine-tuning"},"GPT-2 Fine Tuning"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("a",{parentName:"li",href:"https://minimaxir.com/2019/09/howto-gpt2/"},"How To Make Custom AI-Generated Text With GPT-2"))),(0,s.kt)("h4",{id:"autoregressive-language-generation"},"Autoregressive Language Generation"),(0,s.kt)("p",null,"It based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions. The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step."),(0,s.kt)("h4",{id:"word-level-generation-vs-character-level-generation"},(0,s.kt)("strong",{parentName:"h4"},"Word-Level Generation vs Character-Level Generation")),(0,s.kt)("p",null,'In general, word-level language models tend to display higher accuracy than character-level language models. This is because they can form shorter representations of sentences and preserve the context between words easier than character-level language models. However, large corpora are needed to sufficiently train word-level language models, and one-hot encoding isn\'t very feasible for word-level models. In contrast, character-level language models are often quicker to train, requiring less memory and having faster inference than word-based models. This is because the "vocabulary" (the number of training features) for the model is likely to be much smaller overall, limited to hundreds of characters rather than hundreds of thousands of words.'),(0,s.kt)("h2",{id:"transformers"},"Transformers"),(0,s.kt)("p",null,"The old, obsolete, 1980 architecture of Recurrent Neural Networks(RNNs) including the LSTMs were simply not producing good results anymore. In less than two years, transformer models wiped RNNs off the map and even outperformed human baselines for many tasks."),(0,s.kt)("p",null,"Here\u2019s what the transformer block looks like in PyTorch:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class TransformerBlock(nn.Module):\n  def __init__(self, k, heads):\n    super().__init__()\n\n    self.attention = SelfAttention(k, heads=heads)\n\n    self.norm1 = nn.LayerNorm(k)\n    self.norm2 = nn.LayerNorm(k)\n\n    self.ff = nn.Sequential(\n      nn.Linear(k, 4 * k),\n      nn.ReLU(),\n      nn.Linear(4 * k, k))\n\n  def forward(self, x):\n    attended = self.attention(x)\n    x = self.norm1(attended + x)\n  \n    fedforward = self.ff(x)\n    return self.norm2(fedforward + x)\n")),(0,s.kt)("p",null,"Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only."),(0,s.kt)("p",null,"That is, the block applies, in sequence: a self attention layer, layer normalization, a feed forward layer (a single MLP applied independently to each vector), and another layer normalization. Residual connections are added around both, before the normalization. The order of the various components is not set in stone; the important thing is to combine self-attention with a local feedforward, and to add normalization and residual connections."),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823120-90db30f9-41ee-496e-b6f1-cca0f90df433.png",alt:"content-concepts-raw-nlp-transformers-untitled"})),(0,s.kt)("h3",{id:"the-transformer-block"},"The transformer block"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},"Multi-head attention"),(0,s.kt)("li",{parentName:"ol"},"Scaling the dot product"),(0,s.kt)("li",{parentName:"ol"},"Queries, keys and values")),(0,s.kt)("p",null,"The actual self-attention used in modern transformers relies on three additional tricks."),(0,s.kt)("h4",{id:"additional-tricks"},"Additional tricks"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nimport torch.nn.functional as F\n\n## assume we have some tensor x with size (b, t, k)\nx = ...\n\nraw_weights = torch.bmm(x, x.transpose(1, 2))\n## - torch.bmm is a batched matrix multiplication. It \n##   applies matrix multiplication over batches of \n##   matrices.\n\nweights = F.softmax(raw_weights, dim=2)\n\ny = torch.bmm(weights, x)\n")),(0,s.kt)("p",null,"We\u2019ll represent the input, a sequence of t vectors of dimension k as a t by k matrix \ud835\udc17. Including a minibatch dimension b, gives us an input tensor of size (b,t,k). The set of all raw dot products w\u2032ij forms a matrix, which we can compute simply by multiplying \ud835\udc17 by its transpose. Then, to turn the raw weights w\u2032ij into positive values that sum to one, we apply a row-wise softmax. Finally, to compute the output sequence, we just multiply the weight matrix by \ud835\udc17. This results in a batch of output matrices \ud835\udc18 of size (b, t, k) whose rows are weighted sums over the rows of \ud835\udc17. That\u2019s all. Two matrix multiplications and one softmax gives us a basic self-attention."),(0,s.kt)("h4",{id:"in-pytorch-basic-self-attention"},"In Pytorch: basic self-attention"),(0,s.kt)("p",null,"Let's understand with RecSys analogy. The tokens are both users and items. In movie recommenders e.g., to know a user's interest in different movies, we take a dot product of his embedding vector with movies' embedding vectors. Here in self-attention, we take a dot of the given token with all other tokens to know how much the given token is connected to other tokens."),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"http://peterbloem.nl/blog/transformers"},"Transformers from scratch")),(0,s.kt)("p",null,"In effect, there are five processes we need to understand to implement this model:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Embedding the inputs"),(0,s.kt)("li",{parentName:"ul"},"The Positional Encodings"),(0,s.kt)("li",{parentName:"ul"},"Creating Masks"),(0,s.kt)("li",{parentName:"ul"},"The Multi-Head Attention layer"),(0,s.kt)("li",{parentName:"ul"},"The Feed-Forward layer")),(0,s.kt)("h4",{id:"embedding-1"},"Embedding"),(0,s.kt)("p",null,"Embedding is handled simply in PyTorch:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class Embedder(nn.Module):\n    def __init__(self, vocab_size, d_model):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n    def forward(self, x):\n        return self.embed(x)\n")),(0,s.kt)("p",null,"When each word is fed into the network, this code will perform a look-up and retrieve its embedding vector. These vectors will then be learnt as a parameters by the model, adjusted with each iteration of gradient descent."),(0,s.kt)("h4",{id:"giving-our-words-context-the-positional-encodings"},"Giving our words context: The Positional Encodings"),(0,s.kt)("p",null,"In order for the model to make sense of a sentence, it needs to know two things about each word: what does the word mean? And what is its position in the sentence?"),(0,s.kt)("p",null,"The embedding vector for each word will learn the meaning, so now we need to input something that tells the network about the word\u2019s position."),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"Vasmani et al"),"\xa0answered this problem by using these functions to create a constant of position-specific values:"),(0,s.kt)("div",{className:"math math-display"},(0,s.kt)("span",{parentName:"div",className:"katex-display"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"P"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"E"),(0,s.kt)("mrow",{parentName:"msub"},(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"p"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mn",{parentName:"mrow"},"2"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"))),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"p"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,s.kt)("mn",{parentName:"mrow"},"1000"),(0,s.kt)("msup",{parentName:"mrow"},(0,s.kt)("mn",{parentName:"msup"},"0"),(0,s.kt)("mrow",{parentName:"msup"},(0,s.kt)("mn",{parentName:"mrow"},"2"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"d"),(0,s.kt)("mrow",{parentName:"msub"},(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"l"))))),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.0385em",verticalAlign:"-0.3552em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.13889em"}},"P"),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05764em"}},"E"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3448em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.5198em",marginLeft:"-0.0576em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mopen mtight"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"p"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"os"),(0,s.kt)("span",{parentName:"span",className:"mpunct mtight"},","),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,s.kt)("span",{parentName:"span",className:"mclose mtight"},")"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3552em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.188em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"in"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"p"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"os"),(0,s.kt)("span",{parentName:"span",className:"mord"},"/1000"),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord"},"0"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.938em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.113em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"/"),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"d"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3448em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.3488em",marginLeft:"0em",marginRight:"0.0714em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.5em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size3 size1 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"m"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"o"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.01968em"}},"l"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1512em"}},(0,s.kt)("span",{parentName:"span"})))))))))))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},")")))))),(0,s.kt)("div",{className:"math math-display"},(0,s.kt)("span",{parentName:"div",className:"katex-display"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"P"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"E"),(0,s.kt)("mrow",{parentName:"msub"},(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"p"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mn",{parentName:"mrow"},"2"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mo",{parentName:"mrow"},"+"),(0,s.kt)("mn",{parentName:"mrow"},"1"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"))),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mi",{parentName:"mrow"},"c"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"p"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,s.kt)("mn",{parentName:"mrow"},"1000"),(0,s.kt)("msup",{parentName:"mrow"},(0,s.kt)("mn",{parentName:"msup"},"0"),(0,s.kt)("mrow",{parentName:"msup"},(0,s.kt)("mn",{parentName:"mrow"},"2"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"d"),(0,s.kt)("mrow",{parentName:"msub"},(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"l"))))),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.0385em",verticalAlign:"-0.3552em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.13889em"}},"P"),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.05764em"}},"E"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3448em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.5198em",marginLeft:"-0.0576em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mopen mtight"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"p"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"os"),(0,s.kt)("span",{parentName:"span",className:"mpunct mtight"},","),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,s.kt)("span",{parentName:"span",className:"mbin mtight"},"+"),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"1"),(0,s.kt)("span",{parentName:"span",className:"mclose mtight"},")"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3552em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.188em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"cos"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"p"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"os"),(0,s.kt)("span",{parentName:"span",className:"mord"},"/1000"),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord"},"0"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.938em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.113em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"i"),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"/"),(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"d"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3448em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.3488em",marginLeft:"0em",marginRight:"0.0714em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.5em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size3 size1 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"m"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"o"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.01968em"}},"l"))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1512em"}},(0,s.kt)("span",{parentName:"span"})))))))))))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},")")))))),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216824749-ff74e78e-e69e-4b87-b76b-d805dda8095b.png",alt:"cbimage"})),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823110-6e0dd58f-234d-4ae1-804f-f076e00e2903.png",alt:"content-concepts-raw-nlp-transformers-untitled-1"})),(0,s.kt)("p",null,"The positional encoding matrix is a constant whose values are defined by the above equations. When added to the embedding matrix, each word embedding is altered in a way specific to its position."),(0,s.kt)("p",null,"An intuitive way of coding our Positional Encoder looks like this:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class PositionalEncoder(nn.Module):\n    def __init__(self, d_model, max_seq_len = 80):\n        super().__init__()\n        self.d_model = d_model\n      \n        ## create constant 'pe' matrix with values dependant on \n        ## pos and i\n        pe = torch.zeros(max_seq_len, d_model)\n        for pos in range(max_seq_len):\n            for i in range(0, d_model, 2):\n                pe[pos, i] = \\\n                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = \\\n                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n              \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n \n  \n    def forward(self, x):\n        ## make embeddings relatively larger\n        x = x * math.sqrt(self.d_model)\n        #add constant to embedding\n        seq_len = x.size(1)\n        x = x + Variable(self.pe[:,:seq_len], \\\n        requires_grad=False).cuda()\n        return x\n")),(0,s.kt)("p",null,"The above module lets us add the positional encoding to the embedding vector, providing information about structure to the model."),(0,s.kt)("h4",{id:"creating-our-masks"},(0,s.kt)("strong",{parentName:"h4"},"Creating Our Masks")),(0,s.kt)("p",null,"Masking plays an important role in the transformer. It serves two purposes:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"In the encoder and decoder: To zero attention outputs wherever there is just padding in the input sentences."),(0,s.kt)("li",{parentName:"ul"},"In the decoder: To prevent the decoder \u2018peaking\u2019 ahead at the rest of the translated sentence when predicting the next word.")),(0,s.kt)("p",null,"Creating the mask for the input is simple:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"batch = next(iter(train_iter))\ninput_seq = batch.English.transpose(0,1)\ninput_pad = EN_TEXT.vocab.stoi['<pad>']## creates mask with 0s wherever there is padding in the input\ninput_msk = (input_seq != input_pad).unsqueeze(1)\n")),(0,s.kt)("p",null,"For the target_seq we do the same, but then create an additional step:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"## create mask as beforetarget_seq = batch.French.transpose(0,1)\ntarget_pad = FR_TEXT.vocab.stoi['<pad>']\ntarget_msk = (target_seq != target_pad).unsqueeze(1)size = target_seq.size(1) ## get seq_len for matrixnopeak_mask= np.triu(np.ones(1, size, size),\nk=1).astype('uint8')\nnopeak_mask = Variable(torch.from_numpy(nopeak_mask)== 0)target_msk = target_msk & nopeak_mask\n")),(0,s.kt)("h4",{id:"multi-headed-attention"},(0,s.kt)("strong",{parentName:"h4"},"Multi-Headed Attention")),(0,s.kt)("p",null,"Once we have our embedded values (with positional encodings) and our masks, we can start building the layers of our model."),(0,s.kt)("p",null,"Here is an overview of the multi-headed attention layer:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823111-f1364a42-3cd3-447b-aa9a-17438528c84f.png",alt:"content-concepts-raw-nlp-transformers-untitled-2"})),(0,s.kt)("p",null,"Multi-headed attention layer, each input is split into multiple heads which allows the network to simultaneously attend to different subsections of each embedding."),(0,s.kt)("p",null,"A scaled dot-product attention mechanism is very similar to a self-attention (dot-product) mechanism except it uses a scaling factor. The multi-head part, on the other hand, ensures the model is capable of looking at various aspects of input at all levels. Transformer models attend to encoder annotations and the hidden values from past layers. The architecture of the Transformer model does not have a recurrent step-by-step flow; instead, it uses positional encoding in order to have information about the position of each token in the input sequence. The concatenated values of the embeddings (randomly initialized) and the fixed values of positional encoding are the input fed into the layers in the first encoder part and are propagated through the architecture."),(0,s.kt)("p",null,"In the case of the Encoder,\xa0",(0,s.kt)("em",{parentName:"p"},"V, K"),"\xa0and\xa0",(0,s.kt)("em",{parentName:"p"},"G"),"\xa0will simply be identical copies of the embedding vector (plus positional encoding). They will have the dimensions Batch_size ",(0,s.kt)("em",{parentName:"p"}," seq_len ")," d_model."),(0,s.kt)("p",null,"In multi-head attention we split the embedding vector into\xa0",(0,s.kt)("em",{parentName:"p"},"N"),"\xa0heads, so they will then have the dimensions ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"b"),(0,s.kt)("mi",{parentName:"mrow"},"a"),(0,s.kt)("mi",{parentName:"mrow"},"t"),(0,s.kt)("mi",{parentName:"mrow"},"c"),(0,s.kt)("mi",{parentName:"mrow"},"h"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"_"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mi",{parentName:"mrow"},"z"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mo",{parentName:"mrow"},"\u2217"),(0,s.kt)("mi",{parentName:"mrow"},"N"),(0,s.kt)("mo",{parentName:"mrow"},"\u2217"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"q"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"_"),(0,s.kt)("mi",{parentName:"mrow"},"l"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("mo",{parentName:"mrow"},"\u2217"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"_"),(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"l"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,s.kt)("mi",{parentName:"mrow"},"N"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},".")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"batch\\_size * N * seq\\_len * (d\\_model / N).")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.0044em",verticalAlign:"-0.31em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ba"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"c"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"h"),(0,s.kt)("span",{parentName:"span",className:"mord",style:{marginRight:"0.02778em"}},"_"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"i"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ze"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"\u2217"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.6833em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"\u2217"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.0044em",verticalAlign:"-0.31em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"se"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"q"),(0,s.kt)("span",{parentName:"span",className:"mord",style:{marginRight:"0.02778em"}},"_"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.01968em"}},"l"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}}),(0,s.kt)("span",{parentName:"span",className:"mbin"},"\u2217"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2222em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.06em",verticalAlign:"-0.31em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord",style:{marginRight:"0.02778em"}},"_"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"m"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"o"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.01968em"}},"l"),(0,s.kt)("span",{parentName:"span",className:"mord"},"/"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mord"},".")))))),(0,s.kt)("p",null,"This final dimension ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"_"),(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"l"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"/"),(0,s.kt)("mi",{parentName:"mrow"},"N"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"(d\\_model / N )")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.06em",verticalAlign:"-0.31em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord",style:{marginRight:"0.02778em"}},"_"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"m"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"o"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.01968em"}},"l"),(0,s.kt)("span",{parentName:"span",className:"mord"},"/"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.10903em"}},"N"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")")))))," we will refer to as ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"_"),(0,s.kt)("mi",{parentName:"mrow"},"k")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"d\\_k")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.0044em",verticalAlign:"-0.31em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord",style:{marginRight:"0.02778em"}},"_"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"))))),"."),(0,s.kt)("p",null,"Let\u2019s see the code for the decoder module:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class MultiHeadAttention(nn.Module):\n    def __init__(self, heads, d_model, dropout = 0.1):\n        super().__init__()\n      \n        self.d_model = d_model\n        self.d_k = d_model // heads\n        self.h = heads\n      \n        self.q_linear = nn.Linear(d_model, d_model)\n        self.v_linear = nn.Linear(d_model, d_model)\n        self.k_linear = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.out = nn.Linear(d_model, d_model)\n  \n    def forward(self, q, k, v, mask=None):\n      \n        bs = q.size(0)\n      \n        ## perform linear operation and split into h heads\n      \n        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n      \n        ## transpose to get dimensions bs * h * sl * d_model\n     \n        k = k.transpose(1,2)\n        q = q.transpose(1,2)\n        v = v.transpose(1,2)\n                ## calculate attention using function we will define next\n        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n      \n        ## concatenate heads and put through final linear layer\n        concat = scores.transpose(1,2).contiguous()\\\n        .view(bs, -1, self.d_model)\n      \n        output = self.out(concat)\n  \n        return output\n")),(0,s.kt)("h4",{id:"calculating-attention"},"Calculating Attention"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823113-928794b0-a9fe-40c5-9611-5bda8dc27e63.png",alt:"content-concepts-raw-nlp-transformers-untitled-3"})),(0,s.kt)("div",{className:"math math-display"},(0,s.kt)("span",{parentName:"div",className:"katex-display"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"A"),(0,s.kt)("mi",{parentName:"mrow"},"t"),(0,s.kt)("mi",{parentName:"mrow"},"t"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("mi",{parentName:"mrow"},"t"),(0,s.kt)("mi",{parentName:"mrow"},"i"),(0,s.kt)("mi",{parentName:"mrow"},"o"),(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"("),(0,s.kt)("mi",{parentName:"mrow"},"Q"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mi",{parentName:"mrow"},"K"),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mi",{parentName:"mrow"},"V"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},")"),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("mrow",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"s"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"o"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"f"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"t"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"m"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"a"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"x")),(0,s.kt)("mrow",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"mrow",fence:"true"},"("),(0,s.kt)("mfrac",{parentName:"mrow"},(0,s.kt)("mrow",{parentName:"mfrac"},(0,s.kt)("mi",{parentName:"mrow",mathvariant:"bold"},"Q"),(0,s.kt)("msup",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msup",mathvariant:"bold"},"K"),(0,s.kt)("mi",{parentName:"msup",mathvariant:"normal"},"\u22a4"))),(0,s.kt)("msqrt",{parentName:"mfrac"},(0,s.kt)("mi",{parentName:"msqrt"},"d"))),(0,s.kt)("mo",{parentName:"mrow",fence:"true"},")")),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"bold"},"V"),(0,s.kt)("mo",{parentName:"mrow"},"\u2208"),(0,s.kt)("msup",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msup",mathvariant:"double-struck"},"R"),(0,s.kt)("mrow",{parentName:"msup"},(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("mo",{parentName:"mrow"},"\xd7"),(0,s.kt)("mi",{parentName:"mrow"},"v")))),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"Attention(Q,K,V) = \\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"A"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"tt"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"i"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"o"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,s.kt)("span",{parentName:"span",className:"mopen"},"("),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"Q"),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.07153em"}},"K"),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.22222em"}},"V"),(0,s.kt)("span",{parentName:"span",className:"mclose"},")"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"2.4761em",verticalAlign:"-0.95em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathrm"},"softmax")),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"minner"},(0,s.kt)("span",{parentName:"span",className:"mopen delimcenter",style:{top:"0em"}},(0,s.kt)("span",{parentName:"span",className:"delimsizing size3"},"(")),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mopen nulldelimiter"}),(0,s.kt)("span",{parentName:"span",className:"mfrac"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"1.5261em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.1778em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord sqrt"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.9322em"}},(0,s.kt)("span",{parentName:"span",className:"svg-align",style:{top:"-3em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord",style:{paddingLeft:"0.833em"}},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"))),(0,s.kt)("span",{parentName:"span",style:{top:"-2.8922em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"hide-tail",style:{minWidth:"0.853em",height:"1.08em"}},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"1.08em",viewBox:"0 0 400000 1080",preserveAspectRatio:"xMinYMin slice"},(0,s.kt)("path",{parentName:"svg",d:"M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z"}))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.1078em"}},(0,s.kt)("span",{parentName:"span"}))))))),(0,s.kt)("span",{parentName:"span",style:{top:"-3.23em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"frac-line",style:{borderBottomWidth:"0.04em"}})),(0,s.kt)("span",{parentName:"span",style:{top:"-3.677em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathbf"},"Q"),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathbf"},"K"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.8491em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.063em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"\u22a4"))))))))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.93em"}},(0,s.kt)("span",{parentName:"span"}))))),(0,s.kt)("span",{parentName:"span",className:"mclose nulldelimiter"})),(0,s.kt)("span",{parentName:"span",className:"mclose delimcenter",style:{top:"0em"}},(0,s.kt)("span",{parentName:"span",className:"delimsizing size3"},")"))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathbf",style:{marginRight:"0.01597em"}},"V"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"\u2208"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}})),(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"0.8213em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathbb"},"R"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.8213em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.113em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight"},"n"),(0,s.kt)("span",{parentName:"span",className:"mbin mtight"},"\xd7"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal mtight",style:{marginRight:"0.03588em"}},"v")))))))))))))),(0,s.kt)("p",null,"Initially we must multiply Q by the transpose of K. This is then \u2018scaled\u2019 by dividing the output by the square root of ",(0,s.kt)("span",{parentName:"p",className:"math math-inline"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mrow",{parentName:"semantics"},(0,s.kt)("mi",{parentName:"mrow"},"d"),(0,s.kt)("mi",{parentName:"mrow",mathvariant:"normal"},"_"),(0,s.kt)("mi",{parentName:"mrow"},"k")),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"d\\_k")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"1.0044em",verticalAlign:"-0.31em"}}),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"d"),(0,s.kt)("span",{parentName:"span",className:"mord",style:{marginRight:"0.02778em"}},"_"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"))))),". Before we perform Softmax, we apply our mask and hence reduce values where the input is padding (or in the decoder, also where the input is ahead of the current word). Finally, the last step is doing a dot product between the result so far and V."),(0,s.kt)("p",null,"Here is the code for the attention function:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"def attention(q, k, v, d_k, mask=None, dropout=None):\n    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n\n        if mask is not None:\n        mask = mask.unsqueeze(1)\n        scores = scores.masked_fill(mask == 0, -1e9)\n\n        scores = F.softmax(scores, dim=-1)\n  \n    if dropout is not None:\n        scores = dropout(scores)\n      \n    output = torch.matmul(scores, v)\n    return output\n")),(0,s.kt)("p",null,"In PyTorch, it looks like this:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"from torch import Tensor\nimport torch.nn.functional as f\n\ndef scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n    temp = query.bmm(key.transpose(1, 2))\n    scale = query.size(-1) ** 0.5\n    softmax = f.softmax(temp / scale, dim=-1)\n    return softmax.bmm(value)\n")),(0,s.kt)("p",null,"Note that MatMul operations are translated to ",(0,s.kt)("inlineCode",{parentName:"p"},"torch.bmm")," in PyTorch. That\u2019s because Q, K, and V (query, key, and value arrays) are batches of matrices, each with shape ",(0,s.kt)("inlineCode",{parentName:"p"},"(batch_size, sequence_length, num_features)"),". Batch matrix multiplication is only performed over the last two dimensions."),(0,s.kt)("p",null,"The attention head will then become:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"import torch\nfrom torch import nn\n\nclass AttentionHead(nn.Module):\n    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\n        super().__init__()\n        self.q = nn.Linear(dim_in, dim_k)\n        self.k = nn.Linear(dim_in, dim_k)\n        self.v = nn.Linear(dim_in, dim_v)\n\n    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\n")),(0,s.kt)("p",null,"Now, it\u2019s very easy to build the multi-head attention layer. Just combine ",(0,s.kt)("inlineCode",{parentName:"p"},"num_heads")," different attention heads and a Linear layer for the output."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\n        super().__init__()\n        self.heads = nn.ModuleList(\n            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n        )\n        self.linear = nn.Linear(num_heads * dim_v, dim_in)\n\n    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n        return self.linear(\n            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n        )\n")),(0,s.kt)("p",null,"Let\u2019s pause again to examine what\u2019s going on in the ",(0,s.kt)("inlineCode",{parentName:"p"},"MultiHeadAttention")," layer. Each attention head computes its own query, key, and value arrays, and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows us to \u201cpay attention\u201d to more parts of the sequence at once, which makes the model more powerful."),(0,s.kt)("h4",{id:"the-feed-forward-network"},(0,s.kt)("strong",{parentName:"h4"},"The Feed-Forward Network")),(0,s.kt)("p",null,"This layer just consists of two linear operations, with a relu and dropout operation in between them."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n        super().__init__() \n        ## We set d_ff as a default to 2048\n        self.linear_1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model)\n    def forward(self, x):\n        x = self.dropout(F.relu(self.linear_1(x)))\n        x = self.linear_2(x)\n        return x\n")),(0,s.kt)("p",null,"The feed-forward layer simply deepens our network, employing linear layers to analyze patterns in the attention layers output."),(0,s.kt)("h4",{id:"one-last-thing--normalization"},(0,s.kt)("strong",{parentName:"h4"},"One Last Thing : Normalization")),(0,s.kt)("p",null,"Normalisation is highly important in deep neural networks. It prevents the range of values in the layers changing too much, meaning the model trains faster and has better ability to generalise."),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823115-b6dec0a3-49cb-49c6-870f-b97ffbe135fa.png",alt:"content-concepts-raw-nlp-transformers-untitled-4"})),(0,s.kt)("p",null,"We will be normalizing our results between each layer in the encoder/decoder, so before building our model let\u2019s define that function:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class Norm(nn.Module):\n    def __init__(self, d_model, eps = 1e-6):\n        super().__init__()\n  \n        self.size = d_model\n        ## create two learnable parameters to calibrate normalisation\n        self.alpha = nn.Parameter(torch.ones(self.size))\n        self.bias = nn.Parameter(torch.zeros(self.size))\n        self.eps = eps\n    def forward(self, x):\n        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n        return norm\n")),(0,s.kt)("h4",{id:"putting-it-all-together"},"Putting it all together!"),(0,s.kt)("p",null,"Let\u2019s have another look at the over-all architecture and start building:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823117-03997de3-387a-4efd-a4c5-92e5b2116264.png",alt:"content-concepts-raw-nlp-transformers-untitled-5"})),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"One last Variable:"),"\xa0If you look at the diagram closely you can see a \u2018Nx\u2019 next to the encoder and decoder architectures. In reality, the encoder and decoder in the diagram above represent one layer of an encoder and one of the decoder. N is the variable for the number of layers there will be. Eg. if N=6, the data goes through six encoder layers (with the architecture seen above), then these outputs are passed to the decoder which also consists of six repeating decoder layers."),(0,s.kt)("p",null,"We will now build EncoderLayer and DecoderLayer modules with the architecture shown in the model above. Then when we build the encoder and decoder we can define how many of these layers to have."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"## build an encoder layer with one multi-head attention layer and one ## feed-forward layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout = 0.1):\n        super().__init__()\n        self.norm_1 = Norm(d_model)\n        self.norm_2 = Norm(d_model)\n        self.attn = MultiHeadAttention(heads, d_model)\n        self.ff = FeedForward(d_model)\n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n      \n    def forward(self, x, mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.ff(x2))\n        return x\n  \n## build a decoder layer with two multi-head attention layers and\n## one feed-forward layer\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, heads, dropout=0.1):\n        super().__init__()\n        self.norm_1 = Norm(d_model)\n        self.norm_2 = Norm(d_model)\n        self.norm_3 = Norm(d_model)\n      \n        self.dropout_1 = nn.Dropout(dropout)\n        self.dropout_2 = nn.Dropout(dropout)\n        self.dropout_3 = nn.Dropout(dropout)\n      \n        self.attn_1 = MultiHeadAttention(heads, d_model)\n        self.attn_2 = MultiHeadAttention(heads, d_model)\n        self.ff = FeedForward(d_model).cuda()\n\n        def forward(self, x, e_outputs, src_mask, trg_mask):\n        x2 = self.norm_1(x)\n        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n        x2 = self.norm_2(x)\n        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n        src_mask))\n        x2 = self.norm_3(x)\n        x = x + self.dropout_3(self.ff(x2))\n        return x\n\n        ## We can then build a convenient cloning function that can generate multiple layers:\n        def get_clones(module, N):\n            return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n")),(0,s.kt)("p",null,"We\u2019re now ready to build the encoder and decoder:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class Encoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.embed = Embedder(vocab_size, d_model)\n        self.pe = PositionalEncoder(d_model)\n        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n        self.norm = Norm(d_model)\n    def forward(self, src, mask):\n        x = self.embed(src)\n        x = self.pe(x)\n        for i in range(N):\n            x = self.layers[i](x, mask)\n        return self.norm(x)\n  \nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, d_model, N, heads):\n        super().__init__()\n        self.N = N\n        self.embed = Embedder(vocab_size, d_model)\n        self.pe = PositionalEncoder(d_model)\n        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n        self.norm = Norm(d_model)\n    def forward(self, trg, e_outputs, src_mask, trg_mask):\n        x = self.embed(trg)\n        x = self.pe(x)\n        for i in range(self.N):\n            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n        return self.norm(x)\n")),(0,s.kt)("p",null,"And finally\u2026 The transformer!"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-python"},"class Transformer(nn.Module):\n    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n        super().__init__()\n        self.encoder = Encoder(src_vocab, d_model, N, heads)\n        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n        self.out = nn.Linear(d_model, trg_vocab)\n    def forward(self, src, trg, src_mask, trg_mask):\n        e_outputs = self.encoder(src, src_mask)\n        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n        output = self.out(d_output)\n        return output\n## we don't perform softmax on the output as this will be handled \n## automatically by our loss function\n")),(0,s.kt)("p",null,"The original Transformer model is a stack of 6 layers. The output of layer l is the input of layer l+1 until the final prediction is reached. There is a 6-layer encoder stack on the left and a 6-layer decoder stack on the right:"),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823118-ce558916-d199-430d-8b8b-9f4dcd037e2a.png",alt:"content-concepts-raw-nlp-transformers-untitled-6"})),(0,s.kt)("p",null,"The architecture of the Transformer"),(0,s.kt)("p",null,"On the left, the inputs enter the encoder side of the Transformer through an attention sub-layer and FeedForward Network (FFN) sub-layer. On the right, the target outputs go into the decoder side of the Transformer through two attention sub-layers and an FFN sub-layer. We immediately notice that there is no RNN, LSTM, or CNN. Recurrence has been abandoned. Attention has replaced recurrence, which requires an increasing number of operations as the distance between two words increases."),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},'The attention mechanism is a "word-to-word" operation. The attention mechanism will find how each word is related to all other words in a sequence, including the word being analyzed itself.')),(0,s.kt)("h2",{id:"language-modeling"},"Language Modeling"),(0,s.kt)("p",null,"Language Models (LMs) estimate the probability of different linguistic units: symbols, tokens, token sequences."),(0,s.kt)("p",null,"We see language models in action every day - look at some examples. Usually models in large commercial services are a bit more complicated than the ones we will discuss today, but the idea is the same: if we can estimate probabilities of words/sentences/etc, we can use them in various, sometimes even unexpected, ways."),(0,s.kt)("p",null,'We, humans, already have some feeling of "probability" when it comes to natural language. For example, when we talk, usually we understand each other quite well (at least, what\'s being said). We disambiguate between different options which sound similar without even realizing it!'),(0,s.kt)("p",null,"But how a machine is supposed to understand this? A machine needs a language model, which estimates the probabilities of sentences. If a language model is good, it will assign a larger probability to a correct option."),(0,s.kt)("p",null,"Read ",(0,s.kt)("a",{parentName:"p",href:"https://lena-voita.github.io/nlp_course/language_modeling.html"},"this")," article to understand the concept of ",(0,s.kt)("inlineCode",{parentName:"p"},"language models")," in depth."),(0,s.kt)("h3",{id:"masked-language-modeling"},(0,s.kt)("strong",{parentName:"h3"},"Masked language modeling")),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Masked language modeling"),"\xa0is\xa0the task of training a model on input (a sentence with some masked tokens) and obtaining the output as the whole sentence with the masked tokens filled. But how and why does it help a model to obtain better results on downstream tasks such as classification? The answer is simple: if the model can do a cloze test (a linguistic test for evaluating language understanding by filling in blanks), then it has a general understanding of the language itself. For other tasks, it has been pretrained (by language modeling) and will perform better."),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823093-700825ee-7f47-42c8-8a47-ba2a5bfb544f.png",alt:"content-concepts-raw-nlp-language-modeling-untitled"})),(0,s.kt)("p",null,"Here's an example of a cloze test:"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},"George Washington was the first President of the ___ States.")),(0,s.kt)("p",null,"It is expected that\xa0",(0,s.kt)("em",{parentName:"p"},"United"),"\xa0should fill in the blank. For a masked language model, the same task is applied, and it is required to fill in the masked tokens. However, masked tokens are selected randomly from a sentence."),(0,s.kt)("p",null,"In BERT4Rec, authors used Cloze task technique (also known as \u201cMasked Language Model) to train the bi-directional model. In this, we randomly mask some items (i.e., replace them with a special token ","[mask]",") in the input sequences, and then predict the ids of those masked items based on their surrounding context."),(0,s.kt)("div",{className:"math math-display"},(0,s.kt)("span",{parentName:"div",className:"katex-display"},(0,s.kt)("span",{parentName:"span",className:"katex"},(0,s.kt)("span",{parentName:"span",className:"katex-mathml"},(0,s.kt)("math",{parentName:"span",xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},(0,s.kt)("semantics",{parentName:"math"},(0,s.kt)("mtable",{parentName:"semantics",rowspacing:"0.25em",columnalign:"right",columnspacing:""},(0,s.kt)("mtr",{parentName:"mtable"},(0,s.kt)("mtd",{parentName:"mtr",className:"mtr-glue"}),(0,s.kt)("mtd",{parentName:"mtr"},(0,s.kt)("mstyle",{parentName:"mtd",scriptlevel:"0",displaystyle:"true"},(0,s.kt)("mrow",{parentName:"mstyle"},(0,s.kt)("mi",{parentName:"mrow"},"I"),(0,s.kt)("mi",{parentName:"mrow"},"n"),(0,s.kt)("mi",{parentName:"mrow"},"p"),(0,s.kt)("mi",{parentName:"mrow"},"u"),(0,s.kt)("mi",{parentName:"mrow"},"t"),(0,s.kt)("mo",{parentName:"mrow"},":"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"1")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"2")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"3")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"4")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"5")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"]"),(0,s.kt)("mover",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"mover",stretchy:"true",minsize:"3.0em"},"\u2192"),(0,s.kt)("mpadded",{parentName:"mover",width:"+0.6em",lspace:"0.3em"},(0,s.kt)("mtext",{parentName:"mpadded"},"randomly\xa0mask"))),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"1")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"a"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"k"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"msub",stretchy:"false"},"]"),(0,s.kt)("mn",{parentName:"msub"},"1")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"3")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"a"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"k"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"msub",stretchy:"false"},"]"),(0,s.kt)("mn",{parentName:"msub"},"2")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"5")),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"]")))),(0,s.kt)("mtd",{parentName:"mtr",className:"mtr-glue"}),(0,s.kt)("mtd",{parentName:"mtr",className:"mml-eqn-num"})),(0,s.kt)("mtr",{parentName:"mtable"},(0,s.kt)("mtd",{parentName:"mtr",className:"mtr-glue"}),(0,s.kt)("mtd",{parentName:"mtr"},(0,s.kt)("mstyle",{parentName:"mtd",scriptlevel:"0",displaystyle:"true"},(0,s.kt)("mrow",{parentName:"mstyle"},(0,s.kt)("mi",{parentName:"mrow"},"L"),(0,s.kt)("mi",{parentName:"mrow"},"a"),(0,s.kt)("mi",{parentName:"mrow"},"b"),(0,s.kt)("mi",{parentName:"mrow"},"e"),(0,s.kt)("mi",{parentName:"mrow"},"l"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mo",{parentName:"mrow"},":"),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"a"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"k"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"msub",stretchy:"false"},"]"),(0,s.kt)("mn",{parentName:"msub"},"1")),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"2")),(0,s.kt)("mo",{parentName:"mrow",separator:"true"},","),(0,s.kt)("mo",{parentName:"mrow",stretchy:"false"},"["),(0,s.kt)("mi",{parentName:"mrow"},"m"),(0,s.kt)("mi",{parentName:"mrow"},"a"),(0,s.kt)("mi",{parentName:"mrow"},"s"),(0,s.kt)("mi",{parentName:"mrow"},"k"),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mo",{parentName:"msub",stretchy:"false"},"]"),(0,s.kt)("mn",{parentName:"msub"},"2")),(0,s.kt)("mo",{parentName:"mrow"},"="),(0,s.kt)("msub",{parentName:"mrow"},(0,s.kt)("mi",{parentName:"msub"},"v"),(0,s.kt)("mn",{parentName:"msub"},"4"))))),(0,s.kt)("mtd",{parentName:"mtr",className:"mtr-glue"}),(0,s.kt)("mtd",{parentName:"mtr",className:"mml-eqn-num"}))),(0,s.kt)("annotation",{parentName:"semantics",encoding:"application/x-tex"},"\\begin{align} Input: [v_1, v_2, v_3, v_4, v_5] \\xrightarrow{\\text{randomly mask}} [v_1, [mask]_1, v_3, [mask]_2, v_5]\\\\ Labels: [mask]_1 = v_2, [mask]_2 = v_4 \\end{align}")))),(0,s.kt)("span",{parentName:"span",className:"katex-html","aria-hidden":"true"},(0,s.kt)("span",{parentName:"span",className:"base"},(0,s.kt)("span",{parentName:"span",className:"strut",style:{height:"3.2681em",verticalAlign:"-1.3841em"}}),(0,s.kt)("span",{parentName:"span",className:"mtable"},(0,s.kt)("span",{parentName:"span",className:"col-align-r"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"1.8841em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.8841em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3.1081em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.07847em"}},"I"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"n"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"p"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"u"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"t"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},":"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"3")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"4")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"5")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel x-arrow"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"1.1081em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.322em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight x-arrow-pad"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},(0,s.kt)("span",{parentName:"span",className:"mord text mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"randomly\xa0mask"))))),(0,s.kt)("span",{parentName:"span",className:"svg-align",style:{top:"-2.689em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"hide-tail",style:{height:"0.522em",minWidth:"1.469em"}},(0,s.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"400em",height:"0.522em",viewBox:"0 0 400000 522",preserveAspectRatio:"xMaxYMin slice"},(0,s.kt)("path",{parentName:"svg",d:"M0 241v40h399891c-47.3 35.3-84 78-110 128\n-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20\n 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7\n 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85\n-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5\n-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67\n 151.7 139 205zm0 0v40h399900v-40z"}))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.011em"}},(0,s.kt)("span",{parentName:"span"}))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ma"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"),(0,s.kt)("span",{parentName:"span",className:"mclose"},(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"3")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ma"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"),(0,s.kt)("span",{parentName:"span",className:"mclose"},(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"5")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"))),(0,s.kt)("span",{parentName:"span",style:{top:"-2.3841em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3.1081em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"L"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ab"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"e"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.01968em"}},"l"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},":"),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ma"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"),(0,s.kt)("span",{parentName:"span",className:"mclose"},(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"1")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mpunct"},","),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.1667em"}}),(0,s.kt)("span",{parentName:"span",className:"mopen"},"["),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"ma"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal"},"s"),(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03148em"}},"k"),(0,s.kt)("span",{parentName:"span",className:"mclose"},(0,s.kt)("span",{parentName:"span",className:"mclose"},"]"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"0em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"2")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"})))))),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mrel"},"="),(0,s.kt)("span",{parentName:"span",className:"mspace",style:{marginRight:"0.2778em"}}),(0,s.kt)("span",{parentName:"span",className:"mord"},(0,s.kt)("span",{parentName:"span",className:"mord mathnormal",style:{marginRight:"0.03588em"}},"v"),(0,s.kt)("span",{parentName:"span",className:"msupsub"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.3011em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-2.55em",marginLeft:"-0.0359em",marginRight:"0.05em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"2.7em"}}),(0,s.kt)("span",{parentName:"span",className:"sizing reset-size6 size3 mtight"},(0,s.kt)("span",{parentName:"span",className:"mord mtight"},"4")))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"0.15em"}},(0,s.kt)("span",{parentName:"span"}))))))))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"1.3841em"}},(0,s.kt)("span",{parentName:"span"}))))))),(0,s.kt)("span",{parentName:"span",className:"tag"},(0,s.kt)("span",{parentName:"span",className:"vlist-t vlist-t2"},(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"1.8841em"}},(0,s.kt)("span",{parentName:"span",style:{top:"-3.8841em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3.1081em"}}),(0,s.kt)("span",{parentName:"span",className:"eqn-num"})),(0,s.kt)("span",{parentName:"span",style:{top:"-2.3841em"}},(0,s.kt)("span",{parentName:"span",className:"pstrut",style:{height:"3.1081em"}}),(0,s.kt)("span",{parentName:"span",className:"eqn-num"}))),(0,s.kt)("span",{parentName:"span",className:"vlist-s"},"\u200b")),(0,s.kt)("span",{parentName:"span",className:"vlist-r"},(0,s.kt)("span",{parentName:"span",className:"vlist",style:{height:"1.3841em"}},(0,s.kt)("span",{parentName:"span"}))))))))),(0,s.kt)("p",null,"Let's take another example:"),(0,s.kt)("p",null,"In Autumn the ",(0,s.kt)("strong",{parentName:"p"},"__")," fall from the trees."),(0,s.kt)("p",null,"Do you know the answer? Most likely you do, and you do because you have considered the context of the sentence."),(0,s.kt)("p",null,"We see the words\xa0",(0,s.kt)("em",{parentName:"p"},"fall"),"\xa0and\xa0",(0,s.kt)("em",{parentName:"p"},"trees"),"\xa0\u2014 we know that the missing word is something that\xa0",(0,s.kt)("em",{parentName:"p"},"falls from trees"),"."),(0,s.kt)("p",null,"A lot of things fall from trees, acorns, branches, leaves \u2014 but we have another condition,\xa0",(0,s.kt)("em",{parentName:"p"},"in Autumn"),"\xa0\u2014 that narrows down our search, the most probable thing to fall from a tree in Autumn are\xa0",(0,s.kt)("em",{parentName:"p"},"leaves"),"."),(0,s.kt)("p",null,"As humans, we use a mix of general world knowledge, and linguistic understanding to come to that conclusion. For BERT, this guess will come from reading\xa0",(0,s.kt)("em",{parentName:"p"},"a lot"),"\xa0\u2014 and learning linguistic patterns incredibly well."),(0,s.kt)("p",null,"BERT may not know what Autumn, trees, and leaves are \u2014 but it does know that given linguistic patterns, and the context of these words, the answer is most likely to be\xa0",(0,s.kt)("em",{parentName:"p"},"leaves"),"."),(0,s.kt)("p",null,"The outcome of this process \u2014 for BERT \u2014 is an improved comprehension of the style of language being used."),(0,s.kt)("h3",{id:"causal-language-modeling"},"Causal language modeling"),(0,s.kt)("p",null,"Causal language modeling is the task of predicting the token following a sequence of tokens. In this situation, the model only attends to the left context (tokens on the left of the mask). Such a training is particularly interesting for generation tasks."),(0,s.kt)("aside",null,"\ud83d\udc4c\ud83c\udffc I think it's because pre-BERT, causal language modeling was actually just called language modeling. When the BERT paper arrived they coined the task of predicting random masked tokens as masked language modeling, which led to subsequent papers presenting transformer-like models for translation or generation to use the term causal language modeling for clarity. ~ [https://www.reddit.com/user/keramitas/](https://www.reddit.com/user/keramitas/)"),(0,s.kt)("h3",{id:"permutation-language-modeling"},"Permutation language modeling"),(0,s.kt)("p",null,"PLM is the idea of capturing bidirectional context by training an autoregressive model on all possible permutation of words in a sentence. Instead of fixed left-right or right-left modeling, XLNET maximizes expected log likelihood over all possible permutations of the sequence. In expectation, each position learns to utilize contextual information from all positions thereby capturing bidirectional context. No ","[MASK]"," is needed and input data need not be corrupted."),(0,s.kt)("p",null,(0,s.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/216823090-cb38a865-e721-42ac-a5d5-23c1e288e69e.png",alt:"content-concepts-raw-nlp-language-modeling-untitled-1"})),(0,s.kt)("p",null,"The above diagram illustrates PLM. Let us consider that we are learning x3 (the token at the 3rd position in the sentence). PLM trains an autoregressive model with various permutations of the tokens in the sentence, so that in the end of all such permutations, we would have learnt x3, given all other words in the sentence. In the above illustration, we can see that the next layer takes as inputs only the tokens preceding x3 in the permutation sequence. This way, autoregression is also achieved."),(0,s.kt)("h2",{id:"text-preprocessing-and-cleaning"},"Text Preprocessing and Cleaning"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"import nltk\nfrom nltk.corpus import stopwords\nstopwords = list(set(stopwords.words('english')))\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()\n\ndef preprocess(text):\n  text = ' ' + text + ' '\n  text = re.sub(confidential_label, 'confidential', text)\n  text = text.lower()\n  text = re.sub(r'[^a-z ]', ' ', text)\n  text = re.sub(r'\\b[a-z]\\b', ' ', text)\n  text = ' '.join([lemmatizer.lemmatize(w, 'v') for w in text.split()])\n  text = ' '.join([w for w in text.split() if not w in stopwords])\n  text = ' '.join([w for w in text.split() if not w in custom_stopwords])\n  seen = set()\n  seen_add = seen.add\n  text = ' '.join([x for x in text.split() if not (x in seen or seen_add(x))])\n  return text\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"import pandas as pd\nimport re\nimport nltk\nfrom bs4 import BeautifulSoup\nfrom itertools import groupby\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nps = PorterStemmer()\nnltk.download('stopwords')\nstopwords = list(set(stopwords.words('english')))\nfrom nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()\nnltk.download('wordnet')\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"!pip install ekphrasis\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\n\ntext_processor = TextPreProcessor(\n  normalize=['url', 'email', 'percent', 'money', 'phone', \n              'user', 'time', 'date', 'number'],\n  ### annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n  ###           'emphasis', 'censored'},\n  fix_html=True,\n  segmenter=\"twitter\",\n  corrector=\"twitter\", \n  unpack_hashtags=True,\n  unpack_contractions=True,\n  spell_correct_elong=False,\n  tokenizer=SocialTokenizer(lowercase=False).tokenize,\n  dicts=[emoticons]\n  )\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"!pip install autocorrect\nimport autocorrect\nspeller = autocorrect.Speller()\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"!pip install clean-text[gpl]\nfrom cleantext import clean\n\ndef clean_text(text):\n    if type(text) is float:\n        return ' '\n    text = ' ' + text + ' '\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n    ### noise removal\n    rules = [\n        {r'>\\s+': u'>'},  ### remove spaces after a tag opens or closes\n        {r'\\s+': u' '},  ### replace consecutive spaces\n        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  ### newline after a <br>\n        {r'</(div)\\s*>\\s*': u'\\n'},  ### newline after </p> and </div> and <h1/>...\n        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  ### newline after </p> and </div> and <h1/>...\n        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  ### remove <head> to </head>\n        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  ### show links instead of texts\n        {r'[ \\t]*<[^<]*?/?>': u''},  ### remove remaining tags\n        {r'^\\s+': u''}  ### remove spaces at the beginning\n    ]\n    for rule in rules:\n        for (k, v) in rule.items():\n            regex = re.compile(k)\n            text = regex.sub(v, text)\n    text = text.rstrip()\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r\"#\", \"\", text)\n    text = re.sub(r'<.*?>', ' ', text)\n    text = re.sub(r'\\{[^{}]*\\}', ' ', text)\n    text = re.sub(r'\\s', ' ', text)\n    text = ' '.join(text_processor.pre_process_doc(text))\n    text = ' '.join(text.split())\n    text = re.sub(r'(?:\\d+[a-zA-Z]+|[a-zA-Z]+\\d+)', '<hash>', text)\n    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n    text = ' '.join([k for k,v in groupby(text.split())])\n    text = text.lower()\n    text = clean(text,\n        fix_unicode=True,               ### fix various unicode errors\n        to_ascii=True,                  ### transliterate to closest ASCII representation\n        lower=True,                     ### lowercase text\n        no_line_breaks=False,           ### fully strip line breaks as opposed to only normalizing them\n        no_urls=True,                  ### replace all URLs with a special token\n        no_emails=True,                ### replace all email addresses with a special token\n        no_phone_numbers=True,         ### replace all phone numbers with a special token\n        no_numbers=True,               ### replace all numbers with a special token\n        no_digits=True,                ### replace all digits with a special token\n        no_currency_symbols=True,      ### replace all currency symbols with a special token\n        no_punct=True,                 ### remove punctuations\n        replace_with_punct=\"\",          ### instead of removing punctuations you may replace them\n        replace_with_url=\"<URL>\",\n        replace_with_email=\"<EMAIL>\",\n        replace_with_phone_number=\"<PHONE>\",\n        replace_with_number=\"<NUMBER>\",\n        replace_with_digit=\"0\",\n        replace_with_currency_symbol=\"<CUR>\",\n        lang=\"en\"                       ### set to 'de' for German special handling\n    )\n    text = re.sub(r'[^a-z<> ]', ' ', text)\n    text = re.sub(r'\\b[a-z]\\b', ' ', text)\n    ### text = speller.autocorrect_sentence(text)\n    ### text = ' '.join([ps.stem(w) for w in text.split()])\n    ### text = ' '.join([lemmatizer.lemmatize(w, 'v') for w in text.split()])\n    ### text = ' '.join([w for w in text.split() if not w in stopwords])\n    ### seen = set()\n    ### seen_add = seen.add\n    ### text = ' '.join([x for x in text.split() if not (x in seen or seen_add(x))])\n    text = ' '.join(text.split())\n    return text\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"import re\nNON_ALPHANUM = re.compile(r'[\\W]')\nNON_ASCII = re.compile(r'[^a-z0-1\\s]')\n\ndef normalize_texts(texts):\n  normalized_texts = ''\n  lower = texts.lower()\n  no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n  no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n  return no_non_ascii\n\ndef get_ntc(text):\n    name = text.split('-')[0].strip().strip(',')\n    text1 = ' '.join(text.split('-')[1:])\n    title = text1.split('/')[0].strip().strip(',')\n    company = '/'.join(text.split('/')[1:]).split('Phone')[0].split('Email')[0].strip().strip(',')\n    return [x.strip() for x in [name, title, company]]\n\ndef get_phone_number(text):\n    try:\n        return text.split('Phone:')[1].split('Email')[0].strip().strip(',')\n    except:\n        return None\n\n### phoneRegex = re.compile(r'''((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))''', re.VERBOSE)\nemailRegex = re.compile(r'''(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])''', re.VERBOSE)\n\ndef get_email(text):\n    matches = []\n    for groups in emailRegex.findall(text):\n        matches.append(groups) \n    return ','.join(matches).strip().strip(',')\n\ndef split_phone(text):\n    text = text.replace('/',',')\n    text = re.sub(\"[^+,0-9]\", \" \", text)\n    return text.strip().strip(',')\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"import ast\n\ndef pandas_list_parse(lst):\n    return ','.join(list(set(ast.literal_eval(lst))))\n\ndef pandas_list_explode(df, col, prefix=None, drop_original=True, explode_type=1, split_sign=',', n=5):\n    if prefix is None:\n        prefix = col+'_'\n    if explode_type==1:\n        df[col] = df[col].apply(lambda x: '///'.join(x))\n        _df = df[col].str.split('///', expand=True).add_prefix(prefix)\n    else:\n        _df = df[col].str.split(split_sign, expand=True).add_prefix(prefix)\n    if _df.shape[1]>n:\n        _df = _df.iloc[:,:n]\n    df = df.join(_df)\n    if drop_original:\n        df.drop(col, axis=1, inplace=True)\n    return df\n")),(0,s.kt)("h2",{id:"tf-idf-vectorizor"},"TF-IDF Vectorizor"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english', max_df=0.70, min_df=5, ngram_range=(1,2))\nX = vect.fit_transform(text)\n\nX_dense = X.todense()\nfrom sklearn.decomposition import PCA\ncoords = PCA(n_components=2).fit_transform(X_dense)\nplt.scatter(coords[:, 0], coords[:, 1], c='m')\nplt.show()\n\ndef top_tfidf_feats(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n    return df\ndef top_feats_in_doc(X, features, row_id, top_n=25):\n    row = np.squeeze(X[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\ndf1 = df.copy()\ndf1['text'] = df1.text.apply(preprocess)\n### df1[df1.text.str.contains('avast')]\n\nfeatures = vect.get_feature_names()\nprint(top_feats_in_doc(X, features, 10, 10))\n\ndef top_mean_feats(X, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n  if grp_ids:\n      D = X[grp_ids].toarray()\n  else:\n      D = X.toarray()\n  D[D < min_tfidf] = 0\n  tfidf_means = np.mean(D, axis=0)\n  return top_tfidf_feats(tfidf_means, features, top_n)\n\n  print(top_mean_feats(X, features, top_n=10))\n")),(0,s.kt)("h2",{id:"count-vectorizer"},"Count Vectorizer"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(stop_words='english', max_df=0.5, min_df=10, ngram_range=(1,3))\nvectorizer.fit_transform(df.clean.tolist())\n\nidx = 100\nprint(df.text.iloc[[idx]].tolist()[0])\npd.DataFrame(vectorizer.inverse_transform(vectorizer.transform([df.clean.tolist()[idx]]))).T[0]\n")),(0,s.kt)("h2",{id:"wordcloud"},"Wordcloud"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},"from wordcloud import WordCloud\nall_words = ''.join([word for word in df['clean_text']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\nplt.figure(figsize=(15, 8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title(\"Some frequent words used in the headlines\", weight='bold', fontsize=14)\nplt.show()\n")),(0,s.kt)("h2",{id:"labs"},"Labs"),(0,s.kt)("ol",null,(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-07-09-concept-nlp-basics.ipynb"},"Natural Language Processing 101")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-07-10-concept-embedding-gensim-fasttext.ipynb"},"Training Embeddings Using Gensim and FastText")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2022-01-29-nlp.ipynb"},"NLP Playground")),(0,s.kt)("li",{parentName:"ol"},(0,s.kt)("a",{parentName:"li",href:"https://nbviewer.org/github/recohut/notebook/blob/master/_notebooks/2022-01-02-email-classification.ipynb"},"Email Classification"))))}c.isMDXComponent=!0},77226:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/content-concepts-raw-text-classification-img-eb03940957e68d21cdb50689827b00ad.png"},75026:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/nlp_vec_avg-24152a7e9e98f2e56d7522eef666b863.png"}}]);