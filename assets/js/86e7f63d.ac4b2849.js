"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[93299],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>c});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},u=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),m=p(a),c=i,h=m["".concat(s,".").concat(c)]||m[c]||d[c]||o;return a?n.createElement(h,r(r({ref:t},u),{},{components:a})):n.createElement(h,r({ref:t},u))}));function c(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:i,r[1]=l;for(var p=2;p<o;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},4454:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var n=a(87462),i=(a(67294),a(3905));const o={},r="GCP Dataflow Streaming Pipeline",l={unversionedId:"processing/lab-gcp-dataflow-stream-pipeline",id:"processing/lab-gcp-dataflow-stream-pipeline",title:"GCP Dataflow Streaming Pipeline",description:"Objective",source:"@site/docs/03-processing/lab-gcp-dataflow-stream-pipeline.md",sourceDirName:"03-processing",slug:"/processing/lab-gcp-dataflow-stream-pipeline",permalink:"/docs/processing/lab-gcp-dataflow-stream-pipeline",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"GCP Dataflow Size Inputs",permalink:"/docs/processing/lab-gcp-dataflow-side-inputs"},next:{title:"GCP Dataprep",permalink:"/docs/processing/lab-gcp-dataprep"}},s={},p=[{value:"Objective",id:"objective",level:2},{value:"Preparation",id:"preparation",level:2},{value:"Jupyter notebook-based development environment setup",id:"jupyter-notebook-based-development-environment-setup",level:3},{value:"Download Code Repository",id:"download-code-repository",level:3},{value:"Open the appropriate lab",id:"open-the-appropriate-lab",level:3},{value:"Task 1. Reading from a streaming source",id:"task-1-reading-from-a-streaming-source",level:2},{value:"Task 2. Window the data",id:"task-2-window-the-data",level:2},{value:"Task 3. Aggregate the data",id:"task-3-aggregate-the-data",level:2},{value:"Task 4. Write to BigQuery",id:"task-4-write-to-bigquery",level:2},{value:"Write aggregated data to BigQuery",id:"write-aggregated-data-to-bigquery",level:3},{value:"BigQuery insertion method",id:"bigquery-insertion-method",level:3},{value:"Task 5. Run your pipeline",id:"task-5-run-your-pipeline",level:2},{value:"Task 6. Generate lag-less streaming input",id:"task-6-generate-lag-less-streaming-input",level:2},{value:"Examine the results",id:"examine-the-results",level:3},{value:"Task 7. Introduce lag to streaming input",id:"task-7-introduce-lag-to-streaming-input",level:2},{value:"Generate streaming input with lag",id:"generate-streaming-input-with-lag",level:3},{value:"Examine the results",id:"examine-the-results-1",level:3}],u={toc:p};function d(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"gcp-dataflow-streaming-pipeline"},"GCP Dataflow Streaming Pipeline"),(0,i.kt)("h2",{id:"objective"},"Objective"),(0,i.kt)("p",null,"Serverless Data Processing with Dataflow - Using Dataflow for Streaming Analytics (Python)"),(0,i.kt)("p",null,"In this lab, you take many of the concepts introduced in a batch context and apply them in a streaming context to create a pipeline similar to batch_minute_traffic_pipeline, but which operates in real time. The finished pipeline will first read JSON messages from Pub/Sub and parse those messages before branching. One branch writes some raw data to BigQuery and takes note of event and processing time. The other branch windows and aggregates the data and then writes the results to BigQuery."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Objectives")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Read data from a streaming source."),(0,i.kt)("li",{parentName:"ul"},"Write data to a streaming sink."),(0,i.kt)("li",{parentName:"ul"},"Window data in a streaming context."),(0,i.kt)("li",{parentName:"ul"},"Experimentally verify the effects of lag.")),(0,i.kt)("p",null,"You will build the following pipeline:"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003347-8efa44e9-831b-49f4-a1d1-a1261b6436de.png",alt:null})),(0,i.kt)("h2",{id:"preparation"},"Preparation"),(0,i.kt)("h3",{id:"jupyter-notebook-based-development-environment-setup"},"Jupyter notebook-based development environment setup"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In the Console, expand the Navigation menu (Navigation menu icon), then select Vertex AI > Workbench."),(0,i.kt)("li",{parentName:"ul"},"Enable Notebooks API."),(0,i.kt)("li",{parentName:"ul"},"At the top of the page click New Notebook, and select Smart Analytics Framework > Apache Beam > Without GPUs"),(0,i.kt)("li",{parentName:"ul"},"In the dialog box that appears, set the region to us-central1 and then click CREATE at the bottom."),(0,i.kt)("li",{parentName:"ul"},"Once the environment is ready, click the OPEN JUPYTERLAB link next to your Notebook name. This will open up your environment in a new tab in your browser."),(0,i.kt)("li",{parentName:"ul"},"Next, click Terminal. This will open up a terminal where you can run all the commands in this lab.")),(0,i.kt)("h3",{id:"download-code-repository"},"Download Code Repository"),(0,i.kt)("p",null,"Next you will download a code repository for use in this lab."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In the terminal you just opened, enter the following:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"git clone https://github.com/GoogleCloudPlatform/training-data-analyst\ncd /home/jupyter/training-data-analyst/quests/dataflow_python/\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"On the left panel of your notebook environment, in the file browser, you will notice the training-data-analyst repo added.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Navigate into the cloned repo /training-data-analyst/quests/dataflow_python/. You will see a folder for each lab, which is further divided into a lab sub-folder with code to be completed by you, and a solution sub-folder with a fully workable example to reference if you get stuck."))),(0,i.kt)("p",null,"Note: To open a file for editing purposes, simply navigate to the file and click on it. This will open the file, where you can add or modify code."),(0,i.kt)("h3",{id:"open-the-appropriate-lab"},"Open the appropriate lab"),(0,i.kt)("p",null,"In your terminal, run the following commands to change to the directory you will use for this lab:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"# Change directory into the lab\ncd 5_Streaming_Analytics/lab\nexport BASE_DIR=$(pwd)\n")),(0,i.kt)("p",null,"Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies. Execute the following to create a virtual environment for your work in this lab:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},'sudo apt-get update && sudo apt-get install -y python3-venv\npython3 -m venv df-env\nsource df-env/bin/activate\npython3 -m pip install -q --upgrade pip setuptools wheel\npython3 -m pip install apache-beam[gcp]\ngcloud services enable dataflow.googleapis.com\n\n# Finally, grant the dataflow.worker role to the Compute Engine default service account:\nPROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NUMBER=$(gcloud projects list --filter="$PROJECT_ID" --format="value(PROJECT_NUMBER)")\nexport serviceAccount=""$PROJECT_NUMBER"-compute@developer.gserviceaccount.com"\n\n# Create GCS buckets and BQ dataset\ncd $BASE_DIR/../..\nsource create_streaming_sinks.sh\n# Generate event dataflow\nsource generate_batch_events.sh\n# Change to the directory containing the practice version of the code\ncd $BASE_DIR\n')),(0,i.kt)("h2",{id:"task-1-reading-from-a-streaming-source"},"Task 1. Reading from a streaming source"),(0,i.kt)("p",null,'In the previous labs, you used beam.io.ReadFromText to read from Google Cloud Storage. In this lab, instead of Google Cloud Storage, you use Pub/Sub. Pub/Sub is a fully managed real-time messaging service that allows publishers to send messages to a "topic," to which subscribers can subscribe via a "subscription."'),(0,i.kt)("p",null,"The pipeline you create subscribes to a topic called\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"my_topic"),"\xa0that you just created via\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"create_streaming_sinks.sh"),"\xa0script. In a production situation, this topic will often be created by the publishing team. You can view it in the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/cloudpubsub/topic/list"},"Pub/Sub portion of the console"),"."),(0,i.kt)("p",null,"In the file explorer, navigate to\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"training-data-analyst/quest/dataflow_python/5_Streaming_Analytics/lab/"),"\xa0and open the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"streaming_minute_traffic_pipeline.py"),"\xa0file."),(0,i.kt)("p",null,"To read from Pub/Sub using Apache Beam's IO connectors, add a transform to the pipeline which uses the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.io.gcp.pubsub.html?highlight=pubsub#apache_beam.io.gcp.pubsub.ReadFromPubSub"},(0,i.kt)("inlineCode",{parentName:"a"},"beam.io.ReadFromPubSub()")),"\xa0class. This class has attributes for specifying the source topic as well as the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"timestamp_attribute"),". By default, this attribute is set to the message publishing time."),(0,i.kt)("p",null,"Note:\xa0Publication time is the time when the Pub/Sub service first receives the message. In systems where there may be a delay between the actual event time and publish time (i.e., late data) and you would like to take this into account, the client code publishing the message needs to set a 'timestamp' metadata attribute on the message and provide the actual event timestamp, since Pub/Sub will not natively know how to extract the event timestamp embedded in the payload. You can see the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/efc7ed26b88d54bc1d8c0c0376ed01558d1f3b59/quests/dataflow/streaming_event_generator.py#L112"},"client code generating the messages you'll use here"),"."),(0,i.kt)("p",null,"To complete this task:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Add a transform that reads from the Pub/Sub topic specified by the\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"input_topic"),"\xa0command-line parameter."),(0,i.kt)("li",{parentName:"ul"},"Then, use the provided function,\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"parse_json"),"\xa0with\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"beam.Map"),"\xa0to convert each JSON string into a\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"CommonLog"),"\xa0instance."),(0,i.kt)("li",{parentName:"ul"},"Collect the results from this transform into a\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"PCollection"),"\xa0of\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"CommonLog"),"\xa0instances using\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"with_output_types()"),".")),(0,i.kt)("p",null,"In the first\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"#TODO"),", add the following code:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"beam.io.ReadFromPubSub(input_topic)\n")),(0,i.kt)("h2",{id:"task-2-window-the-data"},"Task 2. Window the data"),(0,i.kt)("p",null,"In the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_minute_traffic_pipeline.py"},"previous non-SQL lab"),", you implemented fixed-time windowing in order to group events by event time into mutually-exclusive windows of fixed size. Do the same thing here with the streaming inputs. Feel free to reference the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_minute_traffic_pipeline.py"},"previous lab's code"),"\xa0or the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/5_Streaming_Analytics/solution/streaming_minute_traffic_pipeline.py"},"solution"),"\xa0if you get stuck."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Window into one-minute windows")),(0,i.kt)("p",null,"To complete this task:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Add a transform to your pipeline that accepts the\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"PCollection"),"\xa0of\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"CommonLog"),"\xa0data and windows elements into windows of\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"window_duration"),"\xa0seconds long, with\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"window_duration"),"\xa0as another command-line parameter."),(0,i.kt)("li",{parentName:"ol"},"Use the following code to add a transform to your pipeline that windows elements into one-minute windows:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'"WindowByMinute" >> beam.WindowInto(beam.window.FixedWindows(60))\n')),(0,i.kt)("h2",{id:"task-3-aggregate-the-data"},"Task 3. Aggregate the data"),(0,i.kt)("p",null,"In the previous lab, you used the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Count"},(0,i.kt)("inlineCode",{parentName:"a"},"CountCombineFn()")),"\xa0combiner to count the number of events per window. Do the same here."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Count events per window")),(0,i.kt)("p",null,"To complete this task:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Pass the windowed\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"PCollection"),"\xa0as input to a transform that counts the number of events per window."),(0,i.kt)("li",{parentName:"ol"},"After this, use the provided\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"DoFn"),",\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"GetTimestampFn"),", with\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"beam.ParDo"),"\xa0to include the window start timestamp."),(0,i.kt)("li",{parentName:"ol"},"Use the following code to add a transform to your pipeline that counts the number of events per window:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'"CountPerMinute" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n')),(0,i.kt)("h2",{id:"task-4-write-to-bigquery"},"Task 4. Write to BigQuery"),(0,i.kt)("p",null,"This pipeline writes to BigQuery in two separate branches. The first branch writes the aggregated data to BigQuery. The second branch, which has already been authored for you, writes out some metadata regarding each raw event, including the event timestamp and the actual processing timestamp. Both write directly to BigQuery via streaming inserts."),(0,i.kt)("h3",{id:"write-aggregated-data-to-bigquery"},"Write aggregated data to BigQuery"),(0,i.kt)("p",null,"Writing to BigQuery has been covered extensively in previous labs, so the basic mechanics will not be covered in depth here."),(0,i.kt)("p",null,"To complete this task:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Create a new command-line parameter called\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"agg_table_name"),"\xa0for the table intended to house aggregated data."),(0,i.kt)("li",{parentName:"ul"},"Add a transfrom as before that writes to BigQuery.")),(0,i.kt)("p",null,"Note:\xa0When in a streaming context,\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"beam.io.WriteToBigQuery()"),"\xa0does not support\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"write_disposition"),"\xa0of\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"WRITE_TRUNCATE"),"\xa0in which the table is dropped and recreated. In this example, use\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"WRITE_APPEND"),"."),(0,i.kt)("h3",{id:"bigquery-insertion-method"},"BigQuery insertion method"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"beam.io.WriteToBigQuery"),"\xa0will default to either\xa0",(0,i.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery/streaming-data-into-bigquery"},"streaming inserts"),"\xa0for unbounded PCollections or\xa0",(0,i.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage"},"batch file load jobs"),"\xa0for bounded PCollections. Streaming inserts can be particularly useful when you want data to show up in aggregations immediately, but does incur\xa0",(0,i.kt)("a",{parentName:"p",href:"https://cloud.google.com/bigquery/pricing#streaming_pricing"},"extra charges"),". In streaming use cases where you are OK with periodic batch uploads on the order of every couple minutes, you can specify this behavior via the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"method"),"\xa0keyword argument, and also set the frequency with the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"triggering_frequency"),"\xa0keyword argument. Learn more from the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.io.gcp.bigquery.html#apache_beam.io.gcp.bigquery.WriteToBigQuery"},"Write data to BigQuery section of the apache_beam.io.gcp.bigquery module documentation"),"."),(0,i.kt)("p",null,"Use the following code to add a transform to your pipeline that writes aggregated data to the BigQuery table."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n  agg_table_name,\n  schema=agg_table_schema,\n  create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n  write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n  )\n")),(0,i.kt)("h2",{id:"task-5-run-your-pipeline"},"Task 5. Run your pipeline"),(0,i.kt)("p",null,"Return to the terminal and execute the following code to run your pipeline:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"export PROJECT_ID=$(gcloud config get-value project)\nexport REGION='us-central1'\nexport BUCKET=gs://${PROJECT_ID}\nexport PIPELINE_FOLDER=${BUCKET}\nexport RUNNER=DataflowRunner\nexport PUBSUB_TOPIC=projects/${PROJECT_ID}/topics/my_topic\nexport WINDOW_DURATION=60\nexport AGGREGATE_TABLE_NAME=${PROJECT_ID}:logs.windowed_traffic\nexport RAW_TABLE_NAME=${PROJECT_ID}:logs.raw\npython3 streaming_minute_traffic_pipeline.py\\\n--project=${PROJECT_ID}\\\n--region=${REGION}\\\n--staging_location=${PIPELINE_FOLDER}/staging\\\n--temp_location=${PIPELINE_FOLDER}/temp\\\n--runner=${RUNNER}\\\n--input_topic=${PUBSUB_TOPIC}\\\n--window_duration=${WINDOW_DURATION}\\\n--agg_table_name=${AGGREGATE_TABLE_NAME}\\\n--raw_table_name=${RAW_TABLE_NAME}\n")),(0,i.kt)("p",null,"Note:\xa0If you get a Dataflow pipeline failed error saying that it is unable to open the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"pipeline.py"),"\xa0file, run the pipeline again and it should run with no issues."),(0,i.kt)("p",null,"Ensure in the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/dataflow/jobs"},"Dataflow UI"),"\xa0that it executes successfully without errors. Note that there is no data yet being created and ingested by the pipeline, so it will be running but not processing anything. You will introduce data in the next step."),(0,i.kt)("h2",{id:"task-6-generate-lag-less-streaming-input"},"Task 6. Generate lag-less streaming input"),(0,i.kt)("p",null,"Because this is a streaming pipeline, it subscribes to the streaming source and will await input; there is none currently. In this section, you generate data with no lag. Actual data will almost invariably contain lag. However, it is instructive to understand lag-less streaming inputs."),(0,i.kt)("p",null,"The code for this quest includes a script for publishing JSON events using Pub/Sub."),(0,i.kt)("p",null,"To complete this task and start publishing messages, open a\xa0new terminal\xa0side-by-side with your current one and run the following script. It will keep publishing messages until you kill the script. Make sure you are in the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"training-data-analyst/quests/dataflow_python"),"\xa0folder."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"bash generate_streaming_events.sh\n")),(0,i.kt)("h3",{id:"examine-the-results"},"Examine the results"),(0,i.kt)("p",null,"Wait a couple minutes for the data to start to populate. Then navigate to\xa0",(0,i.kt)("a",{parentName:"p",href:"http://console.cloud.google.com/bigquery"},"BigQuery"),"\xa0and query the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"logs.minute_traffic"),"\xa0table with the following query:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"SELECT timestamp, page_views\nFROM `logs.windowed_traffic`\nORDER BY timestamp ASC\n")),(0,i.kt)("p",null,"You should see that the number of pageviews hovered around 100 views a minute."),(0,i.kt)("p",null,"Alternatively, you can use the BigQuery command-line tool as a quick way to confirm results are being written:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"bq head logs.raw\nbq head logs.windowed_traffic\n")),(0,i.kt)("p",null,"Now, enter the following query:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"SELECT\n  UNIX_MILLIS(TIMESTAMP(event_timestamp)) - min_millis.min_event_millis AS event_millis,\n  UNIX_MILLIS(TIMESTAMP(processing_timestamp)) - min_millis.min_event_millis AS processing_millis,\n  user_id,\n  -- added as unique label so we see all the points\n  CAST(UNIX_MILLIS(TIMESTAMP(event_timestamp)) - min_millis.min_event_millis AS STRING) AS label\nFROM\n  `logs.raw`\nCROSS JOIN (\n  SELECT\n    MIN(UNIX_MILLIS(TIMESTAMP(event_timestamp))) AS min_event_millis\n  FROM\n    `logs.raw`) min_millis\nWHERE\n  event_timestamp IS NOT NULL\nORDER BY\n  event_millis ASC\n")),(0,i.kt)("p",null,"This query illustrates the gap between event time and processing time. However, it can be hard to see the big picture by looking at just the raw tabular data. We will use Data Studio, a lightweight data visualization and BI engine."),(0,i.kt)("p",null,"To enable Data Studio:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Visit\xa0",(0,i.kt)("a",{parentName:"li",href:"https://datastudio.google.com/"},"https://datastudio.google.com"),"."),(0,i.kt)("li",{parentName:"ul"},"Click\xa0Create\xa0in the upper left."),(0,i.kt)("li",{parentName:"ul"},"Click\xa0Report."),(0,i.kt)("li",{parentName:"ul"},"Click through the Terms of Service and then click\xa0Done."),(0,i.kt)("li",{parentName:"ul"},"Return to the BigQuery UI."),(0,i.kt)("li",{parentName:"ul"},"In the BigQuery UI, click on the\xa0Explore data\xa0button and choose\xa0Explore With Data Studio."),(0,i.kt)("li",{parentName:"ul"},"This will open a new window."),(0,i.kt)("li",{parentName:"ul"},"In the panel on the right-hand side of this window, select the\xa0scatter chart\xa0type.")),(0,i.kt)("p",null,"In the\xa0Data\xa0column of the panel on the right hand side, set the following values:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Dimension: label"),(0,i.kt)("li",{parentName:"ul"},"Hierarchy: disabled"),(0,i.kt)("li",{parentName:"ul"},"Metric X: event_millis"),(0,i.kt)("li",{parentName:"ul"},"Metric Y: processing_millis")),(0,i.kt)("p",null,"The chart will transform to be a scatterplot, where all points are on the diagonal. This is because in the streaming data currently being generated, events are processed immediately after they were generated --- there was no lag. If you started the data generation script quickly, i.e. before the Dataflow job was fully up and running, you may see a hockey stick, as there were messages queuing in Pub/Sub that were all processed more or less at once."),(0,i.kt)("p",null,"But in the real world, lag is something that pipelines need to cope with."),(0,i.kt)("h2",{id:"task-7-introduce-lag-to-streaming-input"},"Task 7. Introduce lag to streaming input"),(0,i.kt)("p",null,"The streaming event script is capable of generating events with simulated lag."),(0,i.kt)("p",null,"This represents scenarios where there is a time delay between when the events are generated and published to Pub/Sub, for example when a mobile client goes into offline mode if a user has no service, but events are collected on the device and all published at once when the device is back online."),(0,i.kt)("h3",{id:"generate-streaming-input-with-lag"},"Generate streaming input with lag"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"First, close the Data Studio window."),(0,i.kt)("li",{parentName:"ol"},"Then, to turn on lag, return to the terminal and stop the running script using\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"CTRL+C"),"."),(0,i.kt)("li",{parentName:"ol"},"Then, run the following:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"bash generate_streaming_events.sh true\n")),(0,i.kt)("h3",{id:"examine-the-results-1"},"Examine the results"),(0,i.kt)("p",null,"Return to the BigQuery UI, rerun the query, and then recreate the Data Studio view as before. The new data that arrive, which should appear on the right side of the chart, should no longer be perfect; instead, some will appear above the diagonal, indicating that they were processed after the events transpired."),(0,i.kt)("p",null,"Chart Type: Scatter"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Dimension: label"),(0,i.kt)("li",{parentName:"ul"},"Hierarchy: disabled"),(0,i.kt)("li",{parentName:"ul"},"Metric X: event_millis"),(0,i.kt)("li",{parentName:"ul"},"Metric Y: processing_millis")))}d.isMDXComponent=!0}}]);