"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[27766],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>m});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),p=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=p(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=p(a),m=i,h=u["".concat(l,".").concat(m)]||u[m]||c[m]||o;return a?n.createElement(h,r(r({ref:t},d),{},{components:a})):n.createElement(h,r({ref:t},d))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=a[p];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},28712:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var n=a(87462),i=(a(67294),a(3905));const o={},r="Delta Lake Optimizations",s={unversionedId:"processing/databricks/lab-deltalake-optimizations/README",id:"processing/databricks/lab-deltalake-optimizations/README",title:"Delta Lake Optimizations",description:"In this lab, we will learn about various Delta Lake optimizations that help us build a more performant Lakehouse.",source:"@site/docs/03-processing/databricks/lab-deltalake-optimizations/README.md",sourceDirName:"03-processing/databricks/lab-deltalake-optimizations",slug:"/processing/databricks/lab-deltalake-optimizations/",permalink:"/docs/processing/databricks/lab-deltalake-optimizations/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"S3 Postgres Scala",permalink:"/docs/processing/databricks/lab-databricks-scala-postgres-s3/"},next:{title:"dlt vs dbt",permalink:"/docs/processing/databricks/lab-dlt-dbt/"}},l={},p=[{value:"Environment Setup",id:"environment-setup",level:2},{value:"Working with the OPTIMIZE and ZORDER commands",id:"working-with-the-optimize-and-zorder-commands",level:2},{value:"Using Auto Optimize",id:"using-auto-optimize",level:2},{value:"Understanding optimized writes",id:"understanding-optimized-writes",level:3},{value:"Understanding Auto Compaction",id:"understanding-auto-compaction",level:3},{value:"Learning about delta caching",id:"learning-about-delta-caching",level:2},{value:"Learning about dynamic partition pruning",id:"learning-about-dynamic-partition-pruning",level:2},{value:"Understanding bloom filter indexing",id:"understanding-bloom-filter-indexing",level:2},{value:"Summary",id:"summary",level:2}],d={toc:p};function c(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"delta-lake-optimizations"},"Delta Lake Optimizations"),(0,i.kt)("p",null,"In this lab, we will learn about various Delta Lake optimizations that help us build a more performant Lakehouse."),(0,i.kt)("p",null,"We will cover the following topics:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Working with the\xa0",(0,i.kt)("strong",{parentName:"li"},"OPTIMIZE"),"\xa0and\xa0",(0,i.kt)("strong",{parentName:"li"},"ZORDER"),"\xa0commands"),(0,i.kt)("li",{parentName:"ul"},"Using\xa0",(0,i.kt)("strong",{parentName:"li"},"AUTO OPTIMIZE")),(0,i.kt)("li",{parentName:"ul"},"Learning about delta caching"),(0,i.kt)("li",{parentName:"ul"},"Learning about dynamic partition pruning"),(0,i.kt)("li",{parentName:"ul"},"Understanding bloom filter indexing")),(0,i.kt)("p",null,"Delta Lake is an open source storage layer that provides functionalities to data in the data lake that only exist in data warehouses. When combined with cloud storage,\xa0",(0,i.kt)("strong",{parentName:"p"},"Databricks"),"\xa0and Delta Lake lead to the formation of a\xa0",(0,i.kt)("strong",{parentName:"p"},"Lakehouse"),". A Lakehouse simply provides the best of both worlds --\xa0",(0,i.kt)("strong",{parentName:"p"},"data lakes"),"\xa0and\xa0",(0,i.kt)("strong",{parentName:"p"},"data warehouses"),". In today's world, a Lakehouse provides the same set of capabilities as a traditional data warehouse and at a much lower cost. This is made possible due to cheap cloud storage such as Azure Data Lake, Spark as the processing engine, and data being stored in the Delta Lake format. In this lab, we will learn about various Delta Lake optimizations that help us build a more performant Lakehouse."),(0,i.kt)("p",null,"In this lab, we will cover the following topics:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Working with the\xa0",(0,i.kt)("strong",{parentName:"li"},"OPTIMIZE"),"\xa0and\xa0",(0,i.kt)("strong",{parentName:"li"},"ZORDER"),"\xa0commands"),(0,i.kt)("li",{parentName:"ul"},"Using\xa0",(0,i.kt)("strong",{parentName:"li"},"AUTO OPTIMIZE")),(0,i.kt)("li",{parentName:"ul"},"Learning about delta caching"),(0,i.kt)("li",{parentName:"ul"},"Learning about dynamic partition pruning"),(0,i.kt)("li",{parentName:"ul"},"Understanding bloom filter indexing")),(0,i.kt)("h2",{id:"environment-setup"},"Environment Setup"),(0,i.kt)("p",null,"You will find a dbc file in assets folder. Import this in your databricks workspace."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/218763268-0f50845e-eb1f-4213-8bf7-4594ff3946ea.png",alt:null})),(0,i.kt)("h2",{id:"working-with-the-optimize-and-zorder-commands"},"Working with the OPTIMIZE and ZORDER commands"),(0,i.kt)("p",null,"Delta lake on Databricks\xa0lets you speed up queries by changing\xa0the layout of the data stored in the cloud storage. The algorithms that support this functionality are as follows:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Bin-packing"),": This uses the\xa0",(0,i.kt)("strong",{parentName:"li"},"OPTIMIZE"),"\xa0command and helps coalesce small files\xa0into larger ones."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Z-Ordering"),": This uses the\xa0",(0,i.kt)("strong",{parentName:"li"},"ZORDER"),"\xa0command and helps collocate data in the same set of files. This co-locality\xa0helps reduce the amount of data that's read by Spark while processing.")),(0,i.kt)("p",null,"Use ",(0,i.kt)("inlineCode",{parentName:"p"},"1. Working with OPTIMIZE and ZORDER")," notebook for this recipe."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"OPTIMIZE"),"\xa0and\xa0",(0,i.kt)("strong",{parentName:"p"},"ZORDER"),"\xa0can be used to speed up Databricks queries. As a best practice,\xa0",(0,i.kt)("strong",{parentName:"p"},"ZORDER"),"\xa0should be used on columns that are commonly used in queries to filter data and have high cardinality. But\xa0",(0,i.kt)("em",{parentName:"p"},"Z-Ordering"),"\xa0on too many columns can also degrade performance. Hence, the columns to Z-Order on should be chosen wisely.\xa0",(0,i.kt)("em",{parentName:"p"},"Bin-packing"),"\xa0should always be used when different transactions such as inserts, deletes, or updates are being executed on a delta table. Also, it is an idempotent process, meaning that if the\xa0",(0,i.kt)("strong",{parentName:"p"},"OPTIMIZE"),"\xa0command is run twice on a table, the second run will have no effect."),(0,i.kt)("h2",{id:"using-auto-optimize"},"Using Auto Optimize"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Auto Optimize"),"\xa0is a feature that helps us automatically compact small files while an individual writes to\xa0a delta table. Unlike bin-packing, we do not need to\xa0run a command every time Auto Optimize is executed. It consists of two components:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Optimized Writes"),": Databricks dynamically optimizes Spark partition sizes to write 128 MB\xa0chunks of table partitions."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Auto Compaction"),": Here, Databricks runs an optimized job when the data writing process\xa0has been completed and compacts small files. It tries to coalesce small files into 128 MB files. This works on data that has the greatest number of small files.")),(0,i.kt)("p",null,"Use ",(0,i.kt)("inlineCode",{parentName:"p"},"2. Using AUTO OPTIMIZE")," notebook for this recipe."),(0,i.kt)("p",null,"In this recipe, we will understand how Auto Optimize has been applied to our newly written file in delta format. But Auto Optimize may not always be useful in every scenario. To understand this, we will learn when to opt in and when to opt out of the Auto Optimize features."),(0,i.kt)("h3",{id:"understanding-optimized-writes"},"Understanding optimized writes"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Optimized writes"),"\xa0help dynamically optimize Spark partition sizes to write 128 MB chunks of table partitions. Here are\xa0some best practices regarding optimized writes:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Optimized writes involve shuffling data across the executors, so they should only be used if a\xa0minutes' worth of latency is acceptable in streaming jobs."),(0,i.kt)("li",{parentName:"ul"},"It should be used when SQL commands such as\xa0",(0,i.kt)("strong",{parentName:"li"},"UPDATE"),",\xa0",(0,i.kt)("strong",{parentName:"li"},"DELETE"),", and more are frequently used."),(0,i.kt)("li",{parentName:"ul"},"It should not be used when terabytes of data is being processed and storage optimized node instances are not available.")),(0,i.kt)("p",null,"Next, let's learn about Auto Compaction."),(0,i.kt)("h3",{id:"understanding-auto-compaction"},"Understanding Auto Compaction"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Auto Compaction"),"\xa0tries to coalesce small files into 128 MB files and works on data that has the greatest number\xa0of small files. Here are some best practices regarding optimized writes:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Auto Compaction should be used when a minutes' worth of latency is acceptable in streaming jobs."),(0,i.kt)("li",{parentName:"ul"},"If Bin-Packing is not being\xa0done on a delta table, Auto Compaction should be used."),(0,i.kt)("li",{parentName:"ul"},"This feature should not be used when operations such as\xa0",(0,i.kt)("strong",{parentName:"li"},"DELETE"),",\xa0",(0,i.kt)("strong",{parentName:"li"},"UPDATE"),", and more are being applied on a Delta table. This is because Auto Compaction is performed on a table after the write has succeeded. Hence, there could be a transactional conflict between the jobs.")),(0,i.kt)("p",null,"NOTE"),(0,i.kt)("blockquote",null,(0,i.kt)("p",{parentName:"blockquote"},"If Auto Compaction fails due to a conflict, Databricks does not fail or retry the compaction.")),(0,i.kt)("p",null,"This concludes this\xa0section on Auto Optimize. In the next section, we will learn about delta caching."),(0,i.kt)("h2",{id:"learning-about-delta-caching"},"Learning about delta caching"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Delta caching"),"\xa0is an optimization technique that helps speed up queries by storing the data in the cluster node's local storage. The delta cache stores local copies of data that resides in remote\xa0locations such as\xa0",(0,i.kt)("strong",{parentName:"p"},"Azure Data Lake"),"\xa0or\xa0",(0,i.kt)("strong",{parentName:"p"},"Azure Blob Storage"),". It improves the\xa0performance of a wide range of queries but cannot store\xa0the results of arbitrary subqueries."),(0,i.kt)("p",null,"Once delta caching has been enabled, any data that is fetched from an external location is automatically added to the cache. This process does not require action. To preload data into the delta cache, the\xa0",(0,i.kt)("strong",{parentName:"p"},"CACHE"),"\xa0command can be used. Any changes that have been made to\xa0the data persisted in the delta cache are automatically detected by the delta cache. The easiest way to use delta caching is to provision a cluster with\xa0",(0,i.kt)("strong",{parentName:"p"},"Standard_L"),"\xa0series worker types (",(0,i.kt)("strong",{parentName:"p"},"Delta Cache Accelerated"),")."),(0,i.kt)("p",null,"Use ",(0,i.kt)("inlineCode",{parentName:"p"},"3. Learning Delta Caching")," notebook for this recipe."),(0,i.kt)("h2",{id:"learning-about-dynamic-partition-pruning"},"Learning about dynamic partition pruning"),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},"Dynamic partition pruning"),"\xa0is a\xa0",(0,i.kt)("em",{parentName:"p"},"data-skipping technique"),"\xa0that can drastically speed up query execution time. Delta lake collects metadata on the partition files it manages so that data can be skipped without the need to access it. This technique is very useful for\xa0",(0,i.kt)("em",{parentName:"p"},"star schema"),"\xa0types of queries as it can dynamically skip partitions and their respective files. Using this technique, we can prune the partitions of a fact table during the join to a dimension\xa0table. This is made possible when the filter that's applied to a dimension table to prune its partitions is dynamically applied to the fact table."),(0,i.kt)("p",null,"Use ",(0,i.kt)("inlineCode",{parentName:"p"},"4. Learning Dynamic Partition Pruning")," notebook for this recipe."),(0,i.kt)("h2",{id:"understanding-bloom-filter-indexing"},"Understanding bloom filter indexing"),(0,i.kt)("p",null,"A\xa0",(0,i.kt)("strong",{parentName:"p"},"bloom filter index"),"\xa0is a data structure that provides data skipping on columns, especially on fields containing arbitrary text. The filter works by either stating that certain data is definitely\xa0not in a file or that it is probably in the file, which is defined by a\xa0",(0,i.kt)("strong",{parentName:"p"},"false positive probability"),"\xa0(",(0,i.kt)("strong",{parentName:"p"},"FPP"),"). The bloom filter\xa0index can help speed up\xa0",(0,i.kt)("em",{parentName:"p"},"needle in a haystack"),"\xa0type of queries, which are not speed up by other techniques."),(0,i.kt)("p",null,"Use ",(0,i.kt)("inlineCode",{parentName:"p"},"5. Understanding Bloom Filter Indexing")," notebook for this recipe."),(0,i.kt)("h2",{id:"summary"},"Summary"),(0,i.kt)("p",null,"In this lab, we learned about several optimization techniques concerning Databricks Delta Lake. We started with file compaction and clustering techniques and ended with techniques for efficient data skipping. These optimization techniques play a crucial role in making querying and data engineering workloads in Databricks quicker and more efficient."))}c.isMDXComponent=!0}}]);