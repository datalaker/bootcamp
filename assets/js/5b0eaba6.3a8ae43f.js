"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[56776],{3905:(e,n,t)=>{t.d(n,{Zo:()=>d,kt:()=>f});var i=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,i,r=function(e,n){if(null==e)return{};var t,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=i.createContext({}),s=function(e){var n=i.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},d=function(e){var n=s(e.components);return i.createElement(p.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},u=i.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,p=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=s(t),f=r,_=u["".concat(p,".").concat(f)]||u[f]||m[f]||o;return t?i.createElement(_,a(a({ref:n},d),{},{components:t})):i.createElement(_,a({ref:n},d))}));function f(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,a=new Array(o);a[0]=u;var l={};for(var p in n)hasOwnProperty.call(n,p)&&(l[p]=n[p]);l.originalType=e,l.mdxType="string"==typeof e?e:r,a[1]=l;for(var s=2;s<o;s++)a[s]=t[s];return i.createElement.apply(null,a)}return i.createElement.apply(null,t)}u.displayName="MDXCreateElement"},51546:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>s});var i=t(87462),r=(t(67294),t(3905));const o={},a="Model Optimization",l={unversionedId:"foundations/data-engineering-foundations/model-optimization",id:"foundations/data-engineering-foundations/model-optimization",title:"Model Optimization",description:"Keras Model Pruning",source:"@site/docs/01-foundations/02-data-engineering-foundations/model-optimization.md",sourceDirName:"01-foundations/02-data-engineering-foundations",slug:"/foundations/data-engineering-foundations/model-optimization",permalink:"/docs/foundations/data-engineering-foundations/model-optimization",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Map Reduce",permalink:"/docs/foundations/data-engineering-foundations/map-reduce"},next:{title:"OLTP vs OLAP",permalink:"/docs/foundations/data-engineering-foundations/oltp-vs-olap"}},p={},s=[{value:"Keras Model Pruning",id:"keras-model-pruning",level:2},{value:"Keras Model Quantization",id:"keras-model-quantization",level:2}],d={toc:s};function m(e){let{components:n,...t}=e;return(0,r.kt)("wrapper",(0,i.Z)({},d,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"model-optimization"},"Model Optimization"),(0,r.kt)("h2",{id:"keras-model-pruning"},"Keras Model Pruning"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"!pip install -q tensorflow-model-optimization\n\nimport tempfile\n\n_, keras_file = tempfile.mkstemp('.h5')\ntf.keras.models.save_model(model, keras_file, include_optimizer=True)\nprint('Saved baseline model to:', keras_file)\n\n# Compute end step to finish pruning after 2 epochs.\nbatch_size = 32\nepochs = 100\nvalidation_split = 0.\n\nnum_samples = X_train.shape[0] * (1 - validation_split)\nend_step = np.ceil(num_samples / batch_size).astype(np.int32) * epochs\nend_step\n\nimport tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\n# Define model for pruning.\npruning_params = {\n      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n                                                               final_sparsity=0.80,\n                                                               begin_step=0,\n                                                               end_step=end_step)\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# `prune_low_magnitude` requires a recompile.\nmodel_for_pruning.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n\nmodel_for_pruning.summary()\n\nlogdir = tempfile.mkdtemp()\n\ncallbacks = [\n  tfmot.sparsity.keras.UpdatePruningStep(),\n  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n  es_callback\n]\n\nhistory_prune = model_for_pruning.fit(X_train, y_train,\n                                    batch_size=batch_size, epochs=epochs,\n                                    callbacks=callbacks,\n                                    validation_data=(X_test, y_test))\n\nmodel_for_pruning_score = r2_score(y_test, model_for_pruning.predict(X_test))\n\nprint('Baseline test score:', baseline_model_score) \nprint('Pruned test score:', model_for_pruning_score)\n\n# summarize history for mae\nplt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.plot(history_prune.history['mae'])\nplt.plot(history_prune.history['val_mae'])\nplt.title('model Mean Absolute Error (MAE)')\nplt.ylabel('MAE')\nplt.xlabel('epoch')\nplt.legend(['train', 'test', 'pruned train', 'pruned test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history_prune.history['loss'])\nplt.plot(history_prune.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test', 'pruned train', 'pruned test'], loc='upper left')\nplt.show()\n\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n\n_, pruned_keras_file = tempfile.mkstemp('.h5')\ntf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\nprint('Saved pruned Keras model to:', pruned_keras_file)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\npruned_tflite_model = converter.convert()\n\n_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n\nwith open(pruned_tflite_file, 'wb') as f:\n  f.write(pruned_tflite_model)\n\nprint('Saved pruned TFLite model to:', pruned_tflite_file)\n\ndef get_gzipped_model_size(file):\n  # Returns size of gzipped model, in bytes.\n  import os\n  import zipfile\n\n  _, zipped_file = tempfile.mkstemp('.zip')\n  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(file)\n\n  return os.path.getsize(zipped_file)\n\n\nprint(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\nprint(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\nprint(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))\n")),(0,r.kt)("h2",{id:"keras-model-quantization"},"Keras Model Quantization"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-py"},"# applying post-training quantization to the pruned model for additional benefits\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_and_pruned_tflite_model = converter.convert()\n\n_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n\nwith open(quantized_and_pruned_tflite_file, 'wb') as f:\n  f.write(quantized_and_pruned_tflite_model)\n\nprint('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n\nprint(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\nprint(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n")))}m.isMDXComponent=!0}}]);