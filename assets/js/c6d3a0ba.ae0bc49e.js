"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[86154],{3905:(e,n,t)=>{t.d(n,{Zo:()=>l,kt:()=>d});var r=t(67294);function s(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){s(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,r,s=function(e,n){if(null==e)return{};var t,r,s={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(s[t]=e[t]);return s}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(s[t]=e[t])}return s}var c=r.createContext({}),_=function(e){var n=r.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):a(a({},n),e)),t},l=function(e){var n=_(e.components);return r.createElement(c.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,s=e.mdxType,o=e.originalType,c=e.parentName,l=i(e,["components","mdxType","originalType","parentName"]),u=_(t),d=s,m=u["".concat(c,".").concat(d)]||u[d]||p[d]||o;return t?r.createElement(m,a(a({ref:n},l),{},{components:t})):r.createElement(m,a({ref:n},l))}));function d(e,n){var t=arguments,s=n&&n.mdxType;if("string"==typeof e||s){var o=t.length,a=new Array(o);a[0]=u;var i={};for(var c in n)hasOwnProperty.call(n,c)&&(i[c]=n[c]);i.originalType=e,i.mdxType="string"==typeof e?e:s,a[1]=i;for(var _=2;_<o;_++)a[_]=t[_];return r.createElement.apply(null,a)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},45647:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>_});var r=t(87462),s=(t(67294),t(3905));const o={},a="Code Snippets",i={unversionedId:"storage/code-snippets",id:"storage/code-snippets",title:"Code Snippets",description:"Connect to Redshift using Python",source:"@site/docs/02-storage/code-snippets.md",sourceDirName:"02-storage",slug:"/storage/code-snippets",permalink:"/docs/storage/code-snippets",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{}},c={},_=[{value:"Connect to Redshift using Python",id:"connect-to-redshift-using-python",level:2},{value:"Create AWS Redshift Cluster",id:"create-aws-redshift-cluster",level:2}],l={toc:_};function p(e){let{components:n,...t}=e;return(0,s.kt)("wrapper",(0,r.Z)({},l,t,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"code-snippets"},"Code Snippets"),(0,s.kt)("h2",{id:"connect-to-redshift-using-python"},"Connect to Redshift using Python"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},'import pandas as pd\nimport psycopg2\nimport boto3\nimport json\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import text\n\ndef get_secret(secret_name=\'wysde\'):\n    region_name = "us-east-1"\n    session = boto3.session.Session()\n    client = session.client(\n        service_name=\'secretsmanager\',\n        region_name=region_name)\n    get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n    get_secret_value_response = json.loads(get_secret_value_response[\'SecretString\'])\n    return get_secret_value_response\n\nsecret_vals = get_secret()\n\nredshift_endpoint = secret_vals[\'REDSHIFT_HOST\']\nredshift_user = secret_vals[\'REDSHIFT_USERNAME\']\nredshift_pass = secret_vals[\'REDSHIFT_PASSWORD\']\nport = 5439\ndbname = "dev"\n\nengine_string = "postgresql+psycopg2://%s:%s@%s:%d/%s" \\\n% (redshift_user, redshift_pass, redshift_endpoint, port, dbname)\nengine = create_engine(engine_string)\n\nquery = """\nSELECT *\nFROM pg_catalog.pg_tables\nWHERE schemaname != \'pg_catalog\' AND \n    schemaname != \'information_schema\';\n"""\ndf = pd.read_sql_query(text(query), engine)\n\nquery = """\nSELECT * FROM "dev"."public"."users";\n"""\ndf = pd.read_sql_query(text(query), engine)\n')),(0,s.kt)("h2",{id:"create-aws-redshift-cluster"},"Create AWS Redshift Cluster"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-py"},'    import pandas as pd\n    import boto3\n    import json\n    import psycopg2\n\n    from botocore.exceptions import ClientError\n    import configparser\n\n    from random import random\n    import threading\n    import time\n\n    # Tracking Cluster Creation Progress\n    progress = 0\n    cluster_status = \'\'\n    cluster_event = threading.Event()\n\n    def initialize():\n        """\n        Summary line. \n        This function starts the create_cluster function. \n\n        Parameters: \n        NONE\n\n        Returns: \n        None\n        """  \n        # Get the config properties from dwh.cfg file\n        config = configparser.ConfigParser()\n        config.read_file(open(\'./aws/aws-capstone.cfg\'))\n\n        KEY                    = config.get(\'AWS\',\'KEY\')\n        SECRET                 = config.get(\'AWS\',\'SECRET\')\n\n        DWH_CLUSTER_TYPE       = config.get("CLUSTER","DWH_CLUSTER_TYPE")\n        DWH_NUM_NODES          = config.get("CLUSTER","DWH_NUM_NODES")\n        DWH_NODE_TYPE          = config.get("CLUSTER","DWH_NODE_TYPE")\n\n        DWH_CLUSTER_IDENTIFIER = config.get("CLUSTER","DWH_CLUSTER_IDENTIFIER")\n        DWH_DB                 = config.get("CLUSTER","DWH_DB")\n        DWH_DB_USER            = config.get("CLUSTER","DWH_DB_USER")\n        DWH_DB_PASSWORD        = config.get("CLUSTER","DWH_DB_PASSWORD")\n        DWH_PORT               = config.get("CLUSTER","DWH_PORT")\n\n        DWH_IAM_ROLE_NAME      = config.get("IAM_ROLE", "DWH_IAM_ROLE_NAME")\n\n\n        df = pd.DataFrame({"Param":\n                        ["DWH_CLUSTER_TYPE", "DWH_NUM_NODES", "DWH_NODE_TYPE", "DWH_CLUSTER_IDENTIFIER", "DWH_DB", "DWH_DB_USER", "DWH_DB_PASSWORD", "DWH_PORT", "DWH_IAM_ROLE_NAME"],\n                    "Value":\n                        [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n                    })\n\n        print(df)\n\n\n        ec2 = boto3.resource(\'ec2\',\n                            region_name="us-west-2",\n                            aws_access_key_id=KEY,\n                            aws_secret_access_key=SECRET\n                            )\n\n        s3 = boto3.resource(\'s3\',\n                            region_name="us-west-2",\n                            aws_access_key_id=KEY,\n                            aws_secret_access_key=SECRET\n                        )\n\n        iam = boto3.client(\'iam\',aws_access_key_id=KEY,\n                            aws_secret_access_key=SECRET,\n                            region_name=\'us-west-2\'\n                        )\n\n        redshift = boto3.client(\'redshift\',\n                            region_name="us-west-2",\n                            aws_access_key_id=KEY,\n                            aws_secret_access_key=SECRET\n                            )\n\n\n        roleArn = create_iam_role(iam, DWH_IAM_ROLE_NAME)\n\n        create_cluster(redshift, roleArn, DWH_CLUSTER_TYPE, DWH_NODE_TYPE, DWH_NUM_NODES, DWH_DB, DWH_CLUSTER_IDENTIFIER, DWH_DB_USER, DWH_DB_PASSWORD)\n\n        #thread = threading.Thread(target=check_cluster_status)\n        thread = threading.Thread(target=lambda : check_cluster_status(redshift, DWH_CLUSTER_IDENTIFIER, \'create\', \'available\'))\n        #thread = threading.Thread(target=lambda : check_cluster_status(redshift, DWH_CLUSTER_IDENTIFIER, \'available\'))\n        thread.start()\n\n        # wait here for the result to be available before continuing\n        while not cluster_event.wait(timeout=5):  \n            print(\'\\r{:5}Waited for {} seconds. Redshift Cluster Creation in-progress...\'.format(\'\', progress), end=\'\', flush=True)\n        print(\'\\r{:5}Cluster creation completed. Took {} seconds.\'.format(\'\', progress))  \n\n        myClusterProps = get_cluster_properties(redshift, DWH_CLUSTER_IDENTIFIER)\n        #print(myClusterProps)\n        prettyRedshiftProps(myClusterProps[0])\n        DWH_ENDPOINT = myClusterProps[1]\n        DWH_ROLE_ARN = myClusterProps[2]\n        print(\'DWH_ENDPOINT = {}\'.format(DWH_ENDPOINT))\n        print(\'DWH_ROLE_ARN = {}\'.format(DWH_ROLE_ARN))\n\n        open_ports(ec2, myClusterProps[0], DWH_PORT)\n\n        conn = psycopg2.connect("host={} dbname={} user={} password={} port={}".format( DWH_ENDPOINT, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT ))\n        cur = conn.cursor()\n\n        print(\'Connected\')  \n\n        conn.close()\n        print(\'Done!\')\n\n    def create_iam_role(iam, DWH_IAM_ROLE_NAME):\n        """\n        Summary line. \n        Creates IAM Role that allows Redshift clusters to call AWS services on your behalf\n\n        Parameters: \n        arg1 : IAM Object\n        arg2 : IAM Role name\n\n        Returns: \n        NONE\n        """  \n        try:\n            print("1.1 Creating a new IAM Role") \n            dwhRole = iam.create_role(\n                Path=\'/\',\n                RoleName=DWH_IAM_ROLE_NAME,\n                Description = "Allows Redshift clusters to call AWS services on your behalf.",\n                AssumeRolePolicyDocument=json.dumps(\n                    {\'Statement\': [{\'Action\': \'sts:AssumeRole\',\n                    \'Effect\': \'Allow\',\n                    \'Principal\': {\'Service\': \'redshift.amazonaws.com\'}}],\n                    \'Version\': \'2012-10-17\'})\n            )  \n        except Exception as e:\n            print(e)\n\n        print("1.2 Attaching Policy")\n        iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n                            PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"\n                            )[\'ResponseMetadata\'][\'HTTPStatusCode\']\n\n        print("1.3 Get the IAM role ARN")\n        roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)[\'Role\'][\'Arn\']\n\n        print(\'{:5} ARN : {}\'.format(\'\',roleArn))\n        return roleArn\n\n    def create_cluster(redshift, roleArn, DWH_CLUSTER_TYPE, DWH_NODE_TYPE, DWH_NUM_NODES, DWH_DB, DWH_CLUSTER_IDENTIFIER, DWH_DB_USER, DWH_DB_PASSWORD):\n        """\n        Summary line. \n        Creates Redshift Cluster\n\n        Parameters: \n        arg1 : Redshift Object\n        arg2 : Cluster Name\n\n        Returns: \n        None\n        """  \n        print(\'1.4 Starting Redshift Cluster Creation\')\n        try:\n            response = redshift.create_cluster(  \n                #HW\n                ClusterType=DWH_CLUSTER_TYPE,\n                NodeType=DWH_NODE_TYPE,\n                NumberOfNodes=int(DWH_NUM_NODES),\n\n                #Identifiers & Credentials\n                DBName=DWH_DB,\n                ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n                MasterUsername=DWH_DB_USER,\n                MasterUserPassword=DWH_DB_PASSWORD,\n\n                #Roles (for s3 access)\n                IamRoles=[roleArn]  \n            )\n        except Exception as e:\n            print(e)\n\n    def prettyRedshiftProps(props):\n        """\n        Summary line. \n        Returns the Redshift Cluster Properties in a dataframe\n\n        Parameters: \n        arg1 : Redshift Properties\n\n        Returns: \n        dataframe with column key, value\n        """  \n\n        pd.set_option(\'display.max_colwidth\', -1)\n        keysToShow = ["ClusterIdentifier", "NodeType", "ClusterStatus", "MasterUsername", "DBName", "Endpoint", "NumberOfNodes", \'VpcId\']\n        #print(props)\n        x = [(k, v) for k,v in props.items() if k in keysToShow]\n        \'\'\'\n        #(OR) Below is longer version above is shorter version\n        xx = []\n        for k in props:\n            if k in keysToShow:\n                v = props.get(k)\n                xx.append((k,v))\n                print(\'{} : {}\'.format(k, v))  \n        print(\'XX = \',xx)\n        \'\'\'\n        #print(\'X = \',x)\n        return pd.DataFrame(data=x, columns=["Key", "Value"])\n\n    def check_cluster_status(redshift, DWH_CLUSTER_IDENTIFIER, action, status):\n        """\n        Summary line. \n        Check the cluster status in a loop till it becomes available/none. \n        Once the desired status is set, updates the threading event variable\n\n        Parameters: \n        arg1 : Redshift Object\n        arg2 : Cluster Name\n        arg3 : action which can be (create or delete)\n        arg4 : status value to check \n\n        Returns: \n        NONE\n        """  \n\n        global progress\n        global cluster_status\n\n        # wait here for the result to be available before continuing  \n        while cluster_status.lower() != status:\n            time.sleep(5)\n            progress+=5\n            if action == \'create\':\n                myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)[\'Clusters\'][0]\n                #print(myClusterProps)\n                df = prettyRedshiftProps(myClusterProps)\n                #print(df)\n                #In keysToShow 2 is ClusterStatus\n                cluster_status = df.at[2, \'Value\']    \n            elif action ==\'delete\':\n                myClusterProps = redshift.describe_clusters()\n                #print(myClusterProps)\n                if len(myClusterProps[\'Clusters\']) == 0 :\n                    cluster_status = \'none\'\n                else:\n                    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)[\'Clusters\'][0]\n                    #print(myClusterProps)\n                    df = prettyRedshiftProps(myClusterProps)\n                    #print(df)\n                    #In keysToShow 2 is ClusterStatus\n                    cluster_status = df.at[2, \'Value\']                    \n\n            print(\'Cluster Status = \',cluster_status)  \n    \n        # when the calculation is done, the result is stored in a global variable\n        cluster_event.set()\n        # Thats it\n\n    def get_cluster_properties(redshift, DWH_CLUSTER_IDENTIFIER):\n        """\n        Summary line. \n        Retrieve Redshift clusters properties\n\n        Parameters: \n        arg1 : Redshift Object\n        arg2 : Cluster Name\n\n        Returns: \n        myClusterProps=Cluster Properties, DWH_ENDPOINT=Host URL, DWH_ROLE_ARN=Role Amazon Resource Name\n        """  \n        myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)[\'Clusters\'][0]\n        DWH_ENDPOINT = myClusterProps[\'Endpoint\'][\'Address\']\n        DWH_ROLE_ARN = myClusterProps[\'IamRoles\'][0][\'IamRoleArn\']\n        print("DWH_ENDPOINT :: ", DWH_ENDPOINT)\n        print("DWH_ROLE_ARN :: ", DWH_ROLE_ARN)\n        return myClusterProps, DWH_ENDPOINT, DWH_ROLE_ARN\n\n    def open_ports(ec2, myClusterProps, DWH_PORT):\n        """\n        Summary line. \n        Update clusters security group to allow access through redshift port\n\n        Parameters: \n        arg1 : ec2 Object\n        arg2 : Cluster Properties\n        arg3 : Redshift Port\n\n        Returns: \n        NONE\n        """  \n        try:\n            vpc = ec2.Vpc(id=myClusterProps[\'VpcId\'])\n            defaultSg = list(vpc.security_groups.all())[0]\n            print(defaultSg)\n            defaultSg.authorize_ingress(\n                GroupName=defaultSg.group_name,\n                CidrIp=\'0.0.0.0/0\',\n                IpProtocol=\'TCP\',\n                FromPort=int(DWH_PORT),\n                ToPort=int(DWH_PORT)\n            )\n        except Exception as e:\n            print(e)\n\n    def main():\n        initialize()\n\n    if __name__ == "__main__":\n        main()\n')))}p.isMDXComponent=!0}}]);