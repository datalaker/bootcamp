"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[187],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>c});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),m=p(n),c=i,h=m["".concat(l,".").concat(c)]||m[c]||d[c]||o;return n?a.createElement(h,r(r({ref:t},u),{},{components:n})):a.createElement(h,r({ref:t},u))}));function c(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var p=2;p<o;p++)r[p]=n[p];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3950:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>p});var a=n(87462),i=(n(67294),n(3905));const o={},r="Lab: GCP Dataflow Batch Pipeline",s={unversionedId:"processing/lab-gcp-dataflow-batch-pipeline",id:"processing/lab-gcp-dataflow-batch-pipeline",title:"Lab: GCP Dataflow Batch Pipeline",description:"Objective: Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Cloud Dataflow (Python)",source:"@site/docs/03-processing/lab-gcp-dataflow-batch-pipeline.md",sourceDirName:"03-processing",slug:"/processing/lab-gcp-dataflow-batch-pipeline",permalink:"/docs/processing/lab-gcp-dataflow-batch-pipeline",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681732641,formattedLastUpdatedAt:"Apr 17, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Lab: GCP Dataflow Pipeline - A Simple Dataflow Pipeline (Python)",permalink:"/docs/processing/lab-gcp-dataflow-pipeline"},next:{title:"Lab: GCP Dataflow Size Inputs",permalink:"/docs/processing/lab-gcp-dataflow-side-inputs"}},l={},p=[{value:"Preparation",id:"preparation",level:2},{value:"Jupyter notebook-based development environment setup",id:"jupyter-notebook-based-development-environment-setup",level:3},{value:"Download Code Repository",id:"download-code-repository",level:3},{value:"Aggregating site traffic by user",id:"aggregating-site-traffic-by-user",level:2},{value:"Task 1. Generate synthetic data",id:"task-1-generate-synthetic-data",level:3},{value:"Task 2. Sum page views per user",id:"task-2-sum-page-views-per-user",level:3},{value:"Task 3. Run your pipeline",id:"task-3-run-your-pipeline",level:3},{value:"Task 4. Verify results in BigQuery",id:"task-4-verify-results-in-bigquery",level:3},{value:"Aggregating site traffic by minute",id:"aggregating-site-traffic-by-minute",level:2},{value:"Task 5. Add timestamps to each element",id:"task-5-add-timestamps-to-each-element",level:3},{value:"Task 6. Window into one-minute windows",id:"task-6-window-into-one-minute-windows",level:3},{value:"Task 7. Count events per window",id:"task-7-count-events-per-window",level:3},{value:"Task 8. Convert back to a row and add timestamp",id:"task-8-convert-back-to-a-row-and-add-timestamp",level:3},{value:"Task 9. Run the pipeline",id:"task-9-run-the-pipeline",level:3},{value:"Task 10. Verify the results",id:"task-10-verify-the-results",level:3}],u={toc:p};function d(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"lab-gcp-dataflow-batch-pipeline"},"Lab: GCP Dataflow Batch Pipeline"),(0,i.kt)("p",null,"Objective: Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Cloud Dataflow (Python)"),(0,i.kt)("p",null,"In this lab, you:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Write a pipeline that aggregates site traffic by user."),(0,i.kt)("li",{parentName:"ul"},"Write a pipeline that aggregates site traffic by minute."),(0,i.kt)("li",{parentName:"ul"},"Implement windowing on time series data.")),(0,i.kt)("h2",{id:"preparation"},"Preparation"),(0,i.kt)("h3",{id:"jupyter-notebook-based-development-environment-setup"},"Jupyter notebook-based development environment setup"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In the Console, expand the Navigation menu (Navigation menu icon), then select Vertex AI > Workbench."),(0,i.kt)("li",{parentName:"ul"},"Enable Notebooks API."),(0,i.kt)("li",{parentName:"ul"},"At the top of the page click New Notebook, and select Smart Analytics Framework > Apache Beam > Without GPUs"),(0,i.kt)("li",{parentName:"ul"},"In the dialog box that appears, set the region to us-central1 and then click CREATE at the bottom."),(0,i.kt)("li",{parentName:"ul"},"Once the environment is ready, click the OPEN JUPYTERLAB link next to your Notebook name. This will open up your environment in a new tab in your browser."),(0,i.kt)("li",{parentName:"ul"},"Next, click Terminal. This will open up a terminal where you can run all the commands in this lab.")),(0,i.kt)("h3",{id:"download-code-repository"},"Download Code Repository"),(0,i.kt)("p",null,"Next you will download a code repository for use in this lab."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"In the terminal you just opened, enter the following:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"git clone https://github.com/GoogleCloudPlatform/training-data-analyst\ncd /home/jupyter/training-data-analyst/quests/dataflow_python/\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"On the left panel of your notebook environment, in the file browser, you will notice the training-data-analyst repo added.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Navigate into the cloned repo /training-data-analyst/quests/dataflow_python/. You will see a folder for each lab, which is further divided into a lab sub-folder with code to be completed by you, and a solution sub-folder with a fully workable example to reference if you get stuck."))),(0,i.kt)("p",null,"Note: To open a file for editing purposes, simply navigate to the file and click on it. This will open the file, where you can add or modify code."),(0,i.kt)("h2",{id:"aggregating-site-traffic-by-user"},"Aggregating site traffic by user"),(0,i.kt)("p",null,"In this part of the lab, you write a pipeline that:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Reads the day's traffic from a file in Cloud Storage."),(0,i.kt)("li",{parentName:"ol"},"Converts each event into a\xa0",(0,i.kt)("inlineCode",{parentName:"li"},"CommonLog"),"\xa0object."),(0,i.kt)("li",{parentName:"ol"},"Sums the number of hits for each unique user by grouping each object by user ID and combining the values to get the total number of hits for that particular user."),(0,i.kt)("li",{parentName:"ol"},"Performs additional aggregations on each user."),(0,i.kt)("li",{parentName:"ol"},"Writes the resulting data to BigQuery.")),(0,i.kt)("h3",{id:"task-1-generate-synthetic-data"},"Task 1. Generate synthetic data"),(0,i.kt)("p",null,"The first step is to generate data for the pipeline to process. You will open the lab environment and generate the data:"),(0,i.kt)("p",null,"In the terminal in your IDE environment, run the following commands:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"cd 3_Batch_Analytics/lab\nexport BASE_DIR=$(pwd)\n")),(0,i.kt)("p",null,"Before you can begin editing the actual pipeline code, you need to ensure that you have installed the necessary dependencies. Execute the following to create a virtual environment for your work in this lab:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"sudo apt-get update && sudo apt-get install -y python3-venv\npython3 -m venv df-env\nsource df-env/bin/activate\npython3 -m pip install -q --upgrade pip setuptools wheel\npython3 -m pip install apache-beam[gcp]\ngcloud services enable dataflow.googleapis.com\n\n# Create GCS buckets and BQ dataset\ncd $BASE_DIR/../..\nsource create_batch_sinks.sh\n# Generate event dataflow\nsource generate_batch_events.sh\n# Change to the directory containing the practice version of the code\ncd $BASE_DIR\n")),(0,i.kt)("p",null,"The script creates a file called\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"events.json"),"\xa0containing lines resembling the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'{"user_id": "-6434255326544341291", "ip": "192.175.49.116", "timestamp": "2019-06-19T16:06:45.118306Z", "http_request": "\\"GET eucharya.html HTTP/1.0\\"", "lat": 37.751, "lng": -97.822, "http_response": 200, "user_agent": "Mozilla/5.0 (compatible; MSIE 7.0; Windows NT 5.01; Trident/5.1)", "num_bytes": 182}\n')),(0,i.kt)("p",null,"It then automatically copies this file to your Google Cloud Storage bucket at\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Cloud Storage path"),"."),(0,i.kt)("h3",{id:"task-2-sum-page-views-per-user"},"Task 2. Sum page views per user"),(0,i.kt)("p",null,"In the file explorer, navigate to\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"training-data-analyst/quest/dataflow_python/3_Batch_Analytics/lab"),"\xa0and open the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"batch_user_traffic_pipeline.py"),"\xa0file. This pipeline already contains the necessary code to accept command-line options for the input path and the output table name, as well as code to read in events from Google Cloud Storage, parse those events, and write results to BigQuery. However, some important parts are missing."),(0,i.kt)("p",null,"The next step in the pipeline is to aggregate the events by each unique\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"user_id"),"\xa0and count page views for each. An easy way to do this on objects of type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"beam.Row"),"\xa0or objects with a Beam schema is to use the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"GroupBy"),"\xa0transform and then perform some aggregations on the resulting group. For example: ",(0,i.kt)("inlineCode",{parentName:"p"},"purchases | GroupBy('user_id', 'address')")," will return a PCollection of rows with two fields."),(0,i.kt)("p",null,"The first is a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Row"),"\xa0with schema representing every unique combination of\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"'user_id'"),"\xa0and\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"address"),'\xa0(both strings), "key", and "values". The second field is an iterable of type\xa0',(0,i.kt)("inlineCode",{parentName:"p"},"Row"),"\xa0containing all of the objects in the unique group from the first field."),(0,i.kt)("p",null,"This is most useful when you can perform aggregate calculations on this grouping and name the resulting fields, like so:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'(purchases | GroupBy(\'user_id\')\n             .aggregate_field("item_id", CountCombineFn(), "num_purchases")\n             .aggregate_field("cost_cents", sum, "total_spend_cents")\n             .aggregate_field("cost_cents", max, "largest_purchases"))\n             .with_output_types(UserPurchases)\n')),(0,i.kt)("p",null,"This returns a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Row"),'\xa0with fields corresponding to the "key(s)" we grouped by and the corresponding aggregations computed here.'),(0,i.kt)("p",null,"The\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"aggregate_field"),"\xa0method takes three arguments. The first argument is a string, referring to the name of the field we wish to aggregate in the input PCollection's schema. The second is the combiner we wish to apply, implemented as a subclass of\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.CombineFn"},"CombineFn"),". The third argument is a string that we use to identify the aggregation in the schema of the output PCollection."),(0,i.kt)("p",null,"Certain aggregation functions, such as\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"sum"),"\xa0and\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"max"),", are implemented directly as combiners in Beam Python\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.core.html#apache_beam.transforms.core.CombinePerKey"},"(Link)"),". Count is implemented via\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Count"},"CountCombineFn"),"."),(0,i.kt)("p",null,"The output PCollection by default is a PCollection of type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Row"),", but we can also apply our own custom types with schema using\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"with_output_types"),". We see that above with\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"UserPurchases"),". However, this means that we need to define a schema for type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"UserPurchases"),". We can do so easily by creating a subclass of\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"typing.NamedTuple"),"\xa0or via creating the schema ad hoc using\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"beam.Row"),"\xa0or\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"beam.Select"),". We will cover the first case here. For the second please refer to the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/documentation/programming-guide/#schema-definition"},"Beam programming guide"),"."),(0,i.kt)("p",null,"The output of our aggregation above has four fields:\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"user_id"),"\xa0(type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"str"),"),\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"num_purchases"),",\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"total_spend_cents"),", and\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"largest_purchases"),"\xa0(all type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"int"),")."),(0,i.kt)("p",null,"We create a subclass of\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"NamedTuple"),"\xa0with these field names and types then register the coder for the schema:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"class UserPurchases(typing.NamedTuple):\n  user_id : str\n  num_purchases : int\n  total_spend_cents : int\n  largest_purchases : int\nbeam.coders.registry.register_coder(UserPurchases, beam.coders.RowCoder)\n")),(0,i.kt)("p",null,"Note:\xa0In this example you could aggregate on any of the fields for ",(0,i.kt)("inlineCode",{parentName:"p"},"CountCombineFn()"),", or even on the wildcard field ",(0,i.kt)("inlineCode",{parentName:"p"},"*"),", as this transform is simply counting how many elements are in the entire group."),(0,i.kt)("p",null,"The next step in the pipeline is to aggregate events by user_id, sum the pageviews, and also calculate some additional aggregations on num_bytes, for example total user bytes, maximum user bytes, and minimum user bytes."),(0,i.kt)("p",null,"To complete this task, add another transform to the pipeline that groups the events by\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"user_id"),"\xa0and then performs the relevant aggregations. Keep in mind the input, the CombineFns to use, and how you name the output fields. After this, create a new output type with schema (call it\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"PerUserAggregation"),") and ensure that the output\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"Row"),"\xa0is converted into this type."),(0,i.kt)("h3",{id:"task-3-run-your-pipeline"},"Task 3. Run your pipeline"),(0,i.kt)("p",null,"Return to Cloud Shell and execute the following command to run your pipeline using the Cloud Dataflow service. You can run it with DirectRunner if you're having trouble, or refer to the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_user_traffic_pipeline.py"},"solution"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"export PROJECT_ID=$(gcloud config get-value project)\nexport REGION=Region\nexport BUCKET=gs://${PROJECT_ID}\nexport PIPELINE_FOLDER=${BUCKET}\nexport RUNNER=DataflowRunner\nexport INPUT_PATH=${PIPELINE_FOLDER}/events.json\nexport TABLE_NAME=${PROJECT_ID}:logs.user_traffic\ncd $BASE_DIR\npython3 batch_user_traffic_pipeline.py\\\n--project=${PROJECT_ID}\\\n--region=${REGION}\\\n--staging_location=${PIPELINE_FOLDER}/staging\\\n--temp_location=${PIPELINE_FOLDER}/temp\\\n--runner=${RUNNER}\\\n--input_path=${INPUT_PATH}\\\n--table_name=${TABLE_NAME}\n")),(0,i.kt)("h3",{id:"task-4-verify-results-in-bigquery"},"Task 4. Verify results in BigQuery"),(0,i.kt)("p",null,"To complete this task, wait a few minutes for the pipeline to complete, then navigate to\xa0",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/bigquery"},"BigQuery"),"\xa0and query the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"user_traffic"),"\xa0table."),(0,i.kt)("h2",{id:"aggregating-site-traffic-by-minute"},"Aggregating site traffic by minute"),(0,i.kt)("p",null,"In this part of the lab, you create a new pipeline called\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"batch_minute_traffic"),".\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"batch_minute_traffic"),"\xa0expands on the basic batch analysis principles used in\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"batch_user_traffic"),"\xa0and, instead of aggregating by users across the entire batch, aggregates by when events occurred."),(0,i.kt)("p",null,"In the IDE, open the file\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"batch_minute_traffic_pipeline"),"\xa0inside\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"3_Batch_Analytics/lab"),"."),(0,i.kt)("h3",{id:"task-5-add-timestamps-to-each-element"},"Task 5. Add timestamps to each element"),(0,i.kt)("p",null,"An unbounded source provides a timestamp for each element. Depending on your unbounded source, you may need to configure how the timestamp is extracted from the raw data stream."),(0,i.kt)("p",null,"However, bounded sources (such as a file from TextIO, as is used in this pipeline) do not provide timestamps."),(0,i.kt)("p",null,"You can parse the timestamp field from each record and use the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.window.html#apache_beam.transforms.window.TimestampedValue"},"beam.window.TimestampedValue"),"\xa0transform to attach the timestamps to each element in your PCollection."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"def add_timestamp(element):\n  ts = # Do Something\n  return beam.window.TimestampedValue(element, ts)\nunstamped = ...\nstamped = unstamped | beam.Map(add_timestamp)\n")),(0,i.kt)("p",null,"To complete this task, add a transform to the pipeline that adds timestamps to each element of the pipeline. To do this, leverage the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/datetime.html"},(0,i.kt)("inlineCode",{parentName:"a"},"datetime")),"\xa0package to convert the timestamp field of the element into a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"datetime"),"\xa0object. You may need to explore the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://docs.python.org/3/library/datetime.html"},(0,i.kt)("inlineCode",{parentName:"a"},"datetime.strptime")),"\xa0function to do so."),(0,i.kt)("h3",{id:"task-6-window-into-one-minute-windows"},"Task 6. Window into one-minute windows"),(0,i.kt)("p",null,"Windowing subdivides a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"PCollection"),"\xa0according to the timestamps of its individual elements. Transforms that aggregate multiple elements, such as\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"GroupByKey"),"\xa0and Combine, work implicitly on a per-window basis --- they process each\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"PCollection"),"\xa0as a succession of multiple, finite windows, though the entire collection itself may be of unbounded size."),(0,i.kt)("p",null,"You can define different kinds of windows to divide the elements of your\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"PCollection"),". Beam provides several windowing functions, including:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Fixed-time windows"),(0,i.kt)("li",{parentName:"ul"},"Sliding-time windows"),(0,i.kt)("li",{parentName:"ul"},"Per-session windows"),(0,i.kt)("li",{parentName:"ul"},"Single global window"),(0,i.kt)("li",{parentName:"ul"},"Calendar-based windows (not supported by the Beam SDK for Python, as of when this lab was written)")),(0,i.kt)("p",null,"In this lab, you use fixed-time windows. A fixed-time window represents a non-overlapping time interval of consistent duration in the data stream. Consider windows with a five-minute duration: all of the elements in your unbounded\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"PCollection"),"\xa0with timestamp values from 0:00:00 up to (but not including) 0:05:00 belong to the first window, elements with timestamp values from 0:05:00 up to (but not including) 0:10:00 belong to the second window, and so on."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003375-5f8bcc72-b62c-42fa-a9f3-f5ab9eeee4f7.png",alt:null})),(0,i.kt)("p",null,"Implement a fixed-time window with a five-minute duration as follows:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"p = ...\np_windowed = p | beam.WindowInto(beam.window.FixedWindows(5*60))\n")),(0,i.kt)("p",null,"To complete this task, add a transform to your pipeline that windows elements into fixed windows one minute long."),(0,i.kt)("p",null,"To learn more about other types of windowing, read the Apache Beam documentation\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/documentation/programming-guide/#provided-windowing-functions"},"Section 8.2. Provided windowing functions"),"."),(0,i.kt)("h3",{id:"task-7-count-events-per-window"},"Task 7. Count events per window"),(0,i.kt)("p",null,"Next, the pipeline needs to compute the number of events that occurred within each window. In the batch_user_traffic pipeline, a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"sum"),"\xa0transform was used to sum per key. However, unlike in that pipeline, in this case the elements have been windowed and the desired computation needs to respect window boundaries."),(0,i.kt)("p",null,"Despite this new constraint, the Combine transform is still appropriate. That's because Combine transforms automatically respect window boundaries."),(0,i.kt)("p",null,"Refer to the documentation for\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/releases/pydoc/2.28.0/apache_beam.transforms.combiners.html#apache_beam.transforms.combiners.Count"},"Count"),"\xa0for how to add a new transform that counts the number of elements per window."),(0,i.kt)("p",null,"As of Beam 2.28, the best option to count elements of rows while windowing is to use\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"beam.CombineGlobally(CountCombineFn()).without_defaults()"),"\xa0(that is, without using full-on SQL, which we will cover more in the next lab). This transform will output a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"PCollection"),"\xa0of type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"int"),"\xa0which, you'll notice, is no longer using Beam schemas."),(0,i.kt)("p",null,"To complete this task, add a transform that counts all the elements in each window. Remember to refer to the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/quests/dataflow_python/3_Batch_Analytics/solution/batch_minute_traffic_pipeline.py"},"solution"),"\xa0if you get stuck."),(0,i.kt)("h3",{id:"task-8-convert-back-to-a-row-and-add-timestamp"},"Task 8. Convert back to a row and add timestamp"),(0,i.kt)("p",null,"In order to write to BigQuery, each element needs to be converted to a\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"dict"),'\xa0object with "page_views" as a field and additional field called "timestamp". The idea is to use the boundary of each window as one field and the combined number of pageviews as the other.'),(0,i.kt)("p",null,"One other issue, at this point, is that the Count transform is only providing elements of type\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"int"),"\xa0that no longer bear any sort of timestamp information."),(0,i.kt)("p",null,"In fact, however, they do, though not in so obvious a way. Apache Beam runners know by default how to supply the value for a number of additional parameters, including event timestamps, windows, and pipeline options; for a full list refer to the\xa0",(0,i.kt)("a",{parentName:"p",href:"https://beam.apache.org/documentation/programming-guide/#other-dofn-parameters"},"Apache's DoFn parameters documentation"),"."),(0,i.kt)("p",null,"To complete this task, write a ParDo function that accepts elements of type int, passes in the additional parameter to access window information,\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"beam.DoFn.WindowParam"),", and emits dictionaries with the fields mentioned above. Note that the timestamp field in the BigQuery table schema is a STRING, so you will have to convert the timestamp to a string. The\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"datetime.strftime"),"\xa0function will be helpful here."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-py"},"class GetTimestampFn(beam.DoFn):\n    def process(self, element, window=beam.DoFn.WindowParam):\n        window_start = #Do something!\n        output = {'page_views': element, 'timestamp': window_start}\n        yield output\n")),(0,i.kt)("h3",{id:"task-9-run-the-pipeline"},"Task 9. Run the pipeline"),(0,i.kt)("p",null,"Once you've finished coding, run the pipeline using the command below. Keep in mind that, while testing your code, it will be much faster to change the RUNNER environment variable to DirectRunner, which will run the pipeline locally."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-sh"},"export PROJECT_ID=$(gcloud config get-value project)\nexport REGION=Region\nexport BUCKET=gs://${PROJECT_ID}\nexport PIPELINE_FOLDER=${BUCKET}\nexport RUNNER=DataflowRunner\nexport INPUT_PATH=${PIPELINE_FOLDER}/events.json\nexport TABLE_NAME=${PROJECT_ID}:logs.minute_traffic\ncd $BASE_DIR\npython3 batch_minute_traffic_pipeline.py\\\n--project=${PROJECT_ID}\\\n--region=${REGION}\\\n--staging_location=${PIPELINE_FOLDER}/staging\\\n--temp_location=${PIPELINE_FOLDER}/temp\\\n--runner=${RUNNER}\\\n--input_path=${INPUT_PATH}\\\n--table_name=${TABLE_NAME}\n")),(0,i.kt)("h3",{id:"task-10-verify-the-results"},"Task 10. Verify the results"),(0,i.kt)("p",null,"To complete this task, wait a few minutes for the pipeline to execute, then navigate to\xa0",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/bigquery"},"BigQuery"),"\xa0and query the\xa0",(0,i.kt)("inlineCode",{parentName:"p"},"minute_traffic"),"\xa0table."))}d.isMDXComponent=!0}}]);