"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[94092],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>f});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=c(a),f=r,m=u["".concat(l,".").concat(f)]||u[f]||p[f]||i;return a?n.createElement(m,o(o({ref:t},d),{},{components:a})):n.createElement(m,o({ref:t},d))}));function f(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},7355:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const i={},o="Data Splits",s={unversionedId:"datascience/data-splits",id:"datascience/data-splits",title:"Data Splits",description:"Training and Validation sets",source:"@site/docs/10-datascience/data-splits.md",sourceDirName:"10-datascience",slug:"/datascience/data-splits",permalink:"/docs/datascience/data-splits",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Model Deployment",permalink:"/docs/foundations/basics/deployment"},next:{title:"Bias-Variance Trade-Off",permalink:"/docs/datascience/bias-variance-tradeoff"}},l={},c=[{value:"Training and Validation sets",id:"training-and-validation-sets",level:2},{value:"K-fold Evaluation",id:"k-fold-evaluation",level:2}],d={toc:c};function p(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"data-splits"},"Data Splits"),(0,r.kt)("h2",{id:"training-and-validation-sets"},"Training and Validation sets"),(0,r.kt)("p",null,"The ecommerce platform randomly splits the customer data into a train dataset and a validation dataset (in this example, with a 0.6 \u201ctrain split,\u201d meaning that 60% of the original dataset with labels is placed in the training set). They train the model on the training set and evaluate it on the validation data to obtain an honest report of its performance. If they\u2019re not satisfied with the model\u2019s validation performance, they can tweak the modeling system."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230721324-c2616c84-f8af-4b79-a674-9556e4a352af.png",alt:"525591_1_En_1_Fig11_HTML"})),(0,r.kt)("h2",{id:"k-fold-evaluation"},"K-fold Evaluation"),(0,r.kt)("p",null,"In this evaluation schema, the dataset is randomly split into k folds (equally sized sets). For each of the k folds, we train a model from scratch on the other k \u2013 1 folds and evaluate it on that fold. Then, we average (or aggregate through some other method) the validation performance for each of the k-folds."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230721452-10c15eff-69c5-49c8-a2e5-526e6f30b9ba.png",alt:"525591_1_En_1_Fig19_HTML"})),(0,r.kt)("p",null,"This method allows us to obtain model validation performance on the \u201centire\u201d dataset. However, note that it may be expensive to train k models \u2013 an alternative is simply to randomly split into training and validation sets without a random seed a given number of times without regard for which specific parts of data are allocated into either set."),(0,r.kt)("p",null,"This approach is also known as k-fold cross-validation, because it allows us to validate our model across the entire dataset."))}p.isMDXComponent=!0}}]);