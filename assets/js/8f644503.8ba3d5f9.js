"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[83961],{3905:(e,t,r)=>{r.d(t,{Zo:()=>d,kt:()=>m});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function o(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function c(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)r=i[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var s=n.createContext({}),l=function(e){var t=n.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):o(o({},t),e)),r},d=function(e){var t=l(e.components);return n.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,d=c(e,["components","mdxType","originalType","parentName"]),u=l(r),m=a,g=u["".concat(s,".").concat(m)]||u[m]||p[m]||i;return r?n.createElement(g,o(o({ref:t},d),{},{components:r})):n.createElement(g,o({ref:t},d))}));function m(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=r.length,o=new Array(i);o[0]=u;var c={};for(var s in t)hasOwnProperty.call(t,s)&&(c[s]=t[s]);c.originalType=e,c.mdxType="string"==typeof e?e:a,o[1]=c;for(var l=2;l<i;l++)o[l]=r[l];return n.createElement.apply(null,o)}return n.createElement.apply(null,r)}u.displayName="MDXCreateElement"},31971:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>c,toc:()=>l});var n=r(87462),a=(r(67294),r(3905));const i={},o="Project: AdvancedBricks",c={unversionedId:"processing/databricks/project-advancedbricks/README",id:"processing/databricks/project-advancedbricks/README",title:"Project: AdvancedBricks",description:"Advanced Data Engineering with Databricks",source:"@site/docs/03-processing/databricks/project-advancedbricks/README.md",sourceDirName:"03-processing/databricks/project-advancedbricks",slug:"/processing/databricks/project-advancedbricks/",permalink:"/docs/processing/databricks/project-advancedbricks/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681583434,formattedLastUpdatedAt:"Apr 15, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Project: Data Engineer Learner Path with Databricks",permalink:"/docs/processing/databricks/project-learnerbricks/"},next:{title:"Project: BedBricks",permalink:"/docs/processing/databricks/project-bedbricks/"}},s={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Learning objectives",id:"learning-objectives",level:3},{value:"Prerequisites",id:"prerequisites",level:3}],d={toc:l};function p(e){let{components:t,...r}=e;return(0,a.kt)("wrapper",(0,n.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"project-advancedbricks"},"Project: AdvancedBricks"),(0,a.kt)("blockquote",null,(0,a.kt)("p",{parentName:"blockquote"},"Advanced Data Engineering with Databricks")),(0,a.kt)("h2",{id:"introduction"},"Introduction"),(0,a.kt)("p",null,"In this course, students will build upon their existing knowledge of Apache Spark, Structured Streaming, and Delta Lake to unlock the full potential of the data lakehouse by utilizing the suite of tools provided by Databricks. This course places a heavy emphasis on designs favoring incremental data processing, enabling systems optimized to continuously ingest and analyze ever-growing data. By designing workloads that leverage built-in platform optimizations, data engineers can reduce the burden of code maintenance and on-call emergencies, and quickly adapt production code to new demands with minimal refactoring or downtime. The topics in this course should be mastered prior to attempting the Databricks Certified s Data Engineering Professional exam.\xa0"),(0,a.kt)("p",null,(0,a.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214502374-93a5f21a-cdf3-498c-8f24-8d032bcf2451.png",alt:null})),(0,a.kt)("h3",{id:"learning-objectives"},"Learning objectives"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Design databases and pipelines optimized for the Databricks Lakehouse Platform."),(0,a.kt)("li",{parentName:"ul"},"Implement efficient incremental data processing to validate and enrich data driving business decisions and applications."),(0,a.kt)("li",{parentName:"ul"},"Leverage Databricks-native features for managing access to sensitive data and fulfilling right-to-be-forgotten requests."),(0,a.kt)("li",{parentName:"ul"},"Manage code promotion, task orchestration, and production job monitoring using Databricks tools.")),(0,a.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Experience using PySpark APIs to perform advanced data transformations"),(0,a.kt)("li",{parentName:"ul"},"Familiarity implementing classes with Python"),(0,a.kt)("li",{parentName:"ul"},"Experience using SQL in production data warehouse or data lake implementations"),(0,a.kt)("li",{parentName:"ul"},"Experience working in Databricks notebooks and configuring clusters"),(0,a.kt)("li",{parentName:"ul"},"Familiarity with creating and manipulating data in Delta Lake tables with SQL"),(0,a.kt)("li",{parentName:"ul"},"Ability to use Spark Structured Streaming to incrementally read from a Delta table")))}p.isMDXComponent=!0}}]);