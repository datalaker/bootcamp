"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[94756],{3905:(e,t,r)=>{r.d(t,{Zo:()=>d,kt:()=>c});var a=r(67294);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function n(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?n(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):n(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function i(e,t){if(null==e)return{};var r,a,o=function(e,t){if(null==e)return{};var r,a,o={},n=Object.keys(e);for(a=0;a<n.length;a++)r=n[a],t.indexOf(r)>=0||(o[r]=e[r]);return o}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(a=0;a<n.length;a++)r=n[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(o[r]=e[r])}return o}var u=a.createContext({}),l=function(e){var t=a.useContext(u),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},d=function(e){var t=l(e.components);return a.createElement(u.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},p=a.forwardRef((function(e,t){var r=e.components,o=e.mdxType,n=e.originalType,u=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),p=l(r),c=o,m=p["".concat(u,".").concat(c)]||p[c]||h[c]||n;return r?a.createElement(m,s(s({ref:t},d),{},{components:r})):a.createElement(m,s({ref:t},d))}));function c(e,t){var r=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var n=r.length,s=new Array(n);s[0]=p;var i={};for(var u in t)hasOwnProperty.call(t,u)&&(i[u]=t[u]);i.originalType=e,i.mdxType="string"==typeof e?e:o,s[1]=i;for(var l=2;l<n;l++)s[l]=r[l];return a.createElement.apply(null,s)}return a.createElement.apply(null,r)}p.displayName="MDXCreateElement"},97547:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>u,contentTitle:()=>s,default:()=>h,frontMatter:()=>n,metadata:()=>i,toc:()=>l});var a=r(87462),o=(r(67294),r(3905));const n={},s="Lab: Uber Data Analysis in Pyspark",i={unversionedId:"foundations/language/pyspark/lab-uber-analysis/README",id:"foundations/language/pyspark/lab-uber-analysis/README",title:"Lab: Uber Data Analysis in Pyspark",description:"This lab can be used as a take-home assignment to learn Pyspark and Data Engineering.",source:"@site/docs/01-foundations/language/pyspark/lab-uber-analysis/README.md",sourceDirName:"01-foundations/language/pyspark/lab-uber-analysis",slug:"/foundations/language/pyspark/lab-uber-analysis/",permalink:"/docs/foundations/language/pyspark/lab-uber-analysis/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681539985,formattedLastUpdatedAt:"Apr 15, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Lab: Spark Optimizations for Analytics Workloads",permalink:"/docs/foundations/language/pyspark/lab-spark-optimizations/"},next:{title:"Lab: Understanding Spark Query Execution",permalink:"/docs/foundations/language/pyspark/lab-understand-spark-query-execution/"}},u={},l=[{value:"Data Description",id:"data-description",level:2},{value:"Assignment",id:"assignment",level:2},{value:"Solution",id:"solution",level:2}],d={toc:l};function h(e){let{components:t,...r}=e;return(0,o.kt)("wrapper",(0,a.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"lab-uber-data-analysis-in-pyspark"},"Lab: Uber Data Analysis in Pyspark"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"This lab can be used as a take-home assignment to learn Pyspark and Data Engineering.")),(0,o.kt)("h2",{id:"data-description"},"Data Description"),(0,o.kt)("p",null,"To answer the question, use the dataset from the file dataset.csv. For example, consider a row from this dataset:"),(0,o.kt)("p",null,(0,o.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/25612446/219965433-956a1dc0-2acf-4d0d-b1cb-40723249d349.png",alt:"image"})),(0,o.kt)("p",null,"This means that during the hour beginning at 4pm (hour 16), on September 10th, 2012, 11 people opened the Uber app (Eyeballs). 2 of them did not see any car (Zeroes) and 4 of them requested a car (Requests). Of the 4 requests, only 3 complete trips actually resulted (Completed Trips). During this time, there were a total of 6 drivers who logged in (Unique Drivers)."),(0,o.kt)("h2",{id:"assignment"},"Assignment"),(0,o.kt)("p",null,"Using the provided dataset, answer the following questions:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"\u26a1 Which date had the most completed trips during the two week period?"),(0,o.kt)("li",{parentName:"ul"},"\u26a1 What was the highest number of completed trips within a 24 hour period?"),(0,o.kt)("li",{parentName:"ul"},"\u26a1 Which hour of the day had the most requests during the two week period?"),(0,o.kt)("li",{parentName:"ul"},"\u26a1 What percentages of all zeroes during the two week period occurred on weekend (Friday at 5 pm to Sunday at 3 am)? Tip: The local time value is the start of the hour (e.g. 15 is the hour from 3:00pm - 4:00pm)"),(0,o.kt)("li",{parentName:"ul"},'\u26a1 What is the weighted average ratio of completed trips per driver during the two week period? Tip: "Weighted average" means your answer should account for the total trip volume in each hour to determine the most accurate number in whole period.'),(0,o.kt)("li",{parentName:"ul"},"\u26a1 In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two week period in terms of unique requests? A new shift starts in every 8 hours. Assume that a driver will work same shift each day."),(0,o.kt)("li",{parentName:"ul"},"\u26a1 True or False: Driver supply always increases when demand increases during the two week period. Tip: Visualize the data to confirm your answer if needed."),(0,o.kt)("li",{parentName:"ul"},"\u26a1 In which 72 hour period is the ratio of Zeroes to Eyeballs the highest?"),(0,o.kt)("li",{parentName:"ul"},"\u26a1 If you could add 5 drivers to any single hour of every day during the two week period, which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing"),(0,o.kt)("li",{parentName:"ul"},'\u26a1 Looking at the data from all two weeks, which time might make the most sense to consider a true "end day" instead of midnight? (i.e when are supply and demand at both their natural minimums) Tip: Visualize the data to confirm your answer if needed.')),(0,o.kt)("h2",{id:"solution"},"Solution"),(0,o.kt)("p",null,"To solve the questions using PySpark, we need to first create a SparkSession and load the dataset into a DataFrame. Here's how we can do it:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\n# Create a SparkSession\nspark = SparkSession.builder.appName("UberDataAnalysis").getOrCreate()\n\n# Load the dataset into a DataFrame\ndf = spark.read.csv("uber.csv", header=True, inferSchema=True) \n')),(0,o.kt)("p",null,"Now that we have loaded the dataset into a data frame, we can start answering the questions.\nWhich date had the most completed trips during the two-week period?"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"To find the date with the most completed trips, you can group the data by date and sum the completed trips column. Then, sort the results in descending order and select the top row.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import max\n\n# Read the data from CSV file\nuber = spark.read.csv("uber.csv", header=True, inferSchema=True)\n\n# Group the data by date and sum the completed trips\ncompleted_trips_by_date = uber.groupBy("Date").sum("Completed Trips")\n\n# Find the date with the most completed trips\ndate_with_most_completed_trips = completed_trips_by_date \\\n    .orderBy("sum(Completed Trips)", ascending=False) \\\n    .select("Date") \\\n    .first()["Date"]\n\nprint(date_with_most_completed_trips)\n\n#Output:  2012-09-15\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"What was the highest number of completed trips within a 24-hour period?")),(0,o.kt)("p",null,"To find the highest number of completed trips within a 24-hour period, you can group the data by date and use a window function to sum the completed trips column over a rolling 24-hour period. Then, you can sort the results in descending order and select the top row."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import sum, window\n\n# Read the data from CSV file\nuber = spark.read.csv("uber.csv", header=True, inferSchema=True)\n\n# Group the data by 24-hour windows and sum the completed trips\ncompleted_trips_by_window = uber \\\n    .groupBy(window("Time (Local)", "24 hours")) \\\n    .agg(sum("Completed Trips").alias("Total Completed Trips")) \\\n    .orderBy("Total Completed Trips", ascending=False)\n\n# Get the highest number of completed trips within a 24-hour period\nhighest_completed_trips_in_24_hours = completed_trips_by_window \\\n    .select("Total Completed Trips") \\\n    .first()["Total Completed Trips"]\n\nprint(highest_completed_trips_in_24_hours)\n\n#Output 2102\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Which hour of the day had the most requests during the two-week period?")),(0,o.kt)("p",null,'To answer this question, we need to group the data by an hour and sum the "Requests" column for each hour. We can then sort the result by the sum of requests and select the hour with the highest sum.'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import hour, sum\n\nhourly_requests = df.groupBy(hour("Time (Local)").alias("hour")).agg(sum("Requests").alias("total_requests")).orderBy("total_requests", ascending=False)\n\nmost_requested_hour = hourly_requests.select("hour").first()[0]\nprint("The hour with the most requests is:", most_requested_hour)\n\n#The hour with the most requests is: 17\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"What percentages of all zeroes during the two-week period occurred on weekends (Friday at 5 pm to Sunday at 3 am)?")),(0,o.kt)("p",null,"To answer this question, we need to filter the data to select only the rows that fall within the specified time range, count the total number of zeros, and count the number of zeros that occurred on weekends. We can then calculate the percentage of zeros that occurred on weekends."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import dayofweek, hour\n\nweekend_zeros = df.filter((hour("Time (Local)") >= 17) | (hour("Time (Local)") < 3)).filter((dayofweek("Date") == 6) | (dayofweek("Date") == 7)).agg(sum("Zeroes").alias("weekend_zeros")).collect()[0]["weekend_zeros"]\n\ntotal_zeros = df.agg(sum("Zeroes").alias("total_zeros")).collect()[0]["total_zeros"]\n\npercent_weekend_zeros = weekend_zeros / total_zeros * 100\n\nprint("The percentage of zeros that occurred on weekends is:", percent_weekend_zeros, "%")\n\n#The percentage of zeros that occurred on weekends is: 41.333414829040026 %\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"What is the weighted average ratio of completed trips per driver during the two-week period?")),(0,o.kt)("p",null,"To answer this question, we need to calculate the ratio of completed trips to unique drivers for each hour, multiply the ratio by the total number of completed trips for that hour, and then sum the results. We can then divide this sum by the total number of completed trips for the entire period."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import avg\n\nweighted_avg = df.withColumn("completed_per_driver", df["Completed Trips"] / df["Unique Drivers"]) \\\n                 .groupBy("Date", "Time (Local)") \\\n                 .agg(avg("completed_per_driver").alias("avg_completed_per_driver"), sum("Completed Trips").alias("total_completed_trips")) \\\n                 .withColumn("weighted_ratio", col("avg_completed_per_driver") * col("total_completed_trips")) \\\n                 .agg(sum("weighted_ratio") / sum("total_completed_trips")).collect()[0][0]\n\nprint("The weighted average ratio of completed trips per driver is:", weighted_avg)\n\n#Output: The weighted average ratio of completed trips per driver is: 1.2869201507713425\n')),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.")),(0,o.kt)("p",null,"To solve this, we can first calculate the number of unique requests for each hour of the day, and then slide a window of 8 hours across the hours to find the 8 consecutive hours with the highest number of unique requests. Here's the code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import col, hour, countDistinct\nfrom pyspark.sql.window import Window\n\n# Calculate the number of unique requests for each hour of the day\nhourly_unique_requests = (df\n  .groupBy(hour("Time (Local)").alias("hour"))\n  .agg(countDistinct("Requests").alias("unique_requests"))\n)\n\n# Slide a window of 8 hours to find the busiest 8 consecutive hours\nwindow = Window.orderBy(col("unique_requests").desc()).rowsBetween(0, 7)\nbusiest_8_consecutive_hours = (hourly_unique_requests\n  .select("*", sum("unique_requests").over(window).alias("consecutive_sum"))\n  .orderBy(col("consecutive_sum").desc())\n  .limit(1)\n)\n\n# Print the result\nbusiest_8_consecutive_hours.show()\n')),(0,o.kt)("p",null,"This will output the busiest 8 consecutive hours in terms of unique requests, along with the number of unique requests during that time period."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"True or False: Driver supply always increases when demand increases during the two-week period.")),(0,o.kt)("p",null,"This statement is false. There are multiple reasons why driver supply might not always increase when demand increases. For example, some drivers might choose not to work during peak demand times, or there might be external factors that affect driver availability (such as traffic, weather conditions, or events in the city). To confirm this, we would need to analyze the data and identify instances where demand increased but driver supply did not."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?")),(0,o.kt)("p",null,"To answer this question, we can group the data by 72-hour periods and calculate the ratio of zeroes to eyeballs for each period. We can then find the period with the highest ratio. Here's the code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'from pyspark.sql.functions import col, sum\n\n# Group the data by 72-hour periods and calculate the ratio of zeroes to eyeballs for each period\nperiod_ratios = (df\n  .groupBy(((col("Date").cast("timestamp").cast("long") / (72*3600)).cast("int")).alias("period"))\n  .agg(sum("Zeroes").alias("zeroes"), sum("Eyeballs").alias("eyeballs"))\n  .withColumn("ratio", col("zeroes") / col("eyeballs"))\n)\n\n# Find the period with the highest ratio\nhighest_ratio_period = period_ratios.orderBy(col("ratio").desc()).limit(1)\n\n# Print the result\nhighest_ratio_period.show()\n')),(0,o.kt)("p",null,"This will output the 72-hour period with the highest ratio of zeroes to eyeballs."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing.")),(0,o.kt)("p",null,"To determine which hour to add 5 drivers too, we want to look for an hour where there are a high number of rider eyeballs and a low number of unique drivers. One way to approach this is to calculate the ratio of requests to unique drivers for each hour and then choose the hour with the highest ratio. The idea here is that adding more drivers to an hour with a high ratio will result in more completed trips.\nWe can use the following PySpark code to calculate the ratio for each hour:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Calculate requests per unique driver for each hour\nrequests_per_driver = (df.groupBy('Time (Local)').agg(\n    (F.sum('Requests') / F.countDistinct('Unique Drivers')).alias('requests_per_driver'))\n)\n\n# Show the hour with the highest ratio\nrequests_per_driver.orderBy(F.desc('requests_per_driver')).show(1)\n")),(0,o.kt)("p",null,"This will output the hour with the highest requests per unique driver ratio, which is where we should add 5 drivers."),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},'Looking at the data from all two weeks, which time might make the most sense to consider a true "end day" instead of midnight? (i.e when are supply and demand at both their natural minimums)')),(0,o.kt)("p",null,"One way to approach this question is to calculate the average number of completed trips and unique drivers for each hour of the day over the entire two-week period. We can then look for the hour with the lowest number of completed trips and unique drivers to find the time when supply and demand are at their natural minimums.\nWe can use the following PySpark code to calculate the average number of completed trips and unique drivers for each hour:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"# Calculate average completed trips and unique drivers for each hour\navg_trips_and_drivers = (df.groupBy('Time (Local)').agg(\n    F.mean('Completed Trips').alias('avg_completed_trips'),\n    F.mean('Unique Drivers').alias('avg_unique_drivers')\n))\n\n# Show the hour with the lowest average completed trips and unique drivers\navg_trips_and_drivers.orderBy('avg_completed_trips', 'avg_unique_drivers').show(1)\n")),(0,o.kt)("p",null,'This will output the hour with the lowest average number of completed trips and unique drivers, which is when supply and demand are at their natural minimums and might make the most sense to consider as the "end day".'))}h.isMDXComponent=!0}}]);