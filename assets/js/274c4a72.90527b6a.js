"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8434],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>m});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},d=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=l(a),m=i,g=u["".concat(c,".").concat(m)]||u[m]||h[m]||o;return a?n.createElement(g,r(r({ref:t},d),{},{components:a})):n.createElement(g,r({ref:t},d))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=u;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var l=2;l<o;l++)r[l]=a[l];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},73253:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var n=a(87462),i=(a(67294),a(3905));const o={},r="Data Encoding",s={unversionedId:"datascience/data-encoding",id:"datascience/data-encoding",title:"Data Encoding",description:"Discrete Data",source:"@site/docs/10-datascience/data-encoding.md",sourceDirName:"10-datascience",slug:"/datascience/data-encoding",permalink:"/docs/datascience/data-encoding",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Image Analytics with Tensorflow",permalink:"/docs/datascience/computer-vision/lab-image-analytics-tensorflow/"},next:{title:"Data Preparation",permalink:"/docs/datascience/data-preparation"}},c={},l=[{value:"Discrete Data",id:"discrete-data",level:2},{value:"Label Encoding",id:"label-encoding",level:3},{value:"One-Hot Encoding",id:"one-hot-encoding",level:3},{value:"Binary Encoding",id:"binary-encoding",level:3},{value:"Frequency Encoding",id:"frequency-encoding",level:3},{value:"Target Encoding",id:"target-encoding",level:3},{value:"Leave-One-Out Encoding",id:"leave-one-out-encoding",level:3},{value:"James-Stein Encoding",id:"james-stein-encoding",level:3},{value:"Weight of Evidence",id:"weight-of-evidence",level:3},{value:"Continuous Data",id:"continuous-data",level:2},{value:"Min-Max Scaling",id:"min-max-scaling",level:3},{value:"Robust Scaling",id:"robust-scaling",level:3},{value:"Standardization",id:"standardization",level:3},{value:"Text Data",id:"text-data",level:2},{value:"Raw Vectorization",id:"raw-vectorization",level:3},{value:"Bag of Words",id:"bag-of-words",level:3},{value:"N-Grams",id:"n-grams",level:3},{value:"TF-IDF",id:"tf-idf",level:3},{value:"Word2Vec",id:"word2vec",level:3},{value:"Time Data",id:"time-data",level:2},{value:"Geographical Data",id:"geographical-data",level:2}],d={toc:l};function h(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"data-encoding"},"Data Encoding"),(0,i.kt)("h2",{id:"discrete-data"},"Discrete Data"),(0,i.kt)("h3",{id:"label-encoding"},"Label Encoding"),(0,i.kt)("p",null,"Label encoding is perhaps the simplest and most direct method of encoding discrete data \u2013 each unique category is associated with a single integer label. This is almost always not the final encoding that you should use for categorical variables, since attaching encodings in this way forces us to make arbitrary decisions that lead to meaningful outcomes. If we associate a category value \u201cDog\u201d with 1 but \u201cSnake\u201d with 2, the model has access to the explicitly coded quantitative relationship that \u201cSnake\u201d is two times \u201cDog\u201d in magnitude or that \u201cSnake\u201d is larger than \u201cDog.\u201d Moreover, there is no good reason \u201cDog\u201d should be labeled 1 and \u201cSnake\u201d should be labeled 2 instead of vice versa. However, label encoding is the basis/primary step upon which many other encodings can be applied. Thus, it is useful to understand how to implement it."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230728515-0bcd376f-f8ad-465b-abfb-6c0e86995d88.png",alt:"label_encoding"})),(0,i.kt)("h3",{id:"one-hot-encoding"},"One-Hot Encoding"),(0,i.kt)("p",null,"In cases of categorical variables in which no definitive quantitative label can be attached, the simplest satisfactory choice is generally one-hot encoding. If there are n unique classes, we create n binary columns, each representing whether the item belongs to that class or not. Thus, there will be one \u201c1\u201d across each of the n columns for every row (and all others as \u201c0\u201d)."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230728517-dc3be36c-a669-42a5-8052-daeaea45ebaa.png",alt:"onehot-encoding"})),(0,i.kt)("p",null,"One problem that may arise from using one-hot encoding, however, is multicollinearity. Multicollinearity occurs when several features are highly correlated such that one can reliably be predicted as a linear relationship of the others. In a one-hot encoding, the sum of feature values across each row is always 1; if we know the values of all other features for some row, we also know the value of the remaining feature."),(0,i.kt)("p",null,"This can become problematic because each feature is no longer independent, whereas many machine learning algorithms like K-Nearest Neighbors (KNN) and regression assume that each dimension of the dataset is not correlated with any other. While multicollinearity may only have a marginal negative effect on model performance, the larger problem is the effect on parameter interpretation. If two independent variables in a Linear Regression model are highly correlated, their resulting parameters after training become almost meaningless because the model could have performed just as well with another set of parameters (e.g., switching the two parameters, ambiguous multiplicity of solutions). Highly correlated features act as approximate duplicates, which means that the corresponding coefficients are halved as well."),(0,i.kt)("p",null,"One simple method to address multicollinearity in one-hot encoding is to randomly drop one (or several) of the features in the encoded feature set. This has the effect of disrupting a uniform sum of 1 across each row while still retaining a unique combination of values for each item (one of the categories will be defined by all zeros, since the feature that would have been marked as \u201c1\u201d was dropped). The disadvantage is that the equality of representation across classes of the encoding is now unbalanced, which may disrupt certain machine learning models \u2013 especially ones that utilize regularization. The take-away for best performance: choose either regularization + feature selection or column dropping, but not both."),(0,i.kt)("h3",{id:"binary-encoding"},"Binary Encoding"),(0,i.kt)("p",null,"Two weaknesses of one-hot encoding \u2013 sparsity and multicollinearity \u2013 can be addressed, or at least improved, with binary encoding. The categorical feature is label encoded (i.e., each unique category is associated with an integer); the labels are converted to binary form and transferred to a set of features in which each column is one place value. That is, a column is created for each digit place of the binary representation."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230728572-d774acbd-89cf-4cce-89aa-8d885ccff95e.png",alt:"binary-encoding"})),(0,i.kt)("p",null,"Because we use binary representations rather than allocating one column per unique class, we more compactly represent the same information (i.e., the same class has the same combination of \u201c1\u201ds and \u201c0\u201ds across the features) at the cost of decreased interpretability (i.e., it\u2019s not clear what each column represents). Moreover, there is no reliable multicollinearity between each of the features used to represent the categorical information."),(0,i.kt)("h3",{id:"frequency-encoding"},"Frequency Encoding"),(0,i.kt)("p",null,"Label encoding, one-hot encoding, and binary encoding each offer methods of encoding that reflect the \u201cpure identity\u201d of each unique class; that is, we devise quantitative methods to assign a unique identity to each class."),(0,i.kt)("p",null,"However, we can both assign unique values to each class and communicate additional information about each class at once. To frequency-encode a feature, we replace each categorical value with the proportion of how often that class appears in the dataset. With frequency encoding, we communicate how often that class appears in the dataset, which may be of value to whatever algorithm is processing it."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230728632-f5ed3606-ede0-4846-b51e-67c6bfd09076.png",alt:"frequency-encoding"})),(0,i.kt)("p",null,"Like one-hot encoding and binary encoding, we attach a unique quantitative representation to each class (although frequency encoding does not guarantee a unique quantitative representation, especially in small datasets). With this encoding scheme, however, the actual value of the representation lies on a continuous scale, and quantitative relationships between the encodings are not arbitrary but instead communicate some piece of information. In the preceding example, \u201cDenver\u201d is \u201cthree times\u201d Miami because it appears three times as often in the dataset."),(0,i.kt)("p",null,"Frequency encoding is the most powerful when the dataset is representative and free of bias. If this is not the case, the actual quantitative encodings may be meaningless in the sense of not providing relevant and truthful/representative information for the purposes of modeling."),(0,i.kt)("h3",{id:"target-encoding"},"Target Encoding"),(0,i.kt)("p",null,"Frequency encoding is often unsatisfactory because it often doesn\u2019t directly reflect information in a class that is directly relevant to a model that uses it. Target encoding is an attempt to model the relationship more directly between the categorical class x and the dependent variable y to be predicted by replacing each class with the mean or median (respectively) value of y for that class. It is assumed that the target class is already in quantitative form, although the target does not necessarily need to be continuous (i.e. a regression problem) to be used in target encoding. For instance, taking the mean of binary classification labels, which are either 0 or 1, gives insight into the proportion of items in the dataset with that class that were associated with class 0. This can be interpreted as the probability the item belongs to a target class given only one independent feature."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230728947-2e21e162-0a82-4747-8bd9-240d143e136c.png",alt:"525591_1_En_2_Fig12_HTML"})),(0,i.kt)("p",null,"Figure: Target encoding using the mean"),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230728948-7dd2208a-5b83-4c32-bf91-33c77ca2f189.png",alt:"525591_1_En_2_Fig13_HTML"})),(0,i.kt)("p",null,"Figure: Target encoding using the median"),(0,i.kt)("p",null,"Note that target encoding can lead to data leakage if encoding is performed before the training and validation sets are split, since the averaging function incorporates information from both the training and validation sets. In order to prevent this, encode the training and validation sets separately after splitting. If the validation dataset is too small, target-encoding the set independently may yield skewed, unrepresentative encodings. In this case, you can use averages per class from the training dataset. This form of \u201cdata leakage\u201d is not inherently problematic, since we are using training data to inform operation on the validation set rather than using validation data to inform operation on the training set."),(0,i.kt)("h3",{id:"leave-one-out-encoding"},"Leave-One-Out Encoding"),(0,i.kt)("p",null,"Mean-based target encoding can be quite powerful, but it suffers from the presence of outliers. If outliers are present that skew the mean, their effects are imprinted across the entire dataset. Leave-one-out encoding is a variation on the target encoding scheme by leaving the \u201ccurrent\u201d item/row out of consideration when calculating the mean for all items of that class. Like target encoding, encoding should be performed separately on training and validation sets to prevent data leakage."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230729170-913e381b-941d-451a-99c8-f3e9e2aece07.png",alt:"525591_1_En_2_Fig15_HTML"})),(0,i.kt)("h3",{id:"james-stein-encoding"},"James-Stein Encoding"),(0,i.kt)("p",null,"Target encoding and leave-one-out encoding assume that each categorical feature is directly and linearly related to the dependent variable. We can take a more sophisticated approach to encoding with James-Stein encoding by incorporating both the overall mean for a feature and the individual mean per class for a feature into an encoding (Figure 2-17). This is achieved by defining the encoding for a category as a weighted sum of the overall mean and individual mean per class via a parameter \u03b2, which is bounded by 0 \u2264 \u03b2 \u2264 1."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230729233-a7cfcf61-0db4-413b-8235-55243b45b0b0.png",alt:"525591_1_En_2_Fig17_HTML"})),(0,i.kt)("p",null,"When \u03b2 = 0, James-Stein encoding is the same as mean-based target encoding. On the other hand, when \u03b2 = 1, James-Stein encoding replaces all values in a column with the average dependent variable value, regardless of individual class values."),(0,i.kt)("h3",{id:"weight-of-evidence"},"Weight of Evidence"),(0,i.kt)("p",null,"The weight of evidence (WoE) technique originated from credit scoring; it was used to measure how \u201cseparable\u201d good customers (paid back a loan) were from bad customers (defaulted on a loan) across a group i (this could be something like customer location, history, etc.)"),(0,i.kt)("p",null,"Weight of evidence is often presented as representing how much the evidence undermines or supports the hypothesis. In the context of categorical encoding, the \u201chypothesis\u201d is that the selected categorical feature can cleanly divide classes such that we can reliably predict which class an item falls in given only information about its inclusion or exclusion from the group i. The \u201cevidence\u201d is the actual distribution of target values within a certain group i."),(0,i.kt)("p",null,"We can also generalize this to multiclass problems by finding the WoE for each class, in which \u201cclass 0\u201d is \u201cin class\u201d and \u201cclass 1\u201d is \u201cnot in class\u201d; the weight of evidence of the complete dataset can then be found by somehow aggregating the individual class-specific WoE calculation, for example, by taking the mean."),(0,i.kt)("h2",{id:"continuous-data"},"Continuous Data"),(0,i.kt)("h3",{id:"min-max-scaling"},"Min-Max Scaling"),(0,i.kt)("p",null,"Min-max scaling generally refers to the scaling of the range of a dataset such that it is between 0 and 1 \u2013 the minimum value of the dataset is 0, and the maximum is 1, but the relative distances between points remain the same."),(0,i.kt)("h3",{id:"robust-scaling"},"Robust Scaling"),(0,i.kt)("p",null,"From the formula for min-max scaling, we see that each scaled value of the dataset is directly impacted by the maximum and minimum values. Hence, outliers significantly impact the scaling operation. Robust scaling subtracts the median value from all values in the dataset and divides by the interquartile range."),(0,i.kt)("h3",{id:"standardization"},"Standardization"),(0,i.kt)("p",null,"More commonly, machine learning algorithms assume that data is standardized \u2013 that is, in the form of a normal distribution with unit variance (standard deviation of 1) and zero mean (centered at 0). Assuming the input data is already somewhat normally distributed, standardization subtracts the dataset\u2019s mean and divides by the dataset\u2019s standard deviation. This has the effect of shifting the dataset mean to 0 and scaling the standard deviation to 1."),(0,i.kt)("h2",{id:"text-data"},"Text Data"),(0,i.kt)("h3",{id:"raw-vectorization"},"Raw Vectorization"),(0,i.kt)("p",null,"Raw vectorization can be thought of as \u201cone-hot encoding\u201d for text: it is an explicit quantitative representation of the information contained within text. Rather than assigning each text a unique class, texts are generally vectorized as a sequence of language units, like characters or words. These are also referred to as tokens. Each of these words or characters is considered to be a unique class, which can be one-hot encoded. Then, a passage of text is a sequence of one-hot encodings."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230732506-ccfe1770-c289-4702-a652-73ffb8abdc97.png",alt:"525591_1_En_2_Fig28_HTML"})),(0,i.kt)("h3",{id:"bag-of-words"},"Bag of Words"),(0,i.kt)("p",null,"In order to reduce the sheer dimensionality/size of a raw vectorization text representation, we can use the Bag of Words (BoW) model to \u201ccollapse\u201d raw vectorizations. In Bag of Words, we count how many times each language unit appears in a text sample while ignoring the specific order and context in which the language units were used."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230732553-d1e7cec3-760f-4f8d-9e8c-0c5a153572db.png",alt:"525591_1_En_2_Fig29_HTML"})),(0,i.kt)("h3",{id:"n-grams"},"N-Grams"),(0,i.kt)("p",null,"We can be more sophisticated than the Bag of Words model by counting the number of unique two-word combinations, or bigrams. This can help reveal context and multiplicity of word meaning; for instance, the Paris in the stripped (no punctuation, no capitalization) text except \u201cparis france\u201d is very different from the Paris in \u201cparis hilton.\u201d We can consider each bigram to be its own term and encode it as such."),(0,i.kt)("p",null,(0,i.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/230732621-0c4d3fd4-f080-4647-b7ad-62cfbcb51011.png",alt:"525591_1_En_2_Fig30_HTML"})),(0,i.kt)("h3",{id:"tf-idf"},"TF-IDF"),(0,i.kt)("p",null,"Another weakness of the Bag of Words model is that the number of times a word appears in the text may not be a good indicator of how important or relevant it is. For instance, the word \u201cthe\u201d appears seven times in this paragraph, more than any other. Does this mean that the word \u201cthe\u201d is the most significant or holds the most meaning?"),(0,i.kt)("p",null,"No, the word \u201cthe\u201d is primarily an artifact of grammar/syntactic structure and reflects little semantic meaning, at least in contexts we are generally concerned with. We usually address the problem of text-saturating syntactic tokens by removing so-called \u201cstop words\u201d from a corpus before encoding it."),(0,i.kt)("p",null,"However, there are many words left over from stop word screening that hold semantic value but suffer from another problem that the word \u201cthe\u201d creates: Because of the structure of the corpus, certain words inherently appear very often throughout the text. This does not mean that they are more important. Consider a corpus of customer reviews for a jacket: naturally, the word \u201cjacket\u201d will appear very often (e.g., \u201cI bought this jacket\u2026,\u201d \u201cThis jacket arrived at my house\u2026\u201d), but in actuality it is not very relevant to our analysis. We know that the corpus is about the jacket and care instead about words that may occur less but mean more, like \u201cbad\u201d (e.g., \u201cThis jacket is bad\u201d), \u201cdurable\u201d (e.g., \u201cSuch a durable jacket!\u201d), or \u201cgood\u201d (e.g., \u201cThis was a good buy\u201d)."),(0,i.kt)("p",null,"We can formalize this intuition by using TF-IDF, or Term Frequency\u2013Inverse Document Frequency, encoding. The logic behind TF-IDF encoding is that we care more about terms that appear often in one document (Term Frequency) but not very often across the entire corpus (Inverse Document Frequency). TF-IDF is calculated by weighting these two effects against each other."),(0,i.kt)("h3",{id:"word2vec"},"Word2Vec"),(0,i.kt)("p",null,"Previous discussion on encoding methods focused on relatively simplistic attempts to capture a text sample\u2019s meaning by attempting to extract one \u201cdimension\u201d or perspective. The Bag of Words model, for instance, captures meaning simply by counting how often a word appears in the text. The Term Frequency\u2013Inverse Document Frequency encoding method attempts to improve upon this scheme by defining a slightly more sophisticated level of meaning by balancing the occurrence of a word in a document with its occurrence in the complete corpus. In these encoding schemes, there is always one perspective or dimension of the text that we leave out and simply cannot capture."),(0,i.kt)("p",null,"With deep neural networks, however, we can capture more complex relationships between text samples \u2013 the nuances of word usage (e.g., \u201cParis,\u201d \u201cHilton,\u201d and \u201cParis Hilton\u201d all mean very different things!), grammatical exceptions, conventions, cultural significance, etc. The Word2Vec family of algorithms associates each word with a fixed-length vector representing latent (\u201chidden\u201d, \u201cimplicit\u201d) features."),(0,i.kt)("h2",{id:"time-data"},"Time Data"),(0,i.kt)("p",null,"Time/temporal data often appears in practical tabular datasets. For instance, a tabular dataset of online customer reviews may have a timestamp down to the second indicating exactly when it was posted. Alternatively, a tabular dataset of medical data might be associated with the day it was collected, but not the exact time. A tabular dataset of quarterly company earnings reports will contain temporal data by quarter. Time is a dynamic and complex data type that takes on many different forms and sizes. Luckily, because time is both so rich with information and well-understood, it is relatively easy to encode time or temporal features."),(0,i.kt)("p",null,"There are several methods to convert time data into a quantitative representation to make it readable to machine learning and deep learning models. The simplest method is simply to assign a time unit as a base unit and represent each time value as a multiple of base units from a starting time. The base unit should generally be the most relevant unit of time to the prediction problem; for instance, if time is stored as a month, date, and year and the prediction task is to predict sales, the base unit is a day, and we would represent each date as the number of days since a starting date (a convenient starting position like January 1, 1900, or simply the earliest date in the dataset). On the other hand, in a physics lab, we may need a base unit of a nanosecond due to high required precision, and time may be represented as the number of nanoseconds since some determined starting time."),(0,i.kt)("h2",{id:"geographical-data"},"Geographical Data"),(0,i.kt)("p",null,"Many tabular datasets will contain geographical data, in which a location is somehow specified in the dataset. Similarly to temporal/time data, geographical data can exist in several different levels of scope \u2013 by continent, country, state/province, city, zip code, address, or longitude and latitude, to name a few. Because of the information-rich and highly context-dependent nature of geographical data, there aren\u2019t well-established, sweeping guidelines on encoding geographical data. However, you can use many of the previously discussed encoding tools and strategies to your advantage here."),(0,i.kt)("p",null,"If your dataset contains geographical data in categorical form, like by country or state/province, you can use previously discussed categorical encoding methods, like one-hot encoding or target encoding."),(0,i.kt)("p",null,"Latitude and longitude are precise geospatial location indicators already in quantitative form, so there is no requirement for further encoding. However, you may find it valuable to add relevant abstract information derived from the latitude and longitude to the dataset, like which country the location falls in."),(0,i.kt)("p",null,"When working with specific addresses, you can extract multiple relevant features, like the country, state/province, zip code, and so on. You can also derive the exact longitude and latitude from the address and append both to the dataset as continuous quantitative representations of the address location."))}h.isMDXComponent=!0}}]);