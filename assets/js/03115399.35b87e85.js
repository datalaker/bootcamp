"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6620],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>m});var n=a(67294);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var l=n.createContext({}),c=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},p=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=c(a),m=i,h=u["".concat(l,".").concat(m)]||u[m]||d[m]||o;return a?n.createElement(h,r(r({ref:t},p),{},{components:a})):n.createElement(h,r({ref:t},p))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=a.length,r=new Array(o);r[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var c=2;c<o;c++)r[c]=a[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},59897:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var n=a(87462),i=(a(67294),a(3905));const o={},r="Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics",s={unversionedId:"capstones/kinesis-flink-etl/README",id:"capstones/kinesis-flink-etl/README",title:"Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics",description:"We will create an Amazon Kinesis Data Analytics for Apache Flink application with Amazon Kinesis Data Streams as a source and a Amazon S3 bucket as a sink. Random data is ingested using Amazon Kinesis Data Generator. The Apache Flink application code performs a word count on the streaming random data using a tumbling window of 5 minutes. The generated word count is then stored in the specified Amazon S3 bucket. Amazon Athena is used to query data generated in the Amazon S3 bucket to validate the end results.",source:"@site/docs/12-capstones/kinesis-flink-etl/README.md",sourceDirName:"12-capstones/kinesis-flink-etl",slug:"/capstones/kinesis-flink-etl/",permalink:"/docs/capstones/kinesis-flink-etl/",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Log Analytics and Processing in Real-Time",permalink:"/docs/capstones/kinesis-flink-beam/"},next:{title:"Kortex",permalink:"/docs/capstones/kortex/"}},l={},c=[{value:"Create an Amazon Kinesis Data Stream",id:"create-an-amazon-kinesis-data-stream",level:2},{value:"Set up Amazon Kinesis Data Generator",id:"set-up-amazon-kinesis-data-generator",level:2},{value:"Send sample data to an Amazon Kinesis Data Stream",id:"send-sample-data-to-an-amazon-kinesis-data-stream",level:2},{value:"Create Amazon S3 buckets",id:"create-amazon-s3-buckets",level:2},{value:"Modify the code for Amazon Kinesis Data Analytics application",id:"modify-the-code-for-amazon-kinesis-data-analytics-application",level:2},{value:"Compile the application code",id:"compile-the-application-code",level:2},{value:"Upload the Apache Flink Streaming Java Code to an Amazon S3 bucket",id:"upload-the-apache-flink-streaming-java-code-to-an-amazon-s3-bucket",level:2},{value:"Create, configure, and start the Kinesis Data Analytics application",id:"create-configure-and-start-the-kinesis-data-analytics-application",level:2},{value:"Project Structure",id:"project-structure",level:2}],p={toc:c};function d(e){let{components:t,...a}=e;return(0,i.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"streaming-etl-pipeline-with-apache-flink-and-amazon-kinesis-data-analytics"},"Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics"),(0,i.kt)("p",null,"We will create an Amazon Kinesis Data Analytics for Apache Flink application with Amazon Kinesis Data Streams as a source and a Amazon S3 bucket as a sink. Random data is ingested using Amazon Kinesis Data Generator. The Apache Flink application code performs a word count on the streaming random data using a tumbling window of 5 minutes. The generated word count is then stored in the specified Amazon S3 bucket. Amazon Athena is used to query data generated in the Amazon S3 bucket to validate the end results."),(0,i.kt)("h2",{id:"create-an-amazon-kinesis-data-stream"},"Create an Amazon Kinesis Data Stream"),(0,i.kt)("p",null,"Open the ",(0,i.kt)("a",{parentName:"p",href:"https://console.aws.amazon.com/kinesis/"},"https://console.aws.amazon.com/kinesis/")," and create an amazon Kinesis Data Stream ",(0,i.kt)("inlineCode",{parentName:"p"},"kinesisDataStream1")," with on-demand capacity."),(0,i.kt)("h2",{id:"set-up-amazon-kinesis-data-generator"},"Set up Amazon Kinesis Data Generator"),(0,i.kt)("p",null,"Go to ",(0,i.kt)("a",{parentName:"p",href:"https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html"},"https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html")," and follow the instructions to create a new Kinesis Data Generator via CloudFormation. On the CloudFormation outputs tab, You will get a URL. Go there and login with the user id and password that you provided in CloudFormation."),(0,i.kt)("h2",{id:"send-sample-data-to-an-amazon-kinesis-data-stream"},"Send sample data to an Amazon Kinesis Data Stream"),(0,i.kt)("p",null,"In the Kinesis Data Generator, select contant as 1 and use the below template in Template 1:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'{{random.arrayElement([\n    "Hi",\n    "Hello there",\n    "How are you",\n    "Hi how are you"\n])}}\n')),(0,i.kt)("p",null,"Click on ",(0,i.kt)("inlineCode",{parentName:"p"},"Send data"),". To verify, go to your KDS Monitoring tab and in the Stream metrics section, find the ",(0,i.kt)("inlineCode",{parentName:"p"},"Put records successful records - average (Percent)")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Put records \u2013 sum (bytes)")," metrics to validate record ingestion."),(0,i.kt)("h2",{id:"create-amazon-s3-buckets"},"Create Amazon S3 buckets"),(0,i.kt)("p",null,"Run the ",(0,i.kt)("inlineCode",{parentName:"p"},"create-buckets.sh")," script."),(0,i.kt)("h2",{id:"modify-the-code-for-amazon-kinesis-data-analytics-application"},"Modify the code for Amazon Kinesis Data Analytics application"),(0,i.kt)("p",null,"In ",(0,i.kt)("a",{parentName:"p",href:"./S3Sink/src/main/java/com/amazonaws/services/kinesisanalytics/S3StreamingSinkJob.java"},"this")," file, you will the find the code that we will use. "),(0,i.kt)("p",null,"The incoming stream of records from Kinesis Data Stream, converts each record to lowercase and splits the record body at tab or spaces. For each word retrieved after the split, it creates a two dimensional tuple with word string and its count."),(0,i.kt)("p",null,"For example :"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"\u201cHi\u201d becomes (Hi,1)"),(0,i.kt)("li",{parentName:"ul"},"\u201cHi how are you\u201d becomes (Hi,1) (how,1) (are, 1) (you,1)")),(0,i.kt)("p",null,"Then, it partitions the stream of tuples using the first key (i.e. the word string) and performs a sum on second key (i.e. the count) for each partition under a tumbling window of one minute."),(0,i.kt)("p",null,"In the previous example, if no new records come up in the stream after 1 minute then the output of sum method would be :"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"(Hi,2) (how,1) (are, 1) (you,1)")),(0,i.kt)("p",null,"Then, a map is created using the sum output and send it to S3Sink. This means that if the record ingestion is sustained then a file in the ",(0,i.kt)("inlineCode",{parentName:"p"},"kda-word-count-tutorial-bucket-<unique-name>")," S3 bucket is created every minute with new-line separated character strings of following format:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Hi count: 824 "),(0,i.kt)("li",{parentName:"ul"},"how count: 124 "),(0,i.kt)("li",{parentName:"ul"},"are count: 210 "),(0,i.kt)("li",{parentName:"ul"},"you count: 100 ")),(0,i.kt)("p",null,"We will modify the following variables:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"region"),(0,i.kt)("li",{parentName:"ul"},"inputStreamName"),(0,i.kt)("li",{parentName:"ul"},"s3SinkPath, type the string ",(0,i.kt)("inlineCode",{parentName:"li"},"s3a://kda-word-count-tutorial-bucket-<unique>/wordCount")," where bucket-palceholder is the name of S3 bucket created in the previous step.")),(0,i.kt)("h2",{id:"compile-the-application-code"},"Compile the application code"),(0,i.kt)("p",null,"In the step, you will compile the application code to create a .jar file of the modified code. This .jar is a required input for Kinesis Data Analytics."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"cd S3Sink/\nmvn package -Dflink.version=1.11.1\n")),(0,i.kt)("p",null,"If Maven is not installed locally (confirm with ",(0,i.kt)("inlineCode",{parentName:"p"},"mnv -v"),"), you can use Colab to install and compile with Maven."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"apt-get install maven\napt-get install openjdk-8-jdk\napt-get install openjfx\nupdate-alternatives --config java\nmvn -v\n")),(0,i.kt)("p",null,"The previous command creates a .jar file of the application at target/aws-kinesis-analytics-java-apps-1.0.jar."),(0,i.kt)("h2",{id:"upload-the-apache-flink-streaming-java-code-to-an-amazon-s3-bucket"},"Upload the Apache Flink Streaming Java Code to an Amazon S3 bucket"),(0,i.kt)("p",null,"Upload the jar file to ",(0,i.kt)("inlineCode",{parentName:"p"},"kda-word-count-ka-app-code-location-<unique-name>")," S3 bucket."),(0,i.kt)("h2",{id:"create-configure-and-start-the-kinesis-data-analytics-application"},"Create, configure, and start the Kinesis Data Analytics application"),(0,i.kt)("p",null,"In that same Kinesis console, create an analytics streaming application ",(0,i.kt)("inlineCode",{parentName:"p"},"kda-word-count"),". After creation, configure the code path and select the S3 Jar file that you uploaded."),(0,i.kt)("p",null,"Now go to the Role and edit its policy to add the S3 buckets and the Kinesis Data Streams to the list of resources."),(0,i.kt)("p",null,"Go back to the Kinesis application and run without snapshot."),(0,i.kt)("p",null,"Open the Flink dashboard and check the running jobs."),(0,i.kt)("h2",{id:"project-structure"},"Project Structure"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"\u251c\u2500\u2500 [4.3K]  README.md\n\u251c\u2500\u2500 [ 155]  create-buckets.sh\n\u251c\u2500\u2500 [6.4K]  dependency-reduced-pom.xml\n\u251c\u2500\u2500 [  58]  download.sh\n\u251c\u2500\u2500 [5.1K]  pom.xml\n\u2514\u2500\u2500 [4.1K]  src\n    \u2514\u2500\u2500 [4.0K]  main\n        \u2514\u2500\u2500 [3.9K]  java\n            \u2514\u2500\u2500 [3.8K]  com\n                \u2514\u2500\u2500 [3.7K]  amazonaws\n                    \u2514\u2500\u2500 [3.6K]  services\n                        \u2514\u2500\u2500 [3.6K]  kinesisanalytics\n                            \u2514\u2500\u2500 [3.5K]  S3StreamingSinkJob.java\n\n  20K used in 7 directories, 6 files\n")))}d.isMDXComponent=!0}}]);