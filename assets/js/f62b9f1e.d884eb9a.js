"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[34509],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>d});var n=a(67294);function l(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){l(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,l=function(e,t){if(null==e)return{};var a,n,l={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(l[a]=e[a]);return l}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(l[a]=e[a])}return l}var p=n.createContext({}),c=function(e){var t=n.useContext(p),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=c(e.components);return n.createElement(p.Provider,{value:t},e.children)},s={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var a=e.components,l=e.mdxType,i=e.originalType,p=e.parentName,u=r(e,["components","mdxType","originalType","parentName"]),m=c(a),d=l,h=m["".concat(p,".").concat(d)]||m[d]||s[d]||i;return a?n.createElement(h,o(o({ref:t},u),{},{components:a})):n.createElement(h,o({ref:t},u))}));function d(e,t){var a=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var i=a.length,o=new Array(i);o[0]=m;var r={};for(var p in t)hasOwnProperty.call(t,p)&&(r[p]=t[p]);r.originalType=e,r.mdxType="string"==typeof e?e:l,o[1]=r;for(var c=2;c<i;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},11081:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>p,contentTitle:()=>o,default:()=>s,frontMatter:()=>i,metadata:()=>r,toc:()=>c});var n=a(87462),l=(a(67294),a(3905));const i={},o="Lab: GCP Dataflow Pipeline - A Simple Dataflow Pipeline (Python)",r={unversionedId:"processing/lab-gcp-dataflow-pipeline",id:"processing/lab-gcp-dataflow-pipeline",title:"Lab: GCP Dataflow Pipeline - A Simple Dataflow Pipeline (Python)",description:"Objective",source:"@site/docs/03-processing/lab-gcp-dataflow-pipeline.md",sourceDirName:"03-processing",slug:"/processing/lab-gcp-dataflow-pipeline",permalink:"/docs/processing/lab-gcp-dataflow-pipeline",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681732641,formattedLastUpdatedAt:"Apr 17, 2023",frontMatter:{},sidebar:"docs",previous:{title:"Lab: MapReduce in Beam using Python",permalink:"/docs/processing/lab-gcp-beam-mapreduce/"},next:{title:"Lab: GCP Dataflow Batch Pipeline",permalink:"/docs/processing/lab-gcp-dataflow-batch-pipeline"}},p={},c=[{value:"Objective",id:"objective",level:2},{value:"Task 1. Ensure that the Dataflow API is successfully enabled",id:"task-1-ensure-that-the-dataflow-api-is-successfully-enabled",level:2},{value:"Task 2. Preparation",id:"task-2-preparation",level:2},{value:"Open the SSH terminal and connect to the training VM",id:"open-the-ssh-terminal-and-connect-to-the-training-vm",level:3},{value:"Download code repository",id:"download-code-repository",level:3},{value:"Create a Cloud Storage bucket",id:"create-a-cloud-storage-bucket",level:3},{value:"Task 3. Pipeline filtering",id:"task-3-pipeline-filtering",level:2},{value:"Task 4. Execute the pipeline locally",id:"task-4-execute-the-pipeline-locally",level:2},{value:"Task 5. Execute the pipeline on the cloud",id:"task-5-execute-the-pipeline-on-the-cloud",level:3}],u={toc:c};function s(e){let{components:t,...a}=e;return(0,l.kt)("wrapper",(0,n.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"lab-gcp-dataflow-pipeline---a-simple-dataflow-pipeline-python"},"Lab: GCP Dataflow Pipeline - A Simple Dataflow Pipeline (Python)"),(0,l.kt)("h2",{id:"objective"},"Objective"),(0,l.kt)("p",null,"In this lab, you will open a Dataflow project, use pipeline filtering, and execute the pipeline locally and on the cloud."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Open Dataflow project"),(0,l.kt)("li",{parentName:"ul"},"Pipeline filtering"),(0,l.kt)("li",{parentName:"ul"},"Execute the pipeline locally and on the cloud")),(0,l.kt)("p",null,"In this lab, you learn how to write a simple Dataflow pipeline and run it both locally and on the cloud."),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Setup a Python Dataflow project using Apache Beam"),(0,l.kt)("li",{parentName:"ul"},"Write a simple pipeline in Python"),(0,l.kt)("li",{parentName:"ul"},"Execute the query on the local machine"),(0,l.kt)("li",{parentName:"ul"},"Execute the query on the cloud")),(0,l.kt)("h2",{id:"task-1-ensure-that-the-dataflow-api-is-successfully-enabled"},"Task 1. Ensure that the Dataflow API is successfully enabled"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Execute the following block of code in the Cloud Shell:")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"gcloud services disable dataflow.googleapis.com --force\ngcloud services enable dataflow.googleapis.com\n")),(0,l.kt)("h2",{id:"task-2-preparation"},"Task 2. Preparation"),(0,l.kt)("h3",{id:"open-the-ssh-terminal-and-connect-to-the-training-vm"},"Open the SSH terminal and connect to the training VM"),(0,l.kt)("p",null,"You will be running all code from a curated training VM."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"In the console, on the\xa0Navigation menu, click\xa0Compute Engine\xa0>\xa0VM instances."),(0,l.kt)("li",{parentName:"ol"},"Locate the line with the instance called\xa0training-vm."),(0,l.kt)("li",{parentName:"ol"},"On the far right, under\xa0Connect, click on\xa0SSH\xa0to open a terminal window."),(0,l.kt)("li",{parentName:"ol"},"In this lab, you will enter CLI commands on the\xa0training-vm.")),(0,l.kt)("h3",{id:"download-code-repository"},"Download code repository"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Download a code repository to use in this lab. In the\xa0training-vm\xa0SSH terminal enter the following:")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n")),(0,l.kt)("h3",{id:"create-a-cloud-storage-bucket"},"Create a Cloud Storage bucket"),(0,l.kt)("p",null,"Follow these instructions to create a bucket."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"In the console, on the\xa0Navigation menu, click\xa0Home."),(0,l.kt)("li",{parentName:"ol"},"Select and copy\xa0the Project ID."),(0,l.kt)("li",{parentName:"ol"},"In the console, on the\xa0Navigation menu, click\xa0Cloud Storage\xa0>\xa0Browser."),(0,l.kt)("li",{parentName:"ol"},"Click\xa0Create Bucket."),(0,l.kt)("li",{parentName:"ol"},"Specify the following, and leave the remaining settings as their defaults:\n| Property | Value (type value or select option as specified) |\n| -------- | ------------------------------------------------ |\n| Name | ",(0,l.kt)("inlineCode",{parentName:"li"},"<your unique bucket name (Project ID)>")," |\n| Location type | ",(0,l.kt)("inlineCode",{parentName:"li"},"Multi-Region")," |\n| Location | ",(0,l.kt)("inlineCode",{parentName:"li"},"<Your location>")," |"),(0,l.kt)("li",{parentName:"ol"},"Click\xa0Create."),(0,l.kt)("li",{parentName:"ol"},"Record the name of your bucket to use in subsequent tasks."),(0,l.kt)("li",{parentName:"ol"},'In the\xa0training-vm\xa0SSH terminal enter the following to create an environment variable named "BUCKET" and verify that it exists with the echo command:',(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},'BUCKET="<your unique bucket name (Project ID)>"\necho $BUCKET\n')))),(0,l.kt)("h2",{id:"task-3-pipeline-filtering"},"Task 3. Pipeline filtering"),(0,l.kt)("p",null,"The goal of this lab is to become familiar with the structure of a Dataflow project and learn how to execute a Dataflow pipeline."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Return to the\xa0training-vm\xa0SSH terminal and navigate to the directory\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"/training-data-analyst/courses/data_analysis/lab2/python"),"\xa0and view the file\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"grep.py"),"."),(0,l.kt)("li",{parentName:"ol"},"View the file with Nano.\xa0Do not make any changes to the code:",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"cd ~/training-data-analyst/courses/data_analysis/lab2/python\nnano grep.py\n"))),(0,l.kt)("li",{parentName:"ol"},"Press CTRL+X to exit Nano.")),(0,l.kt)("p",null,"Can you answer these questions about the file\xa0",(0,l.kt)("inlineCode",{parentName:"p"},"grep.py"),"?"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"What files are being read?"),(0,l.kt)("li",{parentName:"ul"},"What is the search term?"),(0,l.kt)("li",{parentName:"ul"},"Where does the output go?")),(0,l.kt)("p",null,"There are three transforms in the pipeline:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"What does the transform do?"),(0,l.kt)("li",{parentName:"ul"},"What does the second transform do?"),(0,l.kt)("li",{parentName:"ul"},"Where does its input come from?"),(0,l.kt)("li",{parentName:"ul"},"What does it do with this input?"),(0,l.kt)("li",{parentName:"ul"},"What does it write to its output?"),(0,l.kt)("li",{parentName:"ul"},"Where does the output go?"),(0,l.kt)("li",{parentName:"ul"},"What does the third transform do?")),(0,l.kt)("h2",{id:"task-4-execute-the-pipeline-locally"},"Task 4. Execute the pipeline locally"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"In the\xa0training-vm\xa0SSH terminal, locally execute\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"grep.py"),":",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"python3 grep.py\n"))),(0,l.kt)("li",{parentName:"ol"},"The output file will be\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"output.txt"),". If the output is large enough, it will be sharded into separate parts with names like:\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"output-00000-of-00001"),"."),(0,l.kt)("li",{parentName:"ol"},"Locate the correct file by examining the file's time:",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"ls -al /tmp\n"))),(0,l.kt)("li",{parentName:"ol"},"Examine the output file(s)."),(0,l.kt)("li",{parentName:"ol"},'You can replace "-*" below with the appropriate suffix:',(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"cat /tmp/output-*\n")))),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003251-70c4c151-62ec-436e-ba71-8f7edc283da4.png",alt:null})),(0,l.kt)("p",null,"Does the output seem logical?"),(0,l.kt)("h3",{id:"task-5-execute-the-pipeline-on-the-cloud"},"Task 5. Execute the pipeline on the cloud"),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Copy some Java files to the cloud. In the\xa0training-vm\xa0SSH terminal, enter the following command:",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"gsutil cp ../javahelp/src/main/java/com/google/cloud/training/dataanalyst/javahelp/*.java gs://$BUCKET/javahelp\n"))),(0,l.kt)("li",{parentName:"ol"},"Using Nano, edit the Dataflow pipeline in\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"grepc.py"),":",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"nano grepc.py\n"))),(0,l.kt)("li",{parentName:"ol"},"Replace PROJECT and BUCKET with your Project ID and Bucket name."),(0,l.kt)("li",{parentName:"ol"},"Save the file and close Nano by pressing the CTRL+X key, then type Y, and press Enter."),(0,l.kt)("li",{parentName:"ol"},"Submit the Dataflow job to the cloud:",(0,l.kt)("pre",{parentName:"li"},(0,l.kt)("code",{parentName:"pre"},"python3 grepc.py\n"))),(0,l.kt)("li",{parentName:"ol"},"Note:\xa0Ignore the message:\xa0",(0,l.kt)("strong",{parentName:"li"},"WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter."),"\xa0Your Dataflow job will start successfully.")),(0,l.kt)("p",null,"Because this is such a small job, running on the cloud will take significantly longer than running it locally (on the order of 7-10 minutes)."),(0,l.kt)("ol",null,(0,l.kt)("li",{parentName:"ol"},"Return to the browser tab for the console."),(0,l.kt)("li",{parentName:"ol"},"On the\xa0Navigation menu, click\xa0Dataflow\xa0and click on your job to monitor progress."),(0,l.kt)("li",{parentName:"ol"},"Wait for the\xa0Job status\xa0to be\xa0Succeeded."),(0,l.kt)("li",{parentName:"ol"},"Examine the output in the Cloud Storage bucket."),(0,l.kt)("li",{parentName:"ol"},"On the\xa0Navigation menu, click\xa0Cloud Storage > Browser\xa0and click on your bucket."),(0,l.kt)("li",{parentName:"ol"},"Click the\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"javahelp"),"\xa0directory. This job generates the file\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"output.txt"),". If the file is large enough, it will be sharded into multiple parts with names like:\xa0",(0,l.kt)("inlineCode",{parentName:"li"},"output-0000x-of-000y"),". You can identify the most recent file by name or by the\xa0Last modified\xa0field."),(0,l.kt)("li",{parentName:"ol"},"Click on the file to view it.")),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003230-dfe6d1bb-8c8b-4027-a1aa-8d196953a3e8.png",alt:null})),(0,l.kt)("p",null,(0,l.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/214003210-11693a50-df04-4ea4-b5cb-4313491016b3.png",alt:null})),(0,l.kt)("p",null,"Alternatively, you can download the file via the\xa0training-vm\xa0SSH terminal and view it:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"gsutil cp gs://$BUCKET/javahelp/output* .\ncat output*\n")),(0,l.kt)("p",null,"Congratulations!"))}s.isMDXComponent=!0}}]);