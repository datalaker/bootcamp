"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[35433],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>m});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},c=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(a),m=r,h=p["".concat(l,".").concat(m)]||p[m]||u[m]||i;return a?n.createElement(h,o(o({ref:t},c),{},{components:a})):n.createElement(h,o({ref:t},c))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var d=2;d<i;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},65350:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var n=a(87462),r=(a(67294),a(3905));const i={},o="Ray",s={unversionedId:"processing/ray",id:"processing/ray",title:"Ray",description:"What is Ray?",source:"@site/docs/03-processing/ray.md",sourceDirName:"03-processing",slug:"/processing/ray",permalink:"/docs/processing/ray",draft:!1,tags:[],version:"current",lastUpdatedBy:"sparsh",lastUpdatedAt:1681047270,formattedLastUpdatedAt:"Apr 9, 2023",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Building an event-driven IKEA app with Kafka",permalink:"/docs/processing/project-kafka-ikea/"},next:{title:"Snowpark",permalink:"/docs/processing/snowpark"}},l={},d=[{value:"What is Ray?",id:"what-is-ray",level:2},{value:"Core, Libraries and Ecosystem",id:"core-libraries-and-ecosystem",level:2},{value:"Ray AIR and the Data Science Workflow",id:"ray-air-and-the-data-science-workflow",level:2},{value:"Data Processing with Ray Datasets",id:"data-processing-with-ray-datasets",level:3},{value:"Reinforcement learning with Ray RLlib",id:"reinforcement-learning-with-ray-rllib",level:3},{value:"Distributed training with Ray Train",id:"distributed-training-with-ray-train",level:3},{value:"Hyperparameter tuning with Ray Tune",id:"hyperparameter-tuning-with-ray-tune",level:3},{value:"Model Serving with Ray Serve",id:"model-serving-with-ray-serve",level:3},{value:"Ray Core",id:"ray-core",level:2},{value:"Ray Core Major API methods",id:"ray-core-major-api-methods",level:3},{value:"Ray Core API System Components",id:"ray-core-api-system-components",level:3}],c={toc:d};function u(e){let{components:t,...a}=e;return(0,r.kt)("wrapper",(0,n.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"ray"},"Ray"),(0,r.kt)("h2",{id:"what-is-ray"},"What is Ray?"),(0,r.kt)("p",null,"Ray is a great computing framework for the Python data science community because it is flexible and distributed, making it easy to use and understand. It allows you to efficiently parallelize Python programs on your own computer and run them on a cluster without much modification. Additionally, its high-level libraries are easy to set up and can be used together smoothly, and some of them, such as the reinforcement learning library, have a promising future as standalone projects. Even though its core is written in C++, Ray has always been focused on Python and integrates well with many important data science tools. It also has a expanding ecosystem."),(0,r.kt)("p",null,"Ray is not the first framework for distributed Python, nor will it be the last, but it stands out for its ability to handle custom machine learning tasks with ease. Its various modules work well together, allowing for the flexible execution of complex workloads using familiar Python tools. This book aims to teach how to use Ray to effectively utilize distributed Python for machine learning purposes."),(0,r.kt)("p",null,'Programming distributed systems can be challenging because it requires specific skills and experience. While these systems are designed to be efficient and allow users to focus on their tasks, they often have "leaky abstractions" that can make it difficult to get clusters of computers to work as desired. In addition, many software systems require more resources than a single server can provide, and modern systems need to be able to handle failures and offer high availability. This means that applications may need to run on multiple machines or even in different data centers in order to function reliably.'),(0,r.kt)("p",null,"Even if you are not very familiar with machine learning (ML) or artificial intelligence (AI), you have probably heard about recent advances in these fields. Some examples of these advances include Deepmind's Alpha-Fold, which is a system for solving the protein folding problem, and OpenAI's Codex, which helps software developers with the tedious parts of their job. It is commonly known that ML systems require a lot of data to be trained and that ML models tend to become larger. OpenAI has demonstrated that the amount of computing power needed to train AI models has been increasing exponentially, as shown in their paper \"AI and Compute.\" In their study, the operations needed for AI systems were measured in petaflops (thousands of trillion operations per second) and have doubled every 3.4 months since 2012."),(0,r.kt)("p",null,"While Moore's Law suggests that computer transistors will double every two years, the use of distributed computing in machine learning can significantly increase the speed at which tasks are completed. While distributed computing may be seen as challenging, it would be beneficial to develop abstractions that allow for code to run on clusters without constantly considering individual machines and their interactions. By focusing specifically on AI workloads, it may be possible to make distributed computing more accessible and efficient."),(0,r.kt)("h2",{id:"core-libraries-and-ecosystem"},"Core, Libraries and Ecosystem"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/226094436-823ccc97-832e-4069-b232-52bb61ca3930.png",alt:"1"})),(0,r.kt)("h2",{id:"ray-air-and-the-data-science-workflow"},"Ray AIR and the Data Science Workflow"),(0,r.kt)("p",null,"Ray has dedicated libraries for each of the four ML-specific steps. Specifically, you can take care of your data processing needs with ",(0,r.kt)("em",{parentName:"p"},"Ray Datasets"),", run distributed model training with ",(0,r.kt)("em",{parentName:"p"},"Ray Train"),", run your reinforcement learning workloads with ",(0,r.kt)("em",{parentName:"p"},"Ray RLlib"),", tune your hyperparameters efficiently with ",(0,r.kt)("em",{parentName:"p"},"Ray Tune"),", and serve your models with ",(0,r.kt)("em",{parentName:"p"},"Ray Serve"),". And the way Ray is built, all these libraries are ",(0,r.kt)("em",{parentName:"p"},"distributed by design"),"."),(0,r.kt)("p",null,"What\u2019s more is that all of these steps are part of a process and are rarely tackled in isolation. Not only do you want all the libraries involved to seamlessly interoperate, it can also be a decisive advantage if you can work with a consistent API throughout the whole data science process. This is exactly what Ray AIR was built for: having a common runtime and API for your experiments and the ability to scale your workloads when you\u2019re ready."),(0,r.kt)("h3",{id:"data-processing-with-ray-datasets"},"Data Processing with Ray Datasets"),(0,r.kt)("p",null,"The first high-level library of Ray we\u2019ll talk about is Ray Datasets. This library contains a data structure aptly called Dataset, a multitude of connectors for loading data from various formats and systems, an API for transforming such datasets, a way to build data processing pipelines with them, and many integrations with other data processing frameworks. The Dataset abstraction builds on the powerful Arrow framework."),(0,r.kt)("h3",{id:"reinforcement-learning-with-ray-rllib"},"Reinforcement learning with Ray RLlib"),(0,r.kt)("p",null,"Let\u2019s start with ",(0,r.kt)("em",{parentName:"p"},"Ray RLlib")," for reinforcement learning (RL). This library is powered by the modern ML frameworks TensorFlow and PyTorch, and you can choose which one to use. Both frameworks seem to converge more and more conceptually, so you can pick the one you like most without losing much in the process."),(0,r.kt)("h3",{id:"distributed-training-with-ray-train"},"Distributed training with Ray Train"),(0,r.kt)("p",null,"Ray RLlib is dedicated to reinforcement learning, but what do you do if you need to train models for other types of machine learning, like supervised learning? You can use another Ray library for distributed training in this case: ",(0,r.kt)("em",{parentName:"p"},"Ray Train"),"."),(0,r.kt)("h3",{id:"hyperparameter-tuning-with-ray-tune"},"Hyperparameter tuning with Ray Tune"),(0,r.kt)("p",null,"Naming things is hard, but ",(0,r.kt)("em",{parentName:"p"},"Ray Tune"),", which you can use to tune all sorts of parameters, hits the spot. It was built specifically to find good hyperparameters for machine learning models. The typical setup is as follows:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"You want to run an extremely computationally expensive training function. In ML, it\u2019s not uncommon to run training procedures that take days, if not weeks, but let\u2019s say you\u2019re dealing with just a couple of minutes."),(0,r.kt)("li",{parentName:"ul"},"As a result of training, you compute a so-called objective function. Usually you want to either maximize your gains or minimize your losses in terms of performance of your experiment."),(0,r.kt)("li",{parentName:"ul"},"The tricky bit is that your training function might depend on certain parameters, called hyperparameters, that influence the value of your objective function."),(0,r.kt)("li",{parentName:"ul"},"You may have a hunch what individual hyperparameters should be, but tuning them all can be difficult. Even if you can restrict these parameters to a sensible range, it\u2019s usually prohibitive to test a wide range of combinations. Your training function is simply too expensive.\n")),(0,r.kt)("p",null,"What can you do to efficiently sample hyperparameters and get \u201cgood enough\u201d results on your objective? The field concerned with solving this problem is called ",(0,r.kt)("em",{parentName:"p"},"hyperparameter optimization")," (HPO), and Ray Tune has an enormous suite of algorithms for tackling it."),(0,r.kt)("h3",{id:"model-serving-with-ray-serve"},"Model Serving with Ray Serve"),(0,r.kt)("p",null,"The last of Ray\u2019s high-level libraries we\u2019ll discuss specializes in model serving and is simply called Ray Serve."),(0,r.kt)("h2",{id:"ray-core"},"Ray Core"),(0,r.kt)("h3",{id:"ray-core-major-api-methods"},"Ray Core Major API methods"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"API call"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"ray.init()")),(0,r.kt)("td",{parentName:"tr",align:null},"Initializes your Ray Cluster. Pass in an\xa0",(0,r.kt)("inlineCode",{parentName:"td"},"address"),"\xa0to connect to an existing cluster.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"@ray.remote")),(0,r.kt)("td",{parentName:"tr",align:null},"Turns functions into tasks and classes into actors.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"ray.put()")),(0,r.kt)("td",{parentName:"tr",align:null},"Puts values into Ray\u2019s object store.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"ray.get()")),(0,r.kt)("td",{parentName:"tr",align:null},"Gets values from the object store. Returns the values you\u2019ve\xa0",(0,r.kt)("inlineCode",{parentName:"td"},"put"),"\xa0there or that were computed by a task or actor.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},".remote()")),(0,r.kt)("td",{parentName:"tr",align:null},"Runs actor methods or tasks on your Ray Cluster and is used to instantiate actors.")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"ray.wait()")),(0,r.kt)("td",{parentName:"tr",align:null},"Returns two lists of object references, one with finished tasks we\u2019re waiting for and one with unfinished tasks.")))),(0,r.kt)("h3",{id:"ray-core-api-system-components"},"Ray Core API System Components"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://user-images.githubusercontent.com/62965911/226097632-1ab3c123-7c91-470e-a7fe-ea89e4aca0a7.png",alt:"3"})))}u.isMDXComponent=!0}}]);