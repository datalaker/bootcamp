{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create the datasets using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "datasets_name = ['raw_bikesharing','dwh_bikesharing','dm_bikesharing']\n",
    "location = 'US'\n",
    "\n",
    "def create_bigquery_dataset(dataset_name):\n",
    "    \"\"\"Create bigquery dataset. Check first if the dataset exists\n",
    "        Args:\n",
    "            dataset_name: String\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_id = \"{}.{}\".format(client.project, dataset_name)\n",
    "    try:\n",
    "        client.get_dataset(dataset_id)\n",
    "        print(\"Dataset {} already exists\".format(dataset_id))\n",
    "    except NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_id)\n",
    "        dataset.location = location\n",
    "        dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "        print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "\n",
    "for name in datasets_name:\n",
    "    create_bigquery_dataset(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Initial loading of the trips table into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# TODO : Change to your project id\n",
    "PROJECT_ID = \"sparsh-data-eng-on-gcp\"\n",
    "GCS_URI = \"gs://{}-data-bucket/data/trips/20180101/*.json\".format(\n",
    "    project_id)\n",
    "# This uri for load data from 2018-01-02\n",
    "#GCS_URI = \"gs://{}-data-bucket/data/trips/20180102/*.json\".format(project_id)\n",
    "TABLE_ID = \"{}.raw_bikesharing.trips\".format(PROJECT_ID)\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "\n",
    "def load_gcs_to_bigquery_event_data(GCS_URI, TABLE_ID, table_schema):\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=table_schema,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "        write_disposition='WRITE_APPEND'\n",
    "    )\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        GCS_URI, TABLE_ID, job_config=job_config\n",
    "    )\n",
    "\n",
    "    load_job.result()\n",
    "    table = client.get_table(TABLE_ID)\n",
    "\n",
    "    print(\"Loaded {} rows to table {}\".format(table.num_rows, TABLE_ID))\n",
    "\n",
    "\n",
    "bigquery_table_schema = [\n",
    "    bigquery.SchemaField(\"trip_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"duration_sec\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"start_date\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"start_station_name\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"start_station_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"end_date\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"end_station_name\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"end_station_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"member_gender\", \"STRING\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gcs_to_bigquery_event_data(GCS_URI, TABLE_ID, bigquery_table_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code will load trips data from gcs to BigQuery. There are a few things that you need to pay attention to in the code.\n",
    "\n",
    "The GCS file path contains date information, for example, **20180101**. We will use the folder name in our **gcs** file path like this:\n",
    "\n",
    "```\n",
    "gcs_uri = \"gs://{}-data-bucket/data/trips/20180101/*.json\".format(project_id)\n",
    "```\n",
    "\n",
    "The data stored in **NEWLINE DELIMITED JSON** is compressed in **gzip** files. The BigQuery load job config accepts the **NEWLINE_DELIMITED_JSON** file format, and not standard JSON. In case you have standard JSON, you need to transform it first to the correct JSON format. In the code, we need to define the format like this:\n",
    "\n",
    "```\n",
    "source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "```\n",
    "\n",
    "The write disposition is **WRITE_APPEND**. This won't matter during the initial load, but is an important configuration for handling new data. We will revisit this later in the next steps:\n",
    "\n",
    "```\n",
    "write_disposition = 'WRITE_APPEND'\n",
    "```\n",
    "\n",
    "Lastly, you will need to change the **project_id** variable to that of your **project_id** variable, since you want to load data from your own GCS bucket. See the following line:\n",
    "\n",
    "```\n",
    "project_id = \"sparsh-data-eng-on-gcp\"\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Initial loading of the regions table into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# TODO : Change to your project id\n",
    "PROJECT_ID = \"sparsh-data-eng-on-gcp\"\n",
    "PUBLIC_TABLE_ID = \"bigquery-public-data.san_francisco_bikeshare.bikeshare_regions\"\n",
    "TARGET_TABLE_ID = \"{}.raw_bikesharing.regions\".format(PROJECT_ID)\n",
    "\n",
    "\n",
    "def load_data_from_bigquery_public(PUBLIC_TABLE_ID, TARGET_TABLE_ID):\n",
    "    client = bigquery.Client()\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        destination=TARGET_TABLE_ID,\n",
    "        write_disposition='WRITE_TRUNCATE')\n",
    "\n",
    "    sql = \"SELECT * FROM `{}`;\".format(PUBLIC_TABLE_ID)\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "\n",
    "    try:\n",
    "        query_job.result()\n",
    "        print(\"Query success\")\n",
    "    except Exception as exception:\n",
    "        print(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_from_bigquery_public(PUBLIC_TABLE_ID, TARGET_TABLE_ID)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3a: Handle the daily batch data loading for the trips table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "# TODO : Change to your project id\n",
    "PROJECT_ID = \"sparsh-data-eng-on-gcp\"\n",
    "GCS_URI = \"gs://{}-data-bucket/data/trips/20180102/*.json\".format(\n",
    "    project_id)\n",
    "TABLE_ID = \"{}.raw_bikesharing.trips\".format(PROJECT_ID)\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "\n",
    "def load_gcs_to_bigquery_event_data(GCS_URI, TABLE_ID, table_schema):\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=table_schema,\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "        write_disposition='WRITE_APPEND'\n",
    "    )\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        GCS_URI, TABLE_ID, job_config=job_config\n",
    "    )\n",
    "\n",
    "    load_job.result()\n",
    "    table = client.get_table(TABLE_ID)\n",
    "\n",
    "    print(\"Loaded {} rows to table {}\".format(table.num_rows, TABLE_ID))\n",
    "\n",
    "\n",
    "bigquery_table_schema = [\n",
    "    bigquery.SchemaField(\"trip_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"duration_sec\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"start_date\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"start_station_name\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"start_station_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"end_date\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"end_station_name\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"end_station_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"member_gender\", \"STRING\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gcs_to_bigquery_event_data(GCS_URI, TABLE_ID, bigquery_table_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether we have data from both 2018-01-01 and 2018-01-02 by using this SQL query on the BigQuery console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT distinct(date(start_date))\n",
    "FROM `[your project id].raw_bikesharing.trips`;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we want to make sure that no records have been duplicated by using this SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT count(*) cnt_trip_id, trip_id \n",
    "FROM `[your project id].raw_bikesharing.trips`\n",
    "GROUP BY trip_id \n",
    "HAVING cnt_trip_id > 1;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3b: Handle the daily batch data loading for the stations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# TODO : Change to your project id\n",
    "PROJECT_ID = \"sparsh-data-eng-on-gcp\"\n",
    "TABLE_ID = \"{}.raw_bikesharing.stations\".format(PROJECT_ID)\n",
    "GCS_URI = \"gs://{}-data-bucket/mysql_export/stations/20180102/stations.csv\".format(PROJECT_ID)\n",
    "\n",
    "\n",
    "def load_gcs_to_bigquery_snapshot_data(GCS_URI, TABLE_ID, table_schema):\n",
    "    client = bigquery.Client()\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=table_schema,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        write_disposition='WRITE_TRUNCATE'\n",
    "    )\n",
    "\n",
    "    load_job = client.load_table_from_uri(\n",
    "        GCS_URI, TABLE_ID, job_config=job_config\n",
    "    )\n",
    "    load_job.result()\n",
    "    table = client.get_table(TABLE_ID)\n",
    "\n",
    "    print(\"Loaded {} rows to table {}\".format(table.num_rows, TABLE_ID))\n",
    "\n",
    "\n",
    "bigquery_table_schema = [\n",
    "    bigquery.SchemaField(\"station_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"name\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"region_id\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"capacity\", \"INTEGER\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_gcs_to_bigquery_snapshot_data(GCS_URI, TABLE_ID, bigquery_table_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4a: Create Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# TODO : Change to your project id\n",
    "PROJECT_ID = \"sparsh-data-eng-on-gcp\"\n",
    "TARGET_TABLE_ID = \"{}.dwh_bikesharing.fact_trips_daily\".format(PROJECT_ID)\n",
    "\n",
    "\n",
    "def create_fact_table(PROJECT_ID, TARGET_TABLE_ID, load_date):\n",
    "    print(\"\\nLoad date:\", load_date)\n",
    "\n",
    "    client = bigquery.Client()\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        destination=TARGET_TABLE_ID,\n",
    "        write_disposition='WRITE_APPEND')\n",
    "\n",
    "    sql = \"\"\"SELECT DATE(start_date) as trip_date,\n",
    "          start_station_id,\n",
    "          COUNT(trip_id) as total_trips,\n",
    "          SUM(duration_sec) as sum_duration_sec,\n",
    "          AVG(duration_sec) as avg_duration_sec\n",
    "          FROM `{PROJECT_ID}.raw_bikesharing.trips` trips\n",
    "          JOIN `{load_date}.raw_bikesharing.stations` stations\n",
    "          ON trips.start_station_id = stations.station_id\n",
    "          WhERE DATE(start_date) = DATE('{}')\n",
    "          GROUP BY trip_date, start_station_id\n",
    "          ;\"\"\".format(PROJECT_ID=PROJECT_ID, load_date=load_date)\n",
    "\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "\n",
    "    try:\n",
    "        query_job.result()\n",
    "        print(\"Query success\")\n",
    "    except Exception as exception:\n",
    "        print(exception)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_fact_table(PROJECT_ID, TARGET_TABLE_ID, load_date='2018-01-01')\n",
    "\n",
    "# Run it again to load the next day's data:\n",
    "create_fact_table(PROJECT_ID, TARGET_TABLE_ID, load_date='2018-01-02')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4b: Create dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# TODO : Change to your project id\n",
    "PROJECT_ID = \"sparsh-data-eng-on-gcp\"\n",
    "TARGET_TABLE_ID = \"{}.dwh_bikesharing.dim_stations\".format(PROJECT_ID)\n",
    "\n",
    "\n",
    "def create_dim_table(PROJECT_ID, TARGET_TABLE_ID):\n",
    "    client = bigquery.Client()\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        destination=TARGET_TABLE_ID,\n",
    "        write_disposition='WRITE_TRUNCATE')\n",
    "\n",
    "    sql = \"\"\"SELECT station_id,\n",
    "          stations.name as station_name,\n",
    "          regions.name as region_name,\n",
    "          capacity\n",
    "          FROM `{PROJECT_ID}.raw_bikesharing.stations` stations\n",
    "          JOIN `{PROJECT_ID}.raw_bikesharing.regions` regions\n",
    "          ON stations.region_id = CAST(regions.region_id AS STRING)\n",
    "          ;\"\"\".format(PROJECT_ID=PROJECT_ID)\n",
    "\n",
    "    query_job = client.query(sql, job_config=job_config)\n",
    "\n",
    "    try:\n",
    "        query_job.result()\n",
    "        print(\"Query success\")\n",
    "    except Exception as exception:\n",
    "        print(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dim_table(PROJECT_ID, TARGET_TABLE_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
