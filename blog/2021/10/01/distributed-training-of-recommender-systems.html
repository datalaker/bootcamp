<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Distributed Training of Recommender Systems | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/blog/2021/10/01/distributed-training-of-recommender-systems"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" property="og:title" content="Distributed Training of Recommender Systems | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars."><meta data-rh="true" property="og:description" content="The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2021-10-01T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/sparsh-ai"><meta data-rh="true" property="article:tag" content="distributed,recsys"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/blog/2021/10/01/distributed-training-of-recommender-systems"><link data-rh="true" rel="alternate" href="https://www.recohut.com/blog/2021/10/01/distributed-training-of-recommender-systems" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/blog/2021/10/01/distributed-training-of-recommender-systems" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.099cbecb.css">
<link rel="preload" href="/assets/js/runtime~main.40c54d56.js" as="script">
<link rel="preload" href="/assets/js/main.dbd2f5fe.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/12/24/ab-mab-tests">A/B and Multi-Armed Bandit Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/12/13/sql-to-snowflake-schema-conversion">SQL Server to Snowflake Schema Conversion</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/12/12/the-complete-python-course-2022">The Complete Python Course (2022)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/11/16/opening-material">Opening our Bootcamp material</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2021/10/01/clinical-decision-making">Clinical Decision Making</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">Distributed Training of Recommender Systems</h1><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> Â· <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.</p><p>Recently, Facebook shared its approach to maintain its 12 trillion parameter recommender. Building these large systems is challenging because it requires huge computation and memory resources. And we will soon enter into 100 trillion range. And SMEs will not be left behind due to open-source environment of software architectures and the decreasing cost of hardware, especially on the cloud infrastructure.</p><p>As per one estimate, a model with 100 trillion parameters would require at least 200TB just to store the model, even at 16-bit floating-point accuracy. So we need architectures that can support efficient and distributed training of recommendation models.</p><p><strong><em>Memory-intensive vs Computation-intensive</em></strong>: The increasing parameter comes mostly from the embedding layer which maps each entrance of an ID type feature (such as an user ID and a session ID) into a fixed length low-dimensional embedding vector. Consider the billion scale of entrances for the ID type features in a production recommender system and the wide utilization of feature crosses, the embedding layer usually domains the parameter space, which makes this component extremely <strong>memory-intensive</strong>. On the other hand, these low-dimensional embedding vectors are concatenated with diversified Non-ID type features (e.g., image, audio, video, social network, etc.) to feed a group of increasingly sophisticated neural networks (e.g., convolution, LSTM, multi-head attention) for prediction(s). Furthermore, in practice, multiple objectives can also be combined and optimized simultaneously for multiple tasks. These mechanisms make the rest neural network increasingly <strong>computation-intensive</strong>.</p><p><img loading="lazy" alt="An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network." src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-76057748d7f785bcb03bc9fae4560fc3.png" width="881" height="562" class="img_ev3q"></p><p>An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network.</p><p><a href="https://github.com/alibaba/x-deeplearning" target="_blank" rel="noopener noreferrer">Alibaba&#x27;s XDL</a>, <a href="https://github.com/PaddlePaddle/PaddleRec" target="_blank" rel="noopener noreferrer">Baidu&#x27;s PaddleRec</a>, and <a href="https://github.com/persiaml/persia" target="_blank" rel="noopener noreferrer">Kwai&#x27;s Persia</a> are some open-source frameworks for this large-scale distributed training of recommender systems.</p><aside>ð ***Synchronous vs Asynchronous Algorithms***: Synchronous algorithms always use the up-to-date gradient to update the model to ensure the model accuracy. However, the overhead of communications for synchronous algorithms starts to become too expensive to scale out the training procedure, causing inefficiency in running time. While asynchronous algorithm have better hardware efficiency, it often leads to a âsignificantâ loss in model accuracy at this scaleâfor production recommender systems (e.g., Baiduâs search engine). Recall that even 0.1% drop of accuracy would lead to a noticeable loss in revenue.</aside><h3 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-server-framework">Parameter Server Framework<a class="hash-link" href="#parameter-server-framework" title="Direct link to heading">â</a></h3><p>Existing distributed systems for deep learning based recommender models are usually built on top of the parameter server (PS) framework, where one can add elastic distributed storage to hold the increasingly large amount of parameters of the embedding layer. On the other hand, the computation workload does not scale linearly with the increasing parameter scale of the embedding layerâin fact, with an efficient implementation, a lookup operation over a larger embedding table would introduce almost no additional computations.</p><p><img loading="lazy" alt="Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. [Source](https://arxiv.org/pdf/2111.05897v1.pdf)." src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-1-64afd6c4cb479b89e18f624461bb9641.png" width="1170" height="391" class="img_ev3q"></p><p>Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. <a href="https://arxiv.org/pdf/2111.05897v1.pdf" target="_blank" rel="noopener noreferrer">Source</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="persia">PERSIA<a class="hash-link" href="#persia" title="Direct link to heading">â</a></h3><p><strong>PERSIA</strong>Â (<strong>P</strong>arallel r<strong>E</strong>commendation t<strong>R</strong>ainingÂ <strong>S</strong>ystem with hybr<strong>I</strong>dÂ <strong>A</strong>cceleration) is a PyTorch-based system for training deep learning recommendation models on commodity hardware. It supports models containing more than 100 trillion parameters.</p><p>It uses a hybrid training algorithm to tackle the embedding layer and dense neural network modules differentlyâthe embedding layer is trained in an asynchronous fashion to improve the throughput of training samples, while the rest neural network is trained in a synchronous fashion to preserve the statistical efficiency.</p><p>It also uses a distributed system to manage the hybrid computation resources (CPUs and GPUs) to optimize the co-existence of asynchronicity and synchronicity in the training algorithm.</p><p><img loading="lazy" alt="Untitled" src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-2-f8f92456dbc99598ab43bdb238450ac0.png" width="588" height="509" class="img_ev3q"></p><p><img loading="lazy" alt="Untitled" src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-3-c4a1ad9cdcc4ab78213fff42a2bf2d18.png" width="590" height="428" class="img_ev3q"></p><p>Persia includes a data loader module, a embedding PS (Parameter Server) module, a group of embedding workers over CPU nodes, and a group of NN workers over GPU instances. Each module can be dynamically scaled for different model scales and desired training throughput:</p><ul><li>A data loader that fetches training data from distributed storages such as Hadoop, Kafka, etc;</li><li>A embedding parameter server (embedding PS for short) manages the storage and update of the parameters in the embedding layer $\mathrm{w}^{emb}$;</li><li>A group of embedding workers that runs Algorithm 1 for getting the embedding parameters from the embedding PS; aggregating embedding vectors (potentially) and putting embedding gradients back to embedding PS;</li><li>A group of NN workers that runs the forward-/backward- propagation of the neural network $\mathrm{NN_{w^{nn}}(Â·)}$.</li></ul><p><img loading="lazy" alt="The architecture of Persia." src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-4-717546b660d1b2fcaff856bf274d18e2.png" width="874" height="563" class="img_ev3q"></p><p>The architecture of Persia.</p><p>Logically, the training procedure is conducted by Persia in a data dispatching based paradigm as below:</p><ol><li>The data loader will dispatch the ID type feature $\mathrm{x^{ID}}$ to an embedding workerâthe embedding worker will generate an unique sample ID ð for this sample, buffer this sample ID with the ID type feature $\mathrm{x_\xi^{ID}}$ locally, and returns this ID ð back the data loader; the data loader will associate this sampleâs Non-ID type features and labels with this unique ID.</li><li>Next, the data loader will dispatch the Non-ID type feature and label(s) $\mathrm{(x<em>\xi^{NID},y</em>\xi)}$ to a NN worker.</li><li>Once a NN worker receives this incomplete training sample, it will issue a request to pull the ID type featuresâ $\mathrm{(x<em>\xi^{ID})}$ embedding $\mathrm{w</em>\xi^{emb}}$ from some embedding worker according to the sample ID ðâthis would trigger the forward propagation in Algorithm 1, where the embedding worker will use the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ to get the corresponding $\mathrm{w</em>\xi^{emb}}$ from the embedding PS.</li><li>Then the embedding worker performs some potential aggregation of original embedding vectors. When this computation finishes, the aggregated embedding vector $\mathrm{w_\xi^{emb}}$ will be transmitted to the NN worker that issues the pull request.</li><li>Once the NN worker gets a group of complete inputs for the dense module, it will create a mini-batch and conduct the training computation of the NN according to Algorithm 2. Note that the parameter of the NN always locates in the device RAM of the NN worker, where the NN workers synchronize the gradients by the AllReduce Paradigm.</li><li>When the iteration of Algorithm 2 is finished, the NN worker will send the gradients of the embedding ($\mathrm{F_\xi^{emb&#x27;}}$) back to the embedding worker (also along with the sample ID ð).</li><li>The embedding worker will query the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ according to the sample ID ð; compute gradients $\mathrm{F</em>\xi^{emb&#x27;}}$ of the embedding parameters and send the gradients to the embedding PS, so that the embedding PS can finally compute the updates according the embedding parameterâs gradients by its SGD optimizer and update the embedding parameters.</li></ol></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/distributed">distributed</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/recsys">recsys</a></li></ul></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/2021/10/01/detectron-2"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Detectron 2</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/2021/10/01/document-recommendation"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Document Recommendation</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#parameter-server-framework" class="table-of-contents__link toc-highlight">Parameter Server Framework</a></li><li><a href="#persia" class="table-of-contents__link toc-highlight">PERSIA</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.40c54d56.js"></script>
<script src="/assets/js/main.dbd2f5fe.js"></script>
</body>
</html>