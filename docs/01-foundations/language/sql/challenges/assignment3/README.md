# SQL Assignment

**Part 1: Deforestation Exploration**

SQL is most commonly used to manipulate and analyze data to inform decision making. In this assignment, you will act as a data analyst for an organization on a mission to reduce deforestation around the world and to raise awareness about this important environmental topic. First, you’ll clean any erroneous values in a table, join that table to another lookup table to bring in a new categorical and quantitative variable, and return a new view of all categories greater than a reference value. Then, you will create and execute SQL queries to perform calculations using variables from those disparate data sets to answer questions for stakeholders. Your analysis will help you better understand which countries and regions around the world seem to have forests that have been shrinking in size, and also which countries and regions have the most significant forest area. Lastly, you will compile your answers and summarize your analysis into a report that can be shared to a leadership team.

**Part 2: Reddit, A Social News Aggregator**

- Explain which cases to use particular SQL commands and apply query results to address business problems.
- Databases need to be structured properly to enable efficient and effective querying and analysis of data.
- Build normalized, consistent and performant relational data models.
- Use SQL Database Definition Language (DDL) to create the data schemas designed in Postgres and apply SQL Database Manipulation Language (DML) to migrate data from a denormalized schema to a normalized one.
- Understand the tradeoffs between relational databases and their non-relational counterparts and justify which one is best for different scenarios.

Many of today’s most popular web applications have supporting database structures that allow them to customize and aggregate information within seconds. In this assignment, you will build the supporting data structures for Udiddit, a social media news aggregator site. First, you’ll investigate the provided data model for potential errors such as lack of normalization, consistency rules, and proper indexing. Then, you will create a new, normalized database using DDL based on the denormalized one that is provided. Lastly, you will write DML queries to migrate the data from the denormalized schema to their normalized schema.