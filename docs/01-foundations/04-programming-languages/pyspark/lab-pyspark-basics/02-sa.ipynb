{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    functions as f,\n",
    "    Row,\n",
    "    SparkSession,\n",
    "    types as t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"spark_lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(anInt=1), Row(anInt=2), Row(anInt=3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions.explode(col)\n",
    "# Returns a new row for each element in the given array or map\n",
    "df = spark.createDataFrame([Row(a=1, intlist=[1, 2, 3], mapfield={\"a\" : \"b\"})])\n",
    "\n",
    "df.select(f.explode(df.intlist).alias(\"anInt\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word=['hello', 'world', 'and', 'pyspark'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions.split(str, pattern, limit = 1)\n",
    "# Splits str around matches of the given pattern.\n",
    "df = spark.createDataFrame([Row(word=\"hello world and pyspark\")])\n",
    "df.select(f.split(df.word, ' ').alias(\"word\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- temperature: float (nullable = true)\n",
      " |-- observed_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# types.StructField(name, dataType, nullable=True, metadata=None)\n",
    "table_schema = t.StructType([\n",
    "    t.StructField(\"country\", t.StringType(), True),\n",
    "    t.StructField(\"temperature\", t.FloatType(), True),\n",
    "    t.StructField(\"observed_date\", t.StringType(), True)])\n",
    "\n",
    "csv_file_path = \"data/raw/temp_with_date.csv\"\n",
    "df = spark.read.schema(table_schema).csv(csv_file_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|             country|min(temperature)|\n",
      "+--------------------+----------------+\n",
      "|                Chad|           -24.0|\n",
      "|            Anguilla|           -40.0|\n",
      "|            Paraguay|            30.0|\n",
      "|               Macao|           -34.0|\n",
      "|Heard Island and ...|           -39.0|\n",
      "|               Yemen|           -33.0|\n",
      "|             Senegal|           -21.0|\n",
      "|              Sweden|           -29.0|\n",
      "|             Tokelau|           -35.0|\n",
      "|            Kiribati|           -26.0|\n",
      "|French Southern T...|           -22.0|\n",
      "|   Republic of Korea|           -18.0|\n",
      "|              Guyana|           -28.0|\n",
      "|             Eritrea|           -40.0|\n",
      "|         Philippines|           -34.0|\n",
      "|              Jersey|           -21.0|\n",
      "|      Norfolk Island|           -28.0|\n",
      "|               Tonga|           -40.0|\n",
      "|           Singapore|           -25.0|\n",
      "|            Malaysia|           -21.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = df.select(\"country\", \"temperature\", \"observed_date\")\n",
    "min_temperature = data.groupBy(\"country\").min(\"temperature\")\n",
    "min_temperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|             country|        temperature|\n",
      "+--------------------+-------------------+\n",
      "|                Guam|              -13.0|\n",
      "|                Guam|              102.2|\n",
      "|              Serbia|              -31.0|\n",
      "|       French Guiana|               21.2|\n",
      "|Falkland Islands ...|              -40.0|\n",
      "|              Brazil|               59.0|\n",
      "|             Tunisia|-23.799999999999997|\n",
      "|            Portugal|               44.6|\n",
      "|                Iran| -7.600000000000001|\n",
      "|           Australia|               23.0|\n",
      "|              Gambia|               69.8|\n",
      "|               Italy|               87.8|\n",
      "|          Guadeloupe|              -38.2|\n",
      "|        South Africa|-11.200000000000003|\n",
      "|              Malawi|               21.2|\n",
      "|                Iran|               93.2|\n",
      "|      Norfolk Island|               23.0|\n",
      "|      Virgin Islands|               null|\n",
      "|Lao People's Demo...|               77.0|\n",
      "|   Republic of Korea|-0.3999999999999986|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# celsius to fahrenheit: (0°C × 9/5) + 32 \n",
    "f_temperature = data.withColumn(\n",
    "                    \"temperature\",\n",
    "                    (f.col(\"temperature\") * 9 / 5) + 32)\\\n",
    "                .select(\"country\", \"temperature\")\n",
    "f_temperature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+\n",
      "|    customer_name|cost|\n",
      "+-----------------+----+\n",
      "|     Damion Wolfe|1397|\n",
      "| Benedict Frazier| 998|\n",
      "|  Giuseppe Miller| 997|\n",
      "|    Garret Martin| 997|\n",
      "|Erminia Robertson| 997|\n",
      "|     Milan Gibson| 996|\n",
      "|     Rudy Wheeler| 994|\n",
      "|   Kathey Baldwin| 994|\n",
      "|   Williemae Bell| 992|\n",
      "|Gearldine Aguilar| 988|\n",
      "|      Jewel Parks| 987|\n",
      "|     Hyman Castro| 985|\n",
      "|    Noriko Medina| 984|\n",
      "|     Garfield Day| 982|\n",
      "|      Dacia Adams| 981|\n",
      "|     Taisha Henry| 980|\n",
      "|    Branda Valdez| 978|\n",
      "|     Fumiko Weber| 976|\n",
      "|Geraldo Alexander| 975|\n",
      "|      Walker Pope| 975|\n",
      "+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_schema = t.StructType([\n",
    "    t.StructField(\"customer_name\", t.StringType(), True),\n",
    "    t.StructField(\"product_id\", t.IntegerType(), True),\n",
    "    t.StructField(\"price\", t.IntegerType(), True)])\n",
    "\n",
    "csv_file_path = \"data/raw/product.csv\"\n",
    "df = spark.read.schema(table_schema).csv(csv_file_path)\n",
    "\n",
    "customer_spent = df.groupBy(\"customer_name\").agg(f.round(f.sum(\"price\"), 2).alias(\"cost\"))\n",
    "\n",
    "sorted_customer_spent = customer_spent.orderBy(f.col(\"cost\").desc())\n",
    "sorted_customer_spent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100: 217\n",
      "3801: 203\n",
      "2030: 200\n",
      "3021: 191\n",
      "9382: 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-----------------+\n",
      "|occupation_id|count|  occupation_name|\n",
      "+-------------+-----+-----------------+\n",
      "|         1100|  217|         engineer|\n",
      "|         3801|  203|          painter|\n",
      "|         2030|  200|        developer|\n",
      "|         3021|  191|chemistry teacher|\n",
      "|         9382|  189|           priest|\n",
      "+-------------+-----+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "table_schema = t.StructType([\n",
    "    t.StructField(\"interviewer_id\", t.StringType(), False),\n",
    "    t.StructField(\"occupation_id\", t.StringType(), False),\n",
    "    t.StructField(\"rating\", t.IntegerType(), False)])\n",
    "\n",
    "csv_file_path = \"data/raw/like.csv\"\n",
    "\n",
    "df = spark.read.schema(table_schema).csv(csv_file_path)\n",
    "\n",
    "interviewer_count = df.groupBy(\"occupation_id\").count().orderBy(f.desc(\"count\"))\n",
    "\n",
    "for d in interviewer_count.select(\"occupation_id\", f.col(\"count\").alias(\"cnt\")).collect():\n",
    "    print(f\"{d.occupation_id}: {d.cnt}\")\n",
    "    \n",
    "    \n",
    "# But, What if we want to know what occupation_id is?  \n",
    "# 1100: engineer\n",
    "# 2030: developer\n",
    "# 3801: painter\n",
    "# 3021: chemistry teacher\n",
    "# 9382: priest\n",
    "\n",
    "meta = {\n",
    "    \"1100\": \"engineer\",\n",
    "    \"2030\": \"developer\",\n",
    "    \"3801\": \"painter\",\n",
    "    \"3021\": \"chemistry teacher\",\n",
    "    \"9382\": \"priest\"\n",
    "}\n",
    "\n",
    "occupation_dict = spark.sparkContext.broadcast(meta)\n",
    "\n",
    "def get_occupation_name(occupation_id: str) -> str:\n",
    "    return occupation_dict.value[occupation_id]\n",
    "\n",
    "occupation_lookup_udf = f.udf(get_occupation_name)\n",
    "\n",
    "occupation_with_name = interviewer_count.withColumn(\"occupation_name\", occupation_lookup_udf(f.col(\"occupation_id\")))\n",
    "\n",
    "occupation_with_name.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                hero|          connection|\n",
      "+--------------------+--------------------+\n",
      "|             ABCISSA|ELSIE DEE,FURY, C...|\n",
      "|ABOMINATION/EMIL BLO|PO,LOCUST,WATTS,M...|\n",
      "|             ABSALOM|SHATTERSTAR II/GA...|\n",
      "|ABSORBING MAN | MUTA|VALKYRIE II | MUT...|\n",
      "|ABSORBING MAN/CARL C|SOMMERS, APRIL,HE...|\n",
      "|ADAMS, CONGRESSMAN H|SPIDER-MAN/PETER ...|\n",
      "| ADAMS, NICOLE NIKKI|JUSTICE II/VANCE ...|\n",
      "|    ADAMSON, REBECCA|KABALLA,GOLEM III...|\n",
      "|               ADRIA|DORMAMMU,ANCIENT ...|\n",
      "|   ADVENT/KYLE GROBE|JUSTICE II/VANCE ...|\n",
      "|AGAMEMNON II/ANDREI |BLACK WIDOW/NATASHA |\n",
      "|      AGAMEMNON III/|ASTER, LUCIAN,HOG...|\n",
      "|            AGAMOTTO|SATANNISH,DORMAMM...|\n",
      "|         AGENT AXIS/|HUMAN TORCH ANDRO...|\n",
      "|             AGGAMON|DR. STRANGE/STEPHEN |\n",
      "|              AGINAR|SIF,REJECT/RAN-SA...|\n",
      "|                AGON|MARISTA,BLACK BOL...|\n",
      "|     AGUIRRE, ISOBEL|TERMINUS,HUMAN TO...|\n",
      "|               AINET|STORM/ORORO MUNRO...|\n",
      "|    AKUTAGAWA, OSAMU|HUMAN TORCH/JOHNN...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"data/raw/hero-network.csv\"\n",
    "\n",
    "# read file\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# pyspark.sql.functions.collect_set(col) : Aggregate function: returns a set of objects with duplicate elements eliminated\n",
    "\n",
    "data = df.groupBy(\"hero1\").agg(f.collect_set(\"hero2\").alias(\"connection\")).withColumnRenamed(\"hero1\", \"hero\")\n",
    "\n",
    "# data.show()\n",
    "# pyspark.sql.functions.concat_ws(sep, *cols): Concatenates multiple input string columns together into a single string column, using the given separator.\n",
    "data = data.withColumn(\"connection\", f.concat_ws(\",\", f.col(\"connection\")))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DataFrame.coalesce(numPartitions): Returns a new DataFrame that has exactly numPartitions partitions.\n",
    "data.coalesce(1).write.option(\"header\", True).csv(\"data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                hero|          connection|\n",
      "+--------------------+--------------------+\n",
      "|             ABCISSA|ELSIE DEE,FURY, C...|\n",
      "|ABOMINATION/EMIL BLO|PO,LOCUST,WATTS,M...|\n",
      "|             ABSALOM|SHATTERSTAR II/GA...|\n",
      "|ABSORBING MAN | MUTA|VALKYRIE II | MUT...|\n",
      "|ABSORBING MAN/CARL C|SOMMERS, APRIL,HE...|\n",
      "|ADAMS, CONGRESSMAN H|SPIDER-MAN/PETER ...|\n",
      "| ADAMS, NICOLE NIKKI|JUSTICE II/VANCE ...|\n",
      "|    ADAMSON, REBECCA|KABALLA,GOLEM III...|\n",
      "|               ADRIA|DORMAMMU,ANCIENT ...|\n",
      "|   ADVENT/KYLE GROBE|JUSTICE II/VANCE ...|\n",
      "| AGAMEMNON II/ANDREI| BLACK WIDOW/NATASHA|\n",
      "|      AGAMEMNON III/|ASTER, LUCIAN,HOG...|\n",
      "|            AGAMOTTO|SATANNISH,DORMAMM...|\n",
      "|         AGENT AXIS/|HUMAN TORCH ANDRO...|\n",
      "|             AGGAMON| DR. STRANGE/STEPHEN|\n",
      "|              AGINAR|SIF,REJECT/RAN-SA...|\n",
      "|                AGON|MARISTA,BLACK BOL...|\n",
      "|     AGUIRRE, ISOBEL|TERMINUS,HUMAN TO...|\n",
      "|               AINET|STORM/ORORO MUNRO...|\n",
      "|    AKUTAGAWA, OSAMU|HUMAN TORCH/JOHNN...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the file\n",
    "csv_file_path = \"data/output\"\n",
    "df = spark.read\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .option(\"inferSchema\", \"true\")\\\n",
    "            .csv(csv_file_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|                hero|          connection|connection_size|\n",
      "+--------------------+--------------------+---------------+\n",
      "|     CAPTAIN AMERICA|URICH, DORIS,ARMA...|           1795|\n",
      "|SPIDER-MAN/PETER PAR|RED SHIFT,GAMELIN...|           1737|\n",
      "| IRON MAN/TONY STARK|RED SHIFT,SABRETO...|           1443|\n",
      "|     WOLVERINE/LOGAN|SABRETOOTH/VICTOR...|           1278|\n",
      "|THING/BENJAMIN J. GR|CHORD, ANDREW,CAT...|           1262|\n",
      "| SCARLET WITCH/WANDA|SABRETOOTH/VICTOR...|           1246|\n",
      "|HUMAN TORCH/JOHNNY S|CAT KING,BUZZ,MAK...|           1202|\n",
      "|MR. FANTASTIC/REED R|ARMADILLO/ANTONIO...|           1200|\n",
      "|THOR/DR. DONALD BLAK|PARKER, MAY | TIM...|           1183|\n",
      "| INVISIBLE WOMAN/SUE|CAPTAIN MARVEL II...|           1143|\n",
      "|BEAST/HENRY &HANK& P|AMERICAN EAGLE II...|           1140|\n",
      "|              VISION|PHOSPHORUS,AMERIC...|           1110|\n",
      "|                HAWK|AMERICAN EAGLE II...|           1086|\n",
      "|CYCLOPS/SCOTT SUMMER|SABRETOOTH/VICTOR...|           1004|\n",
      "|HULK/DR. ROBERT BRUC|SABRETOOTH/VICTOR...|            978|\n",
      "|ANT-MAN/DR. HENRY J.|DEVEREAUX, MICHEL...|            953|\n",
      "|STORM/ORORO MUNROE S|SABRETOOTH/VICTOR...|            952|\n",
      "| DR. STRANGE/STEPHEN|AMERICAN EAGLE II...|            946|\n",
      "|COLOSSUS II/PETER RA|SABRETOOTH/VICTOR...|            944|\n",
      "| PROFESSOR X/CHARLES|SABRETOOTH/VICTOR...|            940|\n",
      "+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark.sql.functions.size(col): Collection function: returns the length of the array or map stored in the column.\n",
    "df = df.withColumn(\n",
    "        \"connection_size\",\n",
    "        f.size(\n",
    "            f.split(f.col(\"connection\"), \",\")))\\\n",
    "        .orderBy(f.desc(\"connection_size\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
