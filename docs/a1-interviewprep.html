<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-a1-interviewprep/README">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Interview Preparation | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/a1-interviewprep"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Interview Preparation | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Whether you’re just getting into the data engineer job market or your interview is tomorrow, practice is an essential part of the interview preparation process for a data engineer. Data engineering interview questions assess your data engineering skills and domain expertise. They are based on a company’s tech stack and technology goals, and they test your ability to perform job functions. In an interview for any Engineering role, the interviewer wants to understand if you have good analytical skills, problem-solving ability, communication, work culture and ability to build technical solutions. Specific to Data Engineering, they also want to understand if you have the skills to handle large data and build scalable and robust systems."><meta data-rh="true" property="og:description" content="Whether you’re just getting into the data engineer job market or your interview is tomorrow, practice is an essential part of the interview preparation process for a data engineer. Data engineering interview questions assess your data engineering skills and domain expertise. They are based on a company’s tech stack and technology goals, and they test your ability to perform job functions. In an interview for any Engineering role, the interviewer wants to understand if you have good analytical skills, problem-solving ability, communication, work culture and ability to build technical solutions. Specific to Data Engineering, they also want to understand if you have the skills to handle large data and build scalable and robust systems."><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/a1-interviewprep"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/a1-interviewprep" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/a1-interviewprep" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.47c7b9d5.css">
<link rel="preload" href="/assets/js/runtime~main.251db5a0.js" as="script">
<link rel="preload" href="/assets/js/main.1462881d.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Interview Preparation</h1><p>Whether you’re just getting into the data engineer job market or your interview is tomorrow, practice is an essential part of the interview preparation process for a data engineer. Data engineering interview questions assess your data engineering skills and domain expertise. They are based on a company’s tech stack and technology goals, and they test your ability to perform job functions. In an interview for any Engineering role, the interviewer wants to understand if you have good analytical skills, problem-solving ability, communication, work culture and ability to build technical solutions. Specific to Data Engineering, they also want to understand if you have the skills to handle large data and build scalable and robust systems.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-typical-data-engineering-interview-process">The typical Data Engineering interview process<a class="hash-link" href="#the-typical-data-engineering-interview-process" title="Direct link to heading">​</a></h2><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215105782-4169e1cf-6ec6-4a6f-889b-0346be841ded.svg" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="phone-screen">Phone Screen<a class="hash-link" href="#phone-screen" title="Direct link to heading">​</a></h2><p>There are two types of phone screens: HR, which is generally all behavioral questions, and technical phone screens.</p><p>NOTE</p><blockquote><p>Behavioral questions assess soft skills (e.g., communication, leadership, adaptability), your skill level, and how you fit into the company’s data engineering team.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-1-hr">Round 1: HR<a class="hash-link" href="#round-1-hr" title="Direct link to heading">​</a></h3><p>The HR phone screen is usually 15–30 minutes and conducted by non-technical staff at the company, such as a recruiter. You’ll be asked soft questions such as Why do you want to be a Data Engineer? Where do you see yourself in 5 years? Why do you want to work at our company? And importantly, what salary are you expecting? These questions can seem boring or odd if you don’t know the real reason for them behind the scenes: HR wants to find the right fit for their team. They want a candidate who will be communicating well with their peers and managers and stay at the company for a long time because hiring and onboarding are expensive!</p><p>They are looking for clear communication, a pleasant person to work with, someone who is enthusiastic about the company and has done their research, ideally translating into a loyal employee willing to stay and be happy at their company.</p><p>To prepare:</p><ol><li>Write and practice a script for your background.</li><li>Do a deep dive into company values and tweak your answer accordingly.</li><li>Practice with your peers over the phone (we know it can be awkward).</li><li>Settle in a quiet place with a good Internet connection at least 10 minutes before the interview.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-2-technical">Round 2: Technical<a class="hash-link" href="#round-2-technical" title="Direct link to heading">​</a></h3><p>Just as the HR phone screen is a filter for basic communication ability, the technical phone screen is a filter for basic technical ability. On-site interviews are very costly in terms of time and team resources, so companies don’t want to spend hours on a candidate who can’t code well. An assessment of basic SWE knowledge and the ability to break down complicated ideas to smaller understandable pieces are the most essential reasons for technical phone screens.</p><p>Expect a 30–60 minute teleconference call answering basic DE concepts or classic SWE questions, usually from a member of the engineering team.</p><p>They are looking for people with basic knowledge in SWE and DE, problem-solving skills, and ability to communicate technical information.</p><p>Example questions include what are linked lists? How would you code them in your language of choice? Find all duplicates in a list. When would you use SQL vs. NoSQL databases?</p><p>Interviewers use easy technical questions designed to weed out candidates without the right experience. This question assesses your experience level, comfort with specific tools, and the depth of your domain expertise. Data management, SQL and Python questions are there. Data management questions include Data modeling, Data warehousing and Data pipelines. In SQL, you’ll need to know how to use window functions, aggregate functions, subqueries, joins, and sub-selects, as well as handle performance tuning and optimization. In Python, you’ll need to know how to manipulate data structures and use dictionaries, loops and lists, while showing a good understanding of strings, set operations, etc.</p><p>Technical questions can be primarily divided into these categories: SQL, Python, Data Modeling, Data Warehousing, Big Data &amp; Cloud, Data Pipelining, Project-related and Software Engineering.</p><p>To prepare:</p><ol><li>Read <a href="https://github.com/andkret/Cookbook" target="_blank" rel="noopener noreferrer">Data Engineering Cookbook</a> and answer at least 50 questions.</li><li>Practice random questions from the book with your peers.</li><li>Do 50 easy LeetCode problems.</li><li>Settle in a quiet place with good Internet connection at least 10 minutes before the interview.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="take-home-exam">Take-Home Exam<a class="hash-link" href="#take-home-exam" title="Direct link to heading">​</a></h2><p>Your resume says you have many years of experience, leading multiple projects and being a rock star. How do companies know if you’re really that good? In most cases, there is no access to your old company GitHub repository, and it takes time to read and understand personal GitHub projects — not to mention they won’t know for sure that you wrote the code. A take-home coding challenge is the easiest and fastest way to assess how production-ready your code is, how you account for edge cases and exception handling, and whether you can solve a given problem in an optimal way. There are two main types of exams:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="timed-hackerrank">Timed HackerRank<a class="hash-link" href="#timed-hackerrank" title="Direct link to heading">​</a></h3><p>Expect 1.5–2 hours exam with 3–5 easy-medium HackerRank questions
including SQL, regular expressions, algorithms, and data structures</p><p>They are looking for engineers who know efficient algorithms and data structures for solving standard computer science questions, take edge cases into account, and provide the solution quickly</p><p>To prepare:</p><ol><li>Solve at least 100 LeetCode/HackerRank problems</li><li>Practice with <a href="https://leetcode.com/contest/" target="_blank" rel="noopener noreferrer">Virtual Leetcode Contests</a> — all free past contest that you can take any time, and try to solve problems quickly and correctly on the first try</li><li>Block off a chunk of time where you’ll be in a comfortable environment where you usually do technical work and make sure you won’t be interrupted and have plenty of water and snacks (if needed).</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="coding-challenge">Coding challenge<a class="hash-link" href="#coding-challenge" title="Direct link to heading">​</a></h3><p>Expect 1–7 days to write code to answer 1–10 questions on 1–3 datasets, push it to your GitHub repository and submit the link.</p><p>They are looking for clean and modular code, good README with clear delivered ideas, unit tests, and exception handling.</p><p>Example question: Clean and analyze a dataset of employee salaries and locations. What is the distribution of salaries at different locations? Write a SQL query to do the equivalent task.</p><p>To prepare:</p><ol><li>Read and internalize the Google style guide.</li><li>Practice using the unittest library in Python or equivalent.</li><li>Read GitHub best practices.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="on-site-interview">On-Site Interview<a class="hash-link" href="#on-site-interview" title="Direct link to heading">​</a></h2><p>You should feel very accomplished if you get to the on-site interview, but the hardest part is yet to come! On-sites can be grueling affairs of interviewing with 4–10 people in 3–6 hours, especially if you’re not prepared. Knowing what to expect and doing realistic preparation beforehand go a long way toward reducing fear and nervousness.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-1-problem-solving---sql-and-programming">Round 1: Problem Solving - SQL and Programming<a class="hash-link" href="#round-1-problem-solving---sql-and-programming" title="Direct link to heading">​</a></h3><p>You will be given a problem to solve and you need to apply <strong>Think Out Loud</strong> strategy to solve it. Both Python and SQL could be asked. It can also be performed on a whiteboard instead of IDE. Expect 30–45 minutes interview with 1–2 medium-hard questions to solve on the fly on a whiteboard or on IDE, constantly communicating requirements and solutions with the interviewer.</p><p>The coding interview for data engineer roles is usually lighter on the algorithm side but heavier on the data side, and the interview questions are usually more practical. For instance, write a function to transform the input data and produce the desired output data. You will still be expected to use the most optimal data structures and algorithms possible and gracefully handle all the potential data issues. Since data engineers don’t just use the built-in libraries to process data in the real world, the coding interview might also require you to implement solutions using popular open-sourced libraries, such as Spark and pandas. You are generally allowed to look up documentation during the interview if needed. If the job requires proficiency in specific frameworks, be prepared to use those frameworks in your coding interviews.</p><p>A good data engineer should be capable of translating complicated business questions into SQL queries and data models with good performance. In order to write efficient queries that process as little data as possible, you need to understand how the query engine and optimizer work. For example, sometimes using CASE statements combined with aggregation functions can replace JOIN and UNION and process much less data.</p><p>Sometimes, DSA (Data Structures and Algorithms) based questions can also be asked to check your software engineering foundations.</p><p>To prepare:</p><ol><li>Solve 80–150 <a href="https://leetcode.com/" target="_blank" rel="noopener noreferrer">LeetCode</a> and <a href="https://www.hackerrank.com/" target="_blank" rel="noopener noreferrer">HackerRank</a> problems.</li><li>Get at least 20 practice sessions as an interviewee with peers or professionals.</li><li>Practice writing clean, readable code on a whiteboard.</li><li>For DSA, follow NeetCode <a href="https://neetcode.io/courses/dsa-for-beginners/0" target="_blank" rel="noopener noreferrer">basic</a> and <a href="https://neetcode.io/courses/advanced-algorithms/0" target="_blank" rel="noopener noreferrer">advanced</a> course.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-2-resume-discussion">Round 2: Resume Discussion<a class="hash-link" href="#round-2-resume-discussion" title="Direct link to heading">​</a></h3><p>You will be asked about the Python, SQL and Big Data projects you worked in. Questions will be asked mainly based on your resume. You need to apply <strong>Don’t be fake</strong> strategy to crack this round. You need to also prepare for the projects you add in resume because the questions around those projects will also be asked.</p><p>Your resume is not only the stepping stone to get noticed by recruiters and hiring managers, but also the most important list of projects that you should be ready to discuss in-depth with the interviewers in order to demonstrate your skills, including technical competency, problem-solving, teamwork, communication, and project management.</p><p>I strongly recommend practicing talking through your most significant data projects (to someone with an engineering background, if possible) and making sure to answer these questions in your story:</p><ul><li>What was the motivation for the project? (i.e. What data/business problem did your project try to solve?)</li><li>What teams did you collaborate with? How did you work with them?</li><li>If you were the project owner, how did you plan and drive it?</li><li>What are the technical trade-offs made in the system design? (i.e. Why did you use framework X instead of other alternatives?)</li><li>What were some of the technical statistics related to your project? (e.g. What is the throughput and latency of your data pipeline?)</li><li>What was the impact of the project? (e.g. How much revenue did it generate? How many people used your application?)</li><li>What were the challenges you had? How did you solve them?</li></ul><p>Numbers are important in telling a great project story. Instead of just saying “it processed a lot of data…”, look up some statistics of your project and include them on your resume. Numbers will showcase the scale, impact, and your deep understanding of the project. They also make your project more believable. (In fact, interviewers might find it suspicious if you can’t even tell how much data your applications can process.)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-3-system-design">Round 3: System Design<a class="hash-link" href="#round-3-system-design" title="Direct link to heading">​</a></h3><p>As a Data Engineer, on a day to day basis you are going to design entire systems from scratch or add small features to giant existing pipelines. Even you mention these skills on your resume, it’s crucial for companies to check your ability in a real-life. System design data engineer interview questions are often the most challenging part of technical interviews. Expect 30–45 minutes interview to design a data engineering system to spec. You will be asked questions on topics related to Data modeling, warehousing, data lakes, Transformation and data pipelines. The interviewer can ask you to design a data solution from end to end, usually composed of three parts: data storage, data processing, and data modeling. Scenarios will be provided and you need to design the data pipeline/system. You need to apply <strong>Requirement Gathering</strong> process, <strong>4Vs of Big Data</strong> framework and include the <strong>Fault tolerance + Scalability</strong> factors in your system design to crack this round.</p><p>The initial interview question is often very short and abstract (e.g. design a data warehouse from end to end) and it’s your job to ask follow-up questions to pin down the requirements and use cases, just like solving a real-life data problem.</p><p>In a system design interview, you will design a data solution from end to end, which is usually composed of three parts: data storage, data processing, and data modeling. You have to choose the best combination of data storage systems and data processing frameworks based on the requirements, and sometimes there is more than one optimal solution.</p><p>Example questions: Design Twitter — what are the system blocks needed? What database and schema would you use? What about caching and load balancing? What are the tradeoffs of a system? They are looking for your ability to clearly communicate and scope down requirements, design a concept-level pipeline, and knowledge of distributed systems basics.</p><p>Data modeling is usually the end piece of a system design interview but sometimes it is a part of the SQL interview instead. One example of a data modeling interview question is to design the backend analytical tables for a reservation system for vet clinics. The most important principle in data modeling is to design your data models based on use cases and query patterns. Again, it is your responsibility to get clarifications on the requirements and use cases so that you can make better design choices.</p><p>To prepare:</p><ol><li>Read Data Engineering Cookbook, the Data Engineering Ecosystem and Grokking the System Design Interview.</li><li>Follow NeetCode <a href="https://neetcode.io/courses/system-design-for-beginners/0" target="_blank" rel="noopener noreferrer">course</a> and <a href="https://neetcode.io/courses/system-design-interview/0" target="_blank" rel="noopener noreferrer">Interviews</a></li><li>Practice at least 10 different questions on a whiteboard with peers or mentors.</li><li>Practice drawing clean, readable systems diagrams on the whiteboard.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="round-4-cultural-fit">Round 4: Cultural Fit<a class="hash-link" href="#round-4-cultural-fit" title="Direct link to heading">​</a></h3><p>It’s very important to be technically strong and knowledgeable, but it’s not enough! If you can’t deliver your brilliant ideas, then no one else can understand and use them. Behavioral types of interviews, such as cultural fit, are meant to show how you can tell your story and explain how you’ve handled tough workplace situations.</p><p>Interviews are not exams where you just need the right answers to pass, but rather a series of conversations to see if you can learn quickly and work with a team to solve problems together. Therefore, it is very important to be human and be yourself during interviews.</p><p>Expect a 30–45 minutes interview with 2–4 questions about your past situations.</p><p>They are looking for consistent and complete answers using the STAR (situation, task, action, result) method.</p><p>Example questions include tell us about a time at work where you had a big deadline. Tell us about a time when you had a conflict with another team member. How did you handle these situations?</p><p>Interviewers will put Random and hypothetical situations in front of you and check your positive mindset and how you are going to approach that particular problem. You need to follow <strong>Recall your Past Experiences</strong> to crack this round.</p><p>To prepare:</p><ol><li>Practice at least 30 cultural fit interview questions alone, writing scripts and recording yourself if needed.</li><li>Practice at least 10 questions with peers, following the STAR method.</li><li>Be nice. Nobody wants to work with jerks.</li><li>Have conversations. The best interviews are usually like conversations. Ask questions if you want information or feedback.</li><li>Problem-solving, not answers. Just like in real life, you don’t always know the right answer to a problem immediately. It’s more important to show how you would approach the problem instead of only giving an answer.</li><li>Show your passion for data engineering. What do you do outside your work responsibility to be a better data engineer?</li></ol><p>While the interviewers are interviewing you, you are also interviewing them. Would you enjoy working with them? Would this team provide opportunities for you to grow? Do you agree with the manager’s view and managing style? Finding a good team is hard so ask your questions wisely.</p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Interviewing is very stressful. It is an imperfect process where strangers judge your professional competency only based on one hour of interactions with you and sometimes the interview result is not fair. It is frustrating when you just can’t get any further on interview questions and you feel like the interviewers are looking down at you. Getting rejected over and over again can be devastating to your self-esteem and you may start to think you’re not good enough. I have been there too: never hearing back from most job applications and failing all the coding interviews I could get. I thought I would never be an engineer. But I am glad I didn’t give up.</p><p>If you are feeling overwhelmed, frustrated, or hopeless because of interviews, I want to let you know that you are not alone. If you get rejected for a job, it is their loss. Be patient with yourself and stay hopeful, because things will get better and your just need to keep trying! Always show up to your interviews with confidence, because you are good enough!</p></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sql-questions">SQL Questions<a class="hash-link" href="#sql-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-the-coalesce-function-do">What does the COALESCE function do?<a class="hash-link" href="#what-does-the-coalesce-function-do" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-where-and-having-example-of-where-one-should-use-one-over-the-other">What is the difference between WHERE and HAVING? Example of where one should use one over the other?<a class="hash-link" href="#what-is-the-difference-between-where-and-having-example-of-where-one-should-use-one-over-the-other" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-top-rdbms-engines">What are top RDBMS engines?<a class="hash-link" href="#what-are-top-rdbms-engines" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-an-rdbms-different-from-a-nosql-database">How is an RDBMS different from a NoSQL database?<a class="hash-link" href="#how-is-an-rdbms-different-from-a-nosql-database" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-ddl-dcl-and-dml-stand-for-give-examples-of-commands-for-each">What do DDL, DCL and DML stand for? Give examples of commands for each<a class="hash-link" href="#what-do-ddl-dcl-and-dml-stand-for-give-examples-of-commands-for-each" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-common-data-types-in-sql">What are common data types in SQL?<a class="hash-link" href="#what-are-common-data-types-in-sql" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-attribute-constraints-and-explain-them">What are attribute constraints, and explain them?<a class="hash-link" href="#what-are-attribute-constraints-and-explain-them" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-inner-join-and-left-outer-join">What is the difference between inner join and left outer join?<a class="hash-link" href="#what-is-the-difference-between-inner-join-and-left-outer-join" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-union-and-union-all">What is the difference between UNION and UNION ALL?<a class="hash-link" href="#what-is-the-difference-between-union-and-union-all" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="when-should-one-use-a-cte-over-a-subquery">When should one use a CTE over a subquery?<a class="hash-link" href="#when-should-one-use-a-cte-over-a-subquery" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-window-functions">What are window functions?<a class="hash-link" href="#what-are-window-functions" title="Direct link to heading">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="python-questions">Python Questions<a class="hash-link" href="#python-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-a-function-to-sort-anarrayso-it-produces-only-odd-numbers">Write a function to sort an array so it produces only odd numbers.<a class="hash-link" href="#write-a-function-to-sort-anarrayso-it-produces-only-odd-numbers" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-a-function-to-find-non-duplicate-numbers-in-the-first-list-and-preserve-the-order-of-the-list-1132565----13256">Write a function to find non duplicate numbers in the first list and preserve the order of the list: <!-- -->[1,1,3,2,5,6,5]<!-- --> --&gt; <!-- -->[1,3,2,5,6]<a class="hash-link" href="#write-a-function-to-find-non-duplicate-numbers-in-the-first-list-and-preserve-the-order-of-the-list-1132565----13256" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="given-a-list-return-the-numbers-which-have-maximum-count">Given a list, return the numbers which have maximum count.<a class="hash-link" href="#given-a-list-return-the-numbers-which-have-maximum-count" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="given-a-json-object-with-nested-objects-write-a-function-that-flattens-all-the-objects-to-a-single-key-value-dictionary">Given a json object with nested objects, write a function that flattens all the objects to a single key value dictionary.<a class="hash-link" href="#given-a-json-object-with-nested-objects-write-a-function-that-flattens-all-the-objects-to-a-single-key-value-dictionary" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-code-to-find-the-sum-of-any-two-numbers-in-a-given-array-that-could-be-equal-to-x">Write code to find the sum of any two numbers in a given array that could be equal to x.<a class="hash-link" href="#write-code-to-find-the-sum-of-any-two-numbers-in-a-given-array-that-could-be-equal-to-x" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="write-code-to-find-the-maximum-number-of-combinations-of-infinite-coins-of-that-can-add-up-to-make-20-rupees">Write code to find the maximum number of combinations of infinite coins of that can add up to make 20 rupees.<a class="hash-link" href="#write-code-to-find-the-maximum-number-of-combinations-of-infinite-coins-of-that-can-add-up-to-make-20-rupees" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-implement-astackusing-alinked-list">How do you implement a stack using a linked list?<a class="hash-link" href="#how-do-you-implement-astackusing-alinked-list" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-your-experience-with-x-skill-on-python">What is your experience with X skill on Python?<a class="hash-link" href="#what-is-your-experience-with-x-skill-on-python" title="Direct link to heading">​</a></h4><p>General experience questions like this are jump-off points for more technical case studies. And typically, The interviewer will tailor questions as they pertain to the role. However, you should be comfortable with standard Python and supplemental libraries like Matplotlib, Pandas, and NumPy, know what’s available, and understand when it’s appropriate to use each library.</p><p>One note: Don’t fake it. If you don’t have much experience, be honest. You can also describe a related skill or talk about your comfort level in quickly picking up new Python skills (with an example).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-modeling-questions">Data Modeling Questions<a class="hash-link" href="#data-modeling-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-create-a-schema-that-would-keep-track-of-a-customer-address-where-the-address-changes">How do you create a schema that would keep track of a customer address where the address changes?<a class="hash-link" href="#how-do-you-create-a-schema-that-would-keep-track-of-a-customer-address-where-the-address-changes" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="design-a-data-model-in-order-to-track-product-from-the-vendor-to-the-amazon-warehouse-to-delivery-to-the-customer">Design a data model in order to track product from the vendor to the Amazon warehouse to delivery to the customer.<a class="hash-link" href="#design-a-data-model-in-order-to-track-product-from-the-vendor-to-the-amazon-warehouse-to-delivery-to-the-customer" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="design-a-data-model-for-a-retail-store">Design a data model for a retail store.<a class="hash-link" href="#design-a-data-model-for-a-retail-store" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="create-the-required-tables-for-an-online-store-define-the-necessary-relations-identify-primary-and-foreign-keys-etc">Create the required tables for an online store: define the necessary relations, identify primary and foreign keys, etc.<a class="hash-link" href="#create-the-required-tables-for-an-online-store-define-the-necessary-relations-identify-primary-and-foreign-keys-etc" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-manage-a-table-with-a-large-number-of-updates-while-maintaining-the-availability-of-the-table-for-a-large-number-of-users">How do you manage a table with a large number of updates, while maintaining the availability of the table for a large number of users?<a class="hash-link" href="#how-do-you-manage-a-table-with-a-large-number-of-updates-while-maintaining-the-availability-of-the-table-for-a-large-number-of-users" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="should-we-apply-normalization-rules-on-a-star-schema">Should we apply normalization rules on a star schema?<a class="hash-link" href="#should-we-apply-normalization-rules-on-a-star-schema" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="whats-a-chasm-trap">What&#x27;s a chasm trap?<a class="hash-link" href="#whats-a-chasm-trap" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-cap-theorem">What is CAP theorem?<a class="hash-link" href="#what-is-cap-theorem" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-data-modeling-and-why-you-should-care">What is Data Modeling and Why you should care?<a class="hash-link" href="#what-is-data-modeling-and-why-you-should-care" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-advantages-of-data-modeling">What are the advantages of Data Modeling?<a class="hash-link" href="#what-are-the-advantages-of-data-modeling" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-conceptual-data-model">What is Conceptual Data Model?<a class="hash-link" href="#what-is-conceptual-data-model" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-logical-data-model">What is Logical Data Model?<a class="hash-link" href="#what-is-logical-data-model" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-physical-data-model">What is Physical Data Model?<a class="hash-link" href="#what-is-physical-data-model" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-entity-relation-er-model">What is Entity-Relation (ER) Model?<a class="hash-link" href="#what-is-entity-relation-er-model" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-star-schema">What is Star Schema?<a class="hash-link" href="#what-is-star-schema" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-star-schema-and-snowflake-schema">What are the differences between Star Schema and Snowflake Schema?<a class="hash-link" href="#what-are-the-differences-between-star-schema-and-snowflake-schema" title="Direct link to heading">​</a></h4><p>“Star schema is a type of data warehouse schema that is optimized for query performance. The snowflake schema is a type of data warehouse schema that is normalized for storage efficiency.”</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-relational-data-modeling-with-an-example">Explain the Relational Data Modeling with an example<a class="hash-link" href="#explain-the-relational-data-modeling-with-an-example" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-non-relational-data-modeling-with-an-example">Explain the Non-Relational Data Modeling with an example<a class="hash-link" href="#explain-the-non-relational-data-modeling-with-an-example" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-kimballs-four-step-process-to-dimensional-data-modeling">Explain the Kimball&#x27;s Four Step Process to Dimensional Data Modeling<a class="hash-link" href="#explain-the-kimballs-four-step-process-to-dimensional-data-modeling" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-inmon-data-modeling-approach">What is Inmon Data Modeling Approach?<a class="hash-link" href="#what-is-inmon-data-modeling-approach" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-kimball-and-inmon-data-modeling-approach">What are the differences between Kimball and Inmon Data Modeling Approach?<a class="hash-link" href="#what-are-the-differences-between-kimball-and-inmon-data-modeling-approach" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-fact-and-dimension-tables">What is the difference between Fact and Dimension Tables?<a class="hash-link" href="#what-is-the-difference-between-fact-and-dimension-tables" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-slowly-changing-dimensions-scds">What are Slowly Changing Dimensions (SCDs)?<a class="hash-link" href="#what-are-slowly-changing-dimensions-scds" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-different-types-of-slowly-changing-dimensions-scds">Explain Different Types of Slowly Changing Dimensions (SCDs)<a class="hash-link" href="#explain-different-types-of-slowly-changing-dimensions-scds" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-data-normalization">What is Data Normalization?<a class="hash-link" href="#what-is-data-normalization" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-data-denormalization">What is Data Denormalization?<a class="hash-link" href="#what-is-data-denormalization" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-data-normalization">What are the Pros and Cons of Data Normalization?<a class="hash-link" href="#what-are-the-pros-and-cons-of-data-normalization" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-data-denormalization">What are the Pros and Cons of Data Denormalization?<a class="hash-link" href="#what-are-the-pros-and-cons-of-data-denormalization" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-among-1nf-2nf-and-3nf">What is the difference among 1NF, 2NF and 3NF?<a class="hash-link" href="#what-is-the-difference-among-1nf-2nf-and-3nf" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-categories-of-nosql-databases">What are the different categories of NoSQL Databases?<a class="hash-link" href="#what-are-the-different-categories-of-nosql-databases" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-acid-properties-and-where-these-are-applied">What are the ACID properties and where these are applied?<a class="hash-link" href="#what-are-the-acid-properties-and-where-these-are-applied" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-base-properties-and-where-these-are-applied">What are the BASE properties and where these are applied?<a class="hash-link" href="#what-are-the-base-properties-and-where-these-are-applied" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-sql-and-nosql-data-modeling">What are the differences between SQL and NoSQL data modeling?<a class="hash-link" href="#what-are-the-differences-between-sql-and-nosql-data-modeling" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="describe-a-time-you-had-difficulty-merging-data-how-did-you-solve-this-issue">Describe a time you had difficulty merging data. How did you solve this issue?<a class="hash-link" href="#describe-a-time-you-had-difficulty-merging-data-how-did-you-solve-this-issue" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="describe-a-time-you-had-difficulty-merging-data-how-did-you-solve-this-issue-1">Describe a time you had difficulty merging data. How did you solve this issue?<a class="hash-link" href="#describe-a-time-you-had-difficulty-merging-data-how-did-you-solve-this-issue-1" title="Direct link to heading">​</a></h4><p>Data cleaning and data processing are key job responsibilities in engineering roles. Inevitably unexpected issues will come up. Interviewers ask questions like these to determine:</p><ul><li>How well do you adapt?</li><li>The depth of your experience.</li><li>Your technical problem-solving ability.</li><li>Clearly explain the issue, what you proposed, the steps you took to solve the problem, and the outcome.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-design-schemas-of-data-modeling">What are the design schemas of data modeling?<a class="hash-link" href="#what-are-the-design-schemas-of-data-modeling" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="whats-the-difference-between-structured-and-unstructured-data">What’s the difference between structured and unstructured data?<a class="hash-link" href="#whats-the-difference-between-structured-and-unstructured-data" title="Direct link to heading">​</a></h4><p>With a fundamental question like this, be prepared to answer with a quick definition and then provide an example.</p><p>You could say: “Structured data consists of clearly defined data types and easily searchable information. An example would be customer purchase information stored in a relational database. Unstructured data, on the other hand, does not have a clearly defined format, and therefore, a relational database can’t store it in a relational database. An example would be video or image files.”</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-warehousing-questions">Data Warehousing Questions<a class="hash-link" href="#data-warehousing-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="give-a-schema-for-a-data-warehouse">Give a schema for a data warehouse.<a class="hash-link" href="#give-a-schema-for-a-data-warehouse" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="design-a-data-warehouse-to-capture-sales">Design a data warehouse to capture sales.<a class="hash-link" href="#design-a-data-warehouse-to-capture-sales" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="design-a-data-warehouse-to-help-a-customer-support-team-manage-tickets">Design a data warehouse to help a customer support team manage tickets.<a class="hash-link" href="#design-a-data-warehouse-to-help-a-customer-support-team-manage-tickets" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-design-a-simple-oltp-architecture-that-will-convince-the-redbus-team-to-give-x-project-to-you">Can you design a simple OLTP architecture that will convince the Redbus team to give X project to you?<a class="hash-link" href="#can-you-design-a-simple-oltp-architecture-that-will-convince-the-redbus-team-to-give-x-project-to-you" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-data-warehouse">What is Data Warehouse?<a class="hash-link" href="#what-is-data-warehouse" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-data-warehouse-needed">Why is Data Warehouse needed?<a class="hash-link" href="#why-is-data-warehouse-needed" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-databases-and-data-warehouses">What are the differences between Databases and Data Warehouses?<a class="hash-link" href="#what-are-the-differences-between-databases-and-data-warehouses" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-operational-and-analytical-systems">What are the differences between Operational and Analytical Systems?<a class="hash-link" href="#what-are-the-differences-between-operational-and-analytical-systems" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-data-mart">What is Data Mart?<a class="hash-link" href="#what-is-data-mart" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-main-difference-between-view-and-materialized-view">What is the Main Difference between View and Materialized View?<a class="hash-link" href="#what-is-the-main-difference-between-view-and-materialized-view" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-oltp">What is OLTP?<a class="hash-link" href="#what-is-oltp" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-olap">What is OLAP?<a class="hash-link" href="#what-is-olap" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-oltp-and-olap">What are the differences between OLTP and OLAP?<a class="hash-link" href="#what-are-the-differences-between-oltp-and-olap" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-oltp">What are the Pros and Cons of OLTP?<a class="hash-link" href="#what-are-the-pros-and-cons-of-oltp" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-olap">What are the Pros and Cons of OLAP?<a class="hash-link" href="#what-are-the-pros-and-cons-of-olap" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-data-lakehouse">What is a Data Lakehouse?<a class="hash-link" href="#what-is-a-data-lakehouse" title="Direct link to heading">​</a></h4><p>In short, a Data Lakehouse is an architecture that enables efficient and secure Artificial Intelligence (AI) and Business Intelligence (BI) directly on vast amounts of data stored in Data Lakes.</p><p>Today, the vast majority of enterprise data lands in data lakes, low-cost storage systems that can manage any type of data (structured or unstructured) and have an open interface that any processing tool can run against. These data lakes are where most data transformation and advanced analytics workloads (such as AI) run to take advantage of the full set of data in the organization. Separately, for Business Intelligence (BI) use cases, proprietary data warehouse systems are used on a much smaller subset of the data that is structured. These data warehouses primarily support BI, answering historical analytical questions about the past using SQL (e.g., what was my revenue last quarter), while the data lake stores a much larger amount of data and supports analytics using both SQL and non-SQL interfaces, including predictive analytics and AI (e.g. which of my customers will likely churn, or what coupons to offer at what time to my customers). Historically, to accomplish both AI and BI, you would have to have multiple copies of the data and move it between data lakes and data warehouses.</p><p>The Data Lakehouse enables storing all your data once in a data lake and doing AI and BI on that data directly. It has specific capabilities to efficiently enable both AI and BI on all the enterprise’s data at a massive scale. Namely, it has the SQL and performance capabilities (indexing, caching, MPP processing) to make BI work fast on data lakes. It also has direct file access and direct native support for Python, data science, and AI frameworks without ever forcing it through a SQL-based data warehouse. The key technologies used to implement Data Lakehouses are open source, such as Delta Lake, Hudi, and Iceberg. Vendors who focus on Data Lakehouses include, but are not limited to Databricks, AWS, Dremio, and Starburst. Vendors who provide Data Warehouses include, but are not limited to, Teradata, Snowflake, and Oracle.</p><p>Recently, Bill Inmon, widely considered the father of data warehousing, published a blog post on the Evolution of the Data Lakehouse explaining the unique ability of the lakehouse to manage data in an open environment while combining the data science focus of the data lake with the end-user analytics of the data warehouse.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-data-lake">What is a Data Lake?<a class="hash-link" href="#what-is-a-data-lake" title="Direct link to heading">​</a></h4><p>A data lake is a low-cost, open, durable storage system for any data type - tabular data, text, images, audio, video, JSON, and CSV. In the cloud, every major cloud provider leverages and promotes a data lake, e.g. AWS S3, Azure Data Lake Storage (ADLS), Google Cloud Storage (GCS). As a result, the vast majority of the data of most organizations is stored in cloud data lakes. Over time, most organizations store their data in an open standardized format, typically either Apache Parquet format or ORC format. As a result, a large ecosystem of tools and applications can directly work with these open data formats. This approach of storing data in open formats, at a very low cost has enabled organizations to amass large quantities of data in data lakes while avoiding vendor lock-in. At the same time, data lakes have suffered from three main problems - security, quality, and performance despite these advantages. Since all the data is stored and managed as files, it does not provide fine-grained access control on the contents of files, but only coarse-grained access governing who can access what files or directories. The query performance is poor because the formats are not optimized for fast access, and listing files is computationally expensive. In short, organizations end up moving data into other systems to make use of the data, unless the applications can tolerate noise (i.e. machine learning). Finally, quality is a challenge because it’s hard to prevent data corruption and manage schema changes as more and more data gets ingested to the data lake. Similarly, it is challenging to ensure atomic operations when writing a group of files, and no mechanism to roll back changes. As a result, many argue that most data lakes end up becoming data “swamps”. . Consequently, most organizations move subsets of this data into Data Warehouses, which do not have these three problems, but suffer from other problems.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-data-warehouse">What is a Data Warehouse?<a class="hash-link" href="#what-is-a-data-warehouse" title="Direct link to heading">​</a></h4><p>Data warehouses are proprietary systems that are built to store and manage only structured or semi-structured (primarily JSON format) data for SQL-based analytics and business intelligence. The most valuable business data is curated and uploaded to data warehouses, which are optimized for high performance, concurrency, and reliability but at a much higher cost, as any data processing will have to be at more expensive SQL rates rather than cheap data lake access rates. Historically, data warehouses were capacity constrained and could not support simultaneous ETL and BI queries; much less real-time streaming. Since data warehouses were primarily built for structured data, they do not support unstructured data such as images, sensor data, documents, videos, etc. They have limited support for machine learning and cannot directly support popular open source libraries and tools (TensorFlow, PyTorch, and other Python-based libraries) natively. As a result, most organizations end up keeping these data sets in a data lake, moving subsets into a data warehouse for fast concurrent BI and SQL use cases</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-a-data-lakehouse-different-from-a-data-warehouse">How is a Data Lakehouse different from a Data Warehouse?<a class="hash-link" href="#how-is-a-data-lakehouse-different-from-a-data-warehouse" title="Direct link to heading">​</a></h4><p>The lakehouse builds on top of existing data lakes, which often contain more than 90% of the data in the enterprise. While most data warehouses support “external table” functionality to access that data, they have severe functionality limitations (e.g., only supporting read operations) and performance limitations when doing so. Lakehouse instead adds traditional data warehousing capabilities to existing data lakes, including ACID transactions, fine-grained data security, low-cost updates and deletes, first-class SQL support, optimized performance for SQL queries, and BI style reporting. By building on top of a data lake, the Lakehouse stores and manages all existing data in a data lake, including all varieties of data, such as text, audio and video, in addition to structured data in tables. Lakehouse also natively supports data science and machine learning use cases by providing direct access to data using open APIs and supporting various ML and Python/R libraries, such as PyTorch, Tensorflow or XGBoost, unlike data warehouses. Thus, Lakehouse provides a single system to manage all of an enterprise’s data while supporting the range of analytics from BI and AI.</p><p>On the other hand, data warehouses are proprietary data systems that are purpose-built for SQL-based analytics on structured data, and certain types of semi-structured data. Data warehouses have limited support for machine learning and cannot support running popular open source tools natively without first exporting the data (either through ODBC/JDBC or to a data lake). Today, no data warehouse system has native support for all the existing audio, image, and video data that is already stored in data lakes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-the-data-lakehouse-different-from-a-data-lake">How is the Data Lakehouse different from a Data Lake?<a class="hash-link" href="#how-is-the-data-lakehouse-different-from-a-data-lake" title="Direct link to heading">​</a></h4><p>The most common complaint about data lakes is that they can become data swamps. Anybody can dump any data into a data lake; there is no structure or governance to the data in the lake. Performance is poor, as data is not organized with performance in mind, resulting in limited analytics on data lakes. As a result, most organizations use data lakes as a landing zone for most of their data due to the underlying low-cost object storage data lakes use and then move the data to different downstream systems such as data warehouses to extract value.</p><p>Lakehouse tackles the fundamental issues that make data swamps out of data lakes. It adds ACID transactions to ensure consistency as multiple parties concurrently read or write data. It supports DW schema architectures like star/snowflake-schemas and provides robust governance and auditing mechanisms directly on the data lake. It also leverages various performance optimization techniques, such as caching, multi-dimensional clustering, and data skipping, using file statistics and data compaction to right-size the files enabling fast analytics. And it adds fine-grained security and auditing capabilities for data governance. By adding data management and performance optimizations to the open data lake, lakehouse can natively support BI and ML applications.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-easy-is-it-for-data-analysts-to-use-a-data-lakehouse">How easy is it for data analysts to use a Data Lakehouse?<a class="hash-link" href="#how-easy-is-it-for-data-analysts-to-use-a-data-lakehouse" title="Direct link to heading">​</a></h4><p>Data lakehouse systems implement the same SQL interface as traditional data warehouses, so analysts can connect to them in existing BI and SQL tools without changing their workflows. For example, leading BI products such as Tableau, PowerBI, Qlik, and Looker can all connect to data lakehouse systems, data engineering tools like Fivetran and dbt can run against them, and analysts can export data into desktop tools such as Microsoft Excel. Lakehouse’s support for ANSI SQL, fine-grained access control, and ACID transactions enables administrators to manage them the same way as data warehouse systems but cover all the data in their organization in one system.</p><p>One important advantage of Lakehouse systems in simplicity is that they manage all the data in the organization, so data analysts can be granted access to work with raw and historical data as it arrives instead of only the subset of data loaded into a data warehouse system. An analyst can therefore easily ask questions that span multiple historical datasets or establish a new pipeline for working with a new dataset without blocking on a database administrator or data engineer to load the appropriate data. Built-in support for AI also makes it easy for analysts to run AI models built by a machine learning team on any data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-data-lakehouse-systems-compare-in-performance-and-cost-to-data-warehouses">How do Data Lakehouse systems compare in performance and cost to data warehouses?<a class="hash-link" href="#how-do-data-lakehouse-systems-compare-in-performance-and-cost-to-data-warehouses" title="Direct link to heading">​</a></h4><p>Data Lakehouse systems are built around separate, elastically scaling compute and storage to minimize their cost of operation and maximize performance. Recent systems provide comparable or even better performance per dollar to traditional data warehouses for SQL workloads, using the same optimization techniques inside their engines (e.g., query compilation and storage layout optimizations). In addition, Lakehouse systems often take advantage of cloud provider cost-saving features such as spot instance pricing (which requires the system to tolerate losing worker nodes mid-query) and reduced prices for infrequently accessed storage, which traditional data warehouse engines have usually not been designed to support.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-data-governance-functionality-do-data-lakehouse-systems-support">What data governance functionality do Data Lakehouse systems support?<a class="hash-link" href="#what-data-governance-functionality-do-data-lakehouse-systems-support" title="Direct link to heading">​</a></h4><p>By adding a management interface on top of data lake storage, Lakehouse systems provide a uniform way to manage access control, data quality, and compliance across all of an organization’s data using standard interfaces similar to those in data warehouses. Modern Lakehouse systems support fine-grained (row, column, and view level) access control via SQL, query auditing, attribute-based access control, data versioning, and data quality constraints and monitoring. These features are generally provided using standard interfaces familiar to database administrators (for example, SQL GRANT commands) to allow existing personnel to manage all the data in an organization in a uniform way. Centralizing all the data in a Lakehouse system with a single management interface also reduces the administrative burden and potential for error that comes with managing multiple separate systems.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="does-the-data-lakehouse-have-to-be-centralized-or-can-it-be-decentralized-into-a-data-mesh">Does the Data Lakehouse have to be centralized or can it be decentralized into a Data Mesh?<a class="hash-link" href="#does-the-data-lakehouse-have-to-be-centralized-or-can-it-be-decentralized-into-a-data-mesh" title="Direct link to heading">​</a></h4><p>No, organizations do not need to centralize all their data in one Lakehouse. Many organizations using the Lakehouse architecture take a decentralized approach to store and process data but take a centralized approach to security, governance, and discovery. Depending on organizational structure and business needs, we see a few common approaches:</p><ul><li>Each business unit builds its own Lakehouse to capture its business&#x27; complete view – from product development to customer acquisition to customer service.</li><li>Each functional area, such as product manufacturing, supply chain, sales, and marketing, could build its own Lakehouse to optimize operations within its business area.</li><li>Some organizations also spin up a new Lakehouse to tackle new cross-functional strategic initiatives such as customer 360 or unexpected crises like the COVID pandemic to drive fast, decisive action.</li></ul><p>The unified nature of the Lakehouse architecture enables data architects to build simpler data architectures that align with the business needs without complex orchestration of data movement across siloed data stacks for BI and ML. Furthermore, the openness of the Lakehouse architecture enables organizations to leverage the growing ecosystem of open technologies without fear of lock-in to addressing the unique needs of the different business units or functional areas. Because Lakehouse systems are usually built on separated, scalable cloud storage, it is also simple and efficient to let multiple teams access each lakehouse. Recently, Delta Sharing proposed an open and standard mechanism for data sharing across Lakehouses with support from many different vendors.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-the-data-mesh-relate-to-the-data-lakehouse">How does the Data Mesh relate to the Data Lakehouse?<a class="hash-link" href="#how-does-the-data-mesh-relate-to-the-data-lakehouse" title="Direct link to heading">​</a></h4><p>Zhamak Dehghani has outlined four fundamental organizational principles that embody any data mesh implementation. The Data Lakehouse architecture can be used in implementing these organizational principles:</p><ul><li>Domain-oriented decentralized data ownership and architecture: As discussed in the previous section, the lakehouse architecture takes a decentralized approach to data ownership. Organizations can create many different lakehouses to serve the individual needs of the business groups. Based on their needs, they can store and manage various data – images, video, text, structured tabular data, and related data assets such as machine learning models and associated code to reproduce transformations and insights.</li><li>Data as a product: The lakehouse architecture helps organizations manage data as a product by providing different data team members in domain-specific teams complete control over the data lifecycle. Data team comprising of a data owner, data engineers, analysts, and data scientists can manage data (structured, semi-structured, and unstructured with proper lineage and security controls), code (ETL, data science notebooks, ML training, and deployment), and supporting infrastructure (storage, compute, cluster policies, and various analytics and ML engines). Lakehouse platform features such as ACID transactions, data versioning, and zero-copy cloning make it easy for these teams to publish and maintain their data as a product.</li><li>Self-serve data infrastructure as a platform: The lakehouse architecture provides an end-to-end data platform for data management, data engineering, analytics, data science, and machine learning with integrations to a broad ecosystem of tools. Adding data management on top of existing data lakes simplifies data access and sharing – anyone can request access, the requester pays for cheap blob storage and gets immediate secure access. In addition, using open data formats and enabling direct file access, data teams can use best-of-breed analytics and ML frameworks on the data.</li><li>Federated computational governance: The governance in the lakehouse architecture is implemented by a centralized catalog with fine-grained access controls (row/column level), enabling easy discovery of data and other artifacts like code and ML models. Organizations can assign different administrators to different parts of the catalog to decentralize control and management of data assets. This hybrid approach of a centralized catalog with federated control preserves the independence and agility of the local domain-specific teams while ensuring data asset reuse across these teams and enforcing a common security and governance model globally.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-a-data-warehouse-and-an-operational-database">What is the difference between a data warehouse and an operational database?<a class="hash-link" href="#what-is-the-difference-between-a-data-warehouse-and-an-operational-database" title="Direct link to heading">​</a></h4><p>If you took a database course in college, then you probably learned about how to set up a standard normalized database. This style of database is optimized for transactions that involve Insert, Update, and Delete SQL statements. These are standard operational databases. They need to focus on making transactions quickly without getting bogged down by calculations and data manipulations. Thus, their design is a little more cumbersome for an analysis. Generally, you will have to join several tables just to get a single data point.</p><p>A data warehouse is not concerned as much with dealing with millions of fast transactions every second. Instead, a data warehouse is usually built to support a data analytics product and analysis. This means performance is not geared towards transactions — instead, it’s aimed at aggregations, calculations, and select statements. A data warehouse will have a slightly denormalized structure compared to an operational database. In most data warehouses, a majority of tables will take on two different attributes: a historical transaction table and tables that contain categorical style data. We reference these as fact and dimension tables.</p><p>The fact table is essentially in the center, unlike in a normalized database where you might have to join across several tables to get one data point. A standard data warehouse usually has a focus on the fact tables, and all the dimension tables join to provide categorical information to the fact table. It’s also typically bad practice to join fact table to the fact table, but sometimes it can occur if the data is created correctly.</p><p>These are not the only tables that exist in a data warehouse. There are aggregate tables, snapshots, partitions, and more. The goal is usually a report or dashboard that can be automatically updated quickly.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-would-you-design-a-data-warehouse-given-x-criteria">How would you design a data warehouse given X criteria?<a class="hash-link" href="#how-would-you-design-a-data-warehouse-given-x-criteria" title="Direct link to heading">​</a></h4><p>This example is a fundamental case study question in data engineering, and it requires you to provide a high-level design for a database based on criteria. To answer questions like this:</p><ul><li>Start with clarifying questions and state your assumptions</li><li>Provide a hypothesis or high-level overview of your design</li><li>Then describe how your design would work</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="big-data--cloud-questions">Big Data &amp; Cloud Questions<a class="hash-link" href="#big-data--cloud-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-big-datas-four-vs">What are big data’s four Vs?<a class="hash-link" href="#what-are-big-datas-four-vs" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-key-features-of-hadoop">What are the key features of Hadoop?<a class="hash-link" href="#what-are-the-key-features-of-hadoop" title="Direct link to heading">​</a></h4><p>Some of the Hadoop features you might talk about in a data engineering interview include:</p><ul><li>Fault tolerance</li><li>Distributed processing</li><li>Scalability</li><li>Reliability</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-key-features-of-pyspark">What are the key features of PySpark?<a class="hash-link" href="#what-are-the-key-features-of-pyspark" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-pyspark-is-different-from-python">How PySpark is different from Python?<a class="hash-link" href="#how-pyspark-is-different-from-python" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-etl-tools-do-you-have-experience-using-what-tools-do-you-prefer">What ETL tools do you have experience using? What tools do you prefer?<a class="hash-link" href="#what-etl-tools-do-you-have-experience-using-what-tools-do-you-prefer" title="Direct link to heading">​</a></h4><p>There are many variations to this type of question. A different version would be about a specific ETL tool, “Have you had experienced with Apache Spark or Amazon Redshift?” If a tool is in the job description, it might come up in a question like this. One tip: Include any training, how long you’ve used the tech, and specific tasks you can perform.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-experience-do-you-have-with-cloud-technologies">What experience do you have with cloud technologies?<a class="hash-link" href="#what-experience-do-you-have-with-cloud-technologies" title="Direct link to heading">​</a></h4><p>If cloud technology is in the job description, chances are it will show up in the interview. Some of the most common cloud technologies for data engineer interviews include Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform, and IBM Cloud. Additionally, be prepared to discuss specific tools for each platform, like AWS Glue, EMR, and AWS Athena.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-some-challenges-unique-to-cloud-computing">What are some challenges unique to cloud computing?<a class="hash-link" href="#what-are-some-challenges-unique-to-cloud-computing" title="Direct link to heading">​</a></h4><p>A broad question like this can quickly assess your experience with cloud technologies in data engineering. Some of the challenges you should be prepared to talk about include:</p><ul><li>Security and Compliance</li><li>Cost</li><li>Governance and control</li><li>Performance</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-pipelining-questions">Data Pipelining Questions<a class="hash-link" href="#data-pipelining-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="given-scenario-a-how-would-you-design-the-pipeline-for-ingesting-this-data">Given scenario A, how would you design the pipeline for ingesting this data?<a class="hash-link" href="#given-scenario-a-how-would-you-design-the-pipeline-for-ingesting-this-data" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="given-a-schema-create-a-script-from-scratch-for-an-etl-to-provide-certain-data-writing-a-function-for-each-step-of-the-process">Given a schema, create a script from scratch for an ETL to provide certain data, writing a function for each step of the process.<a class="hash-link" href="#given-a-schema-create-a-script-from-scratch-for-an-etl-to-provide-certain-data-writing-a-function-for-each-step-of-the-process" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-would-you-build-a-data-pipeline-around-an-aws-product-which-is-able-to-handle-increasing-data-volume">How would you build a data pipeline around an AWS product, which is able to handle increasing data volume?<a class="hash-link" href="#how-would-you-build-a-data-pipeline-around-an-aws-product-which-is-able-to-handle-increasing-data-volume" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-tools-you-would-use-to-build-data-pipelines-in-cloud">Which tools you would use to build data pipelines in cloud?<a class="hash-link" href="#which-tools-you-would-use-to-build-data-pipelines-in-cloud" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-would-you-design-a-data-pipeline">How would you design a data pipeline?<a class="hash-link" href="#how-would-you-design-a-data-pipeline" title="Direct link to heading">​</a></h4><p>A broad, beginner case study question like this wants to know how you approach a problem. With all case study questions, you should ask clarifying questions like:</p><ul><li>What type of data is processed?</li><li>How will the information be used?</li><li>What are the requirements for the project?</li><li>How much will data be pulled? How frequently?</li></ul><p>These questions will provide insights into the type of response the interviewer seeks. Then, you can describe your design process, starting with choosing data sources and data ingestion strategies, before moving into your developing data processing and implementation plans.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="tell-me-about-a-time-you-had-performance-issues-with-an-etl-and-how-did-you-fix-it">Tell me About a Time You Had Performance Issues With an ETL and How Did You Fix It?<a class="hash-link" href="#tell-me-about-a-time-you-had-performance-issues-with-an-etl-and-how-did-you-fix-it" title="Direct link to heading">​</a></h4><p>As a data engineer, you will run into performance issues. Either you developed an ETL when the data was smaller and it didn’t scale, or you’re maintaining older architecture that is not scaling. ETLs feature multiple components, multiple table inserts, merges, and updates. This makes it difficult to tell exactly where the ETL issue is occurring. The first step is identifying the problem, so you need to figure out where the bottleneck is occurring.</p><p>Hopefully, whoever set up your ETL has an ETL log table somewhere that tracks when components finish. This makes it easy to spot bottlenecks and the biggest time sucks. If not, it will not be easy to find the issue. Depending on the urgency of the issue, we would recommend setting up an ETL log table and then rerunning to identify the issue. If the fix is needed right away, then you will probably just have to go piece-by-piece through the ETL to try to track down the long-running component. This also depends on how long the ETL takes to run. There are ways you can approach that as well depending on what the component relies on.</p><p>Issues can vary wildly, and they can include table locks, slow transactions, loops getting stuck, and etc. Once you have identified the issue, then you need to figure out a fix. This depends on the problem, but the solutions could require adding an index, removing an index, partitioning tables, and batching the data in smaller pieces (or sometimes even larger pieces — it seems counterintuitive, but this would depend on table scans and indexes). Depending on the storage system you are using, it’s good to look into the activity monitor to see what is happening on an I/O level. This will give you a better idea of the problem.</p><p>When you look at the activity monitor, you can see if there is any data being processed at all. Is there too much data being processed, none, or table locks? Any of these issues can choke an ETL and would need to be addressed.</p><p>If you Google some of the performance issues, then you will find some people blaming the architecture for a lot of the problems. We don’t disagree with them. However, this doesn’t mean you should throw in the towel. There are always various ways to manage performance if you can’t touch the actual structure. Even beyond indexes, there are some parallel processing methods that can be used to speed up the ETL. You can also add temporary support tables to lighten the load.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-would-you-approach-developing-a-new-analytical-product-as-a-data-engineer"><strong>How Would You Approach Developing a New Analytical Product as a Data Engineer?</strong><a class="hash-link" href="#how-would-you-approach-developing-a-new-analytical-product-as-a-data-engineer" title="Direct link to heading">​</a></h4><p>As a data engineer, you control what is possible in the final product. A data scientist can’t build algorithms or metrics without having the right data and the data at the right granularity.</p><p>This means a data engineer needs to understand the entire product. A data engineer can’t just get away with building systems based off of requirements. They need to ask why they are building certain tables and objects.</p><p>It’s helpful if the stakeholders already have a general outline of what they want. If they don’t have an outline, we would want to work with them to develop a general idea of what metrics and algorithms will exist. This drives all the major decisions, including what data should be pulled, how long should it be stored, if should it be archived, and etc.</p><p>Once a general outline exists, the next step would be drilling into the why of each metric. This is because as you’re building different tables at different data granularities, certain issues might arise. Should the unique key be on columns A and B, or A, B, and C. Well, that depends, why is this important? What does that row signify? Is it customer level, store level, or maybe brand level?</p><p>Once your team has gone through the process of working on the outline with your stakeholders and gained an understanding of the why, the next step is to think through as many operational scenarios as possible.</p><p>Will you ever need to reload data? Do your ETLs allow for it? Is it efficient? What happens when X occurs? How do you handle case Y?</p><p>You can’t spend all day doing this, but trying to think of all the issues that could occur will help you develop a more robust system. It also helps create a system that actually meets requirements.</p><p>From there, it’s about developing the design, creating test cases, and testing the tables, stored procedures, and scripts, and then pushing to production. How that occurs usually changes from team to team.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="project-related-questions">Project-related Questions<a class="hash-link" href="#project-related-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="walk-me-through-a-project-you-worked-on-from-start-to-finish">Walk me through a project you worked on from start to finish<a class="hash-link" href="#walk-me-through-a-project-you-worked-on-from-start-to-finish" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-algorithms-did-you-use-on-the-project">What algorithm(s) did you use on the project?<a class="hash-link" href="#what-algorithms-did-you-use-on-the-project" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-tools-did-you-use-on-the-project">What tools did you use on the project?<a class="hash-link" href="#what-tools-did-you-use-on-the-project" title="Direct link to heading">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="software-engineering-questions">Software Engineering Questions<a class="hash-link" href="#software-engineering-questions" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="describe-a-situation-where-you-used-git">Describe a situation where you used git<a class="hash-link" href="#describe-a-situation-where-you-used-git" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-you-version-control-your-codebase">How you version control your codebase<a class="hash-link" href="#how-you-version-control-your-codebase" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-your-experience-with-bash-shell">What is your experience with Bash shell?<a class="hash-link" href="#what-is-your-experience-with-bash-shell" title="Direct link to heading">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-agile-methodology">What is agile methodology<a class="hash-link" href="#what-is-agile-methodology" title="Direct link to heading">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="airflow">Airflow<a class="hash-link" href="#airflow" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-airflow">What is Airflow?<a class="hash-link" href="#what-is-airflow" title="Direct link to heading">​</a></h4><p>Apache Airflow is an open-source workflow management platform. It began in October 2014 at Airbnb as a solution for managing the company&#x27;s increasingly complex workflows. Airbnb&#x27;s creation of Airflow enabled them to programmatically author, schedule, and monitor their workflows via the built-in Airflow user interface. Airflow is a data transformation pipeline ETL (Extract, Transform, Load) workflow orchestration tool.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-issues-does-airflow-resolve">What issues does Airflow resolve?<a class="hash-link" href="#what-issues-does-airflow-resolve" title="Direct link to heading">​</a></h4><p>Crons are an old technique of task scheduling.
Scalable
Cron requires external assistance to log, track, and manage tasks. The Airflow UI is used to track and monitor the workflow&#x27;s execution.
Creating and maintaining a relationship between tasks in cron is a challenge, whereas it is as simple as writing Python code in Airflow.
Cron jobs are not reproducible until they are configured externally. Airflow maintains an audit trail of all tasks completed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-workflow-is-designed-in-airflow">Explain how workflow is designed in Airflow?<a class="hash-link" href="#explain-how-workflow-is-designed-in-airflow" title="Direct link to heading">​</a></h4><p>A directed acyclic graph (DAG) is used to design an Airflow workflow. That is to say, when creating a workflow, consider how it can be divided into tasks that can be completed independently. The tasks can then be combined into a graph to form a logical whole.
The overall logic of your workflow is based on the shape of the graph. An Airflow DAG can have multiple branches, and you can choose which ones to follow and which to skip during workflow execution.
Airflow Pipeline DAG
Airflow could be completely stopped, and able to run workflows would then resume through restarting the last unfinished task.
It is important to remember that airflow operators can be run more than once when designing airflow operators. Each task should be idempotent, or capable of being performed multiple times without causing unintended consequences.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-airflow-architecture-and-its-components">Explain Airflow Architecture and its components?<a class="hash-link" href="#explain-airflow-architecture-and-its-components" title="Direct link to heading">​</a></h4><p>There are four major components to airflow.</p><ul><li>Webserver<ul><li>This is the Airflow UI built on the Flask, which provides an overview of the overall health of various DAGs and helps visualise various components and states of every DAG. For the Airflow setup, the Web Server also allows you to manage users, roles, and different configurations.</li></ul></li><li>Scheduler<ul><li>Every n seconds, the scheduler walks over the DAGs and schedules the task to be executed.Executor</li></ul></li><li>Executor is another internal component of the scheduler.<ul><li>The executors are the components that actually execute the tasks, while the Scheduler orchestrates them. Airflow has different types of executors, including SequentialExecutor, LocalExecutor, CeleryExecutor and KubernetesExecutor. People generally choose the executor which is best for their use case.</li></ul></li><li>Worker<ul><li>Workers are responsible to run the task that the executor has given them.</li></ul></li><li>Metadata Database
Airflow supports a wide range of metadata storage databases. This database contains information about DAGs, their runs, and other Airflow configurations such as users, roles, and connections.
The DAGs&#x27; states and runs are shown by the Web Server from the database. This information is also updated in the metadata database by the Scheduler.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-types-of-executors-in-airflow">What are the types of Executors in Airflow?<a class="hash-link" href="#what-are-the-types-of-executors-in-airflow" title="Direct link to heading">​</a></h4><p>The executors are the components that actually execute the tasks, while the Scheduler orchestrates them. Airflow has different types of executors, including SequentialExecutor, LocalExecutor, CeleryExecutor and KubernetesExecutor. People generally choose the executor which is best for their use case.
Types of Executor</p><ul><li>SequentialExecutor<ul><li>Only one task is executed at a time by SequentialExecutor. The scheduler and the workers both use the same machine.</li></ul></li><li>LocalExecutor<ul><li>LocalExecutor is the same as the Sequential Executor, except it can run multiple tasks at a time.</li></ul></li><li>CeleryExecutor<ul><li>Celery is a Python framework for running distributed asynchronous tasks.
As a result, CeleryExecutor has long been a part of Airflow, even before Kubernetes.
CeleryExecutors has a fixed number of workers on standby to take on tasks when they become available.</li></ul></li><li>KubernetesExecutor<ul><li>Each task is run by KubernetesExecutor in its own Kubernetes pod. It, unlike Celery, spins up worker pods on demand, allowing for the most efficient use of resources.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-sequentialexecutor">What are the pros and cons of SequentialExecutor?<a class="hash-link" href="#what-are-the-pros-and-cons-of-sequentialexecutor" title="Direct link to heading">​</a></h4><p>Pros:</p><ul><li>It&#x27;s simple and straightforward to set up.</li><li>It&#x27;s a good way to test DAGs while they&#x27;re being developed.
Pros:
It isn&#x27;t scalable.
It is not possible to perform many tasks at the same time.
Unsuitable for use in production</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-localexecutor">What are the pros and cons of LocalExecutor?<a class="hash-link" href="#what-are-the-pros-and-cons-of-localexecutor" title="Direct link to heading">​</a></h4><p>Pros:</p><ul><li>Able to perform multiple tasks.</li><li>Can be used to run DAGs during development.
Cons:</li><li>The product isn&#x27;t scalable.</li><li>There is only one point of failure.</li><li>Unsuitable for use in production.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-celeryexecutor">What are the pros and cons of CeleryExecutor?<a class="hash-link" href="#what-are-the-pros-and-cons-of-celeryexecutor" title="Direct link to heading">​</a></h4><p>Pros:</p><ul><li>It allows for scalability.</li><li>Celery is responsible for managing the workers. Celery creates a new one in the case of a failure.
Cons:</li><li>Celery requires RabbitMQ/Redis for task queuing, which is redundant with what Airflow already supports.</li><li>The setup is also complicated due to the above-mentioned dependencies.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-pros-and-cons-of-kubernetesexecutor">What are the pros and cons of KubernetesExecutor?<a class="hash-link" href="#what-are-the-pros-and-cons-of-kubernetesexecutor" title="Direct link to heading">​</a></h4><p>Pros:
It combines the benefits of CeleryExecutor and LocalExecutor in terms of scalability and simplicity.
Fine-grained control over task-allocation resources. At the task level, the amount of CPU/memory needed can be configured.
Cons:
Airflow is newer to Kubernetes, and the documentation is complicated.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-define-a-workflow-in-airflow">How to define a workflow in Airflow?<a class="hash-link" href="#how-to-define-a-workflow-in-airflow" title="Direct link to heading">​</a></h4><p>Python files are used to define workflows.
DAG (Directed Acyclic Graph)
The DAG Python class in Airflow allows you to generate a Directed Acyclic Graph, which is a representation of the workflow.
from Airflow.models import DAG
from airflow.utils.dates import days_ago</p><p>args = {
&#x27;start_date&#x27;: days_ago(0),
}</p><p>dag = DAG(
dag_id=&#x27;bash_operator_example&#x27;,
default_args=args,
schedule_interval=&#x27;* * * * <em>&#x27;,
)
You can use the start date to launch a task on a specific date.
The schedule interval specifies how often each workflow is scheduled to run. &#x27;</em> * * * *&#x27; indicates that the tasks must run every minute.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-make-the-module-available-to-airflow-if-youre-using-docker-compose">How do you make the module available to airflow if you&#x27;re using Docker Compose?<a class="hash-link" href="#how-do-you-make-the-module-available-to-airflow-if-youre-using-docker-compose" title="Direct link to heading">​</a></h4><p>If we are using Docker Compose, then we will need to use a custom image with our own additional dependencies in order to make the module available to Airflow. Refer to the following Airflow Documentation for reasons why we need it and how to do it.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-schedule-dag-in-airflow">How to schedule DAG in Airflow?<a class="hash-link" href="#how-to-schedule-dag-in-airflow" title="Direct link to heading">​</a></h4><p>DAGs could be scheduled by passing a timedelta or a cron expression (or one of the @ presets), which works well enough for DAGs that need to run on a regular basis, but there are many more use cases that are presently difficult to express &quot;natively&quot; in Airflow, or that require some complicated workarounds. You can refer Airflow Improvements Proposals (AIP).
Simply use the following command to start a scheduler:</p><ul><li>airflow scheduler</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-xcoms-in-airflow">What is XComs In Airflow?<a class="hash-link" href="#what-is-xcoms-in-airflow" title="Direct link to heading">​</a></h4><p>XCom (short for cross-communication) are messages that allow data to be sent between tasks. The key, value, timestamp, and task/DAG id are all defined.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-xcom_pull-in-xcom-airflow">What is xcom_pull in XCom Airflow?<a class="hash-link" href="#what-is-xcom_pull-in-xcom-airflow" title="Direct link to heading">​</a></h4><p>The xcom push and xcom pull methods on Task Instances are used to explicitly &quot;push&quot; and &quot;pull&quot; XComs to and from their storage. Whereas if do xcom push parameter is set to True (as it is by default), many operators and @task functions will auto-push their results into an XCom key named return value.
If no key is supplied to xcom pull, it will use this key by default, allowing you to write code like this:
Pulls the return_value XCOM from &quot;pushing_task&quot;
value = task_instance.xcom_pull(task_ids=&#x27;pushing_task&#x27;)</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-jinja-templates">What is Jinja templates?<a class="hash-link" href="#what-is-jinja-templates" title="Direct link to heading">​</a></h4><p>Jinja is a templating engine that is quick, expressive, and extendable. The template has special placeholders that allow you to write code that looks like Python syntax. After that, data is passed to the template in order to render the final document.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-use-airflow-xcoms-in-jinja-templates">How to use Airflow XComs in Jinja templates?<a class="hash-link" href="#how-to-use-airflow-xcoms-in-jinja-templates" title="Direct link to heading">​</a></h4><p>We can use XComs in Jinja templates as given below:</p><ul><li>SELECT * FROM {{ task_instance.xcom_pull(task_ids=&#x27;foo&#x27;, key=&#x27;table_name&#x27;) }}</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="aws">AWS<a class="hash-link" href="#aws" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-ec2">What is EC2?<a class="hash-link" href="#what-is-ec2" title="Direct link to heading">​</a></h4><p>EC2, a Virtual Machine in the cloud on which you have OS-level control. You can run this cloud server whenever you want and can be used when you need to deploy your own servers in the cloud, similar to your on-premises servers, and when you want to have full control over the choice of hardware and the updates on the machine.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-snowball">What is SnowBall?<a class="hash-link" href="#what-is-snowball" title="Direct link to heading">​</a></h4><p>SnowBall is a small application that enables you to transfer terabytes of data inside and outside of the AWS environment.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-cloudwatch">What is CloudWatch?<a class="hash-link" href="#what-is-cloudwatch" title="Direct link to heading">​</a></h4><p>CloudWatch helps you to monitor AWS environments like EC2, RDS Instances, and CPU utilization. It also triggers alarms depending on various metrics.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-elastic-transcoder">What is Elastic Transcoder?<a class="hash-link" href="#what-is-elastic-transcoder" title="Direct link to heading">​</a></h4><p>Elastic Transcoder is an AWS Service Tool that helps you in changing a video’s format and resolution to support various devices like tablets, smartphones, and laptops of different resolutions.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-vpc">What do you understand by VPC?<a class="hash-link" href="#what-do-you-understand-by-vpc" title="Direct link to heading">​</a></h4><p>VPC stands for Virtual Private Cloud. It allows you to customize your networking configuration. VPC is a network that is logically isolated from other networks in the cloud. It allows you to have your private IP Address range, internet gateways, subnets, and security groups.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dns-and-load-balancer-services-come-under-which-type-of-cloud-service">DNS and Load Balancer Services come under which type of Cloud Service?<a class="hash-link" href="#dns-and-load-balancer-services-come-under-which-type-of-cloud-service" title="Direct link to heading">​</a></h4><p>DNS and Load Balancer are a part of IaaS-Storage Cloud Service.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-storage-classes-available-in-amazon-s3">What are the Storage Classes available in Amazon S3?<a class="hash-link" href="#what-are-the-storage-classes-available-in-amazon-s3" title="Direct link to heading">​</a></h4><p>Storage Classes available with Amazon S3 are:</p><ul><li>Amazon S3 Standard</li><li>Amazon S3 Standard-Infrequent Access</li><li>Amazon S3 Reduced Redundancy Storage</li><li>Amazon Glacier</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-t2-instances-are">Explain what T2 instances are?<a class="hash-link" href="#explain-what-t2-instances-are" title="Direct link to heading">​</a></h4><p>T2 Instances are designed to provide moderate baseline performance and the capability to burst to higher performance as required by the workload.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-key-pairs-in-aws">What are Key-Pairs in AWS?<a class="hash-link" href="#what-are-key-pairs-in-aws" title="Direct link to heading">​</a></h4><p>Key-Pairs are secure login information for your Virtual Machines. To connect to the instances, you can use Key-Pairs which contain a Public Key and a Private Key.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-many-subnets-can-you-have-per-vpc">How many Subnets can you have per VPC?<a class="hash-link" href="#how-many-subnets-can-you-have-per-vpc" title="Direct link to heading">​</a></h4><p>You can have 200 Subnets per VPC.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-different-types-of-cloud-services">List different types of Cloud Services.<a class="hash-link" href="#list-different-types-of-cloud-services" title="Direct link to heading">​</a></h4><p>Different types of Cloud Services are:</p><ul><li>Software as a Service (SaaS)</li><li>Data as a Service (DaaS)</li><li>Platform as a Service (PaaS)</li><li>Infrastructure as a Service (IaaS)</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-s3-is">Explain what S3 is?<a class="hash-link" href="#explain-what-s3-is" title="Direct link to heading">​</a></h4><p>S3 stands for Simple Storage Service. You can use the S3 interface to store and retrieve any amount of data, at any time and from anywhere on the web. For S3, the payment model is “pay as you go”.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-amazon-route-53-provide-high-availability-and-low-latency">How does Amazon Route 53 provide high availability and low latency?<a class="hash-link" href="#how-does-amazon-route-53-provide-high-availability-and-low-latency" title="Direct link to heading">​</a></h4><p>Amazon Route 53 uses the following to provide high availability and low latency:</p><ul><li>Globally Distributed Servers - Amazon is a global service and consequently has DNS Servers globally. Any customer creating a query from any part of the world gets to reach a DNS Server local to them that provides low latency.</li><li>Dependency - Route 53 provides a high level of dependability required by critical applications.</li><li>Optimal Locations - Route 53 serves the requests from the nearest data center to the client sending the request. AWS has data-centers across the world. The data can be cached on different data-centers located in different regions of the world depending on the requirements and the configuration chosen. Route 53 enables any server in any data-center which has the required data to respond. This way, it enables the nearest server to serve the client request, thus reducing the time taken to serve.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-send-a-request-to-amazon-s3">How can you send a request to Amazon S3?<a class="hash-link" href="#how-can-you-send-a-request-to-amazon-s3" title="Direct link to heading">​</a></h4><p>Amazon S3 is a REST Service, and you can send a request by using the REST API or the AWS SDK wrapper libraries that wrap the underlying Amazon S3 REST API.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-ami-include">What does AMI include?<a class="hash-link" href="#what-does-ami-include" title="Direct link to heading">​</a></h4><p>An AMI includes the following things:</p><ul><li>A template for the root volume for the instance.</li><li>Launch permissions to decide which AWS accounts can avail the AMI to launch instances.</li><li>A block device mapping that determines the volumes to attach to the instance when it is launched.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-types-of-instances">What are the different types of Instances?<a class="hash-link" href="#what-are-the-different-types-of-instances" title="Direct link to heading">​</a></h4><ul><li>Following are the types of instances:</li><li>Compute Optimized</li><li>Memory-Optimized</li><li>Storage Optimized</li><li>Accelerated Computing</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-relation-between-the-availability-zone-and-region">What is the relation between the Availability Zone and Region?<a class="hash-link" href="#what-is-the-relation-between-the-availability-zone-and-region" title="Direct link to heading">​</a></h4><p>An AWS Availability Zone is a physical location where an Amazon data center is located. On the other hand, an AWS Region is a collection or group of Availability Zones or Data Centers.
This setup helps your services to be more available as you can place your VMs in different data centers within an AWS Region. If one of the data centers fails in a Region, the client requests still get served from the other data centers located in the same Region. This arrangement, thus, helps your service to be available even if a Data Center goes down.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-monitor-amazon-vpc">How do you monitor Amazon VPC?<a class="hash-link" href="#how-do-you-monitor-amazon-vpc" title="Direct link to heading">​</a></h4><p>You can monitor Amazon VPC using:</p><ul><li>CloudWatch</li><li>VPC Flow Logs</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-types-of-ec2-instances-based-on-their-costs">What are the different types of EC2 instances based on their costs?<a class="hash-link" href="#what-are-the-different-types-of-ec2-instances-based-on-their-costs" title="Direct link to heading">​</a></h4><p>The three types of EC2 instances based on the costs are:</p><ul><li>On-Demand Instance - These instances are prepared as and when needed. Whenever you feel the need for a new EC2 instance, you can go ahead and create an on-demand instance. It is cheap for the short-time but not when taken for the long term.</li><li>Spot Instance - These types of instances can be bought through the bidding model. These are comparatively cheaper than On-Demand Instances.</li><li>Reserved Instance - On AWS, you can create instances that you can reserve for a year or so. These types of instances are especially useful when you know in advance that you will be needing an instance for the long term. In such cases, you can create a reserved instance and save heavily on costs.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-stopping-and-terminating-an-ec2-instance">What do you understand by stopping and terminating an EC2 Instance?<a class="hash-link" href="#what-do-you-understand-by-stopping-and-terminating-an-ec2-instance" title="Direct link to heading">​</a></h4><p>Stopping an EC2 instance means to shut it down as you would normally do on your Personal Computer. This will not delete any volumes attached to the instance and the instance can be started again when needed.
On the other hand, terminating an instance is equivalent to deleting an instance. All the volumes attached to the instance get deleted and it is not possible to restart the instance if needed at a later point in time.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-consistency-models-for-modern-dbs-offered-by-aws">What are the consistency models for modern DBs offered by AWS?<a class="hash-link" href="#what-are-the-consistency-models-for-modern-dbs-offered-by-aws" title="Direct link to heading">​</a></h4><p>Eventual Consistency - It means that the data will be consistent eventually, but may not be immediate. This will serve the client requests faster, but chances are that some of the initial read requests may read the stale data. This type of consistency is preferred in systems where data need not be real-time. For example, if you don’t see the recent tweets on Twitter or recent posts on Facebook for a couple of seconds, it is acceptable.
Strong Consistency - It provides an immediate consistency where the data will be consistent across all the DB Servers immediately. Accordingly. This model may take some time to make the data consistent and subsequently start serving the requests again. However, in this model, it is guaranteed that all the responses will always have consistent data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-geo-targeting-in-cloudfront">What is Geo-Targeting in CloudFront?<a class="hash-link" href="#what-is-geo-targeting-in-cloudfront" title="Direct link to heading">​</a></h4><p>Geo-Targeting enables the creation of customized content based on the geographic location of the user. This allows you to serve the content which is more relevant to a user. For example, using Geo-Targeting, you can show the news related to local body elections to a user sitting in India, which you may not want to show to a user sitting in the US. Similarly, the news related to Baseball Tournament can be more relevant to a user sitting in the US, and not so relevant for a user sitting in India.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-advantages-of-aws-iam">What are the advantages of AWS IAM?<a class="hash-link" href="#what-are-the-advantages-of-aws-iam" title="Direct link to heading">​</a></h4><p>AWS IAM enables an administrator to provide granular level access to different users and groups. Different users and user groups may need different levels of access to different resources created. With IAM, you can create roles with specific access-levels and assign the roles to the users.
It also allows you to provide access to the resources to users and applications without creating the IAM Roles, which is known as Federated Access.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-a-security-group">What do you understand by a Security Group?<a class="hash-link" href="#what-do-you-understand-by-a-security-group" title="Direct link to heading">​</a></h4><p>When you create an instance in AWS, you may or may not want that instance to be accessible from the public network. Moreover, you may want that instance to be accessible from some networks and not from others.
Security Groups are a type of rule-based Virtual Firewall using which you can control access to your instances. You can create rules defining the Port Numbers, Networks, or protocols from which you want to allow access or deny access.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-spot-instances-and-on-demand-instances">What are Spot Instances and On-Demand Instances?<a class="hash-link" href="#what-are-spot-instances-and-on-demand-instances" title="Direct link to heading">​</a></h4><p>When AWS creates EC2 instances, there are some blocks of computing capacity and processing power left unused. AWS releases these blocks as Spot Instances. Spot Instances run whenever capacity is available. These are a good option if you are flexible about when your applications can run and if your applications can be interrupted.
On the other hand, On-Demand Instances can be created as and when needed. The prices of such instances are static. Such instances will always be available unless you explicitly terminate them.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-connection-draining">Explain Connection Draining.<a class="hash-link" href="#explain-connection-draining" title="Direct link to heading">​</a></h4><p>Connection Draining is a feature provided by AWS which enables your servers which are either going to be updated or removed, to serve the current requests.
If Connection Draining is enabled, the Load Balancer will allow an outgoing instance to complete the current requests for a specific period but will not send any new request to it. Without Connection Draining, an outgoing instance will immediately go off and the requests pending on that instance will error out.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-stateful-and-a-stateless-firewall">What is a Stateful and a Stateless Firewall?<a class="hash-link" href="#what-is-a-stateful-and-a-stateless-firewall" title="Direct link to heading">​</a></h4><p>A Stateful Firewall is the one that maintains the state of the rules defined. It requires you to define only inbound rules. Based on the inbound rules defined, it automatically allows the outbound traffic to flow.
On the other hand, a Stateless Firewall requires you to explicitly define rules for inbound as well as outbound traffic.
For example, if you allow inbound traffic from Port 80, a Stateful Firewall will allow outbound traffic to Port 80, but a Stateless Firewall will not do so.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-power-user-access-in-aws">What is a Power User Access in AWS?<a class="hash-link" href="#what-is-a-power-user-access-in-aws" title="Direct link to heading">​</a></h4><p>An Administrator User will be similar to the owner of the AWS Resources. He can create, delete, modify or view the resources and also grant permissions to other users for the AWS Resources.
A Power User Access provides Administrator Access without the capability to manage the users and permissions. In other words, a user with Power User Access can create, delete, modify or see the resources, but he cannot grant permissions to other users.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-an-instance-store-volume-and-an-ebs-volume">What is an Instance Store Volume and an EBS Volume?<a class="hash-link" href="#what-is-an-instance-store-volume-and-an-ebs-volume" title="Direct link to heading">​</a></h4><p>An Instance Store Volume is temporary storage that is used to store the temporary data required by an instance to function. The data is available as long as the instance is running. As soon as the instance is turned off, the Instance Store Volume gets removed and the data gets deleted.
On the other hand, an EBS Volume represents a persistent storage disk. The data stored in an EBS Volume will be available even after the instance is turned off.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-recovery-time-objective-and-recovery-point-objective-in-aws">What are Recovery Time Objective and Recovery Point Objective in AWS?<a class="hash-link" href="#what-are-recovery-time-objective-and-recovery-point-objective-in-aws" title="Direct link to heading">​</a></h4><p>Recovery Time Objective - It is the maximum acceptable delay between the interruption of service and restoration of service. This translates to an acceptable time window when the service can be unavailable.
Recover Point Objective - It is the maximum acceptable amount of time since the last data restore point. It translates to the acceptable amount of data loss which lies between the last recovery point and the interruption of service.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-there-a-way-to-upload-a-file-that-is-greater-than-100-megabytes-in-amazon-s3">Is there a way to upload a file that is greater than 100 Megabytes in Amazon S3?<a class="hash-link" href="#is-there-a-way-to-upload-a-file-that-is-greater-than-100-megabytes-in-amazon-s3" title="Direct link to heading">​</a></h4><p>Yes, it is possible by using the Multipart Upload Utility from AWS. With the Multipart Upload Utility, larger files can be uploaded in multiple parts that are uploaded independently. You can also decrease upload time by uploading these parts in parallel. After the upload is done, the parts are merged into a single object or file to create the original file from which the parts were created.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-change-the-private-ip-address-of-an-ec2-instance-while-it-is-running-or-in-a-stopped-state">Can you change the Private IP Address of an EC2 instance while it is running or in a stopped state?<a class="hash-link" href="#can-you-change-the-private-ip-address-of-an-ec2-instance-while-it-is-running-or-in-a-stopped-state" title="Direct link to heading">​</a></h4><p>No, a Private IP Address of an EC2 instance cannot be changed. When an EC2 instance is launched, a private IP Address is assigned to that instance at the boot time. This private IP Address is attached to the instance for its entire lifetime and can never be changed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-lifecycle-hooks-is-autoscaling">What is the use of lifecycle hooks is Autoscaling?<a class="hash-link" href="#what-is-the-use-of-lifecycle-hooks-is-autoscaling" title="Direct link to heading">​</a></h4><p>Lifecycle hooks are used for Auto-scaling to put an additional wait time to a scale-in or a scale-out event.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-policies-that-you-can-set-for-your-users-passwords">What are the policies that you can set for your user’s passwords?<a class="hash-link" href="#what-are-the-policies-that-you-can-set-for-your-users-passwords" title="Direct link to heading">​</a></h4><p>Following are the policies that can be set for user’s passwords:</p><ul><li>You can set a minimum length of the password.</li><li>You can ask the users to add at least one number or special character to the password.</li><li>Assigning the requirements of particular character types, including uppercase letters, lowercase letters, numbers, and non-alphanumeric characters.</li><li>You can enforce automatic password expiration, prevent the reuse of old passwords, and request for a password reset upon their next AWS sign-in.</li><li>You can have the AWS users contact an account administrator when the user has allowed the password to expire.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-tou-know-about-the-amazon-database">What do tou know about the Amazon Database?<a class="hash-link" href="#what-do-tou-know-about-the-amazon-database" title="Direct link to heading">​</a></h4><p>Amazon database is one of the Amazon Web Services that offers managed database along with managed service and NoSQL. It is also a fully managed petabyte-scale data warehouse service and in-memory caching as a service. There are four AWS database services, the user can choose to use one or multiple that meet the requirements. Amazon database services are – DynamoDB, RDS, RedShift, and Elastic ache.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-amazon-relational-database">Explain Amazon Relational Database?<a class="hash-link" href="#explain-amazon-relational-database" title="Direct link to heading">​</a></h4><p>Amazon relational database is a service that helps users with a number of services such as operation, lining up, and scaling an on-line database within the cloud. It automates the admin tasks such as info setup, hardware provisioning, backups, and mending. Amazon relational database provides users with resizable and cost-effective capability. By automating the tasks, it saves time and thus let user concentrate on the applications and provide them high availableness, quick performance, compatibility, and security.
There are a number of AWS RDS engines, such as:</p><ul><li>Mysql</li><li>Oracle</li><li>PostgreSQL</li><li>SQL Server</li><li>MariaDB</li><li>Amazon Aurora</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-features-of-amazon-database">What are the Features of Amazon Database?<a class="hash-link" href="#what-are-the-features-of-amazon-database" title="Direct link to heading">​</a></h4><p>Following are the important features of Amazon Database:</p><ul><li>Easy to administer</li><li>Highly scalable</li><li>Durable and reliable</li><li>Faster performance</li><li>Highly available</li><li>More secure</li><li>Cost-effective</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-of-the-aws-db-service-is-a-nosql-database-and-serverless-and-delivers-consistent-singledigit-millisecond-latency-at-any-scale">Which of the Aws Db Service is a Nosql Database and Serverless and Delivers Consistent singledigit Millisecond Latency at any scale?<a class="hash-link" href="#which-of-the-aws-db-service-is-a-nosql-database-and-serverless-and-delivers-consistent-singledigit-millisecond-latency-at-any-scale" title="Direct link to heading">​</a></h4><p>Amazon DynamoDB.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is--key-value-store">What is  Key Value Store?<a class="hash-link" href="#what-is--key-value-store" title="Direct link to heading">​</a></h4><p>Key-value store is a database service which facilitates the storing, updating, and querying of the objects which are generally identified with the key and values. These objects consist of the keys and values which constitutes the actual content that is stored.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-dynamodb">What is Dynamodb?<a class="hash-link" href="#what-is-dynamodb" title="Direct link to heading">​</a></h4><p>DynamoDB is a NoSQL database service that provides an inevitable and faster performance. DynamoDB is superintendent and offers a high level of scalability. DynamoDB makes users not to worry about the configuration, setup, hardware provisioning, throughput capacity, replication, software patching or cluster scaling. It helps users in offloading the scaling and operating distributed databases to AWS.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-of-the-benefits-of-using-amazon-dynamodb">List of the benefits of using Amazon Dynamodb?<a class="hash-link" href="#list-of-the-benefits-of-using-amazon-dynamodb" title="Direct link to heading">​</a></h4><p>Amazon DynamoDB is the NoSQL service that provides a number of benefits to the users.
Some benefits of AWS DynamoDB are:</p><ul><li>Being a self-managed service, DynamoDB doesn’t require the experts for setup, installation, cluster etc.</li><li>It provides inevitable and faster performance.</li><li>It is highly scalable, available, and durable.</li><li>It provides very high throughput at the low latency.</li><li>It is highly cost-effective.</li><li>It supports and allows the creation of dynamic tables with multi-values attributes i.e. it’s flexible in nature.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-dynamodbmapper-class">What is a Dynamodbmapper Class?<a class="hash-link" href="#what-is-a-dynamodbmapper-class" title="Direct link to heading">​</a></h4><p>The mapper class is the entry point of the DynamoDB. It allows users to enter the DynamoDB and access the endpoint. DynamoDB mapper class helps users access the data stored in various tables, then execute queries, scan them against the tables, and perform CRUD operations on the data items.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-data-types-supported-by-dynamodb">What are the Data Types supported by Dynamodb?<a class="hash-link" href="#what-are-the-data-types-supported-by-dynamodb" title="Direct link to heading">​</a></h4><p>DynamoDB supports different types of data types such as collection data types, scalar data types, and even null values.
Scalar Data Types – The scalar data types supported by DynamoDB are:</p><ul><li>Binary</li><li>Number</li><li>Boolean</li><li>String</li></ul><p>Collection Data Types – The collection data types supported by DynamoDB are:</p><ul><li>Binary Set</li><li>Number Set</li><li>String Set</li><li>Heterogeneous Map</li><li>Heterogeneous List</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-dynamodb-auto-scaling">What do you understand by Dynamodb Auto Scaling?<a class="hash-link" href="#what-do-you-understand-by-dynamodb-auto-scaling" title="Direct link to heading">​</a></h4><p>DynamoDB Auto scaling specifies its specialized feature to automatically scale up and down its own read and write capacity or global secondary index.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-data-warehouse-and-how-aws-redshift-can-play-a-vital-role-in-the-storage">What is a Data Warehouse and how Aws Redshift can play a vital role in the Storage?<a class="hash-link" href="#what-is-a-data-warehouse-and-how-aws-redshift-can-play-a-vital-role-in-the-storage" title="Direct link to heading">​</a></h4><p>A data warehouse can be thought of a repository where the data generated from the company’s systems and other sources is collected and stored. So a data warehouse has three-tier architecture:
In the bottom tier, we have the tools which cleanse and collect the data.
In the middle tier, we have tools which transform the data using Online Analytical Processing Server.
In the top tier, we have different tools where data analysis and data mining is performed at the front end.
Setting up and managing a data warehouse involves a lot of money as the data in an organization continuously increases and the organization has to continuously upgrade their data storage servers. So here AWS RedShift comes into existence where the companies store their data in the cloud-based warehouses provided by Amazon.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-amazon-redshift-and-why-is-it-popular-among-other-cloud-data-warehouses">What is Amazon Redshift and why is it popular among other Cloud Data Warehouses?<a class="hash-link" href="#what-is-amazon-redshift-and-why-is-it-popular-among-other-cloud-data-warehouses" title="Direct link to heading">​</a></h4><p>Amazon RedShift is a fast and scalable data warehouse which is easy to use and is cost-effective to manage all the organization’s data. The database is ranged from gigabytes to 100’s of petabyte of cloud data storage. A person does not need knowledge of any programming language to use this feature, just upload the cluster and tools which are already known to the user he can start using RedShift.
AWS RedShift is popular due to the following reasons:</p><ul><li>AWS RedShift is very easy to use: In the console of AWS RedShift, you will find an option of creating a cluster. Just click on that and leave the rest on the machine programming of RedShift. Just fill the correct details as asked and launch the cluster. Now the cluster is ready to be used as RedShift automated most of the task like managing, monitoring and scaling.</li><li>Scaling of Warehouse is very easy: You just have to resize the cluster size by increasing the number of compute nodes.</li><li>RedShift gives 10x times better and fast performance: It makes use of specific strategies like columnar storage and massive parallel processing strategies to deliver high throughput and response time.</li><li>Economical: As it does not require any setup so cost reduces down to 1/10th of the traditional data warehouse.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-redshift-spectrum">What is Redshift Spectrum?<a class="hash-link" href="#what-is-redshift-spectrum" title="Direct link to heading">​</a></h4><p>The RedShift Spectrum allows you to run queries alongside petabyte of data which is unstructured and that too with no requirement of loading ETL. Spectrum scales millions of queries and allows you to allocate and store the data wherever you want and whatever the type of format is suitable for you.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-leader-node-and-compute-node">What is a Leader Node and Compute Node?<a class="hash-link" href="#what-is-a-leader-node-and-compute-node" title="Direct link to heading">​</a></h4><p>In a leader node the queries from the client application are received and then the queries are parsed and the execution plan is developed. The steps to process these queries are developed and the result is sent back to the client application.
In a compute node the steps assigned in the leader node are executed and the data is transmitted. The result is then sent back to the leader node before sending it to the client application.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-load-data-iin-amazon-redshift">How to load data iIn Amazon Redshift?<a class="hash-link" href="#how-to-load-data-iin-amazon-redshift" title="Direct link to heading">​</a></h4><p>Amazon DynamoDB, Amazon EMR, AWS Glue, AWS Data Pipeline are some of the data sources by which you can load data in RedShift data warehouse. The clients can also connect to RedShift with the help of ODBC or JDBC and give the SQL command insert to load the data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-the-database-engines-which-are-supported-by-amazon-rds">Mention the database engines which are supported by Amazon Rds?<a class="hash-link" href="#mention-the-database-engines-which-are-supported-by-amazon-rds" title="Direct link to heading">​</a></h4><p>The database engines that are supported by Amazon RDS are Amazon Aurora, Mysql, MariaDB, Oracle, SQL Server, and PostgreSQL database engine.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-work-of-amazon-rds">What is the work of Amazon Rds?<a class="hash-link" href="#what-is-the-work-of-amazon-rds" title="Direct link to heading">​</a></h4><p>When a user wants to set up a relational database then Amazon RDS is used. It provisions the infrastructure capacity that a user requests to install the database software. Once the database is set up and functional RDS automates the tasks like patching up of the software, taking the backup of the data and management of synchronous data replication with automatic failover.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-purpose-of-standby-rds-instance">What is the purpose of standby Rds Instance?<a class="hash-link" href="#what-is-the-purpose-of-standby-rds-instance" title="Direct link to heading">​</a></h4><p>The main purpose of launching a standby RDS instance is to prevent the infrastructure failure (in case failure occurs) so it is stored in a different availability zone which is a totally different infrastructure physically and independent.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="are-rds-instances-upgradable-or-down-gradable-according-to-the-need">Are Rds instances upgradable or down gradable according to the Need?<a class="hash-link" href="#are-rds-instances-upgradable-or-down-gradable-according-to-the-need" title="Direct link to heading">​</a></h4><p>Yes, you can upgrade the RDS instances with the help of following command: modify-db-instance. If you are unable to detect the amount of CPU needed to upgrade then start with db.m1.small DB instance class and monitor the utilization of CPU with the help of tool Amazon Cloud Watch Service.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-amazon-elastic-ache">What is Amazon Elastic Ache?<a class="hash-link" href="#what-is-amazon-elastic-ache" title="Direct link to heading">​</a></h4><p>Amazon Elastic ache is an in-memory key-value store which is capable of supporting two key-value engines – Redis and Memcached. It is a fully managed and zero administrations which are hardened by Amazon. With the help of Amazon Elastic ache, you can either build a new high-performance application or improve the existing application. You can find the various application of Elastic ache in the field of Gaming, Healthcare, etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-amazon-elastic-ache">What is the use of Amazon Elastic Ache?<a class="hash-link" href="#what-is-the-use-of-amazon-elastic-ache" title="Direct link to heading">​</a></h4><p>The performance of web applications could be improved with the help of the caching of information that is used again and again. The information can be accessed very fast using in-memory-caching. With Elastic ache there is no need of managing a separate caching server. You can easily deploy or run an open source compatible in-memory data source with high throughput and low latency.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-benefits-of-amazon-elastic-ache">What are the Benefits of Amazon Elastic Ache?<a class="hash-link" href="#what-are-the-benefits-of-amazon-elastic-ache" title="Direct link to heading">​</a></h4><p>There are various benefits of using Amazon Elastic ache some of which are discussed below:</p><ul><li>The cache node failures are automatically detected and recovered.</li><li>It can be easily integrated with other AWS to provide a high performance and secured in-memory cache.</li><li>As most of the data is managed by Elastic ache such as setup, configuration, and monitoring so that the user can focus on other high-value applications.</li><li>The performance is enhanced greatly as it only supports the applications which require a very less response time.</li><li>The Elastic ache can easily scale itself up or scale down according to the need.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-types-of-engines-in-elastic-ache">Explain the Types of Engines in Elastic Ache?<a class="hash-link" href="#explain-the-types-of-engines-in-elastic-ache" title="Direct link to heading">​</a></h4><p>There is two type of engine supported in Elastic ache: Memcached and Redis.</p><ul><li>Memcached:<ul><li>It is a popular in-memory data store which the developers use for the high-performance cache to speed up applications. By storing the data in memory instead of disk Memcached can retrieve the data in less than a millisecond. It works by keeping every value of the key for every other data to be stored and uniquely identifies each data and lets Memcached quickly find the record.</li></ul></li><li>Redis:<ul><li>Today’s applications need low latency and high throughput performance for real-time processing. Due to the performance, simplicity, and capability of redis, it is most favored by the developers. It provides high performance for real-time apps and sub-millisecond latency. It supports complex data types i.n. string, hashes, etc and has a backup and restore capabilities. While Memcached supports key names and values up to 1 MB only redis supports up to 512 MB.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-possible-to-run-multiple-db-instances-for-free-for-amazon-rds">Is it possible to run Multiple Db Instances for free for Amazon Rds?<a class="hash-link" href="#is-it-possible-to-run-multiple-db-instances-for-free-for-amazon-rds" title="Direct link to heading">​</a></h4><p>Yes, it is possible to run more than one Single-AZ micro DB instance for Amazon RDS and that’s for free. However, if the usage exceeds 750 instance hours across all the RDS Single-AZ micro DB instances, billing will be done at the standard Amazon RDS pricing across all the regions and database engines.
For example, consider we are running 2 Single-AZ micro DB instances for 400 hours each in one month only; the accumulated usage will be 800 instance hours from which 750 instance hours will be free. In this case, you will be billed for the remaining 50 hours at the standard pricing of Amazon RDS.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-aws-services-will-you-choose-for-collecting-and-processing-ecommerce-data-for-realtime-analysis">Which Aws Services will you choose for collecting and processing Ecommerce Data for Realtime Analysis?<a class="hash-link" href="#which-aws-services-will-you-choose-for-collecting-and-processing-ecommerce-data-for-realtime-analysis" title="Direct link to heading">​</a></h4><p>I’ll use DynamoDB for collecting and processing e-commerce data for real-time analysis. DynamoDB is a fully managed NoSQL database service that can be used for any type of unstructured data. It can even be used for the e-commerce data taken from e-commerce websites. On this retrieved e-commerce data, analysis can be then performed using RedShift. Elastic MapReduce can also be used for analysis but we’ll avoid it here as real-time analysis if required.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-will-happen-to-the-db-snapshots-and-backups-if-any-user-deletes-db-instance">What will happen to the Db Snapshots and Backups if any user deletes Db Instance?<a class="hash-link" href="#what-will-happen-to-the-db-snapshots-and-backups-if-any-user-deletes-db-instance" title="Direct link to heading">​</a></h4><p>When a dB instance is deleted, the user receives an option of making a final dB snapshot. If you do that it will restore your information from that snapshot. AWS RDS keeps all these dB snapshots together that are created by the user along with the all other manually created dB snapshots when the dB instance is deleted. At the same time, automated backups are deleted while manually created dB snapshots are preserved.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="cassandra">Cassandra<a class="hash-link" href="#cassandra" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-cassandra">Explain what is Cassandra?<a class="hash-link" href="#explain-what-is-cassandra" title="Direct link to heading">​</a></h4><p>Cassandra is an open source data storage system developed at Facebook for inbox search and designed for storing and managing large amounts of data across commodity servers. It can server as both Real time data store system for online applications Also as a read intensive database for business intelligence system</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-the-benefits-of-using-cassandra">List the benefits of using Cassandra?<a class="hash-link" href="#list-the-benefits-of-using-cassandra" title="Direct link to heading">​</a></h4><p>Unlike traditional or any other database, Apache Cassandra delivers near real-time performance simplifying the work of Developers, Administrators, Data Analysts and Software Engineers.
Instead of master-slave architecture, Cassandra is established on peer-to-peer architecture ensuring no failure.
It also assures phenomenal flexibility as it allows insertion of multiple nodes to any Cassandra cluster in any datacenter. Further, any client can forward its request to any server.
Cassandra facilitates extensible scalability and can be easily scaled up and scaled down as per the requirements. With a high throughput for read and write operations, this NoSQL application need not be restarted while scaling.
Cassandra is also revered for its strong data replication capability as it allows data storage at multiple locations enabling users to retrieve data from another location if one node fails. Users have the option to set up the number of replicas they want to create.
Shows brilliant performance when used for massive datasets and thus, the most preferable NoSQL DB by most organizations.
Operates on column-oriented structure and thus, quickens and simplifies the process of slicing. Even data access and retrieval becomes more efficient with column-based data model.
Further, Apache Cassandra supports schema-free/schema-optional data model, which un-necessitate the purpose of showing all the columns required by your application.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-cassandra-and-why-to-use-cassandra">What is the use of Cassandra and why to use Cassandra?<a class="hash-link" href="#what-is-the-use-of-cassandra-and-why-to-use-cassandra" title="Direct link to heading">​</a></h4><p>Cassandra was designed to handle big data workloads across multiple nodes without any single point of failure. The various factors responsible for using Cassandra are</p><ul><li>It is fault tolerant and consistent</li><li>Gigabytes to petabytes scalabilities</li><li>It is a column-oriented database</li><li>No single point of failure</li><li>No need for separate caching layer</li><li>Flexible schema design</li><li>It has flexible data storage, easy data distribution, and fast writes</li><li>It supports ACID (Atomicity, Consistency, Isolation, and Durability)properties</li><li>Multi-data center and cloud capable</li><li>Data compression</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-concept-of-tunable-consistency-in-cassandra">Explain the concept of tunable consistency in Cassandra?<a class="hash-link" href="#explain-the-concept-of-tunable-consistency-in-cassandra" title="Direct link to heading">​</a></h4><p>Tunable Consistency is a phenomenal characteristic that makes Cassandra a favored database choice of Developers, Analysts and Big data Architects. Consistency refers to the up-to-date and synchronized data rows on all their replicas. Cassandra’s Tunable Consistency allows users to select the consistency level best suited for their use cases. It supports two consistencies -Eventual and Consistency and Strong Consistency.
The former guarantees consistency when no new updates are made on a given data item, all accesses return the last updated value eventually. Systems with eventual consistency are known to have achieved replica convergence.
For Strong consistency, Cassandra supports the following condition:
R + W &gt; N, where
N – Number of replicas
W – Number of nodes that need to agree for a successful write
R – Number of nodes that need to agree for a successful read</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-composite-type-in-cassandra">Explain what is composite type in Cassandra?<a class="hash-link" href="#explain-what-is-composite-type-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, composite type allows to defined key or a column name with a concatenation of data of different type. You can use two types of Composite Type
Row Key
Column Name</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-cassandra-write">How does Cassandra write?<a class="hash-link" href="#how-does-cassandra-write" title="Direct link to heading">​</a></h4><p>Cassandra performs the write function by applying two commits-first it writes to a commit log on disk and then commits to an in-memory structured known as memtable. Once the two commits are successful, the write is achieved. Writes are written in the table structure as SSTable (sorted string table). Cassandra offers speedier write performance.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-cassandra-stores-data">How Cassandra stores data?<a class="hash-link" href="#how-cassandra-stores-data" title="Direct link to heading">​</a></h4><p>All data stored as bytes
When you specify validator, Cassandra ensures those bytes are encoded as per requirement
Then a comparator orders the column based on the ordering specific to the encoding
While composite are just byte arrays with a specific encoding, for each component it stores a two byte length followed by the byte encoded component followed by a termination bit.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-the-management-tools-in-cassandra">Define the management tools in Cassandra?<a class="hash-link" href="#define-the-management-tools-in-cassandra" title="Direct link to heading">​</a></h4><p>DataStaxOpsCenter: internet-based management and monitoring solution for Cassandra cluster and DataStax. It is free to download and includes an additional Edition of OpsCenter
SPM primarily administers Cassandra metrics and various OS and JVM metrics. Besides Cassandra, SPM also monitors Hadoop, Spark, Solr, Storm, zookeeper and other Big Data platforms. The main features of SPM include correlation of events and metrics, distributed transaction tracing, creating real-time graphs with zooming, anomaly detection and heartbeat alerting.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-are-the-main-components-of-cassandra-data-model">Mention what are the main components of Cassandra data model?<a class="hash-link" href="#mention-what-are-the-main-components-of-cassandra-data-model" title="Direct link to heading">​</a></h4><p>The main components of Cassandra Data Model are</p><ul><li>Cluster</li><li>Keyspace</li><li>Column</li><li>Column &amp; Family</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-memtable">Define Memtable?<a class="hash-link" href="#define-memtable" title="Direct link to heading">​</a></h4><p>Similar to table, memtable is in-memory/write-back cache space consisting of content in key and column format. The data in memtable is sorted by key, and each ColumnFamily consist of a distinct memtable that retrieves column data via key. It stores the writes until it is full, and then flushed out.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-a-column-family-in-cassandra">Explain what is a Column Family in Cassandra?<a class="hash-link" href="#explain-what-is-a-column-family-in-cassandra" title="Direct link to heading">​</a></h4><p>Column family in Cassandra is referred for a collection of Rows.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-sstable-and-how-is-it-different-from-other-relational-tables">What is SStable and how is it different from other relational tables?<a class="hash-link" href="#what-is-sstable-and-how-is-it-different-from-other-relational-tables" title="Direct link to heading">​</a></h4><p>SSTable expands to ‘Sorted String Table,’ which refers to an important data file in Cassandra and accepts regular written memtables. They are stored on disk and exist for each Cassandra table. Exhibiting immutability, SStables do not allow any further addition and removal of data items once written. For each SSTable, Cassandra creates three separate files like partition index, partition summary and a bloom filter.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-a-cluster-in-cassandra">Explain what is a Cluster in Cassandra?<a class="hash-link" href="#explain-what-is-a-cluster-in-cassandra" title="Direct link to heading">​</a></h4><p>A cluster is a container for keyspaces. Cassandra database is segmented over several machines that operate together. The cluster is the outermost container which arranges the nodes in a ring format and assigns data to them. These nodes have a replica which takes charge in case of data handling failure.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-concept-of-bloom-filter">Explain the concept of Bloom Filter?<a class="hash-link" href="#explain-the-concept-of-bloom-filter" title="Direct link to heading">​</a></h4><p>Associated with SSTable, Bloom filter is an off-heap (off the Java heap to native memory) data structure to check whether there is any data available in the SSTable before performing any I/O disk operation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-out-the-other-components-of-cassandra">List out the other components of Cassandra?<a class="hash-link" href="#list-out-the-other-components-of-cassandra" title="Direct link to heading">​</a></h4><p>The other components of Cassandra are</p><ul><li>Node</li><li>Data Center</li><li>Cluster</li><li>Commit log</li><li>Mem-table</li><li>SSTable</li><li>Bloom Filter</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-cap-theorem">Explain Cap Theorem?<a class="hash-link" href="#explain-cap-theorem" title="Direct link to heading">​</a></h4><p>With a strong requirement to scale systems when additional resources are needed, CAP Theorem plays a major role in maintaining the scaling strategy. It is an efficient way to handle scaling in distributed systems. Consistency Availability and Partition tolerance (CAP) theorem states that in distributed systems like Cassandra, users can enjoy only two out of these three characteristics.
One of them needs to be sacrificed. Consistency guarantees the return of most recent write for the client, Availability returns a rational response within minimum time and in Partition Tolerance, the system will continue its operations when network partitions occur. The two options available are AP and CP.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-a-keyspace-in-cassandra">Explain what is a Keyspace in Cassandra?<a class="hash-link" href="#explain-what-is-a-keyspace-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, a keyspace is a namespace that determines data replication on nodes. A cluster consist of one keyspace per node.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="state-the-differences-between-node-and-cluster-and-datacenter-in-cassandra">State the differences between Node and Cluster And DataCenter in Cassandra?<a class="hash-link" href="#state-the-differences-between-node-and-cluster-and-datacenter-in-cassandra" title="Direct link to heading">​</a></h4><p>While a node is a single machine running Cassandra, cluster is a collection of nodes that have similar type of data grouped together. DataCentersare useful components when serving customers in different geographical areas. You can group different nodes of a cluster into different data centers.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-are-the-values-stored-in-the-cassandra-column">Mention what are the values stored in the Cassandra Column?<a class="hash-link" href="#mention-what-are-the-values-stored-in-the-cassandra-column" title="Direct link to heading">​</a></h4><p>In Cassandra Column, basically there are three values</p><ul><li>Column Name</li><li>Value</li><li>Time Stamp</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-write-a-query-in-cassandra">How to write a Query in Cassandra?<a class="hash-link" href="#how-to-write-a-query-in-cassandra" title="Direct link to heading">​</a></h4><p>Using CQL (Cassandra Query Language).Cqlsh is used for interacting with database.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-when-you-can-use-alter-keyspace">Mention when you can use Alter Keyspace?<a class="hash-link" href="#mention-when-you-can-use-alter-keyspace" title="Direct link to heading">​</a></h4><p>ALTER KEYSPACE can be used to change properties such as the number of replicas and the durable_write of a keyspace.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-os-cassandra-supports">What os Cassandra supports?<a class="hash-link" href="#what-os-cassandra-supports" title="Direct link to heading">​</a></h4><p>Windows and Linux.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-cassandra-cqlsh">Explain what is Cassandra cqlsh?<a class="hash-link" href="#explain-what-is-cassandra-cqlsh" title="Direct link to heading">​</a></h4><p>Cassandra-Cqlsh is a query language that enables users to communicate with its database. By using Cassandra cqlsh, you can do following things</p><ul><li>Define a schema</li><li>Insert a data and</li><li>Execute a query</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-cassandra-data-model">What is Cassandra Data Model?<a class="hash-link" href="#what-is-cassandra-data-model" title="Direct link to heading">​</a></h4><p>Cassandra Data Model consists of four main components:</p><ul><li>Cluster: Made up of multiple nodes and keyspaces</li><li>Keyspace: a namespace to group multiple column families, especially one per partition</li><li>Column: consists of a column name, value and timestamp</li><li>ColumnFamily: multiple columns with row key reference.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-does-the-shell-commands-capture-and-consistency-determines">Mention what does the Shell Commands capture And consistency determines?<a class="hash-link" href="#mention-what-does-the-shell-commands-capture-and-consistency-determines" title="Direct link to heading">​</a></h4><p>There are various Cqlsh shell commands in Cassandra. Command “Capture”, captures the output of a command and adds it to a file while, command “Consistency” display the current consistency level or set a new consistency level.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-cql">What is Cql?<a class="hash-link" href="#what-is-cql" title="Direct link to heading">​</a></h4><p>CQL is Cassandra Query language to access and query the Apache distributed database. It consists of a CQL parser that incites all the implementation details to the server. The syntax of CQL is similar to SQL but it does not alter the Cassandra data model.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-mandatory-while-creating-a-table-in-cassandra">What is mandatory while creating a table in Cassandra?<a class="hash-link" href="#what-is-mandatory-while-creating-a-table-in-cassandra" title="Direct link to heading">​</a></h4><p>While creating a table primary key is mandatory, it is made up of one or more columns of a table.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-concept-of-compaction-in-cassandra">Explain the concept of compaction in Cassandra?<a class="hash-link" href="#explain-the-concept-of-compaction-in-cassandra" title="Direct link to heading">​</a></h4><p>Compaction refers to a maintenance process in Cassandra , in which, the SSTables are reorganized for data optimization of data structure son the disk. The compaction process is useful during interactive with memtable. There are two type sof compaction in Cassandra:</p><ul><li>Minor compaction: started automatically when a new sstable is created. Here, Cassandra condenses all the equally sized sstables into one.</li><li>Major compaction : is triggered manually using nodetool. Compacts all sstables of a ColumnFamily into one.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-needs-to-be-taken-care-while-adding-a-column">Mention what needs to be taken care while adding a Column?<a class="hash-link" href="#mention-what-needs-to-be-taken-care-while-adding-a-column" title="Direct link to heading">​</a></h4><ul><li>While adding a column you need to take care that the Column name is not conflicting with the existing column names</li><li>Table is not defined with compact storage option</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="does-cassandra-support-acid-transactions">Does Cassandra support ACID transactions?<a class="hash-link" href="#does-cassandra-support-acid-transactions" title="Direct link to heading">​</a></h4><p>Unlike relational databases, Cassandra does not support ACID transactions.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-cassandra-writes-data">Explain how Cassandra writes data?<a class="hash-link" href="#explain-how-cassandra-writes-data" title="Direct link to heading">​</a></h4><p>Cassandra writes data in three components</p><ul><li>Commitlog write</li><li>Memtable write</li><li>SStable write</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-memtable-in-cassandra">Explain what is Memtable in Cassandra?<a class="hash-link" href="#explain-what-is-memtable-in-cassandra" title="Direct link to heading">​</a></h4><p>Cassandra writes the data to a in memory structure known as Memtable.
It is an in-memory cache with content stored as key/column.
By key Memtable data are sorted.
There is a separate Memtable for each ColumnFamily, and it retrieves column data from the key.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-the-consistency-levels-for-read-operations-in-cassandra">Define the Consistency Levels for Read Operations in Cassandra?<a class="hash-link" href="#define-the-consistency-levels-for-read-operations-in-cassandra" title="Direct link to heading">​</a></h4><ul><li>ALL: Highly consistent. A write must be written to commitlog and memtable on all replica nodes in the cluster</li><li>EACH_QUORUM: A write must be written to commitlog and memtable on quorum of replica nodes in all data centers.</li><li>LOCAL_QUORUM:A write must be written to commitlog and memtable on quorum of replica nodes in the same center.</li><li>ONE: A write must be written to commitlog and memtableof at least one replica node.</li><li>TWO, Three: Same as One but at least two and three replica nodes, respectively</li><li>LOCAL_ONE: A write must be written for at least one replica node in the local data center ANY</li><li>SERIAL: Linearizable Consistency to prevent unconditional updates</li><li>LOCAL_SERIAL: Same as Serial but restricted to local data center</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-cassandra-writes-changed-data-into-commitlog">Explain how Cassandra writes changed data into Commitlog?<a class="hash-link" href="#explain-how-cassandra-writes-changed-data-into-commitlog" title="Direct link to heading">​</a></h4><p>Cassandra concatenate changed data to commitlog
Commitlog acts as a crash recovery log for data
Until the changed data is concatenated to commitlog write operation will be never considered successful
Data will not be lost once commitlog is flushed out to file.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-difference-between-column-and-super-column">What is difference between Column and Super Column?<a class="hash-link" href="#what-is-difference-between-column-and-super-column" title="Direct link to heading">​</a></h4><p>Both elements work on the principle of tuple having name and value. However, the former‘s value is a string while the value in latter is a Map of Columns with different data types.
Unlike Columns, Super Columns do not contain the third component of timestamp.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-columnfamily">What is ColumnFamily?<a class="hash-link" href="#what-is-columnfamily" title="Direct link to heading">​</a></h4><p>As the name suggests, ColumnFamily refers to a structure having infinite number of rows. That are referred by a key-value pair, where key is the name of the column and value represents the column data. It is much similar to a hashmap in java or dictionary in Python. Rememeber, the rows are not limited to a predefined list of Columns here. Also, the ColumnFamily is absolutely flexible with one row having 100 Columns while the other only 2 columns.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-cassandra-delete-data">Explain how Cassandra delete data?<a class="hash-link" href="#explain-how-cassandra-delete-data" title="Direct link to heading">​</a></h4><p>SSTables are immutable and cannot remove a row from SSTables. When a row needs to be deleted, Cassandra assigns the column value with a special value called Tombstone. When the data is read, the Tombstone value is considered as deleted.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-the-use-of-source-command-in-cassandra">Define the use of Source Command in Cassandra?<a class="hash-link" href="#define-the-use-of-source-command-in-cassandra" title="Direct link to heading">​</a></h4><p>Source command is used to execute a file consisting of CQL statements.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-thrift">What is Thrift?<a class="hash-link" href="#what-is-thrift" title="Direct link to heading">​</a></h4><p>Thrift is a legacy RPC protocol or API unified with a code generation tool for CQL. The purpose of using Thrift in Cassandra is to facilitate access to the DB across the programming language.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-tombstone-in-cassandra">Explain Tombstone in Cassandra?<a class="hash-link" href="#explain-tombstone-in-cassandra" title="Direct link to heading">​</a></h4><p>Tombstone is row marker indicating a column deletion. These marked columns are deleted during compaction. Tombstones are of great significance as Cassnadra supports eventual consistency, where the data must respond before any successful operation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-platforms-cassandra-runs-on">What Platforms Cassandra runs on?<a class="hash-link" href="#what-platforms-cassandra-runs-on" title="Direct link to heading">​</a></h4><p>Since Cassandra is a Java application, it can successfully run on any Java-driven platform or Java Runtime Environment (JRE) or Java Virtual Machine (JVM). Cassandra also runs on RedHat, CentOS, Debian and Ubuntu Linux platforms.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="name-the-ports-cassandra-uses">Name the ports Cassandra uses?<a class="hash-link" href="#name-the-ports-cassandra-uses" title="Direct link to heading">​</a></h4><p>The default settings state that Cassandra uses 7000 ports for Cluster Management, 9160 for Thrift Clients, 8080 for JMX. These are all TCP ports and can be edited in the configuration file: bin/Cassandra.in.sh</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-add-or-remove-column-families-in-a-working-cluster">Can you Add Or Remove column families in a working cluster?<a class="hash-link" href="#can-you-add-or-remove-column-families-in-a-working-cluster" title="Direct link to heading">​</a></h4><p>Yes, but keeping in mind the following processes.
Do not forget to clear the commitlog with ‘nodetool drain’
Turn off Cassandra to check that there is no data left in commitlog
Delete the sstable files for the removed CFs</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-replication-factor-in-cassandra">What is Replication Factor in Cassandra?<a class="hash-link" href="#what-is-replication-factor-in-cassandra" title="Direct link to heading">​</a></h4><p>Replication Factor is the measure of number of data copies existing. It is important to increase the replication factor to log into the cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-we-change-replication-factor-on-a-live-cluster">Can we change Replication Factor on a Live Cluster?<a class="hash-link" href="#can-we-change-replication-factor-on-a-live-cluster" title="Direct link to heading">​</a></h4><p>Yes, but it will require running repair to alter the replica count of existing data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-iterate-all-rows-in-columnfamily">How to Iterate all rows in ColumnFamily?<a class="hash-link" href="#how-to-iterate-all-rows-in-columnfamily" title="Direct link to heading">​</a></h4><p>Using get_range_slices. You can start iteration with the empty string and after each iteration, the last key read serves as the start key for next iteration.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-cassandra">Explain Cassandra.<a class="hash-link" href="#explain-cassandra" title="Direct link to heading">​</a></h4><p>Cassandra is a popular NOSQL database management system used to handle large amount of data. It is free and open source distributed database that provides high availability without any failure.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="in-which-language-cassandra-is-written">In which language Cassandra is written?<a class="hash-link" href="#in-which-language-cassandra-is-written" title="Direct link to heading">​</a></h4><p>Cassandra is written in Java. It is originally designed by Facebook consisting of flexible schemas. It is highly scalable for big data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-query-language-is-used-in-cassandra-database">Which query language is used in Cassandra database?<a class="hash-link" href="#which-query-language-is-used-in-cassandra-database" title="Direct link to heading">​</a></h4><p>Cassandra introduced its own Cassandra Query Language (CQL). CQL is a simple interface for accessing Cassandra, as an alternative to the traditional Structured Query Language (SQL).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-benefits-and-advantages-of-cassandra">What are the benefits and advantages of Cassandra?<a class="hash-link" href="#what-are-the-benefits-and-advantages-of-cassandra" title="Direct link to heading">​</a></h4><p>Cassandra delivers real-time performance simplifying the work of Developers, Administrators, Data Analysts and Software Engineers.
It provides extensible scalability and can be easily scaled up and scaled down as per the requirements.
Data can be replicated to several nodes for fault-tolerance.
Being a distributed management system, there is no single point of failure.
Every node in a cluster contains different data and able to serve any request.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="where-cassandra-stores-its-data">Where Cassandra stores its data?<a class="hash-link" href="#where-cassandra-stores-its-data" title="Direct link to heading">​</a></h4><p>Cassandra stores its data in the data dictionary.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-was-the-design-goal-of-cassandra">What was the design goal of Cassandra?<a class="hash-link" href="#what-was-the-design-goal-of-cassandra" title="Direct link to heading">​</a></h4><p>The main design goal of Cassandra was to handle big data workloads across multiple nodes without a single point of failure.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-many-types-of-nosql-databases-and-give-some-examples">How many types of NoSQL databases and give some examples.<a class="hash-link" href="#how-many-types-of-nosql-databases-and-give-some-examples" title="Direct link to heading">​</a></h4><p>There are mainly 4 types of NoSQL databases:</p><ul><li>Document store types ( MongoDB and CouchDB)</li><li>Key-Value store types ( Redis and Voldemort)</li><li>Column store types ( Cassandra)</li><li>Graph store types ( Neo4j and Giraph)</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-keyspace-in-cassandra">What is keyspace in Cassandra?<a class="hash-link" href="#what-is-keyspace-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, a keyspace is a namespace that determines data replication on nodes. A cluster contains of one keyspace per node.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-composite-keys-in-cassandra">What are the different composite keys in Cassandra?<a class="hash-link" href="#what-are-the-different-composite-keys-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, composite keys are used to define key or a column name with a concatenation of data of different type. There are two types of Composite key in Cassandra:</p><ul><li>Row Key</li><li>Column Name</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-data-replication-in-cassandra">What is data replication in Cassandra?<a class="hash-link" href="#what-is-data-replication-in-cassandra" title="Direct link to heading">​</a></h4><p>Data replication is an electronic copying of data from a database in one computer or server to a database in another so that all users can share the same level of information. Cassandra stores replicas on multiple nodes to ensure reliability and fault tolerance. The replication strategy decides the nodes where replicas are placed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-node-in-cassandra">What is node in Cassandra?<a class="hash-link" href="#what-is-node-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, node is a place where data is stored.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-data-center-in-cassandra">What do you mean by data center in Cassandra?<a class="hash-link" href="#what-do-you-mean-by-data-center-in-cassandra" title="Direct link to heading">​</a></h4><p>Data center is a complete data of clusters.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-commit-log-in-cassandra">What do you mean by commit log in Cassandra?<a class="hash-link" href="#what-do-you-mean-by-commit-log-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, commit log is a crash-recovery mechanism. Every write operation is written to the commit log.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-column-family-in-cassandra">What do you mean by column family in Cassandra?<a class="hash-link" href="#what-do-you-mean-by-column-family-in-cassandra" title="Direct link to heading">​</a></h4><p>Column family is a table in RDMS that contains an ordered collection of rows.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-consistency-in-cassandra">What do you mean by consistency in Cassandra?<a class="hash-link" href="#what-do-you-mean-by-consistency-in-cassandra" title="Direct link to heading">​</a></h4><p>Consistency in Cassandra specifies how to synchronize and up to date a row of Cassandra data and its replicas.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-cassandra-perform-write-function">How does Cassandra perform write function?<a class="hash-link" href="#how-does-cassandra-perform-write-function" title="Direct link to heading">​</a></h4><p>Cassandra performs the write function by applying two commits:
First commit is applied on disk and then second commit to an in-memory structure known as memtable.
When the both commits are applied successfully, to write is achieved.
Writes are written in the table structure as SSTable (sorted string table).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-sstable">What is SSTable?<a class="hash-link" href="#what-is-sstable" title="Direct link to heading">​</a></h4><p>SSTable is a short form of &#x27;Sorted String Table&#x27;. It refers to an important data file in Cassandra and accepts regular written memtables. They are stored on disk and exist for each Cassandra table.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-the-sstable-is-different-from-other-relational-tables">How the SSTable is different from other relational tables?<a class="hash-link" href="#how-the-sstable-is-different-from-other-relational-tables" title="Direct link to heading">​</a></h4><p>SStables do not allow any further addition and removal of data items once written. For each SSTable, Cassandra creates three separate files like partition index, partition summary and a bloom filter.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-role-of-alter-keyspace">What is the role of ALTER KEYSPACE?<a class="hash-link" href="#what-is-the-role-of-alter-keyspace" title="Direct link to heading">​</a></h4><p>ALTER KEYSPACE is used to change the value of DURABLE_WRITES with its related properties.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-differences-between-node-and-cluster-and-datacenter-in-cassandra">What are the differences between node and cluster and datacenter in Cassandra?<a class="hash-link" href="#what-are-the-differences-between-node-and-cluster-and-datacenter-in-cassandra" title="Direct link to heading">​</a></h4><ul><li>Node: A node is a single machine running Cassandra.</li><li>Cluster: A cluster is a collection of nodes that contains similar types of data together.</li><li>Datacenter: A datacenter is a useful component when serving customers in different geographical areas. Different nodes of a cluster can be grouped into different data centers.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-cassandra-cql-collection">What is the use of Cassandra CQL collection?<a class="hash-link" href="#what-is-the-use-of-cassandra-cql-collection" title="Direct link to heading">​</a></h4><p>Cassandra CQL collection is used to collect the data and store it in a column where each collection represents the same type of data. CQL consist of three types of types:</p><ul><li>SET: It is a collection of unordered list of unique elements.</li><li>List: It is a collection of elements arranged in an order and can contain duplicate values.</li><li>MAP: It is a collection of unique elements in a form of key-value pair.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-bloom-filter-in-cassandra">What is the use of Bloom Filter in Cassandra?<a class="hash-link" href="#what-is-the-use-of-bloom-filter-in-cassandra" title="Direct link to heading">​</a></h4><p>On a request of a data, before doing any disk I/O Bloom filter checks whether the requested data exist in the row of SSTable.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-cassandra-delete-data">How does Cassandra delete data?<a class="hash-link" href="#how-does-cassandra-delete-data" title="Direct link to heading">​</a></h4><p>In Cassandra, to delete a row, it is required to associate the value of column to Tombstone (where Tombstone is a special value).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-supercolumn-in-cassandra">What is SuperColumn in Cassandra?<a class="hash-link" href="#what-is-supercolumn-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, SuperColumn is a unique element containing similar collection of data. They are actually key-value pairs with values as columns.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-hadoop-and-hbase-and-hive-and-cassandra">What are the Hadoop and HBase and Hive and Cassandra?<a class="hash-link" href="#what-are-the-hadoop-and-hbase-and-hive-and-cassandra" title="Direct link to heading">​</a></h4><p>Hadoop, HBase, Hive and Cassandra all are Apache products.
Apache Hadoop supports file storage, grid compute processing via Map reduce. Apache Hive is a SQL like interface on the top of Hadoop.
Apache HBase follows column family storage built like Big Table.
Apache Cassandra also follows column family storage built like Big Table with Dynamo topology and consistency.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-usage-of-void-close-method">What is the usage of void close method?<a class="hash-link" href="#what-is-the-usage-of-void-close-method" title="Direct link to heading">​</a></h4><p>In Cassandra, the void close() method is used to close the current session instance.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-command-is-used-to-start-the-cqlsh-prompt">Which command is used to start the cqlsh prompt?<a class="hash-link" href="#which-command-is-used-to-start-the-cqlsh-prompt" title="Direct link to heading">​</a></h4><p>The cqlsh command is used to start the cqlsh prompt.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-usage-of-cqlsh-version-command">What is the usage of cqlsh version command?<a class="hash-link" href="#what-is-the-usage-of-cqlsh-version-command" title="Direct link to heading">​</a></h4><p>The &quot;cqlsh-version&quot; command is used to provide the version of the cqlsh you are using.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="does-cassandra-work-on-windows">Does Cassandra work on Windows?<a class="hash-link" href="#does-cassandra-work-on-windows" title="Direct link to heading">​</a></h4><p>Yes. Cassandra is compatible on Windows and works pretty well. Now its Linux and Window compatible version are available.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-kundera-in-cassandra">What is Kundera in Cassandra?<a class="hash-link" href="#what-is-kundera-in-cassandra" title="Direct link to heading">​</a></h4><p>In Cassandra, Kundera is an object-relational mapping (ORM) implementation which is written using Java annotations.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-thrift-in-cassandra">What do you mean by Thrift in Cassandra?<a class="hash-link" href="#what-do-you-mean-by-thrift-in-cassandra" title="Direct link to heading">​</a></h4><p>Thrift is the name of RPC client which is used to communicate with the Cassandra Server.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-hector-in-cassandra">What is Hector in Cassandra?<a class="hash-link" href="#what-is-hector-in-cassandra" title="Direct link to heading">​</a></h4><p>Hector was one of the early Cassandra clients. It is an open source project written in Java using the MIT license.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-kafka">Apache Kafka<a class="hash-link" href="#apache-kafka" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-is-apache-kafka">Mention what is Apache Kafka?<a class="hash-link" href="#mention-what-is-apache-kafka" title="Direct link to heading">​</a></h4><p>Apache Kafka is a publish-subscribe messaging system developed by Apache written in Scala. It is a distributed, p#artitioned and replicated log service.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-is-the-traditional-method-of-message-transfer">Mention what is the traditional method of message transfer?<a class="hash-link" href="#mention-what-is-the-traditional-method-of-message-transfer" title="Direct link to heading">​</a></h4><p>The traditional method of message transfer includes two methods</p><ul><li>Queuing: In a queuing, a pool of consumers may read message from the server and each message goes to one of them</li><li>Publish-Subscribe: In this model, messages are broadcasted to all consumers</li></ul><p>Kafka caters single consumer abstraction that generalized b#oth of the above- the consumer group.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-is-the-benefits-of-apache-kafka-over-the-traditional-technique">Mention what is the benefits of Apache Kafka over the traditional technique?<a class="hash-link" href="#mention-what-is-the-benefits-of-apache-kafka-over-the-traditional-technique" title="Direct link to heading">​</a></h4><p>Apache Kafka has following benefits above traditional messaging technique</p><ul><li>Fast: A single Kafka broker can serve thousands of clients by handling megabytes of reads and writes per second</li><li>Scalable: Data are partitioned and streamlined over a cluster of machines to enable larger data</li><li>Durable: Messages are persistent and is replicated within the cluster to prevent data loss</li><li>Distributed by Design: It provides fault tolerance g#uarantees and durability</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-is-the-meaning-of-broker-in-kafka">Mention what is the meaning of Broker in Kafka?<a class="hash-link" href="#mention-what-is-the-meaning-of-broker-in-kafka" title="Direct link to heading">​</a></h4><p>I#n Kafka cluster, broker term is used to refer Server.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-is-the-maximum-size-of-the-message-does-kafka-server-can-receive">Mention what is the Maximum Size of the Message does Kafka server can Receive?<a class="hash-link" href="#mention-what-is-the-maximum-size-of-the-message-does-kafka-server-can-receive" title="Direct link to heading">​</a></h4><p>The maximum size of the message that Kafka server can r#eceive is 1000000 bytes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-what-is-zookeeper-in-kafka-and-can-we-use-kafka-without-zookeeper">Explain what is Zookeeper in Kafka and can we use Kafka without Zookeeper?<a class="hash-link" href="#explain-what-is-zookeeper-in-kafka-and-can-we-use-kafka-without-zookeeper" title="Direct link to heading">​</a></h4><p>Zookeeper is an open source, high-performance co-ordination service used for distributed applications adapted by Kafka.
No, it is not possible to bye-pass Zookeeper and connect straight to the Kafka broker. Once the Zookeeper is down, it cannot serve client request.
Zookeeper is basically used to communicate between different nodes in a cluster
In Kafka, it is used to commit offset, so if node fails in any case it can be retrieved from the previously committed offset
Apart from this it also does other activities like leader detection, distributed synchronization, configuration management, identifies when a new node leaves or joins, t#he cluster, node status in real time, etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-message-is-consumed-by-consumer-in-kafka">Explain how message is consumed by Consumer in Kafka?<a class="hash-link" href="#explain-how-message-is-consumed-by-consumer-in-kafka" title="Direct link to heading">​</a></h4><p>Transfer of messages in Kafka is done by using sendfile API. It enables the transfer of bytes from the socket to disk via kernel space saving copies and call between k#ernel user back to the kernel.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-you-can-improve-the-throughput-of-a-remote-consumer">Explain how you can improve the throughput of a remote consumer?<a class="hash-link" href="#explain-how-you-can-improve-the-throughput-of-a-remote-consumer" title="Direct link to heading">​</a></h4><p>If the consumer is located in a different data center from the broker, you may require to tune the socket buffer size t#o amortize the long network latency.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-you-can-get-exactly-once-messaging-from-kafka-during-data-production">Explain how you can get Exactly Once Messaging from Kafka during data production?<a class="hash-link" href="#explain-how-you-can-get-exactly-once-messaging-from-kafka-during-data-production" title="Direct link to heading">​</a></h4><p>During data, production to get exactly once messaging from Kafka you have to follow two things avoiding duplicates during data consumption and avoiding duplication during data production.
Here are the two ways to get exactly one semantics while data production:
Avail a single writer per partition, every time you get a network error checks the last message in that partition to see if your last write succeeded
In the message include a primary key (UUID or something) a#nd de-duplicate on the consumer</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-how-you-can-reduce-churn-in-isr-and-when-does-broker-leave-the-isr">Explain how you can reduce churn in Isr and when does Broker leave the Isr?<a class="hash-link" href="#explain-how-you-can-reduce-churn-in-isr-and-when-does-broker-leave-the-isr" title="Direct link to heading">​</a></h4><p>ISR is a set of message replicas that are completely synced up with the leaders, in other word ISR has all messages that are committed. ISR should always include all replicas until there is a real failure. A replica will be d#ropped out of ISR if it deviates from the leader.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-replication-is-required-in-kafka">Why Replication is required in Kafka?<a class="hash-link" href="#why-replication-is-required-in-kafka" title="Direct link to heading">​</a></h4><p>Replication of message in Kafka ensures that any published message does not lose and can be consumed in case of machine error, program error or more common software u#pgrades.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-it-indicate-if-replica-stays-out-of-isr-for-a-long-time">What does it indicate if replica stays out of Isr for a long time?<a class="hash-link" href="#what-does-it-indicate-if-replica-stays-out-of-isr-for-a-long-time" title="Direct link to heading">​</a></h4><p>If a replica remains out of ISR for an extended time, it indicates that the follower is unable to fetch data as f#ast as data accumulated at the leader.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-happens-if-the-preferred-replica-is-not-in-the-isr">Mention what happens if the preferred replica is not in the Isr?<a class="hash-link" href="#mention-what-happens-if-the-preferred-replica-is-not-in-the-isr" title="Direct link to heading">​</a></h4><p>If the preferred replica is not in the ISR, the controller w#ill fail to move leadership to the preferred replica.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-possible-to-get-the-message-offset-after-producing">Is it possible to get the Message Offset after Producing?<a class="hash-link" href="#is-it-possible-to-get-the-message-offset-after-producing" title="Direct link to heading">​</a></h4><p>You cannot do that from a class that behaves as a producer like in most queue systems, its role is to fire and forget the messages. The broker will do the rest of the work like appropriate metadata handling with id’s, offsets, etc.
As a consumer of the message, you can get the offset from a Kafka broker. If you gaze in the SimpleConsumer class, you will notice it fetches MultiFetchResponse objects that include offsets as a list. In addition to that, when you iterate the Kafka Message, you will have MessageAndOffset o#bjects that include both, the offset and the message sent.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mention-what-is-the-difference-between-apache-kafka-and-apache-storm">Mention what is the difference between Apache Kafka and Apache Storm?<a class="hash-link" href="#mention-what-is-the-difference-between-apache-kafka-and-apache-storm" title="Direct link to heading">​</a></h4><p>Apach Kafeka: It is a distributed and robust messaging system that can handle huge amount of data and allows passage of messages from one end-point to another.
Apache Storm: It is a real time message processing system, and you can edit or manipulate data in real time. Apache storm pulls the data from Kafka and applies some required m#anipulation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-the-various-components-in-kafka">List the various components in Kafka?<a class="hash-link" href="#list-the-various-components-in-kafka" title="Direct link to heading">​</a></h4><p>The four major components of Kafka are:</p><ul><li>Topic – a stream of messages belonging to the same type</li><li>Producer – that can publish messages to a topic</li><li>Brokers – a set of servers where the publishes messages are stored</li><li>Consumer – that subscribes to various topics and pulls d#ata from the brokers.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-role-of-the-offset">Explain the role of the Offset?<a class="hash-link" href="#explain-the-role-of-the-offset" title="Direct link to heading">​</a></h4><p>Messages contained in the partitions are assigned a unique ID number that is called the offset. The role of the offset is to uniquely identify every message within the p#artition.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-concept-of-leader-and-follower">Explain the concept of Leader and Follower?<a class="hash-link" href="#explain-the-concept-of-leader-and-follower" title="Direct link to heading">​</a></h4><p>Every partition in Kafka has one server which plays the role of a Leader, and none or more servers that act as Followers. The Leader performs the task of all read and write requests for the partition, while the role of the Followers is to passively replicate the leader. In the event of the Leader failing, one of the Followers will take on the role of the Leader. This ensures load b#alancing of the server.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-define-a-partitioning-key">How do you define a Partitioning Key?<a class="hash-link" href="#how-do-you-define-a-partitioning-key" title="Direct link to heading">​</a></h4><p>Within the Producer, the role of a Partitioning Key is to indicate the destination partition of the message. By default, a hashing-based Partitioner is used to determine the partition ID given the key. Alternatively, users can a#lso use customized Partitions.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="in-the-producer-when-does-queuefullexception-occur">In the Producer when does Queuefullexception occur?<a class="hash-link" href="#in-the-producer-when-does-queuefullexception-occur" title="Direct link to heading">​</a></h4><p>QueueFullException typically occurs when the Producer attempts to send messages at a pace that the Broker cannot handle. Since the Producer doesn’t block, users will need to add enough brokers to collaboratively handle the i#ncreased load.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-role-of-the-kafka-producer-api">Explain the role of the Kafka Producer Api.<a class="hash-link" href="#explain-the-role-of-the-kafka-producer-api" title="Direct link to heading">​</a></h4><p>The role of Kafka’s Producer API is to wrap the two producers – kafka.producer.SyncProducer and the kafka.producer.async.AsyncProducer. The goal is to expose all the producer functionality through a single API to the client.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="snowflake">Snowflake<a class="hash-link" href="#snowflake" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-stored-procedure-and-how-it-works">What is a stored procedure and how it works?<a class="hash-link" href="#what-is-a-stored-procedure-and-how-it-works" title="Direct link to heading">​</a></h4><p>A stored procedure is a prepared SQL code that you can save, so the code can be reused over and over again.</p><p>So if you have an SQL query that you write over and over again, save it as a stored procedure, and then just call it to execute it.</p><p>You can also pass parameters to a stored procedure, so that the stored procedure can act based on the parameter value(s) that is passed.</p><p>The JavaScript API for stored procedures is similar to, but not identical to, the APIs in Snowflake connectors and drivers (Node.js, JDBC, Python, etc.).</p><p>The API enables you to perform operations such as:</p><ul><li>Execute a SQL statement.</li><li>Retrieve the results of a query (i.e. a result set).</li><li>Retrieve metadata about the result set (number of columns, data types of the columns, etc.)</li></ul><p>To create a stored procedure we use the below syntax:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">OR</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">REPLACE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">PROCEDURE</span><span class="token plain"> procedure_name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">RETURNS</span><span class="token plain"> STRING</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">LANGUAGE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">SQL</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">AS</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$$</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">BEGIN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sql_statements</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">RETURN</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;Success&#x27;</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">END</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">$$</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>To run/execute the stored procedure we use the below command:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">EXEC</span><span class="token plain"> procedure_name</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-so-you-mean-by-a-task-in-snowflake">What so you mean by a task in Snowflake?<a class="hash-link" href="#what-so-you-mean-by-a-task-in-snowflake" title="Direct link to heading">​</a></h4><p>A task in snowflake is a piece of SQL code that runs either when we call it manually or schedule it to run on specific intervals of time.</p><p>Snowflake Tasks are schedulers that can assist you in scheduling a single SQL Query or Stored Procedure. When paired with streams to create an end-to-end Data Pipeline, a job can be quite beneficial.</p><p>Tasks can execute a single SQL statement or a stored procedure.</p><p>A task can also be scheduled to run at any instance of time. Currently, a task is not able to run multiple SQL statements.</p><p>Since a snowflake task can execute a stored procedure, we can add that stored procedure within that task and schedule that task to run at specific intervals of time.</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">OR</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">REPLACE</span><span class="token plain"> task_name</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">WAREHOUSE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> COMPUTE_WH</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">SCHEDULE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;1 MINUTE&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">AS</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">call</span><span class="token plain"> procedure_name</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-analytical-functions-can-you-explain-with-an-example">What are analytical functions? Can you explain with an example?<a class="hash-link" href="#what-are-analytical-functions-can-you-explain-with-an-example" title="Direct link to heading">​</a></h4><p>Analytical functions are used to calculate an aggregated value from the dataset but are based on a specific set of rows instead of the entire dataset. As compared to aggregate functions like SUM, COUNT, AVG, etc. which return scalar records, these functions can return multiple records based on the conditions. The most common examples of using these functions are to find moving averages, running totals, etc. SQL Server supports the following analytic functions.</p><p>a. CUME_DIST — Find the cumulative distribution of a numerical column
b. FIRST_VALUE — Finds the first value of a column from the group and prints the same for each row
c. LAST_VALUE — Finds the last value of a column from the group and prints the same for each row
d. LAG — Reads values after the specified number of rows for a column
e. LEAD — Reads values before the specified number of rows for a column
f. rank() and dense_rank()</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-types-of-warehouses-in-snowflake">What are the different types of warehouses in Snowflake?<a class="hash-link" href="#what-are-the-different-types-of-warehouses-in-snowflake" title="Direct link to heading">​</a></h4><p>Warehouses are required for queries, as well as all DML operations, including loading data into tables. A warehouse is defined by its size, as well as the other properties that can be set to help control and automate warehouse activity.</p><p>Warehouses can be started and stopped at any time. They can also be resized at any time, even while running, to accommodate the need for more or less compute resources, based on the type of operations being performed by the warehouse.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-my-micro-partition">What do you mean my micro partition?<a class="hash-link" href="#what-do-you-mean-my-micro-partition" title="Direct link to heading">​</a></h4><p>Traditional data warehouses rely on static partitioning of large tables to achieve acceptable performance and enable better scaling. In these systems, a partition is a unit of management that is manipulated independently using specialized DDL and syntax; however, static partitioning has a number of well-known limitations, such as maintenance overhead and data skew, which can result in disproportionately-sized partitions.</p><p>In contrast to a data warehouse, the Snowflake Data Platform implements a powerful and unique form of partitioning, called micro-partitioning, that delivers all the advantages of static partitioning without the known limitations, as well as providing additional significant benefits.</p><p><strong>Benefits of Micro-partitioning:</strong></p><p>a. Snowflake micro-partitions are derived automatically; they don’t need to be explicitly defined up-front or maintained by users.
b. As the name suggests, these are small partitions 50MB to 500MB, which enables extremely efficient DML and fine-grained pruning for faster queries.
c. Helps prevent skew.
d. As the data is stored independently and in columnar storage, only the columns referenced in the query are scanned.
e. Columns are also compressed individually within micro-partitions. Snowflake automatically determines the most efficient compression algorithm for the columns in each micro-partition.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-qualify-function-in-snowflake">What is QUALIFY function in Snowflake?<a class="hash-link" href="#what-is-qualify-function-in-snowflake" title="Direct link to heading">​</a></h4><p>In a SELECT statement, the QUALIFY clause filters the results of window functions.</p><p>QUALIFY does with window functions what HAVING does with aggregate functions and GROUP BY clauses.</p><p>In the execution order of a query, QUALIFY is therefore evaluated after window functions are computed. Typically, a SELECT statement’s clauses are evaluated in the order shown below:</p><ul><li>From</li><li>Where</li><li>Group by</li><li>Having</li><li>Window</li><li>QUALIFY</li><li>Distinct</li><li>Order by</li><li>Limit</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-tell-me-something-about-query-optimization-practices-in-snowflake">Can you tell me something about query optimization practices in Snowflake?<a class="hash-link" href="#can-you-tell-me-something-about-query-optimization-practices-in-snowflake" title="Direct link to heading">​</a></h4><p>Query optimization simply means that tuning the query such that the database doesn’t become slower. Also the queries might return results faster. Following are the techniques of query optimization,</p><p>a. Maximizing Caching
b. Using materialized views wherever possible.
c. Scaling up, means increasing the size of the warehouse for complex queries.
d. Scaling out, means adding more and more data warehouses of the same size.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="tell-me-something-about-storage-integration-object">Tell me something about storage integration object.<a class="hash-link" href="#tell-me-something-about-storage-integration-object" title="Direct link to heading">​</a></h4><p>Creates a new storage integration in the account or replaces an existing integration.</p><p>A storage integration is a Snowflake object that stores a generated identity and access management (IAM) entity for your external cloud storage, along with an optional set of allowed or blocked storage locations (Amazon S3, Google Cloud Storage, or Microsoft Azure).</p><p>What this means is that if we define the storage integration object for Amazon S3 (below code), we are able to access the files from s3 into snowflake. The storage integration object is an object defining the configuration and what is allowed and what is not allowed.</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">//Creating an integration object. </span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">OR</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">REPLACE</span><span class="token plain"> STORAGE INTEGRATION s3_int</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token keyword" style="color:#00009f">TYPE</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> EXTERNAL_STAGE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  STORAGE_PROVIDER </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> S3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  ENABLED </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">TRUE</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  STORAGE_AWS_ROLE_ARN </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  STORAGE_ALLOWED_LOCATIONS </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;s3://&lt;your-bucket-name&gt;/&lt;your-path&gt;/&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;s3://&lt;your-bucket-name&gt;/&lt;your-path&gt;/&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token keyword" style="color:#00009f">COMMENT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;This is an optional comment&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//Describing the integration object.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">DESC</span><span class="token plain"> INTEGRATION s3_int</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-create-a-user-">How do you create a user ?<a class="hash-link" href="#how-do-you-create-a-user-" title="Direct link to heading">​</a></h4><p>A user in snowflake is a person having access to snowflake and also has a role associate to it. To create a user and assign a role to it, we need to create a role first. (If the role is already created then we can just create a user and assign role to it.)</p><p>To create a user type in the following command:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">USER</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;JOHN&#x27;</span><span class="token plain"> PASSWORD </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;Notasimplepassword123!@&#x27;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-share-objects-to-different-users-in-snowflake">How do you share objects to different users in Snowflake?<a class="hash-link" href="#how-do-you-share-objects-to-different-users-in-snowflake" title="Direct link to heading">​</a></h4><p>Since in snowflake each newly created user has a default PUBLIC role assigned to him, we can now directly start giving privileges if we want to.</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">//Granting usage permission to warehouse</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">GRANT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">USAGE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">ON</span><span class="token plain"> WAREHOUSE READ_WH </span><span class="token keyword" style="color:#00009f">TO</span><span class="token plain"> ROLE </span><span class="token keyword" style="color:#00009f">PUBLIC</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//Granting usage permissions to database.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">GRANT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">USAGE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">ON</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">DATABASE</span><span class="token plain"> READ_DB </span><span class="token keyword" style="color:#00009f">TO</span><span class="token plain"> ROLE </span><span class="token keyword" style="color:#00009f">PUBLIC</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//Grant usage permissions to schema.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">GRANT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">USAGE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">ON</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">SCHEMA</span><span class="token plain"> INFORMATION_SCHEMA </span><span class="token keyword" style="color:#00009f">TO</span><span class="token plain"> ROLE </span><span class="token keyword" style="color:#00009f">PUBLIC</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token comment" style="color:#999988;font-style:italic">//Grant usage permissions to tables.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">GRANT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">ON</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">ALL</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">TABLES</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">IN</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">SCHEMA</span><span class="token plain"> INFORMATION_SCHEMA </span><span class="token keyword" style="color:#00009f">TO</span><span class="token plain"> ROLE </span><span class="token keyword" style="color:#00009f">PUBLIC</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-permanent-transient-and-temporary-table">What is the difference between permanent, transient and temporary table?<a class="hash-link" href="#what-is-the-difference-between-permanent-transient-and-temporary-table" title="Direct link to heading">​</a></h4><p>There are 3 types of tables in Snowflake.</p><p>a. Permanent Table — Permanent table is like a normal table which will be stored in a database and dropped until explicitly stated. These have time travel and fail safe functionality depending on which edition you are using.
b. Transient Table — Snowflake supports creating transient tables that persist until explicitly dropped and are available to all users with the appropriate privileges. Transient tables are similar to permanent tables with the key difference that they do not have a Fail-safe period.
As a result, transient tables are specifically designed for transitory data that needs to be maintained beyond each session (in contrast to temporary tables), but does not need the same level of data protection and recovery provided by permanent tables.
c. Temporary Table — Snowflake supports creating temporary tables for storing non-permanent, transitory data (e.g. ETL data, session-specific data). Temporary tables only exist within the session in which they were created and persist only for the remainder of the session. As such, they are not visible to other users or sessions. Once the session ends, data stored in the table is purged completely from the system and, therefore, is not recoverable, either by the user who created the table or Snowflake.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Snowflake also supports creating transient databases and schemas. All tables created in a transient schema, as well as all schemas created in a transient database, are transient by definition.</p></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-you-perform-dml-operations-on-views">Can you perform DML operations on views?<a class="hash-link" href="#can-you-perform-dml-operations-on-views" title="Direct link to heading">​</a></h4><p>DML operations could be performed through a simple view. DML operations could not always be performed through a complex view. INSERT, DELETE and UPDATE are directly possible on a simple view. We cannot apply INSERT, DELETE and UPDATE on complex view directly.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-views-and-materialized-views">What is the difference between views and materialized views?<a class="hash-link" href="#what-is-the-difference-between-views-and-materialized-views" title="Direct link to heading">​</a></h4><p>Similar to views, materialized views are also database objects which are formed based on a SQL Query however unlike views, the contents or data of the materialized views are periodically refreshed based on its configuration.</p><p>The contents of view will get updated automatically when the underlying table (forming the query) data gets changed. However, materialized views can be configured to refresh its contents periodically or can be manually refreshed when needed.</p><p>Creating materialized views can be a very good approach for performance tuning especially when dealing with remote tables.</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">create</span><span class="token plain"> materialized </span><span class="token keyword" style="color:#00009f">view</span><span class="token plain"> mymv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">comment</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;Test view&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">as</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">select</span><span class="token plain"> col1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> col2 </span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> mytable</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-merge-statement">What is a merge statement?<a class="hash-link" href="#what-is-a-merge-statement" title="Direct link to heading">​</a></h4><p>Merge is part of the DML commands in SQL which can be used either perform INSERT or UPDATE based on the data in the respective table.</p><p>If the desired data is present then merge will update the records. If desired data is not present then merge will insert the records.</p><p>Sample merge statement is shown below. Here if the managers and directors table have matching records based the ID field then UPDATE command will be run else if there are no matching records then INSERT statement will be executed.</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">MERGE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">INTO</span><span class="token plain"> managers m</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">USING</span><span class="token plain"> directors d  </span><span class="token keyword" style="color:#00009f">ON</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">m</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> d</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">WHEN</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">MATCHED</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">THEN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">UPDATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">SET</span><span class="token plain"> name </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;TEST&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">WHEN</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">NOT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">MATCHED</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">THEN</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">INSERT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">VALUES</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">d</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">id</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> d</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">name</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-trigger">What is a trigger?<a class="hash-link" href="#what-is-a-trigger" title="Direct link to heading">​</a></h4><p>Trigger is a database object which is similar to a stored procedure which will automatically get invoked or executed when the specified event occurs in the database.</p><p>The most common type of triggers are DML triggers, DDL triggers and Database triggers (also referred as Logon triggers).</p><p>DML triggers are invoked when a DML operation (INSERT, UPDATE, DELETE) occurs on the respective table (table on which the trigger was created). Trigger can be configured to invoke either before the DML operation or after the DML operation.</p><p>DDL triggers are invoked when a DDL operation (CREATE, ALTER, DROP) occurs on the respective table (table on which the trigger was created).</p><p>Database trigger is invoked when the database session is established or shut down.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-difference-between-where-and-having-clause">What is difference between WHERE and HAVING clause?<a class="hash-link" href="#what-is-difference-between-where-and-having-clause" title="Direct link to heading">​</a></h4><p>WHERE clause is used to filter records from the table. We can also specify join conditions between two tables in the WHERE clause. If a SQL query has both WHERE and GROUP BY clause then the records will first get filtered based on the conditions mentioned in WHERE clause before the data gets grouped as per the GROUP BY clause.</p><p>Whereas HAVING clause is used to filter records returned from the GROUP BY clause. So if a SQL query has WHERE, GROUP BY and HAVING clause then first the data gets filtered based on WHERE condition, only after this grouping of data takes place. Finally based on the conditions in HAVING clause the grouped data again gets filtered.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-an-external-table-in-snowflake">What is an external table in Snowflake?<a class="hash-link" href="#what-is-an-external-table-in-snowflake" title="Direct link to heading">​</a></h4><p>In a typical table, the data is stored in the database; however, in an external table, the data is stored in files in an external stage. External tables store file-level metadata about the data files, such as the filename, a version identifier and related properties. This enables querying data stored in files in an external stage as if it were inside a database. External tables can access data stored in any format supported by COPY INTO &lt;table_name&gt; statements.</p><p>External tables are read-only, therefore no DML operations can be performed on them.</p><p>However, external tables can be used for query and join operations. Views can be created against external tables.</p><p>Querying data stored external to the database is likely to be slower than querying native database tables; however, materialized views based on external tables can improve query performance.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-a-secure-view-and-a-normal-view">What is the difference between a secure view and a normal view?<a class="hash-link" href="#what-is-the-difference-between-a-secure-view-and-a-normal-view" title="Direct link to heading">​</a></h4><p>A normal view is when a view is created from a table and then everyone who has access to the view can see all the data in the view along with “THE SQL QUERY THAT HAS BEEN USED TO CREATE THE VIEW”. If the user can see the SQL query then the user will know that something has been filtered in the WHERE clause and the view is not as complete as the table.</p><p>In this scenario, we can make use of secure view.</p><p>A secure view only allows a role to see whatever is written in the SELECT statement “WITHOUT ACTUALLY SEEING THE SQL QUERY THAT HAS BEEN USED TO CREATE THE VIEW”. So that user will see the view but will not know what has been filtered out as he cannot see the SQL query.</p><p>Also you can see a boolean value for a column “is_secure” when you see all the views by using the below query.</p><p>True means that the view is secure.</p><p>False means that the view is not secure. In short the view is normal.</p><div class="theme-admonition theme-admonition-note alert alert--secondary admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</div><div class="admonitionContent_S0QG"><p>Don’t confuse this with Dynamic data masking policy. In a dynamic data masking policy we are showing all the columns but using some sort of special characters like <code>***-**</code> in the column data to hide the data from the user.</p><p>In secure view we are showing all columns already filtered out.</p></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-spark">Apache Spark<a class="hash-link" href="#apache-spark" title="Direct link to heading">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-main-features-of-apache-spark">What are the main features of Apache Spark?<a class="hash-link" href="#what-are-the-main-features-of-apache-spark" title="Direct link to heading">​</a></h4><p>Main features of Apache Spark are as follows:</p><ul><li>Performance: The key feature of Apache Spark is its Performance. With Apache Spark we can run programs up to 100 times faster than Hadoop MapReduce in memory. On disk we can run it 10 times faster than Hadoop.</li><li>Ease of Use: Spark supports Java, Python, R, Scala etc. languages. So it makes it much easier to develop applications for Apache Spark.</li><li>Integrated Solution: In Spark we can create an integrated solution that combines the power of SQL, Streaming and data analytics.
R+ un Everywhere: Apache Spark can run on many platforms. It can run on Hadoop, Mesos, in Cloud or standalone. It can also connect to many data sources like HDFS, Cassandra, HBase, S3 etc.</li><li>Stream Processing: Apache Spark also supports real time stream processing. With real time streaming we can provide real time analytics solutions. This is very useful for real-time data.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-resilient-distribution-dataset-in-apache-spark">What is a Resilient Distribution Dataset in Apache Spark?<a class="hash-link" href="#what-is-a-resilient-distribution-dataset-in-apache-spark" title="Direct link to heading">​</a></h4><p>Resilient Distribution Dataset (RDD) is an abstraction of data in Apache Spark. It is a distributed and resilient collection of records spread over many partitions. RDD hides the data partitioning and distribution behind the scenes. Main	features of RDD 	are as follows:</p><ul><li>Distributed: Data in a RDD is distributed across multiple nodes.</li><li>Resilient: RDD is a fault- tolerant dataset. In case of node failure, Spark can re- compute data.</li><li>Dataset: It is a collection of data similar to collections in Scala.</li><li>Immutable: Data in RDD cannot be modified after creation. But we can transform it using a Transformation.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-transformation-in-apache-spark">What is a Transformation in Apache Spark?<a class="hash-link" href="#what-is-a-transformation-in-apache-spark" title="Direct link to heading">​</a></h4><p>Transformation in Apache Spark is a function that can be applied to a RDD. Output of a Transformation is another RDD. Transformation in Spark is a lazy operation. It means it is not executed immediately. Once we call an action, transformation is executed. A Transformation does not change the input RDD. We can also create a pipeline of certain Transformations to create a Data flow.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-security-options-in-apache-spark">What are security options in Apache Spark?<a class="hash-link" href="#what-are-security-options-in-apache-spark" title="Direct link to heading">​</a></h4><p>Apache Spark provides following security options:
Encryption: Apache Spark supports encryption by SSL. We can use HTTPS protocol for secure data transfer. Therefore data is transmitted in encrypted mode. We can use spark.ssl parameters to set the SSL configuration.</p><p>Authentication: We can perform authentication by a shared secret in Apache Spark. We can use spark.authenticate	to configure authentication in Spark.
Event Logging: If we use Event Logging, then we can set the permissions on the directory where event logs are	stored.	These permissions can ensure access control for Event log.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-will-you-monitor-apache-spark">How will you monitor Apache Spark?<a class="hash-link" href="#how-will-you-monitor-apache-spark" title="Direct link to heading">​</a></h4><p>We can use the Web UI provided by SparkContext to monitor Spark. We can access this Web UI at port 4040 to get the useful information. Some of the information that we can monitor is:</p><ul><li>Scheduler tasks and stages RDD	Sizes	and	Memory usage Spark	Environment Information</li><li>Executors Information</li><li>Spark also provides a Metrics library. This library can be used to send Spark information to HTTP, JMX, CSV files etc.This is another option to collect Spark runtime information for monitoring another dashboard tool.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-main-libraries-of-apache-spark">What are the main libraries of Apache Spark?<a class="hash-link" href="#what-are-the-main-libraries-of-apache-spark" title="Direct link to heading">​</a></h4><p>Main libraries of Apache Spark are as follows:</p><ul><li>MLib: This is Spark’s Machine Learning library. We can use it to create scalable machine learning system. We can use various machine learning algorithms as well as features like pipelining etc with this library.</li><li>GraphX: This library is used for computation of Graphs. It helps in creating a Graph abstraction of data and then use various Graph operators like- SubGraph, joinVertices etc.</li><li>Structured Streaming: This library is used for handling streams in Spark. It is a fault tolerant system built on top of Spark SQL Engine to process streams.</li><li>Spark SQL: This is another popular component that is used	for	processing	SQL queries on Spark platform.</li><li>SparkR: This is a package in Spark to use Spark from R language. We can use R data frames, dplyr etc from this package. We can also start SparkR from RStudio.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-main-functions-of-spark-core-in-apache-spark">What are the main functions of Spark Core in Apache Spark?<a class="hash-link" href="#what-are-the-main-functions-of-spark-core-in-apache-spark" title="Direct link to heading">​</a></h4><p>Spark Core is the central component of Apache Spark. It serves following functions:</p><ul><li>Distributed Task Dispatching</li><li>Job Scheduling</li><li>I/O Functions</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-will-you-do-memory-tuning-in-spark">How will you do memory tuning in Spark?<a class="hash-link" href="#how-will-you-do-memory-tuning-in-spark" title="Direct link to heading">​</a></h4><p>In case of memory tuning we have to take care of these points.
Amount of memory used by objects Cost of accessing objects Overhead	of Garbage Collection
Apache Spark stores objects in memory for caching. So it becomes important to perform memory tuning in a Spark application. First we determine the memory usage by the application. To do this we first create a RDD and put it in cache. Now we can see the size of the RDD in storage page of Web UI. This will tell the amount of memory consumed by RDD. Based on the memory usage, we can estimate the amount of memory needed for our task. In case we need tuning, we can follow these practices to reduce memory usage:
Use data structures like Array of objects or primitives instead of Linked list or HashMap. Fastutil library provides convenient collection classes for primitive types compatible with Java.
We have to reduce the usage of nested data structures with a large number of small objects and pointes. E.g. Linked list has pointers within each node.
It is a good practice to use numeric IDs instead of Strings for keys.
We can also use JVM flag - XX:+UseCompressedOops to	make pointers be four bytes instead of eight.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-two-ways-to-create-rdd-in-spark">What are the two ways to create RDD in Spark?<a class="hash-link" href="#what-are-the-two-ways-to-create-rdd-in-spark" title="Direct link to heading">​</a></h4><p>We can create RDD in Spark in following two ways:</p><ul><li>Internal: We can parallelize an existing collection of data within our Spark Driver program and create a RDD out of it.</li><li>External: We can also create RDD by referencing a Dataset in an external data source like AWS S3, HDFS, HBASE etc.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-main-operations-that-can-be-done-on-a-rdd-in-apache-spark">What are the main operations that can be done on a RDD in Apache Spark?<a class="hash-link" href="#what-are-the-main-operations-that-can-be-done-on-a-rdd-in-apache-spark" title="Direct link to heading">​</a></h4><p>There are two main operations that can be performed on a RDD in Spark:</p><ul><li>Transformation: This is a function that is used to create a new RDD out of an existing RDD.</li><li>Action: This is a function that returns a value to Driver program after running a computation on RDD.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-common-transformations-in-apache-spark">What are the common Transformations in Apache Spark?<a class="hash-link" href="#what-are-the-common-transformations-in-apache-spark" title="Direct link to heading">​</a></h4><p>Some common transformations in Apache Spark are as follows:</p><ul><li>Map(func): This is a basic transformation that returns a new dataset by passing each element of input dataset through func function.</li><li>Filter(func):	This transformation returns a new dataset of elements that return true for func function. It is used to filter elements in a dataset based on criteria in func function.</li><li>Union(other Dataset): This is used to combine a dataset with another dataset to form a union of two datasets.</li><li>Intersection(other Dataset):	This transformation gives the elements common to two datasets.</li><li>Pipe(command, <!-- -->[envVars]<!-- -->): This transformation passes each partition of the dataset through a shell command.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-common-actions-in-apache-spark">What are the common Actions in Apache Spark?<a class="hash-link" href="#what-are-the-common-actions-in-apache-spark" title="Direct link to heading">​</a></h4><p>Some commonly used Actions in Apache Spark are as follows:</p><ul><li>Reduce(func): This Action aggregates the elements of a dataset by using func function.</li><li>Count(): This action gives the total number of elements in a Dataset.</li><li>Collect(): This action will return all the elements of a dataset as an Array to the driver program.</li><li>First(): This action gives the first element of a collection.</li><li>Take(n): This action gives the first n elements of dataset.</li><li>Foreach(func): This action runs each element in dataset through a for loop and executes function func on each element.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-shuffle-operation-in-spark">What is a Shuffle operation in Spark?<a class="hash-link" href="#what-is-a-shuffle-operation-in-spark" title="Direct link to heading">​</a></h4><p>Shuffle operation is used in Spark to re-distribute data across multiple partitions. It is a costly and complex operation. In general a single task in Spark operates on elements in one partition. To execute shuffle, we have to run an operation on all elements of all partitions. It is also called all-to-all operation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-operations-that-can-cause-a-shuffle-in-spark">What are the operations that can cause a shuffle in Spark?<a class="hash-link" href="#what-are-the-operations-that-can-cause-a-shuffle-in-spark" title="Direct link to heading">​</a></h4><p>Some of the common operations that can cause a shuffle internally in Spark are as follows:</p><ul><li>Repartition</li><li>Coalesce</li><li>GroupByKey</li><li>ReduceByKey</li><li>Cogroup</li><li>Join</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-purpose-of-spark-sql">What is purpose of Spark SQL?<a class="hash-link" href="#what-is-purpose-of-spark-sql" title="Direct link to heading">​</a></h4><p>Spark SQL is used for running SQL queries. We can use Spark SQL to interact with SQL as well as Dataset API in Spark. During execution, Spark SQL uses same computation engine for SQL as well as Dataset API. With Spark SQL we can get more information about the structure of data as well as computation being performed. We can also use Spark SQL to read data from an existing Hive installation. Spark SQL can also be accessed by using JDBC/ODBC API as well as command line.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-dataframe-in-spark-sql">What is a DataFrame in Spark SQL?<a class="hash-link" href="#what-is-a-dataframe-in-spark-sql" title="Direct link to heading">​</a></h4><p>A DataFrame in SparkSQL is a Dataset organized into names columns. It is conceptually like a table in SQL.In Java and Scala, a DataFrame is a represented by a DataSet of rows. We can create a DataFrame from an existing RDD, a Hive table or from other Spark data sources.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-parquet-file-in-spark">What is a Parquet file in Spark?<a class="hash-link" href="#what-is-a-parquet-file-in-spark" title="Direct link to heading">​</a></h4><p>Apache Parquet is a columnar storage format that is available to any project in Hadoop ecosystem. Any data processing framework, data model or programming language can use it. It is a compressed, efficient and encoding format common to Hadoop system projects. Spark SQL supports both reading and writing of parquet files. Parquet files also automatically preserves the schema of the original data. During write operations, by default all columns in a parquet file are converted to nullable column.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-apache-spark-and-apache-hadoop-mapreduce">What is the difference between Apache Spark and Apache Hadoop MapReduce?<a class="hash-link" href="#what-is-the-difference-between-apache-spark-and-apache-hadoop-mapreduce" title="Direct link to heading">​</a></h4><p>Some of the main differences between Apache Spark and Hadoop MapReduce are follows:</p><ul><li>Speed: Apache Spark is 10X to 100X faster than Hadoop due to its usage of in memory processing.</li><li>Memory: Apache Spark stores data in memory, whereas	Hadoop MapReduce stores data in hard disk.</li><li>RDD: Spark uses Resilient Distributed Dataset (RDD) that guarantee fault tolerance. Where Apache Hadoop uses replication of data in multiple copies to achieve fault tolerance.</li><li>Streaming: Apache Spark supports Streaming with very less administration. This makes it much easier to use than Hadoop for real-time stream processing.</li><li>API: Spark provides a versatile API that can be used with multiple data sources as well as languages. It is more extensible than the API provided by Apache Hadoop.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-main-languages-supported-by-apache-spark">What are the main languages supported by Apache Spark?<a class="hash-link" href="#what-are-the-main-languages-supported-by-apache-spark" title="Direct link to heading">​</a></h4><p>Some of the main languages supported by Apache Spark are as follows:</p><ul><li>Java: We can use JavaSparkContext object to work with Java in Spark.</li><li>Scala: To use Scala with Spark, we have to create SparkContext	object	in Scala.</li><li>Python: We also used SparkContext to work with Python in Spark.</li><li>R: We can use SparkR module to work with R language in Spark ecosystem.</li><li>SQL: We can also SparkSQL to work with SQL language in Spark.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-file-systems-supported-by-spark">What are the file systems supported by Spark?<a class="hash-link" href="#what-are-the-file-systems-supported-by-spark" title="Direct link to heading">​</a></h4><p>Some of the popular file systems supported by Apache Spark are as follows:</p><ul><li>HDFS</li><li>S3</li><li>Local File System</li><li>Cassandra</li><li>OpenStack Swift</li><li>MapR File System</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-spark-driver">What is a Spark Driver?<a class="hash-link" href="#what-is-a-spark-driver" title="Direct link to heading">​</a></h4><p>Spark Driver is a program that runs on the master node machine. It takes care of declaring any operation- Transformation or Action on a RDD. With Spark Driver was can keep track of all the operations on a Dataset. It can also be used to rebuild a RDD in Spark.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-an-rdd-lineage">What is an RDD Lineage?<a class="hash-link" href="#what-is-an-rdd-lineage" title="Direct link to heading">​</a></h4><p>Resilient Distributed Dataset (RDD) Lineage is a graph of all the parent RDD of a RDD. Since Spark does not replicate data, it is possible to lose some data. In case some Dataset is lost, it is possible to use RDD Lineage to recreate the lost Dataset. Therefore RDD Lineage provides solution for better performance of Spark as well as it helps in building a resilient system.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-two-main-types-of-vector-in-spark">What are the two main types of Vector in Spark?<a class="hash-link" href="#what-are-the-two-main-types-of-vector-in-spark" title="Direct link to heading">​</a></h4><p>There are two main types of Vector in Spark:
Dense Vector: A dense vector is backed by an array of double data type. This array contains the values.
E.g. {1.0 , 0.0, 3.0} Sparse Vector: A sparse vector is backed by two parallel arrays. One array is for indices and the other array is for values. E.g. {3, <!-- -->[0,2]<!-- -->, <!-- -->[1.0,3.0]<!-- -->} In this array, the first element is the number of elements in vector. Second element is the array of indices of non-zero values. Third element is the array of non-zero values.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-deployment-modes-of-apache-spark">What are the different deployment modes of Apache Spark?<a class="hash-link" href="#what-are-the-different-deployment-modes-of-apache-spark" title="Direct link to heading">​</a></h4><p>Some popular deployment modes of Apache Spark are as follows:</p><ul><li>Amazon EC2: We can use AWS cloud product Elastic Compute Cloud (EC2) to deploy and run a Spark cluster.</li><li>Mesos: We can deploy a Spark application in a private cluster by using Apache Mesos.</li><li>YARN: We can also deploy Spark on Apache YARN (Hadoop NextGen)</li><li>Standalone: This is the mode in which we can start Spark by hand. We can launch standalone cluster manually.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-lazy-evaluation-in-apache-spark">What is lazy evaluation in Apache Spark?<a class="hash-link" href="#what-is-lazy-evaluation-in-apache-spark" title="Direct link to heading">​</a></h4><p>Apache Spark uses lazy evaluation as a performance optimization technique. In Laze evaluation as transformation is not applied immediately to a RDD. Spark records the transformations that have to be applied to a RDD. Once an Action is called, Spark executes all the transformations. Since Spark does not perform immediate execution based on transformation, it is	called lazy evaluation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-core-components-of-a-distributed-application-in-apache-spark">What are the core components of a distributed application in Apache Spark?<a class="hash-link" href="#what-are-the-core-components-of-a-distributed-application-in-apache-spark" title="Direct link to heading">​</a></h4><p>Core components of a distributed application in Apache Spark are as follows:</p><ul><li>Cluster Manager: This is the component responsible for launching executors and drivers on multiple nodes. We can use different types of cluster managers based on our requirements. Some of the common types are Standalone, YARN, Mesos etc.</li><li>Driver: This is the main program in Spark that runs the main() function of an application. A Driver program creates the SparkConetxt.	Driver program listens and accepts incoming connections from its executors. Driver program can schedule tasks on the cluster. It runs closer to worker nodes.</li><li>Executor: This is a process on worker node. It is launched on the node to run an application. It can run tasks and use data in memory or disk storage to perform the task.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-in-cache-and-persist-methods-in-apache-spark">What is the difference in cache() and persist() methods in Apache Spark?<a class="hash-link" href="#what-is-the-difference-in-cache-and-persist-methods-in-apache-spark" title="Direct link to heading">​</a></h4><p>Both cache() and persist() functions are used for persisting a RDD in memory across operations. The key difference between persist() and cache() is that in persist() we can specify the Storage level that we select for persisting. Where as in cache(), default strategy is used for persisting. The default storage strategy is MEMORY_ONLY.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-will-you-remove-data-from-cache-in-apache-spark">How will you remove data from cache in Apache Spark?<a class="hash-link" href="#how-will-you-remove-data-from-cache-in-apache-spark" title="Direct link to heading">​</a></h4><p>In general, Apache Spark automatically removes the unused objects from cache. It uses Least Recently Used (LRU) algorithm to drop old partitions. There are automatic monitoring mechanisms in Spark to monitor cache usage on each node. In case   we   want   to   forcibly remove an object from cache in Apache Spark, we can use RDD.unpersist() method.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-sparkcontext-in-apache-spark">What is the use of SparkContext in Apache Spark?<a class="hash-link" href="#what-is-the-use-of-sparkcontext-in-apache-spark" title="Direct link to heading">​</a></h4><p>SparkContext is the central object in Spark that coordinates different Spark applications in a cluster. In a cluster we can use SparkContext to connect to multiple Cluster Managers that allocate resources to multiple applications. For any Spark program we first create SparkContext object. We can access a cluster by using this object. To create a SparkContext object, we first create a SparkConf object. This object contains the configuration information of our application. In Spark Shell, by default we get a SparkContext for the shell.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="do-we-need-hdfs-for-running-spark-application">Do we need HDFS for running Spark application?<a class="hash-link" href="#do-we-need-hdfs-for-running-spark-application" title="Direct link to heading">​</a></h4><p>This is a trick question. Spark supports multiple file-systems. Spark supports HDFS, HBase, local file system, S3, Cassandra etc. So HDFS is not the only file system for running Spark application.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-spark-streaming">What is Spark Streaming?<a class="hash-link" href="#what-is-spark-streaming" title="Direct link to heading">​</a></h4><p>Spark Streaming is a very popular feature of Spark for processing live streams with a large amount of data. Spark Streaming uses Spark API to create a highly scalable, high throughput and fault tolerant system to handle live data streams. Spark Streaming supports ingestion of data from popular sources like- Kafka, Kinesis, Flume etc. We can apply popular functions like map, reduce, join etc on data processed through Spark Streams. The processed data can be written to a file system or sent to databases and live dashboards.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-spark-streaming-work-internally">How does Spark Streaming work internally?<a class="hash-link" href="#how-does-spark-streaming-work-internally" title="Direct link to heading">​</a></h4><p>Spark Streams listen to live data streams from various sources. On receiving data, it is divided into small batches that can be handled by Spark engine. These small batches of data are processed by Spark Engine to generate another output stream of resultant data. Internally, Spark uses an abstraction called DStream or discretized stream. A DStream is a continuous stream of data. We can create DStream from Kafka, Flume, Kinesis etc. A DStream is nothing but a sequence of RDDs in Spark. We can apply transformations and actions on this sequence of RDDs to create further RDDs.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-pipeline-in-apache-spark">What is a Pipeline in Apache Spark?<a class="hash-link" href="#what-is-a-pipeline-in-apache-spark" title="Direct link to heading">​</a></h4><p>Pipeline is a concept from Machine learning. It is a sequence of algorithms that are executed for processing and learning from data. Pipeline is similar to a workflow. There can be one or more stages in a Pipeline.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-pipeline-work-in-apache-spark">How does Pipeline work in Apache Spark?<a class="hash-link" href="#how-does-pipeline-work-in-apache-spark" title="Direct link to heading">​</a></h4><p>A Pipeline is a sequence of stages. Each stage in Pipeline can be a Transformer or an Estimator. We run these stages in an order. Initially a DataFrame is passed as an input to Pipeline. This DataFrame keeps on transforming with each stage of Pipeline. Most of the time, Runtime checking is done on DataFrame passing through the Pipeline. We can also save a Pipeline to a disk. It can be re-read from disk a later point of time.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-transformer-and-estimator-in-apache-spark">What is the difference between Transformer and Estimator in Apache Spark?<a class="hash-link" href="#what-is-the-difference-between-transformer-and-estimator-in-apache-spark" title="Direct link to heading">​</a></h4><p>A Transformer is an abstraction for feature transformer and learned model. A Transformer implements transform() method. It converts one DataFrame to another DataFrame. It appends one or more columns to a DataFrame. In a feature transformer a DataFrame is the input and the output is a new DataFrame with a new mapped column. An Estimator is an abstraction for a learning algorithm that fits or trains on data. An Estimator implements fit() method. The fit() method takes a DataFrame as input and results in a Model.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-types-of-cluster-managers-in-apache-spark">What are the different types of Cluster Managers in Apache Spark?<a class="hash-link" href="#what-are-the-different-types-of-cluster-managers-in-apache-spark" title="Direct link to heading">​</a></h4><p>Main types of Cluster Managers for Apache Spark are as follows:</p><ul><li>Standalone: It is a simple cluster manager that is included with Spark. We can start Spark manually by hand in this mode.</li><li>Spark	on Mesos: In this mode, Mesos master replaces Spark master as the cluster manager. When driver creates a job, Mesos will determine which machine will handle the task.</li><li>Hadoop YARN: In this setup, Hadoop YARN is used in cluster. There are two modes in this setup. In cluster mode, Spark driver runs inside a master process managed by YARN on cluster. In client mode, the Spark driver runs in the client process and application master is used for requesting resources from YARN.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-will-you-minimize-data-transfer-while-working-with-apache-spark">How will you minimize data transfer while working with Apache Spark?<a class="hash-link" href="#how-will-you-minimize-data-transfer-while-working-with-apache-spark" title="Direct link to heading">​</a></h4><p>Generally Shuffle operation in Spark leads to a large amount of data transfer. We can configure Spark Shuffle process for optimum data transfer. Some of the main points are as follows:</p><ul><li>spark.shuffle.compress: This configuration can be set to true to compress map output files. This reduces the amount of data transfer due to compression.</li><li>ByKey operations: We can minimize the use of ByKey operations to minimize the shuffle calls.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-main-use-of-mlib-in-apache-spark">What is the main use of MLib in Apache Spark?<a class="hash-link" href="#what-is-the-main-use-of-mlib-in-apache-spark" title="Direct link to heading">​</a></h4><p>MLib is a machine-learning library in Apache Spark. Some of the main uses of MLib in Spark are as follows:</p><ul><li>ML Algorithms: It contains Machine Learning algorithms such as classification, regression, clustering, and collaborative filtering. Featurization:</li><li>MLib provides algorithms to work with features. Some of these are	feature	extraction, transformation, dimensionality		reduction, and selection.</li><li>Pipelines: It contains tools for constructing, evaluating, and tuning ML Pipelines.</li><li>Persistence: It also provides methods for saving and load algorithms, models, and Pipelines.</li><li>Utilities: It contains utilities for linear algebra, statistics, data handling, etc.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-checkpointing-in-apache-spark">What is the Checkpointing in Apache Spark?<a class="hash-link" href="#what-is-the-checkpointing-in-apache-spark" title="Direct link to heading">​</a></h4><p>In Spark Streaming, there is a concept of Checkpointing to add resiliency in the application. In case of a failure, a streaming application needs a checkpoint to recover. Due to this Spark provides Checkpointing. There are two types of Checkpointing:</p><ul><li>Metadata Checkpointing: Metadata is the configuration information and other information that defines a Streaming application. We can create a Metadata checkpoint for a node to recover from the failure while running the driver application.		Metadata includes	configuration, DStream operations and incomplete batches etc.</li><li>Data Checkpointing: In this checkpoint we save RDD to a reliable storage. This is useful	in stateful transformations where generated RDD depends on RDD   of   previous   batch. There can be a long chain of RDDs in some cases. To avoid such a large recovery time, it is easier to create Data Checkpoint with RDDs at intermediate steps.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-an-accumulator-in-apache-spark">What is an Accumulator in Apache Spark?<a class="hash-link" href="#what-is-an-accumulator-in-apache-spark" title="Direct link to heading">​</a></h4><p>An Accumulator is a variable in Spark that can be added only through an associative and commutative operation. An Accumulator can be supported in parallel. It is generally used to implement a counter or cumulative sum. We can create numeric type Accumulators by default in Spark.An Accumulator variable can be named as well as unnamed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-broadcast-variable-in-apache-spark">What is a Broadcast variable in Apache Spark?<a class="hash-link" href="#what-is-a-broadcast-variable-in-apache-spark" title="Direct link to heading">​</a></h4><p>As per Spark online documentation, “A Broadcast variable allows a programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.” Spark can also distribute broadcast variable with an efficient broadcast algorithm to reduce communication cost. In Shuffle operations, there is a need of common data. This common data is broadcast by Spark as a Broadcast variable. The data in these variables is serialized and de-serialized before running a task.
We can	use SparkContext.broadcast(v) to create a broadcast variable. It is recommended that we should use broadcast variable in place of original variable for running a function on cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-structured-streaming-in-apache-spark">What is Structured Streaming in Apache Spark?<a class="hash-link" href="#what-is-structured-streaming-in-apache-spark" title="Direct link to heading">​</a></h4><p>Structured Streaming is a new feature in Spark 2.1. It is a scalable and fault-tolerant stream- processing engine. It is built on Spark SQL engine. We can use Dataset or DataFrame API to express streaming aggregations, event-time windows etc. The computations are done on the optimized Spark SQL engine.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-will-you-pass-functions-to-apache-spark">How will you pass functions to Apache Spark?<a class="hash-link" href="#how-will-you-pass-functions-to-apache-spark" title="Direct link to heading">​</a></h4><p>In Spark API, we pass functions to driver program so that it can be run on a cluster. Two common ways to pass functions in Spark are as follows:</p><ul><li>Anonymous	Function Syntax: This is used for passing short pieces of code in an anonymous function.</li><li>Static	Methods in a Singleton object: We can also define static methods in an object with only once instance i.e. Singleton. This object along with its methods can be passed to cluster nodes.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-property-graph">What is a Property Graph?<a class="hash-link" href="#what-is-a-property-graph" title="Direct link to heading">​</a></h4><p>A Property Graph is a directed multigraph. We can attach an object on each vertex and edge of a Property Graph.In a directed multigraph, we can have multiple parallel edges that share same source and destination vertex. During modeling the data, the option of parallel edges helps in creating multiple relationships between same pair of vertices. E.g. Two persons can have two relationships Boss as well as Mentor.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-neighborhood-aggregation-in-spark">What is Neighborhood Aggregation in Spark?<a class="hash-link" href="#what-is-neighborhood-aggregation-in-spark" title="Direct link to heading">​</a></h4><p>Neighborhood Aggregation is a concept in Graph module of Spark. It refers to the task of aggregating information	about	the neighborhood of each vertex. E.g. We want to know the number of books referenced in a book. Or number of times a Tweet is retweeted. This concept is used in iterative graph algorithms. Some of the popular uses of this concept are in Page Rank, Shortest Path etc.
We can use aggregateMessages[] and mergeMsg[] operations in Spark for implementing Neighborhood Aggregation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-different-persistence-levels-in-apache-spark">What are different Persistence levels in Apache Spark?<a class="hash-link" href="#what-are-different-persistence-levels-in-apache-spark" title="Direct link to heading">​</a></h4><p>Different Persistence levels in Apache Spark are as follows:</p><ul><li>MEMORY_ONLY: In this level, RDD object is stored as a de-serialized Java object in JVM. If an RDD doesn’t fit in the memory, it will be recomputed.</li><li>MEMORY_AND_DISK: In this level, RDD object is stored as a de-serialized Java object in JVM. If an RDD doesn’t fit in the memory, it will be stored on the Disk.</li><li>MEMORY_ONLY_SER: In this level, RDD object is stored as a serialized Java object in JVM. It is more efficient than de-serialized object.</li><li>MEMORY_AND_DISK_SE In this level, RDD object is stored as a serialized Java object in JVM. If an RDD doesn’t fit in the memory, it will be stored on the Disk.</li><li>DISK_ONLY: In this level, RDD object is stored only on Disk.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-will-you-select-the-storage-level-in-apache-spark">How will you select the storage level in Apache Spark?<a class="hash-link" href="#how-will-you-select-the-storage-level-in-apache-spark" title="Direct link to heading">​</a></h4><p>We use storage level to maintain balance between CPU efficiency and Memory usage. If our RDD objects fit in memory, we use MEMORY_ONLY option. In this option, the performance is very good due to objects being in Memory only. In case our RDD objects cannot fit in memory, we go for MEMORY_ONLY_SER option and select a serialization library that can provide space savings with serialization. This option is also quite fast in performance. In case our RDD object cannot fit in memory with a big gap in memory vs. total object size, we go for MEMORY_AND_DISK option. In this option some RDD object are stored on Disk. For fast fault recovery we use replication of objects to multiple partitions.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-options-in-spark-to-create-a-graph">What are the options in Spark to create a Graph?<a class="hash-link" href="#what-are-the-options-in-spark-to-create-a-graph" title="Direct link to heading">​</a></h4><p>We can create a Graph in Spark from a collection of vertices and edges. Some of the options in Spark to create a Graph are as follows:</p><ul><li>Graph.apply: This is the simplest option to create graph. We use this option to create a graph from RDDs of vertices and edges.</li><li>Graph.fromEdges: We can also create a graph from RDD of edges. In this option, vertices are created automatically and a default value is assigned to each vertex.</li><li>Graph.fromEdgeTuples: We can also create a graph from only an RDD of tuples.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-basic-graph-operators-in-spark">What are the basic Graph operators in Spark?<a class="hash-link" href="#what-are-the-basic-graph-operators-in-spark" title="Direct link to heading">​</a></h4><p>Some common Graph operators in Apache Spark are as follows:</p><ul><li>numEdges</li><li>numVertices</li><li>inDegrees</li><li>outDegrees</li><li>degrees</li><li>vertices</li><li>edges</li><li>persist</li><li>cache</li><li>unpersistVertices</li><li>partitionBy</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-partitioning-approach-used-in-graphx-of-apache-spark">What is the partitioning approach used in GraphX of Apache Spark?<a class="hash-link" href="#what-is-the-partitioning-approach-used-in-graphx-of-apache-spark" title="Direct link to heading">​</a></h4><p>GraphX uses Vertex-cut approach to distributed graph partitioning. In this approach, a graph is not split along edges. Rather we partition graph along vertices. These vertices can span on multiple machines. This approach reduces communication and storage overheads. Edges are assigned to different partitions based on the partition strategy that we select.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-rdd">What is RDD?<a class="hash-link" href="#what-is-rdd" title="Direct link to heading">​</a></h4><p>RDD (Resilient Distribution Datasets) is a fault-tolerant collection of operational elements that run parallel. The partitioned data in RDD is immutable and distributed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="name-the-different-types-of-rdd">Name the different types of RDD<a class="hash-link" href="#name-the-different-types-of-rdd" title="Direct link to heading">​</a></h4><p>There are primarily two types of RDD – parallelized collection and Hadoop datasets.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-methods-of-creating-rdds-in-spark">What are the methods of creating RDDs in Spark?<a class="hash-link" href="#what-are-the-methods-of-creating-rdds-in-spark" title="Direct link to heading">​</a></h4><p>There are two methods :</p><ul><li>By paralleling a collection in your Driver program.</li><li>By loading an external dataset from external storage like HDFS, HBase, shared file system.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-sparse-vector">What is a Sparse Vector?<a class="hash-link" href="#what-is-a-sparse-vector" title="Direct link to heading">​</a></h4><p>A sparse vector has two parallel arrays –one for indices and the other for values.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-languages-supported-by-apache-spark-and-which-is-the-most-popular-one-what-is-jdbc-and-why-it-is-popular">What are the languages supported by Apache Spark and which is the most popular one, What is JDBC and why it is popular?<a class="hash-link" href="#what-are-the-languages-supported-by-apache-spark-and-which-is-the-most-popular-one-what-is-jdbc-and-why-it-is-popular" title="Direct link to heading">​</a></h4><p>There are four languages supported by Apache Spark – Scala, Java, Python, and R. Scala is the most popular one.
<strong>Java Database Connectivity (JDBC)</strong> is an application programming interface (API) that defines database connections in Java environments. Spark is written in Scala, which runs on the Java Virtual Machine (JVM). This makes JDBC the preferred method for connecting to data whenever possible. Hadoop, Hive, and MySQL all run on Java and easily interface with Spark clusters.
Databases are advanced technologies that benefit from decades of research and development. To leverage the inherent efficiencies of database engines, Spark uses an optimization called predicate pushdown. Predicate pushdown uses the database itself to handle certain parts of a query (the predicates). In mathematics and functional programming, a predicate is anything that returns a Boolean. In SQL terms, this often refers to the WHERE clause. Since the database is filtering data before it arrives on the Spark cluster, there less data transfer across the network and fewer records for Spark to process. Spark&#x27;s Catalyst Optimizer includes predicate pushdown communicated through the JDBC API, making JDBC an ideal data source for Spark workloads.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-yarn">What is Yarn?<a class="hash-link" href="#what-is-yarn" title="Direct link to heading">​</a></h4><p>Yarn is one of the key features in Spark, providing a central and resource management platform to deliver scalable operations across the cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="do-you-need-to-install-spark-on-all-nodes-of-yarn-cluster-why">Do you need to install Spark on all nodes of Yarn cluster? Why?<a class="hash-link" href="#do-you-need-to-install-spark-on-all-nodes-of-yarn-cluster-why" title="Direct link to heading">​</a></h4><p>No, because Spark runs on top of Yarn.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-possible-to-run-apache-spark-on-apache-mesos">Is it possible to run Apache Spark on Apache Mesos?<a class="hash-link" href="#is-it-possible-to-run-apache-spark-on-apache-mesos" title="Direct link to heading">​</a></h4><p>Yes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-lineage-graph">What is lineage graph?<a class="hash-link" href="#what-is-lineage-graph" title="Direct link to heading">​</a></h4><p>The RDDs in Spark, depend on one or more other RDDs. The representation of dependencies in between RDDs is known as the lineage graph.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-partitions-in-apache-spark">Define Partitions in Apache Spark<a class="hash-link" href="#define-partitions-in-apache-spark" title="Direct link to heading">​</a></h4><p>Partition is a smaller and logical division of data similar to split in MapReduce. It is a logical chunk of a large distributed data set. Partitioning is the process to derive logical units of data to speed up the processing process.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-dstream">What is a DStream?<a class="hash-link" href="#what-is-a-dstream" title="Direct link to heading">​</a></h4><p>Discretized Stream (DStream) is a sequence of Resilient Distributed Databases that represent a stream of data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-catalyst-framework">What is a Catalyst framework?<a class="hash-link" href="#what-is-a-catalyst-framework" title="Direct link to heading">​</a></h4><p>Catalyst framework is an optimization framework present in Spark SQL. It allows Spark to automatically transform SQL queries by adding new optimizations to build a faster processing system.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-actions-in-spark">What are Actions in Spark?<a class="hash-link" href="#what-are-actions-in-spark" title="Direct link to heading">​</a></h4><p>An action helps in bringing back the data from RDD to the local machine. An action’s execution is the result of all previously created transformations.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-parquet-file">What is a Parquet file?<a class="hash-link" href="#what-is-a-parquet-file" title="Direct link to heading">​</a></h4><p>Parquet is a columnar format file supported by many other data processing systems.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-graphx">What is GraphX?<a class="hash-link" href="#what-is-graphx" title="Direct link to heading">​</a></h4><p>Spark uses GraphX for graph processing to build and transform interactive graphs.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-file-systems-does-spark-support">What file systems does Spark support?<a class="hash-link" href="#what-file-systems-does-spark-support" title="Direct link to heading">​</a></h4><p>Hadoop distributed file system (HDFS), local file system, and Amazon S3.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-types-of-transformations-on-dstreams-explain">What are the different types of transformations on DStreams? Explain.<a class="hash-link" href="#what-are-the-different-types-of-transformations-on-dstreams-explain" title="Direct link to heading">​</a></h4><ul><li>Stateless Transformations – Processing of the batch does not depend on the output of the previous batch. Examples – map (), reduceByKey (), filter ().</li><li>Stateful Transformations – Processing of the batch depends on the intermediary results of the previous batch. Examples –Transformations that depend on sliding windows.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-persist--and-cache-">What is the difference between persist () and cache ()?<a class="hash-link" href="#what-is-the-difference-between-persist--and-cache-" title="Direct link to heading">​</a></h4><p>Persist () allows the user to specify the storage level whereas cache () uses the default storage level.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-schemardd">What do you understand by SchemaRDD?<a class="hash-link" href="#what-do-you-understand-by-schemardd" title="Direct link to heading">​</a></h4><p>SchemaRDD is an RDD that consists of row objects (wrappers around the basic string or integer arrays) with schema information about the type of data in each column.
These are some popular questions asked in an Apache Spark interview. Always be prepared to answer all types of questions — technical skills, interpersonal, leadership or methodology. If you are someone who has recently started your career in big data, you can always get certified in Apache Spark to get the techniques and skills required to be an expert in the field.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-apache-spark">What is Apache Spark?<a class="hash-link" href="#what-is-apache-spark" title="Direct link to heading">​</a></h4><p>Spark is a fast, easy-to-use and flexible data processing framework. It has an advanced execution engine supporting cyclic data  flow and in-memory computing. Spark can run on Hadoop, standalone or in the cloud and is capable of accessing diverse data sources including HDFS, HBase, Cassandra and others.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-key-features-of-spark">Explain key features of Spark.<a class="hash-link" href="#explain-key-features-of-spark" title="Direct link to heading">​</a></h4><p>Allows Integration with Hadoop and files included in HDFS.
Spark has an interactive language shell as it has an independent Scala (the language in which Spark is written) interpreter.
Spark consists of RDD’s (Resilient Distributed Datasets), which can be cached across computing nodes in a cluster.
Spark supports multiple analytic tools that are used for interactive query analysis , real-time analysis and graph      processing</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-rdd">Define RDD?<a class="hash-link" href="#define-rdd" title="Direct link to heading">​</a></h4><p>RDD is the acronym for Resilient Distribution Datasets – a fault-tolerant collection of operational elements that run parallel. The partitioned data in RDD is immutable and distributed. There are primarily two types of RDD:</p><ul><li>Parallelized Collections : The existing RDD’s running parallel with one another.</li><li>Hadoop datasets : perform function on each file record in HDFS or other storage system</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-a-spark-engine-do">What does a Spark Engine do?<a class="hash-link" href="#what-does-a-spark-engine-do" title="Direct link to heading">​</a></h4><p>Spark Engine is responsible for scheduling, distributing and monitoring the data application across the cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-partitions">Define Partitions?<a class="hash-link" href="#define-partitions" title="Direct link to heading">​</a></h4><p>As the name suggests, partition is a smaller and logical division of data  similar to split in MapReduce. Partitioning is the process to derive logical units of data to speed up the processing process. Everything in Spark is a partitioned RDD.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-transformations-in-spark">What do you understand by Transformations in Spark?<a class="hash-link" href="#what-do-you-understand-by-transformations-in-spark" title="Direct link to heading">​</a></h4><p>Transformations are functions applied on RDD, resulting into another RDD. It does not execute until an action occurs. map() and filer() are examples of transformations, where the former applies the function passed to it on each element of RDD and results into another RDD. The filter() creates a new RDD by selecting elements form current RDD that pass function argument.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-actions">Define Actions.<a class="hash-link" href="#define-actions" title="Direct link to heading">​</a></h4><p>An action helps in bringing back the data from RDD to the local machine. An action’s execution is the result of all previously created transformations. reduce() is an action that implements the function passed again and again until one value if left. take() action takes all the values from RDD to local node.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-functions-of-sparkcore">Define functions of SparkCore?<a class="hash-link" href="#define-functions-of-sparkcore" title="Direct link to heading">​</a></h4><p>Serving as the base engine, SparkCore performs various important functions like memory management, monitoring jobs, fault-tolerance, job scheduling and interaction with storage systems.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-rdd-lineage">What is RDD Lineage?<a class="hash-link" href="#what-is-rdd-lineage" title="Direct link to heading">​</a></h4><p>Spark does not support data replication in the memory and thus, if any data is lost, it is rebuild using RDD lineage. RDD lineage is a process that reconstructs lost data partitions. The best is that RDD always remembers how to build from other datasets.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-spark-driver">What is Spark Driver?<a class="hash-link" href="#what-is-spark-driver" title="Direct link to heading">​</a></h4><p>Spark Driver is the program that runs on the master node of the machine and declares transformations and actions on data RDDs. In simple terms, driver in Spark creates SparkContext, connected to a given Spark Master.
The driver also delivers the RDD graphs to Master, where the standalone cluster manager runs.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-hive-on-spark">What is Hive on Spark?<a class="hash-link" href="#what-is-hive-on-spark" title="Direct link to heading">​</a></h4><p>Hive contains significant support for Apache Spark, wherein Hive execution is configured to Spark:
hive&gt; set spark.home=/location/to/sparkHome;
hive&gt; set hive.execution.engine=spark;</p><p>Name commonly-used Spark Ecosystems.</p><ul><li>Spark SQL (Shark)- for developers.</li><li>Spark Streaming for processing live data streams.</li><li>GraphX for generating and computing graphs.</li><li>MLlib (Machine Learning Algorithms).</li><li>SparkR to promote R Programming in Spark engine.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-spark-streaming">Define Spark Streaming.<a class="hash-link" href="#define-spark-streaming" title="Direct link to heading">​</a></h4><p>Spark supports stream processing – an extension to the Spark API , allowing stream processing of live data streams. The data from different sources like Flume, HDFS is streamed and finally processed to file systems, live dashboards and databases. It is similar to batch processing as the input data is divided into streams like batches.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-spark-sql">What is Spark SQL?<a class="hash-link" href="#what-is-spark-sql" title="Direct link to heading">​</a></h4><p>SQL Spark, better known as Shark is a novel module introduced in Spark to work with structured data and perform structured data processing. Through this module, Spark executes relational SQL queries on the data. The core of the component supports an altogether different RDD called SchemaRDD, composed of rows objects and schema objects defining data type of each column in the row. It is similar to a table in relational database.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-the-functions-of-spark-sql">List the functions of Spark SQL?<a class="hash-link" href="#list-the-functions-of-spark-sql" title="Direct link to heading">​</a></h4><p>Spark SQL is capable of:</p><ul><li>Loading data from a variety of structured sources.</li><li>Querying data using SQL statements, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC). For instance, using business intelligence tools like Tableau.</li><li>Providing rich integration between SQL and regular Python/Java/Scala code, including the ability to join RDDs and SQL tables, expose custom functions in SQL, and more.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-benefits-of-spark-over-mapreduce">What are benefits of Spark over MapReduce?<a class="hash-link" href="#what-are-benefits-of-spark-over-mapreduce" title="Direct link to heading">​</a></h4><p>Due to the availability of in-memory processing, Spark implements the processing around 10-100x faster than Hadoop MapReduce. MapReduce makes use of persistence storage for any of the data processing tasks.
Unlike Hadoop, Spark provides in-built libraries to perform multiple tasks form the same core like batch processing, Steaming, Machine learning, Interactive SQL queries. However, Hadoop only supports batch processing.
Hadoop is highly disk-dependent whereas Spark promotes caching and in-memory data storage.
Spark is capable of performing computations multiple times on the same dataset. This is called iterative computation while there is no iterative computing implemented by Hadoop.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-spark-executor">What is Spark Executor?<a class="hash-link" href="#what-is-spark-executor" title="Direct link to heading">​</a></h4><p>When SparkContext connect to a cluster manager, it acquires an Executor on nodes in the cluster. Executors are Spark processes that run computations and store the data on the worker node. The final tasks by SparkContext are transferred to executors for their execution.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-worker-node">What do you understand by worker node?<a class="hash-link" href="#what-do-you-understand-by-worker-node" title="Direct link to heading">​</a></h4><p>Worker node refers to any node that can run the application code in a cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="illustrate-some-demerits-of-using-spark">Illustrate some demerits of using Spark.<a class="hash-link" href="#illustrate-some-demerits-of-using-spark" title="Direct link to heading">​</a></h4><p>Since Spark utilizes more storage space compared to Hadoop and MapReduce, there may arise certain problems. Developers need to be careful while running their applications in Spark. Instead of running everything on a single node, the work must be distributed over multiple clusters.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-advantage-of-a-parquet-file">What is the advantage of a Parquet file?<a class="hash-link" href="#what-is-the-advantage-of-a-parquet-file" title="Direct link to heading">​</a></h4><p>Parquet file is a columnar format file that helps :</p><ul><li>Limit I/O operations</li><li>Consumes less space</li><li>Fetches only required columns.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-different-op-methods-to-get-result">What are different o/p methods to get result?<a class="hash-link" href="#what-are-different-op-methods-to-get-result" title="Direct link to heading">​</a></h4><ul><li>collect()</li><li>show()</li><li>take()</li><li>foreach(println)</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-two-ways-to-attain-a-schema-from-data">What are two ways to attain a schema from data?<a class="hash-link" href="#what-are-two-ways-to-attain-a-schema-from-data" title="Direct link to heading">​</a></h4><p>Allow Spark to infer a schema from your data or provide a user defined schema. Schema inference is the recommended first step; however, you can customize this schema to your use case with a user defined schema.</p><p>Providing a schema increases performance two to three times, depending on the size of the cluster used. Since Spark doesn&#x27;t infer the schema, it doesn&#x27;t have to read through all of the data. This is also why  there are fewer jobs when a schema is provided: Spark doesn&#x27;t need one job for each partition of the data to infer the schema.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-should-you-define-your-own-schema">Why should you define your own schema?<a class="hash-link" href="#why-should-you-define-your-own-schema" title="Direct link to heading">​</a></h4><p>Benefits of user defined schemas include:</p><ul><li>Avoiding the extra scan of your data needed to infer the schema</li><li>Providing alternative data types</li><li>Parsing only the fields you need</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-json-a-common-format-in-big-data-pipelines">Why is JSON a common format in big data pipelines?<a class="hash-link" href="#why-is-json-a-common-format-in-big-data-pipelines" title="Direct link to heading">​</a></h4><p>Semi-structured data works well with hierarchical data and where schemas need to evolve over time. It also easily contains composite data types such as arrays and maps.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="by-default-how-are-corrupt-records-dealt-with-using--sparkreadjson">By default, how are corrupt records dealt with using  spark.read.json()?<a class="hash-link" href="#by-default-how-are-corrupt-records-dealt-with-using--sparkreadjson" title="Direct link to heading">​</a></h4><p>They appear in a column called  _corrupt_record. These are the records that Spark can&#x27;t read (e.g. when characters are missing from a JSON string).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-key-features-of-apache-spark">Explain the key features of Apache Spark.<a class="hash-link" href="#explain-the-key-features-of-apache-spark" title="Direct link to heading">​</a></h4><ul><li>Polyglot: Spark provides high-level APIs in Java, Scala, Python and R. Spark code can be written in any of these four languages. It provides a shell in Scala and Python. The Scala shell can be accessed through ./bin/spark-shell and Python shell through ./bin/pyspark from the installed directory.</li><li>Speed: Spark runs upto 100 times faster than Hadoop MapReduce for large-scale data processing. Spark is able to achieve this speed through controlled partitioning. It manages data using partitions that help parallelize distributed data processing with minimal network traffic.</li><li>Multiple Formats: Spark supports multiple data sources such as Parquet, JSON, Hive and Cassandra. The Data Sources API provides a pluggable mechanism for accessing structured data though Spark SQL. Data sources can be more than just simple pipes that convert data and pull it into Spark.</li><li>Lazy Evaluation: Apache Spark delays its evaluation till it is absolutely necessary. This is one of the key factors contributing to its speed. For transformations, Spark adds them to a DAG of computation and only when the driver requests some data, does this DAG actually gets executed.</li><li>Real Time Computation: Spark’s computation is real-time and has less latency because of its in-memory computation. Spark is designed for massive scalability and the Spark team has documented users of the system running production clusters with thousands of nodes and supports several computational models.</li><li>Hadoop Integration: Apache Spark provides smooth compatibility with Hadoop. This is a great boon for all the Big Data engineers who started their careers with Hadoop. Spark is a potential replacement for the MapReduce functions of Hadoop, while Spark has the ability to run on top of an existing Hadoop cluster using YARN for resource scheduling.</li><li>Machine Learning: Spark’s MLlib is the machine learning component which is handy when it comes to big data processing. It eradicates the need to use multiple tools, one for processing and one for machine learning. Spark provides data engineers and data scientists with a powerful, unified engine that is both fast and easy to use.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-benefits-of-spark-over-mapreduce-1">What are benefits of Spark over MapReduce?<a class="hash-link" href="#what-are-benefits-of-spark-over-mapreduce-1" title="Direct link to heading">​</a></h4><p>Spark has the following benefits over MapReduce:
Due to the availability of in-memory processing, Spark implements the processing around 10 to 100 times faster than Hadoop MapReduce whereas MapReduce makes use of persistence storage for any of the data processing tasks.
Unlike Hadoop, Spark provides inbuilt libraries to perform multiple tasks from the same core like batch processing, Steaming, Machine learning, Interactive SQL queries. However, Hadoop only supports batch processing.
Hadoop is highly disk-dependent whereas Spark promotes caching and in-memory data storage.
Spark is capable of performing computations multiple times on the same dataset. This is called iterative computation while there is no iterative computing implemented by Hadoop.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-yarn-1">What is YARN?<a class="hash-link" href="#what-is-yarn-1" title="Direct link to heading">​</a></h4><p>Similar to Hadoop, YARN is one of the key features in Spark, providing a central and resource management platform to deliver scalable operations across the cluster. YARN is a distributed container manager, like Mesos for example, whereas Spark is a data processing tool. Spark can run on YARN, the same way Hadoop Map Reduce can run on YARN. Running Spark on YARN necessitates a binary distribution of Spark as built on YARN support.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="do-you-need-to-install-spark-on-all-nodes-of-yarn-cluster">Do you need to install Spark on all nodes of YARN cluster?<a class="hash-link" href="#do-you-need-to-install-spark-on-all-nodes-of-yarn-cluster" title="Direct link to heading">​</a></h4><p>No, because Spark runs on top of YARN. Spark runs independently from its installation. Spark has some options to use YARN when dispatching jobs to the cluster, rather than its own built-in manager, or Mesos. Further, there are some configurations to run YARN. They include master, deploy-mode, driver-memory, executor-memory, executor-cores, and queue.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-there-any-benefit-of-learning-mapreduce-if-spark-is-better-than-mapreduce">Is there any benefit of learning MapReduce if Spark is better than MapReduce?<a class="hash-link" href="#is-there-any-benefit-of-learning-mapreduce-if-spark-is-better-than-mapreduce" title="Direct link to heading">​</a></h4><p>Yes, MapReduce is a paradigm used by many big data tools including Spark as well. It is extremely relevant to use MapReduce when the data grows bigger and bigger. Most tools like Pig and Hive convert their queries into MapReduce phases to optimize them better.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-concept-of-resilient-distributed-dataset-rdd">Explain the concept of Resilient Distributed Dataset (RDD).<a class="hash-link" href="#explain-the-concept-of-resilient-distributed-dataset-rdd" title="Direct link to heading">​</a></h4><p>RDD stands for Resilient Distribution Datasets. An RDD is a fault-tolerant collection of operational elements that run in parallel. The partitioned data in RDD is immutable and distributed in nature. There are primarily two types of RDD:</p><ul><li>Parallelized Collections: Here, the existing RDDs running parallel with one another.</li><li>Hadoop Datasets: They perform functions on each file record in HDFS or other storage systems.</li><li>RDDs are basically parts of data that are stored in the memory distributed across many nodes. RDDs are lazily evaluated in Spark. This lazy evaluation is what contributes to Spark’s speed.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-we-create-rdds-in-spark">How do we create RDDs in Spark?<a class="hash-link" href="#how-do-we-create-rdds-in-spark" title="Direct link to heading">​</a></h4><p>Spark provides two methods to create RDD:</p><ol><li>By parallelizing a collection in your Driver program.</li><li>This makes use of SparkContext’s parallelize</li><li>By loading an external dataset from external storage like HDFS, HBase, shared file system.</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-executor-memory-in-a-spark-application">What is Executor Memory in a Spark application?<a class="hash-link" href="#what-is-executor-memory-in-a-spark-application" title="Direct link to heading">​</a></h4><p>Every spark application has same fixed heap size and fixed number of cores for a spark executor. The heap size is what referred to as the Spark executor memory which is controlled with the spark.executor.memory property of the –executor-memory flag. Every spark application will have one executor on each worker node. The executor memory is basically a measure on how much memory of the worker node will the application utilize.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-partitions-in-apache-spark-1">Define Partitions in Apache Spark.<a class="hash-link" href="#define-partitions-in-apache-spark-1" title="Direct link to heading">​</a></h4><p>As the name suggests, partition is a smaller and logical division of data similar to split in MapReduce. It is a logical chunk of a large distributed data set. Partitioning is the process to derive logical units of data to speed up the processing process. Spark manages data using partitions that help parallelize distributed data processing with minimal network traffic for sending data between executors. By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks. Everything in Spark is a partitioned RDD.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-operations-does-rdd-support">What operations does RDD support?<a class="hash-link" href="#what-operations-does-rdd-support" title="Direct link to heading">​</a></h4><p>RDD (Resilient Distributed Dataset) is main logical data unit in Spark. An RDD has distributed a collection of objects. Distributed means, each RDD is divided into multiple partitions. Each of these partitions can reside in memory or stored on the disk of different machines in a cluster. RDDs are immutable (Read Only) data structure. You can’t change original RDD, but you can always transform it into different RDD with all changes you want.
RDDs support two types of operations: transformations and actions.
Transformations: Transformations create new RDD from existing RDD like map, reduceByKey and filter we just saw. Transformations are executed on demand. That means they are computed lazily.
Actions: Actions return final results of RDD computations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-transformations-in-spark-1">What do you understand by Transformations in Spark?<a class="hash-link" href="#what-do-you-understand-by-transformations-in-spark-1" title="Direct link to heading">​</a></h4><p>Transformations are functions applied on RDD, resulting into another RDD. It does not execute until an action occurs. map() and filter() are examples of transformations, where the former applies the function passed to it on each element of RDD and results into another RDD. The filter() creates a new RDD by selecting elements from current RDD that pass function argument.
val rawData=sc.textFile(&quot;path to/movies.txt&quot;)
val moviesData=rawData.map(x=&gt;x.split(&quot;  &quot;))
As we can see here, rawData RDD is transformed into moviesData RDD. Transformations are lazily evaluated.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-actions-in-spark">Define Actions in Spark.<a class="hash-link" href="#define-actions-in-spark" title="Direct link to heading">​</a></h4><p>An action helps in bringing back the data from RDD to the local machine. An action’s execution is the result of all previously created transformations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system.
reduce() is an action that implements the function passed again and again until one value if left. take() action takes all the values from RDD to a local node.
moviesData.saveAsTextFile(“MoviesData.txt”)
As we can see here, moviesData RDD is saved into a text file called MoviesData.txt.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-functions-of-sparkcore-1">Define functions of SparkCore.<a class="hash-link" href="#define-functions-of-sparkcore-1" title="Direct link to heading">​</a></h4><p>Spark Core is the base engine for large-scale parallel and distributed data processing. The core is the distributed execution engine and the Java, Scala, and Python APIs offer a platform for distributed ETL application development. SparkCore performs various important functions like memory management, monitoring jobs, fault-tolerance, job scheduling and interaction with storage systems. Further, additional libraries, built atop the core allow diverse workloads for streaming, SQL, and machine learning. It is responsible for:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="memory-management-and-fault-recovery">Memory management and fault recovery<a class="hash-link" href="#memory-management-and-fault-recovery" title="Direct link to heading">​</a></h4><p>Scheduling, distributing and monitoring jobs on a cluster Interacting with storage systems</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-pair-rdd">What do you understand by Pair RDD?<a class="hash-link" href="#what-do-you-understand-by-pair-rdd" title="Direct link to heading">​</a></h4><p>Apache defines PairRDD functions class as class PairRDDFunctions<!-- -->[K, V]<!-- --> extends Logging with HadoopMapReduceUtil with Serializable
Special operations can be performed on RDDs in Spark using key/value pairs and such RDDs are referred to as Pair RDDs. Pair RDDs allow users to access each key in parallel. They have a reduceByKey() method that collects data based on each key and a join() method that combines different RDDs together, based on the elements having the same key.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-streaming-implemented-in-spark-explain-with-examples">How is Streaming implemented in Spark? Explain with examples.<a class="hash-link" href="#how-is-streaming-implemented-in-spark-explain-with-examples" title="Direct link to heading">​</a></h4><p>Spark Streaming is used for processing real-time streaming data. Thus it is a useful addition to the core Spark API. It enables high-throughput and fault-tolerant stream processing of live data streams. The fundamental stream unit is DStream which is basically a series of RDDs (Resilient Distributed Datasets) to process the real-time data. The data from different sources like Flume, HDFS is streamed and finally processed to file systems, live dashboards and databases. It is similar to batch processing as the input data is divided into streams like batches.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-there-an-api-for-implementing-graphs-in-spark">Is there an API for implementing graphs in Spark?<a class="hash-link" href="#is-there-an-api-for-implementing-graphs-in-spark" title="Direct link to heading">​</a></h4><p>GraphX is the Spark API for graphs and graph-parallel computation. Thus, it extends the Spark RDD with a Resilient Distributed Property Graph.
The property graph is a directed multi-graph which can have multiple edges in parallel. Every edge and vertex have user defined properties associated with it. Here, the parallel edges allow multiple relationships between the same vertices. At a high-level, GraphX extends the Spark RDD abstraction by introducing the Resilient Distributed Property Graph: a directed multigraph with properties attached to each vertex and edge.
To support graph computation, GraphX exposes a set of fundamental operators (e.g., subgraph, joinVertices, and mapReduceTriplets) as well as an optimized variant of the Pregel API. In addition, GraphX includes a growing collection of graph algorithms and builders to simplify graph analytics tasks.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-pagerank-in-graphx">What is PageRank in GraphX?<a class="hash-link" href="#what-is-pagerank-in-graphx" title="Direct link to heading">​</a></h4><p>PageRank measures the importance of each vertex in a graph, assuming an edge from u to v represents an endorsement of v’s importance by u. For example, if a Twitter user is followed by many others, the user will be ranked highly.
GraphX comes with static and dynamic implementations of PageRank as methods on the PageRank Object. Static PageRank runs for a fixed number of iterations, while dynamic PageRank runs until the ranks converge (i.e., stop changing by more than a specified tolerance). GraphOps allows calling these algorithms directly as methods on Graph.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-machine-learning-implemented-in-spark">How is machine learning implemented in Spark?<a class="hash-link" href="#how-is-machine-learning-implemented-in-spark" title="Direct link to heading">​</a></h4><p>MLlib is scalable machine learning library provided by Spark. It aims at making machine learning easy and scalable with common learning algorithms and use cases like clustering, regression filtering, dimensional reduction, and alike.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-there-a-module-to-implement-sql-in-spark-how-does-it-work">Is there a module to implement SQL in Spark? How does it work?<a class="hash-link" href="#is-there-a-module-to-implement-sql-in-spark-how-does-it-work" title="Direct link to heading">​</a></h4><p>Spark SQL is a new module in Spark which integrates relational processing with Spark’s functional programming API. It supports querying data either via SQL or via the Hive Query Language. For those of you familiar with RDBMS, Spark SQL will be an easy transition from your earlier tools where you can extend the boundaries of traditional relational data processing.
Spark SQL integrates relational processing with Spark’s functional programming. Further, it provides support for various data sources and makes it possible to weave SQL queries with code transformations thus resulting in a very powerful tool.
The following are the four libraries of Spark SQL.</p><ul><li>Data Source API</li><li>DataFrame API</li><li>Interpreter &amp; Optimizer</li><li>SQL Service</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-receivers-in-apache-spark-streaming">What are receivers in Apache Spark Streaming?<a class="hash-link" href="#what-are-receivers-in-apache-spark-streaming" title="Direct link to heading">​</a></h4><p>Receivers are those entities that consume data from different data sources and then move them to Spark for processing. They are created by using streaming contexts in the form of long-running tasks that are scheduled for operating in a round-robin fashion. Each receiver is configured to use up only a single core. The receivers are made to run on various executors to accomplish the task of data streaming. There are two types of receivers depending on how the data is sent to Spark:</p><ul><li><p>Reliable receivers: Here, the receiver sends an acknowledegment to the data sources post successful reception of data and its replication on the Spark storage space.</p></li><li><p>Unreliable receiver: Here, there is no acknowledgement sent to the data sources.</p></li><li><p>What is the difference between repartition and coalesce?</p></li><li><p>Repartition</p><ul><li>Usage repartition can increase/decrease the number of data partitions.</li><li>Repartition creates new data partitions and performs a full shuffle of evenly distributed data.</li><li>Repartition internally calls coalesce with shuffle parameter thereby making it slower than coalesce.</li></ul></li><li><p>Coalesce</p><ul><li>Spark coalesce can only reduce the number of data partitions.</li><li>Coalesce makes use of already existing partitions to reduce the amount of shuffled data unevenly.</li><li>Coalesce is faster than repartition. However, if there are unequal-sized data partitions, the speed might be slightly slower.</li></ul></li><li><p>What are the data formats supported by Spark?
Spark supports both the raw files and the structured file formats for efficient reading and processing. File formats like paraquet, JSON, XML, CSV, RC, Avro, TSV, etc are supported by Spark.</p></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-shuffling-in-spark">What do you understand by Shuffling in Spark?<a class="hash-link" href="#what-do-you-understand-by-shuffling-in-spark" title="Direct link to heading">​</a></h4><p>The process of redistribution of data across different partitions which might or might not cause data movement across the JVM processes or the executors on the separate machines is known as shuffling/repartitioning. Partition is nothing but a smaller logical division of data.
It is to be noted that Spark has no control over what partition the data gets distributed across.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-apache-spark-different-from-mapreduce">How is Apache Spark different from MapReduce?<a class="hash-link" href="#how-is-apache-spark-different-from-mapreduce" title="Direct link to heading">​</a></h4><ul><li><p>MapReduce</p><ul><li>MapReduce does only batch-wise processing of data.</li><li>MapReduce does slow processing of large data.</li><li>MapReduce stores data in HDFS (Hadoop Distributed File System) which makes it take a long time to get the data.</li><li>MapReduce highly depends on disk which makes it to be a high latency framework.</li><li>MapReduce requires an external scheduler for jobs.</li></ul></li><li><p>Apache Spark</p><ul><li>Apache Spark can process the data both in real-time and in batches.</li><li>Apache Spark runs approximately 100 times faster than MapReduce for big data processing.</li><li>Spark stores data in memory (RAM) which makes it easier and faster to retrieve data when needed.</li><li>Spark supports in-memory data storage and caching and makes it a low latency computation framework.</li><li>Spark has its own job scheduler due to the in-memory data computation.</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-working-of-spark-with-the-help-of-its-architecture">Explain the working of Spark with the help of its architecture.<a class="hash-link" href="#explain-the-working-of-spark-with-the-help-of-its-architecture" title="Direct link to heading">​</a></h4><p>Spark applications are run in the form of independent processes that are well coordinated by the Driver program by means of a SparkSession object. The cluster manager or the resource manager entity of Spark assigns the tasks of running the Spark jobs to the worker nodes as per one task per partition principle. There are various iterations algorithms that are repeatedly applied to the data to cache the datasets across various iterations. Every task applies its unit of operations to the dataset within its partition and results in the new partitioned dataset. These results are sent back to the main driver application for further processing or to store the data on the disk. The following diagram illustrates this working as described above:</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-working-of-dag-in-spark">What is the working of DAG in Spark?<a class="hash-link" href="#what-is-the-working-of-dag-in-spark" title="Direct link to heading">​</a></h4><p>DAG stands for Direct Acyclic Graph which has a set of finite vertices and edges. The vertices represent RDDs and the edges represent the operations to be performed on RDDs sequentially. The DAG created is submitted to the DAG Scheduler which splits the graphs into stages of tasks based on the transformations applied to the data. The stage view has the details of the RDDs of that stage.
The working of DAG in spark is defined as per the workflow diagram below:
The first task is to interpret the code with the help of an interpreter. If you use the Scala code, then the Scala interpreter interprets the code.
Spark then creates an operator graph when the code is entered in the Spark console.
When the action is called on Spark RDD, the operator graph is submitted to the DAG Scheduler.
The operators are divided into stages of task by the DAG Scheduler. The stage consists of detailed step-by-step operation on the input data. The operators are then pipelined together.
The stages are then passed to the Task Scheduler which launches the task via the cluster manager to work on independently without the dependencies between the stages.
The worker nodes then execute the task.
Each RDD keeps track of the pointer to one/more parent RDD along with its relationship with the parent. For example, consider the operation val childB=parentA.map() on RDD, then we have the RDD childB that keeps track of its parentA which is called RDD lineage.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="under-what-scenarios-do-you-use-client-and-cluster-modes-for-deployment">Under what scenarios do you use Client and Cluster modes for deployment?<a class="hash-link" href="#under-what-scenarios-do-you-use-client-and-cluster-modes-for-deployment" title="Direct link to heading">​</a></h4><p>In case the client machines are not close to the cluster, then the Cluster mode should be used for deployment. This is done to avoid the network latency caused while communication between the executors which would occur in the Client mode. Also, in Client mode, the entire process is lost if the machine goes offline.
If we have the client machine inside the cluster, then the Client mode can be used for deployment. Since the machine is inside the cluster, there won’t be issues of network latency and since the maintenance of the cluster is already handled, there is no cause of worry in cases of failure.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-spark-streaming-and-how-is-it-implemented-in-spark">What is Spark Streaming and how is it implemented in Spark?<a class="hash-link" href="#what-is-spark-streaming-and-how-is-it-implemented-in-spark" title="Direct link to heading">​</a></h4><p>Spark Streaming is one of the most important features provided by Spark. It is nothing but a Spark API extension for supporting stream processing of data from different sources.
Data from sources like Kafka, Kinesis, Flume, etc are processed and pushed to various destinations like databases, dashboards, machine learning APIs, or as simple as file systems. The data is divided into various streams (similar to batches) and is processed accordingly.
Spark streaming supports highly scalable, fault-tolerant continuous stream processing which is mostly used in cases like fraud detection, website monitoring, website click baits, IoT (Internet of Things) sensors, etc.
Spark Streaming first divides the data from the data stream into batches of X seconds which are called Dstreams or Discretized Streams. They are internally nothing but a sequence of multiple RDDs. The Spark application does the task of processing these RDDs using various Spark APIs and the results of this processing are again returned as batches. The following diagram explains the workflow of the spark streaming process.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-can-you-say-about-spark-datasets">What can you say about Spark Datasets?<a class="hash-link" href="#what-can-you-say-about-spark-datasets" title="Direct link to heading">​</a></h4><p>Spark Datasets are those data structures of SparkSQL that provide JVM objects with all the benefits (such as data manipulation using lambda functions) of RDDs alongside Spark SQL-optimised execution engine. This was introduced as part of Spark since version 1.6.
Spark datasets are strongly typed structures that represent the structured queries along with their encoders.
They provide type safety to the data and also give an object-oriented programming interface.
The datasets are more structured and have the lazy query expression which helps in triggering the action. Datasets have the combined powers of both RDD and Dataframes. Internally, each dataset symbolizes a logical plan which informs the computational query about the need for data production. Once the logical plan is analyzed and resolved, then the physical query plan is formed that does the actual query execution.
Datasets have the following features:
Optimized Query feature: Spark datasets provide optimized queries using Tungsten and Catalyst Query Optimizer frameworks. The Catalyst Query Optimizer represents and manipulates a data flow graph (graph of expressions and relational operators). The Tungsten improves and optimizes the speed of execution of Spark job by emphasizing the hardware architecture of the Spark execution platform.
Compile-Time Analysis: Datasets have the flexibility of analyzing and checking the syntaxes at the compile-time which is not technically possible in RDDs or Dataframes or the regular SQL queries.
Interconvertible: The type-safe feature of datasets can be converted to “untyped” Dataframes by making use of the following methods provided by the Datasetholder:
toDS():Dataset<!-- -->[T]<!-- -->
toDF():DataFrame
toDF(columName:String*):DataFrame
Faster Computation: Datasets implementation are much faster than those of the RDDs which helps in increasing the system performance.
Persistent storage qualified: Since the datasets are both queryable and serializable, they can be easily stored in any persistent storages.
Less Memory Consumed: Spark uses the feature of caching to create a more optimal data layout. Hence, less memory is consumed.
Single Interface Multiple Languages: Single API is provided for both Java and Scala languages. These are widely used languages for using Apache Spark. This results in a lesser burden of using libraries for different types of inputs.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-spark-dataframes">Define Spark DataFrames.<a class="hash-link" href="#define-spark-dataframes" title="Direct link to heading">​</a></h4><p>Spark Dataframes are the distributed collection of datasets organized into columns similar to SQL. It is equivalent to a table in the relational database and is mainly optimized for big data operations.
Dataframes can be created from an array of data from different data sources such as external databases, existing RDDs, Hive Tables, etc. Following are the features of Spark Dataframes:
Spark Dataframes have the ability of processing data in sizes ranging from Kilobytes to Petabytes on a single node to large clusters.
They support different data formats like CSV, Avro, elastic search, etc, and various storage systems like HDFS, Cassandra, MySQL, etc.
By making use of SparkSQL catalyst optimizer, state of art optimization is achieved.
It is possible to easily integrate Spark Dataframes with major Big Data tools using SparkCore.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-executor-memory-in-spark">Define Executor Memory in Spark<a class="hash-link" href="#define-executor-memory-in-spark" title="Direct link to heading">​</a></h4><p>The applications developed in Spark have the same fixed cores count and fixed heap size defined for spark executors. The heap size refers to the memory of the Spark executor that is controlled by making use of the property spark.executor.memory that belongs to the -executor-memory flag. Every Spark applications have one allocated executor on each worker node it runs. The executor memory is a measure of the memory consumed by the worker node that the application utilizes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-functions-of-sparkcore">What are the functions of SparkCore?<a class="hash-link" href="#what-are-the-functions-of-sparkcore" title="Direct link to heading">​</a></h4><p>SparkCore is the main engine that is meant for large-scale distributed and parallel data processing. The Spark core consists of the distributed execution engine that offers various APIs in Java, Python, and Scala for developing distributed ETL applications.
Spark Core does important functions such as memory management, job monitoring, fault-tolerance, storage system interactions, job scheduling, and providing support for all the basic I/O functionalities. There are various additional libraries built on top of Spark Core which allows diverse workloads for SQL, streaming, and machine learning. They are responsible for:
Fault recovery
Memory management and Storage system interactions
Job monitoring, scheduling, and distribution
Basic I/O functions</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-worker-node-1">What do you understand by worker node?<a class="hash-link" href="#what-do-you-understand-by-worker-node-1" title="Direct link to heading">​</a></h4><p>Worker nodes are those nodes that run the Spark application in a cluster. The Spark driver program listens for the incoming connections and accepts them from the executors addresses them to the worker nodes for execution. A worker node is like a slave node where it gets the work from its master node and actually executes them. The worker nodes do data processing and report the resources used to the master. The master decides what amount of resources needs to be allocated and then based on their availability, the tasks are scheduled for the worker nodes by the master.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-some-demerits-of-using-spark-in-applications">What are some demerits of using Spark in applications?<a class="hash-link" href="#what-are-some-demerits-of-using-spark-in-applications" title="Direct link to heading">​</a></h4><p>Despite Spark being the powerful data processing engine, there are certain demerits to using Apache Spark in applications. Some of them are:</p><p>Spark makes use of more storage space when compared to MapReduce or Hadoop which may lead to certain memory-based problems.
Care must be taken by the developers while running the applications. The work should be distributed across multiple clusters instead of running everything on a single node.
Since Spark makes use of “in-memory” computations, they can be a bottleneck to cost-efficient big data processing.
While using files present on the path of the local filesystem, the files must be accessible at the same location on all the worker nodes when working on cluster mode as the task execution shuffles between various worker nodes based on the resource availabilities. The files need to be copied on all worker nodes or a separate network-mounted file-sharing system needs to be in place.
One of the biggest problems while using Spark is when using a large number of small files. When Spark is used with Hadoop, we know that HDFS gives a limited number of large files instead of a large number of small files. When there is a large number of small gzipped files, Spark needs to uncompress these files by keeping them on its memory and network. So large amount of time is spent in burning core capacities for unzipping the files in sequence and performing partitions of the resulting RDDs to get data in a manageable format which would require extensive shuffling overall. This impacts the performance of Spark as much time is spent preparing the data instead of processing them.
Spark doesn’t work well in multi-user environments as it is not capable of handling many users concurrently.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-the-data-transfers-be-minimized-while-working-with-spark">How can the data transfers be minimized while working with Spark?<a class="hash-link" href="#how-can-the-data-transfers-be-minimized-while-working-with-spark" title="Direct link to heading">​</a></h4><p>Data transfers correspond to the process of shuffling. Minimizing these transfers results in faster and reliable running Spark applications. There are various ways in which these can be minimized. They are:</p><ul><li>Usage of Broadcast Variables: Broadcast variables increases the efficiency of the join between large and small RDDs.</li><li>Usage of Accumulators: These help to update the variable values parallelly during execution.
Another common way is to avoid the operations which trigger these reshuffles.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-schemardd-in-spark-rdd">What is SchemaRDD in Spark RDD?<a class="hash-link" href="#what-is-schemardd-in-spark-rdd" title="Direct link to heading">​</a></h4><p>SchemaRDD is an RDD consisting of row objects that are wrappers around integer arrays or strings that has schema information regarding the data type of each column. They were designed to ease the lives of developers while debugging the code and while running unit test cases on the SparkSQL modules. They represent the description of the RDD which is similar to the schema of relational databases. SchemaRDD also provides the basic functionalities of the common RDDs along with some relational query interfaces of SparkSQL.
Consider an example. If you have an RDD named Person that represents a person’s data. Then SchemaRDD represents what data each row of Person RDD represents. If the Person has attributes like name and age, then they are represented in SchemaRDD.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-module-is-used-for-implementing-sql-in-apache-spark">What module is used for implementing SQL in Apache Spark?<a class="hash-link" href="#what-module-is-used-for-implementing-sql-in-apache-spark" title="Direct link to heading">​</a></h4><p>Spark provides a powerful module called SparkSQL which performs relational data processing combined with the power of the functional programming feature of Spark. This module also supports either by means of SQL or Hive Query Language. It also provides support for different data sources and helps developers write powerful SQL queries using code transformations.
The four major libraries of SparkSQL are:</p><ul><li>Data Source API</li><li>DataFrame API</li><li>Interpreter &amp; Catalyst Optimizer</li><li>SQL Services
Spark SQL supports the usage of structured and semi-structured data in the following ways:</li><li>Spark supports DataFrame abstraction in various languages like Python, Scala, and Java along with providing good optimization techniques.</li><li>SparkSQL supports data read and writes operations in various structured formats like JSON, Hive, Parquet, etc.
S+ parkSQL allows data querying inside the Spark program and via external tools that do the JDBC/ODBC connections.</li></ul><p>It is recommended to use SparkSQL inside the Spark applications as it empowers the developers to load the data, query the data from databases and write the results to the destination.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-steps-to-calculate-the-executor-memory">What are the steps to calculate the executor memory?<a class="hash-link" href="#what-are-the-steps-to-calculate-the-executor-memory" title="Direct link to heading">​</a></h4><p>Consider you have the below details regarding the cluster:
Number of nodes = 10
Number of cores in each node = 15 cores
RAM of each node = 61GB
To identify the number of cores, we follow the approach:
Number of Cores = number of concurrent tasks that can be run parallelly by the executor. The optimal value as part of a general rule of thumb is 5.
Hence to calculate the number of executors, we follow the below approach:</p><p>Number of executors = Number of cores/Concurrent Task = 15/5 = 3
Number of executors = Number of nodes <em> Number of executor in each node = 10 </em> 3 = 30 executors per Spark job</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-do-we-need-broadcast-variables-in-spark">Why do we need broadcast variables in Spark?<a class="hash-link" href="#why-do-we-need-broadcast-variables-in-spark" title="Direct link to heading">​</a></h4><p>Broadcast variables let the developers maintain read-only variables cached on each machine instead of shipping a copy of it with tasks. They are used to give every node copy of a large input dataset efficiently. These variables are broadcasted to the nodes using different algorithms to reduce the cost of communication.
Differentiate between Spark Datasets, Dataframes and RDDs.
Criteria	Spark Datasets	Spark Dataframes	Spark RDDs
Representation of Data	Spark Datasets is a combination of Dataframes and RDDs with features like static type safety and object-oriented interfaces.	Spark Dataframe is a distributed collection of data that is organized into named columns.	Spark RDDs are a distributed collection of data without schema.
Optimization	Datasets make use of catalyst optimizers for optimization.	Dataframes also makes use of catalyst optimizer for optimization.	There is no built-in optimization engine.
Schema Projection	Datasets find out schema automatically using SQL Engine.	Dataframes also find the schema automatically.	Schema needs to be defined manually in RDDs.
Aggregation Speed	Dataset aggregation is faster than RDD but slower than Dataframes.	Aggregations are faster in Dataframes due to the provision of easy and powerful APIs.	RDDs are slower than both the Dataframes and the Datasets while performing even simple operations like data grouping.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-apache-spark-be-used-along-with-hadoop-if-yes-then-how">Can Apache Spark be used along with Hadoop? If yes, then how?<a class="hash-link" href="#can-apache-spark-be-used-along-with-hadoop-if-yes-then-how" title="Direct link to heading">​</a></h4><p>Yes! The main feature of Spark is its compatibility with Hadoop. This makes it a powerful framework as using the combination of these two helps to leverage the processing capacity of Spark by making use of the best of Hadoop’s YARN and HDFS features.
Hadoop can be integrated with Spark in the following ways:
HDFS: Spark can be configured to run atop HDFS to leverage the feature of distributed replicated storage.
MapReduce: Spark can also be configured to run alongside the MapReduce in the same or different processing framework or Hadoop cluster. Spark and MapReduce can be used together to perform real-time and batch processing respectively.
YARN: Spark applications can be configured to run on YARN which acts as the cluster management framework.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-sparse-vectors-how-are-they-different-from-dense-vectors">What are Sparse Vectors? How are they different from dense vectors?<a class="hash-link" href="#what-are-sparse-vectors-how-are-they-different-from-dense-vectors" title="Direct link to heading">​</a></h4><p>Sparse vectors consist of two parallel arrays where one array is for storing indices and the other for storing values. These vectors are used to store non-zero values for saving space.</p><p>val sparseVec: Vector = Vectors.sparse(5, Array(0, 4), Array(1.0, 2.0))
In the above example, we have the vector of size 5, but the non-zero values are there only at indices 0 and 4.
Sparse vectors are particularly useful when there are very few non-zero values. If there are cases that have only a few zero values, then it is recommended to use dense vectors as usage of sparse vectors would introduce the overhead of indices which could impact the performance.
Dense vectors can be defines as follows:
val denseVec = Vectors.dense(4405d,260100d,400d,5.0,4.0,198.0,9070d,1.0,1.0,2.0,0.0)
Usage of sparse or dense vectors does not impact the results of calculations but when used inappropriately, they impact the memory consumed and the speed of calculation.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-are-automatic-clean-ups-triggered-in-spark-for-handling-the-accumulated-metadata">How are automatic clean-ups triggered in Spark for handling the accumulated metadata?<a class="hash-link" href="#how-are-automatic-clean-ups-triggered-in-spark-for-handling-the-accumulated-metadata" title="Direct link to heading">​</a></h4><p>The clean-up tasks can be triggered automatically either by setting spark.cleaner.ttl parameter or by doing the batch-wise division of the long-running jobs and then writing the intermediary results on the disk.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-caching-relevant-in-spark-streaming">How is Caching relevant in Spark Streaming?<a class="hash-link" href="#how-is-caching-relevant-in-spark-streaming" title="Direct link to heading">​</a></h4><p>Spark Streaming involves the division of data stream’s data into batches of X seconds called DStreams. These DStreams let the developers cache the data into the memory which can be very useful in case the data of DStream is used for multiple computations. The caching of data can be done using the cache() method or using persist() method by using appropriate persistence levels. The default persistence level value for input streams receiving data over the networks such as Kafka, Flume, etc is set to achieve data replication on 2 nodes to accomplish fault tolerance.
Caching using cache method:
val cacheDf = dframe.cache()
Caching using persist method:
val persistDf = dframe.persist(StorageLevel.MEMORY_ONLY)
The main advantages of caching are:</p><ul><li>Cost efficiency: Since Spark computations are expensive, caching helps to achieve reusing of data and this leads to reuse computations which can save the cost of operations.</li><li>Time-efficient: The computation reusage leads to saving a lot of time.</li><li>More Jobs Achieved: By saving time of computation execution, the worker nodes can perform/execute more jobs.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-piping-in-spark">Define Piping in Spark.<a class="hash-link" href="#define-piping-in-spark" title="Direct link to heading">​</a></h4><p>Apache Spark provides the pipe() method on RDDs which gives the opportunity to compose different parts of occupations that can utilize any language as needed as per the UNIX Standard Streams. Using the pipe() method, the RDD transformation can be written which can be used for reading each element of the RDD as String. These can be manipulated as required and the results can be displayed as String.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-api-is-used-for-graph-implementation-in-spark">What API is used for Graph Implementation in Spark?<a class="hash-link" href="#what-api-is-used-for-graph-implementation-in-spark" title="Direct link to heading">​</a></h4><p>Spark provides a powerful API called GraphX that extends Spark RDD for supporting graphs and graph-based computations. The extended property of Spark RDD is called as Resilient Distributed Property Graph which is a directed multi-graph that has multiple parallel edges. Each edge and the vertex has associated user-defined properties. The presence of parallel edges indicates multiple relationships between the same set of vertices. GraphX has a set of operators such as subgraph, mapReduceTriplets, joinVertices, etc that can support graph computation. It also includes a large collection of graph builders and algorithms for simplifying tasks related to graph analytics.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-achieve-machine-learning-in-spark">How can you achieve machine learning in Spark?<a class="hash-link" href="#how-can-you-achieve-machine-learning-in-spark" title="Direct link to heading">​</a></h4><p>Spark provides a very robust, scalable machine learning-based library called MLlib. This library aims at implementing easy and scalable common ML-based algorithms and has the features like classification, clustering, dimensional reduction, regression filtering, etc. More information about this library can be obtained in detail from Spark’s official documentation site here: <a href="https://spark.apache.org/docs/latest/ml-guide.html" target="_blank" rel="noopener noreferrer">https://spark.apache.org/docs/latest/ml-guide.html</a></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-limitations-of-spark">What are the limitations of Spark?<a class="hash-link" href="#what-are-the-limitations-of-spark" title="Direct link to heading">​</a></h4><p>Does not have its file management system. Thus, it needs to integrate with Hadoop or other cloud-based data platforms.
In-memory capability can become a bottleneck. Especially when it comes to cost-efficient processing of Bigdata.
Memory consumption is very high. And the issues for the same are not handled in a user-friendly manner. d. It requires large data.
MLlib lack in some available algorithms, for example, Tanimoto distance.
Read more Apache Spark Limitations in detail.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="compare-hadoop-and-spark">Compare Hadoop and Spark.<a class="hash-link" href="#compare-hadoop-and-spark" title="Direct link to heading">​</a></h4><ul><li>Cost Efficient – In Hadoop, during replication, a large number of servers, huge amount of storage, and the large data center is required. Thus, installing and using Apache Hadoop is expensive. While using Apache Spark is a cost effective solution for big data environment.</li><li>Performance – The basic idea behind Spark was to improve the performance of data processing. And Spark did this to 10x-100x times. And all the credit of faster processing in Spark goes to in-memory processing of data. In Hadoop, the data processing takes place in disc while in Spark the data processing takes place in memory. It moves to the disc only when needed. The Spark in-memory computation is beneficial for iterative algorithms. When it comes to performance, because of batch processing in Hadoop it’s processing is quite slow while the processing speed of Apache is faster as it supports micro-batching.</li><li>Ease of development – The core in Spark is the distributed execution engine. Various languages are supported by Apache Spark for distributed application development. For example, Java, Scala, Python, and R. On the top of spark core, various libraries are built that enables workload. they make use of streaming, SQL, graph and machine learning. Hadoop also supports some of these workloads but Spark eases the development by combining all into the same application. d. Failure recovery: The method of Fault</li><li>Failure recovery – The method of Fault Recovery is different in both Apache Hadoop and Apache Spark. In Hadoop after every operation data is written to disk. The data objects are stored in Spark in RDD distributed across data cluster. The RDDs are either in memory or on disk and provides full recovery from faults or failure.</li><li>File Management System – Hadoop has its own File Management System called HDFS (Hadoop Distributed File System). While Apache Spark an integration with one, it may be even HDFS. Thus, Hadoop can run over Apache Spark.</li><li>Computation model – Apache Hadoop uses batch processing model i.e. it takes a large amount of data and processes it. But Apache Spark adopts micro-batching. Must for handling near real time processing data model. When it comes to performance, because of batch processing in Hadoop it’s processing is quite slow. The processing speed of Apache is faster as it supports micro-batching.</li><li>Lines of code – Apache Hadoop has near about 23, 00,000 lines of code while Apache Spark has 20,000 lines of code.</li><li>Caching – By caching partial result in memory of distributed workers Spark ensures low latency computations. While MapReduce is completely disk oriented, there is no provision of caching.</li><li>Scheduler – Because of in-memory computation in Spark, it acts as its own flow scheduler. While with Hadoop MapReduce we need an extra job scheduler like Azkaban or Oozie so that we can schedule complex flows.</li><li>Spark API – Because of very Strict API in Hadoop MapReduce, it is not versatile. But since Spark discards many low-level details it is more productive.</li><li>Window criteria – Apache Spark has time-based window criteria. But Apache Hadoop does not have window criteria since it does not support streaming.</li><li>Faster – Apache Hadoop executes job 10 to 100 times faster than Apache Hadoop MapReduce.</li><li>License – Both Apache Hadoop and Apache MapReduce has a License Version 2.0.</li><li>DAG() – In Apache Spark, there is cyclic data flow in machine learning algorithm, which is a direct acyclic graph. While in Hadoop MapReduce data flow does not have any loops, rather it is a chain of the image.</li><li>Memory Management – Apache Spark has automatic memory management system. While Memory Management in Apache Hadoop can be either statistic or dynamic.</li><li>Iterative Processing – In Apache Spark, the data iterates in batches. Here processing and scheduling of each iteration are separate. While in Apache Hadoop there is no provision for iterative processing.</li><li>Latency – The time taken for processing by Apache Spark is less as compared to Hadoop since it caches its data on memory by means of RDD, thus the latency of Apache Spark is less as compared to Hadoop.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-lazy-evaluation-in-spark">What is lazy evaluation in Spark?<a class="hash-link" href="#what-is-lazy-evaluation-in-spark" title="Direct link to heading">​</a></h4><p>lazy evaluation known as call-by-need is a strategy that delays the execution until one requires a value. The transformation in Spark is lazy in nature. Spark evaluate them lazily. When we call some operation in RDD it does not execute immediately; Spark maintains the graph of which operation it demands. We can execute the operation at any instance by calling the action on the data. The data does not loads until it is necessary.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-benefits-of-lazy-evaluation">What are the benefits of lazy evaluation?<a class="hash-link" href="#what-are-the-benefits-of-lazy-evaluation" title="Direct link to heading">​</a></h4><p>Using lazy evaluation we can:</p><ul><li>Increase the manageability of the program.</li><li>Saves computation overhead and increases the speed of the system.</li><li>Reduces the time and space complexity.</li><li>provides the optimization by reducing the number of queries.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-persistence">What do you mean by Persistence?<a class="hash-link" href="#what-do-you-mean-by-persistence" title="Direct link to heading">​</a></h4><p>RDD persistence is an optimization technique which saves the result of RDD evaluation. Using this we save the intermediate result for further use. It reduces the computation overhead. We can make persisted RDD through cache() and persist() methods. It is a key tool for the interactive algorithm. Because, when RDD is persisted each node stores any partition of it that it computes in memory. Thus makes it reusable for future use. This process speeds up the further computation ten times.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-run-time-architecture-of-spark">Explain the run time architecture of Spark?<a class="hash-link" href="#explain-the-run-time-architecture-of-spark" title="Direct link to heading">​</a></h4><p>The components of the run-time architecture of Spark are as follows:</p><ul><li>The Driver – The main() method of the program runs in the driver. The process that runs the user code which creates RDDs performs transformation and action, and also creates SparkContext is called diver. When the Spark Shell is launched, this signifies that we have created a driver program. The application finishes, as the driver terminates. Finally, driver program splits the Spark application into the task and schedules them to run on the executor.</li><li>Cluster Manager –  Spark depends on cluster manager to launch executors. In some cases, even the drivers are launched by cluster manager. It is a pluggable component in Spark. On the cluster manager, the Spark scheduler schedules the jobs and action within a spark application in FIFO fashion. Alternatively, the scheduling can also be done in Round Robin fashion. The resources used by a Spark application can also be dynamically adjusted based on the workload. Thus, the application can free unused resources and request them again when there is a demand. This is available on all coarse-grained cluster managers, i.e. standalone mode, YARN mode, and Mesos coarse-grained mode.</li><li>The Executors –  Each task in the Spark job runs in the Spark executors. thus, Executors are launched once in the beginning of Spark Application and then they run for the entire lifetime of an application. Even after the failure of Spark executor, the Spark application can continue with ease.
There are two main roles of the executors:</li><li>Runs the task that makes up the application and returns the result to the driver.</li><li>Provide in-memory storage for RDDs that the user program cache.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-dsm-and-rdd">What is the difference between DSM and RDD?<a class="hash-link" href="#what-is-the-difference-between-dsm-and-rdd" title="Direct link to heading">​</a></h4><p>a) READ
RDD: In RDD the read operation is coarse grained or fine grained. In coarse-grained we can transform the whole dataset but not an individual element. While in fine-grained we do the transformation of an individual element on a dataset.
Distributed Shared Memory: The read operation in Distributed shared memory is fine-grained.
b) Write:
RDD: The write operation is coarse-grained in RDD.
Distributed Shared Memory: In distributed shared system the write operation is fine grained.
c) Consistency:
RDD: The consistency of RDD is trivial meaning it is immutable in nature. Any changes made to an RDD cannot roll back, it is permanent. So the level of consistency is high.
Distributed Shared Memory: The system guarantees that if the programmer follows the rules, the memory will be consistent. It also guarantees that the results of memory operations will be predictable.
d) Fault-recovery mechanism:
RDD: Using lineage graph at any point in time we can easily find the lost data in an RDD.
Distributed Shared Memory: Fault tolerance is achieved by a checkpointing technique. It allows applications to roll back to a recent checkpoint rather than restarting.
e) Straggler mitigation: Stragglers, in general, are those tasks that take more time to complete than their peers.
RDD: in RDD it is possible to mitigate stragglers using backup task.
Distributed Shared Memory: It is quite difficult to achieve straggler mitigation.
f) Behavior if not enough RAM:
RDD: If there is not enough space to store RDD in RAM then the RDDs are shifted to disk.
Distributed Shared Memory: In this type of system the performance decreases if the RAM runs out of storage.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-data-transfer-be-minimized-when-working-with-apache-spark">How can data transfer be minimized when working with Apache Spark?<a class="hash-link" href="#how-can-data-transfer-be-minimized-when-working-with-apache-spark" title="Direct link to heading">​</a></h4><p>By minimizing data transfer and avoiding shuffling of data we can increase the performance. In Apache Spark, we can minimize the data transfer in three ways:
By using a broadcast variable – Since broadcast variable increases the efficiency of joins between small and large RDDs. the broadcast variable allows keeping a read-only variable cached on every machine in place of shipping a copy of it with tasks. We create broadcast variable v by calling SparlContext.broadcast(v) and we can access its value by calling the value method.
Using Accumulator – Using accumulator we can update the value of a variable in parallel while executing. Accumulators can only be added through the associative and commutative operation. We can also implement counters (as in MapReduce) or sums using an accumulator. Users can create named or unnamed accumulator. We can create numeric accumulator by calling SparkContext.longAccumulator() or SparkContext.doubleAccumulator() for Long or Double respectively.
By avoiding operations like ByKey, repartition or any other operation that trigger shuffle. we can minimize the data transfer.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-does-apache-spark-handles-accumulated-metadata">How does Apache Spark handles accumulated Metadata?<a class="hash-link" href="#how-does-apache-spark-handles-accumulated-metadata" title="Direct link to heading">​</a></h4><p>By triggering automatic cleanup Spark handles the automatic Metadata. We can trigger cleanup by setting the parameter “spark.cleaner.ttl“. the default value for this is infinite. It tells for how much duration Spark will remember the metadata. It is periodic cleaner. And also ensure that metadata older than the set duration will vanish. Thus, with its help, we can run Spark for many hours.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-common-faults-of-the-developer-while-using-apache-spark">What are the common faults of the developer while using Apache Spark?<a class="hash-link" href="#what-are-the-common-faults-of-the-developer-while-using-apache-spark" title="Direct link to heading">​</a></h4><p>The common mistake by developers are:
Customer hit web-service several time by using multiple clusters.
Customer runs everything on local node instead of distributing it.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-among-the-two-is-preferable-for-the-project--hadoop-mapreduce-or-apache-spark">Which among the two is preferable for the project- Hadoop MapReduce or Apache Spark?<a class="hash-link" href="#which-among-the-two-is-preferable-for-the-project--hadoop-mapreduce-or-apache-spark" title="Direct link to heading">​</a></h4><p>The answer to this question depends on the type of project one has. As we all know Spark makes use of a large amount of RAM and also needs a dedicated machine to provide an effective result. Thus the answer depends on the project and the budget of the organization.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-the-popular-use-cases-of-apache-spark">List the popular use cases of Apache Spark.<a class="hash-link" href="#list-the-popular-use-cases-of-apache-spark" title="Direct link to heading">​</a></h4><p>The most popular use-cases of Apache Spark are:</p><ol><li>Streaming</li><li>Machine Learning</li><li>interactive Analysis</li><li>fog computing</li><li>Using Spark in the real world</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-sparkexecutormemory-in-a-spark-application">What is Spark.executor.memory in a Spark Application?<a class="hash-link" href="#what-is-sparkexecutormemory-in-a-spark-application" title="Direct link to heading">​</a></h4><p>The default value for this is 1 GB. It refers to the amount of memory that will be used per executor process.
We have categorized the above Spark Interview Questions and Answers for Freshers and Experienced-
Spark Interview Questions and Answers for Fresher – Q.No.1-8, 37
Spark Interview Questions and Answers for Experienced – Q.No. 9-36, 38
Follow this link to read more Spark Basic interview Questions with Answers.
b. Spark SQL Interview Questions and Answers
In this section, we will discuss some basic Spark SQL Interview Questions and Answers.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-dataframes">What is DataFrames?<a class="hash-link" href="#what-is-dataframes" title="Direct link to heading">​</a></h4><p>It is a collection of data which organize in named columns. It is theoretically equivalent to a table in relational database. But it is more optimized. Just like RDD, DataFrames evaluates lazily. Using lazy evaluation we can optimize the execution. It optimizes by applying the techniques such as bytecode generation and predicate push-downs.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-advantages-of-dataframe">What are the advantages of DataFrame?<a class="hash-link" href="#what-are-the-advantages-of-dataframe" title="Direct link to heading">​</a></h4><p>It makes large data set processing even easier. Data Frame also allows developers to impose a structure onto a distributed collection of data. As a result, it allows higher-level abstraction.
Data frame is both space and performance efficient.
It can deal with both structured and unstructured data formats, for example, Avro, CSV etc . And also storage systems like HDFS, HIVE tables, MySQL, etc.
The DataFrame API’s are available in various programming languages. For example Java, Scala, Python, and R.
It provides Hive compatibility. As a result, we can run unmodified Hive queries on existing Hive warehouse.
Catalyst tree transformation uses DataFrame in four phases: a) Analyze logical plan to solve references. b) Logical plan optimization c) Physical planning d) Code generation to compile part of the query to Java bytecode.
It can scale from kilobytes of data on the single laptop to petabytes of data on the large cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-dataset">What is DataSet?<a class="hash-link" href="#what-is-dataset" title="Direct link to heading">​</a></h4><p>Spark Datasets are the extension of Dataframe API. It creates object-oriented programming interface and type-safety. Dataset is Spark 1.6 release. It makes use of Spark’s catalyst optimizer. It reveals expressions and data fields to a query optimizer. Dataset also influences fast in-memory encoding. It also provides provision for compile time type-safety. We can check for errors in an application when they run.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-advantages-of-datasets">What are the advantages of DataSets?<a class="hash-link" href="#what-are-the-advantages-of-datasets" title="Direct link to heading">​</a></h4><p>It provides run-time type safety.
Influences fast in-memory encoding.
It provides a custom view of structured and semi-structured data.
It owns rich semantics and an easy set of domain-specific operations, as a result, it facilitates the use of structured data.
Dataset API decreases the use of memory. As Spark knows the structure of data in the dataset, thus it creates an optimal layout in memory while caching.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-catalyst-framework">Explain Catalyst framework.<a class="hash-link" href="#explain-catalyst-framework" title="Direct link to heading">​</a></h4><p>The Catalyst is a framework which represents and manipulate a DataFrame graph. Data flow graph is a tree of relational operator and expressions. The three main features of catalyst are:
It has a TreeNode library for transforming tree. They are expressed as Scala case classes.
A logical plan representation for relational operator.
Expression library.
The TreeNode builds a query optimizer. It contains a number of the query optimizer. Catalyst Optimizer supports both rule-based and cost-based optimization. In rule-based optimization the optimizer use set of rule to determine how to execute the query. While the cost based optimization finds the most suitable way to carry out SQL statement. In cost-based optimization, many plans are generates using rules. And after this, it computes their cost. Catalyst optimizer makes use of standard features of Scala programming like pattern matching.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-dstream">What is DStream?<a class="hash-link" href="#what-is-dstream" title="Direct link to heading">​</a></h4><p>DStream is the high-level abstraction provided by Spark Streaming. It represents a continuous stream of data. Thus, DStream is internally a sequence of RDDs. There are two ways to create DStream:
by using data from different sources such as Kafka, Flume, and Kinesis.
by applying high-level operations on other DStreams.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-different-transformation-on-dstream">Explain different transformation on DStream.<a class="hash-link" href="#explain-different-transformation-on-dstream" title="Direct link to heading">​</a></h4><p>DStream is a basic abstraction of Spark Streaming. It is a continuous sequence of RDD which represents a continuous stream of data. Like RDD, DStream also supports many transformations which are available on normal Spark RDD. For example, map(func), flatMap(func), filter(func) etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-written-ahead-log-or-journaling">What is written ahead log or journaling?<a class="hash-link" href="#what-is-written-ahead-log-or-journaling" title="Direct link to heading">​</a></h4><p>The write-ahead log is a technique that provides durability in a database system. It works in the way that all the operation that applies on data, we write it to write-ahead log. The logs are durable in nature. Thus, when the failure occurs we can easily recover the data from these logs. When we enable the write-ahead log Spark stores the data in fault-tolerant file system.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-first-operation-in-apache-spark-rdd">Explain first operation in Apache Spark RDD.<a class="hash-link" href="#explain-first-operation-in-apache-spark-rdd" title="Direct link to heading">​</a></h4><p>It is an action and returns the first element of the RDD.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="describe-join-operation-how-is-outer-join-supported">Describe join operation. How is outer join supported?<a class="hash-link" href="#describe-join-operation-how-is-outer-join-supported" title="Direct link to heading">​</a></h4><p>join() is transformation and is in package org.apache.spark.rdd.pairRDDFunction
def join<!-- -->[W]<!-- -->(other: RDD<!-- -->[(K, W)]<!-- -->): RDD<!-- -->[(K, (V, W))]<!-- -->Permalink
Return an RDD containing all pairs of elements with matching keys in this and other.
Each pair of elements will returns as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
It is joining two datasets. When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="describe-coalesce-operation-when-can-you-coalesce-to-a-larger-number-of-partitions-explain">Describe coalesce operation. When can you coalesce to a larger number of partitions? Explain.<a class="hash-link" href="#describe-coalesce-operation-when-can-you-coalesce-to-a-larger-number-of-partitions-explain" title="Direct link to heading">​</a></h4><p>It is a transformation and it’s in a package org.apache.spark.rdd.ShuffledRDD
Return a new RDD that is reduced into numPartitions partitions.
This results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead, each of the 100 new partitions will claim 10 of the current partitions.
However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can pass shuffle = true. This will add a shuffle step but means the current upstream partitions will execut in parallel (per whatever the current partitioning is).
Note: With shuffle = true, you can actually coalesce to a larger number of partitions. This is useful if you have a small number of partitions, say 100, potentially with a few partitions being abnormally large. Calling coalesce(1000, shuffle = true) will result in 1000 partitions with the data distributed using a hash partitioner.
Coalesce() operation changes a number of the partition where data is stored. It combines original partitions to a new number of partitions, so it reduces the number of partitions. Coalesce() operation is an optimized version of repartition that allows data movement, but only if you are decreasing the number of RDD partitions. It runs operations more efficiently after filtering large datasets.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="describe-partition-and-partitioner-in-apache-spark">Describe Partition and Partitioner in Apache Spark.<a class="hash-link" href="#describe-partition-and-partitioner-in-apache-spark" title="Direct link to heading">​</a></h4><p>Partition in Spark is similar to split in HDFS. A partition in Spark is a logical division of data stored on a node in the cluster. They are the basic units of parallelism in Apache Spark. RDDs are a collection of partitions. When some actions are executed, a task is launched per partition.
By default, partitions are automatically created by the framework. However, the number of partitions in Spark are configurable to suit the needs. For the number of partitions, if spark.default.parallelism is set, then we should use the value from SparkContext defaultParallelism, othewrwise we should use the max number of upstream partitions. Unless spark.default.parallelism is set, the number of partitions will be the same as that of the largest upstream RDD, as this would least likely cause out-of-memory errors.
A partitioner is an object that defines how the elements in a key-value pair RDD are partitioned by key, maps each key to a partition ID from 0 to numPartitions – 1. It captures the data distribution at the output. With the help of partitioner, the scheduler can optimize the future operations. The contract of partitioner ensures that records for a given key have to reside on a single partition.
We should choose a partitioner to use for a cogroup-like operations. If any of the RDDs already has a partitioner, we should choose that one. Otherwise, we use a default HashPartitioner.</p><p>There are three types of partitioners in Spark :
Hash Partitioner
Range Partitioner
Custom Partitioner
Hash – Partitioner: Hash- partitioning attempts to spread the data evenly across various partitions based on the key.
Range – Partitioner: In Range- Partitioning method, tuples having keys with same range will appear on the same machine.
RDDs can create with specific partitioning in two ways :
i) Providing explicit partitioner by calling partitionBy method on an RDD
ii) Applying transformations that return RDDs with specific partitioners.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-manually-partition-the-rdd">How can you manually partition the RDD?<a class="hash-link" href="#how-can-you-manually-partition-the-rdd" title="Direct link to heading">​</a></h4><p>When we create the RDD from a file stored in HDFS.
data = context.textFile(&quot;/user/dataflair/file-name&quot;)
By default one partition is created for one block. ie. if we have a file of size 1280 MB (with 128 MB block size) there will be 10 HDFS blocks, hence the similar number of partitions (10) will create.
If you want to create more partitions than the number of blocks, you can specify the same while RDD creation:
data = context.textFile(&quot;/user/dataflair/file-name&quot;, 20)
It will create 20 partitions for the file. ie for each block 2 partitions will create.
NOTE: It is often recommended to have more no of partitions than no of the block, it improves the performance</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-api-create-or-replace-tempview">Explain API create Or Replace TempView.<a class="hash-link" href="#explain-api-create-or-replace-tempview" title="Direct link to heading">​</a></h4><p>It’s basic Dataset function and under org.apache.spark.sql
def createOrReplaceTempView(viewName: String): Unit
Creates a temporary view using the given name.
scala&gt; df.createOrReplaceTempView(&quot;titanicdata&quot;)</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-various-advantages-of-dataframe-over-rdd-in-apache-spark">What are the various advantages of DataFrame over RDD in Apache Spark?<a class="hash-link" href="#what-are-the-various-advantages-of-dataframe-over-rdd-in-apache-spark" title="Direct link to heading">​</a></h4><p>DataFrames are the distributed collection of data. In DataFrame, data is organized into named columns. It is conceptually similar to a table in a relational database.
we can construct DataFrames from a wide array of sources. Such as structured data files, tables in Hive, external databases, or existing RDDs.
As same as RDDs, DataFrames are evaluated lazily(Lazy Evaluation). In other words, computation only happens when an action (e.g. display result, save output) is required.
Out of the box, DataFrame supports reading data from the most popular formats, including JSON files, Parquet files, Hive tables. Also, can read from distributed file systems (HDFS), local file systems, cloud storage (S3), and external relational database systems through JDBC. In addition, through Spark SQL’s external data sources API, DataFrames can extend to support any third-party data formats or sources. Existing third-party extensions already include Avro, CSV, ElasticSearch, and Cassandra.
There is much more to know about DataFrames. Refer link: Spark SQL DataFrame</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-a-dataset-and-what-are-its-advantages-over-dataframe-and-rdd">What is a DataSet and what are its advantages over DataFrame and RDD?<a class="hash-link" href="#what-is-a-dataset-and-what-are-its-advantages-over-dataframe-and-rdd" title="Direct link to heading">​</a></h4><p>In Apache Spark, Datasets are an extension of DataFrame API. It offers object-oriented programming interface. Through Spark SQL, it takes advantage of Spark’s Catalyst optimizer by exposing e data fields to a query planner.
In SparkSQL, Dataset is a data structure which is strongly typed and is a map to a relational schema. Also, represents structured queries with encoders. DataSet has been released in Spark 1.6.
In serialization and deserialization (SerDe) framework, encoder turns out as a primary concept in Spark SQL. Encoders handle all translation process between JVM objects and Spark’s internal binary format. In Spark, we have built-in encoders those are very advanced. Even they generate bytecode to interact with off-heap data.
On-demand access to individual attributes without having to de-serialize an entire object is provided by an encoder. Spark SQL uses a SerDe framework, to make input-output time and space efficient. Due to encoder knows the schema of record, it became possible to achieve serialization as well as deserialization.
Spark Dataset is structured and lazy query expression(lazy Evolution) that triggers the action. Internally dataset represents a logical plan. The logical plan tells the computational query that we need to produce the data. the logical plan is a base catalyst query plan for the logical operator to form a logical query plan. When we analyze this and resolve we can form a physical query plan.
As Dataset introduced after RDD and DataFrame, it clubs the features of both. It offers the following similar features:</p><ol><li>The convenience of RDD.</li><li>Performance optimization of DataFrame.</li><li>Static type-safety of Scala.
Hence, we have observed that Datasets provides a more functional programming interface to work with structured data.
To know more detailed information about DataSets, refer link: Spark Dataset</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="on-what-all-basis-can-you-differentiate-rdd-and-dataframe-and-dataset">On what all basis can you differentiate RDD and DataFrame and DataSet?<a class="hash-link" href="#on-what-all-basis-can-you-differentiate-rdd-and-dataframe-and-dataset" title="Direct link to heading">​</a></h4><p>DataFrame: A Data Frame is used for storing data into tables. It is equivalent to a table in a relational database but with richer optimization. Spark DataFrame is a data abstraction and domain-specific language (DSL) applicable on a structure and semi-structured data. It is distributed the collection of data in the form of named column and row. It has a matrix-like structure whose column may be different types (numeric, logical, factor, or character ). We can say data frame has the two-dimensional array like structure where each column contains the value of one variable and row contains one set of values for each column and combines feature of list and matrices
RDD is the representation of a set of records, immutable collection of objects with distributed computing. RDD is a large collection of data or RDD is an array of reference of partitioned objects. Each and every dataset in RDD is logically partitioned across many servers so that they can compute on different nodes of the cluster. RDDs are fault tolerant i.e. self-recovered/recomputed in the case of failure. The dataset can load externally by the users which can be in the form of JSON file, CSV file, text file or database via JDBC with no specific data structure.
DataSet in Apache Spark, Datasets are an extension of DataFrame API. It offers object-oriented programming interface. Through Spark SQL, it takes advantage of Spark’s Catalyst optimizer by exposing e data fields to a query planner.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-level-of-parallelism-in-spark-streaming">Explain the level of parallelism in Spark Streaming.<a class="hash-link" href="#explain-the-level-of-parallelism-in-spark-streaming" title="Direct link to heading">​</a></h4><p>In order to reduce the processing time, one need to increase the parallelism. In Spark Streaming, there are three ways to increase the parallelism:
Increase the number of receivers : If there are too many records for single receiver (single machine) to read in and distribute so that is bottleneck. So we can increase the no. of receiver depends on scenario.
Re-partition the receive data : If one is not in a position to increase the no. of receivers in that case redistribute the data by re-partitioning.
Increase parallelism in aggregation :
for complete guide on Spark Streaming you may refer to Apache Spark-Streaming guide</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="discuss-writeahead-logging-in-apache-spark-streaming">Discuss writeahead logging in Apache Spark Streaming.<a class="hash-link" href="#discuss-writeahead-logging-in-apache-spark-streaming" title="Direct link to heading">​</a></h4><p>There are two types of failures in any Apache Spark job – Either the driver failure or the worker failure.
When any worker node fails, the executor processes running in that worker node will kill, and the tasks which were scheduled on that worker node will be automatically moved to any of the other running worker nodes, and the tasks will accomplish.
When the driver or master node fails, all of the associated worker nodes running the executors will kill, along with the data in each of the executors’ memory. In the case of files being read from reliable and fault tolerant file systems like HDFS, zero data loss is always guaranteed, as the data is ready to be read anytime from the file system. Checkpointing also ensures fault tolerance in Spark by periodically saving the application data in specific intervals.
In the case of Spark Streaming application, zero data loss is not always guaranteed, as the data will buffer in the executors’ memory until they get processed. If the driver fails, all of the executors will kill, with the data in their memory, and the data cannot recover.
To overcome this data loss scenario, Write Ahead Logging (WAL) has been introduced in Apache Spark 1.2. With WAL enabled, the intention of the operation is first noted down in a log file, such that if the driver fails and is restarted, the noted operations in that log file can apply to the data. For sources that read streaming data, like Kafka or Flume, receivers will be receiving the data, and those will store in the executor’s memory. With WAL enabled, these received data will also store in the log files.
WAL can enable by performing the below:</p><ol><li>Setting the checkpoint directory, by using streamingContext.checkpoint(path)</li><li>Enabling the WAL logging, by setting spark.stream.receiver.WriteAheadLog.enable to True.</li></ol><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-mean-by-speculative-execution-in-apache-spark">What do you mean by Speculative execution in Apache Spark?<a class="hash-link" href="#what-do-you-mean-by-speculative-execution-in-apache-spark" title="Direct link to heading">​</a></h4><p>The Speculative task in Apache Spark is task that runs slower than the rest of the task in the job.It is health check process that verifies the task is speculated, meaning the task that runs slower than the median of successfully completed task in the task sheet. Such tasks are submitted to another worker. It runs the new copy in parallel rather than shutting down the slow task.</p><p>In the cluster deployment mode, the thread starts as TaskSchedulerImp1with spark.speculation enabled. It executes periodically every spark.speculation.interval after the initial spark.speculation.interval passes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-do-you-parse-data-in-xml-which-kind-of-class-do-you-use-with-java-to-pass-data">How do you parse data in XML? Which kind of class do you use with java to pass data?<a class="hash-link" href="#how-do-you-parse-data-in-xml-which-kind-of-class-do-you-use-with-java-to-pass-data" title="Direct link to heading">​</a></h4><p>One way to parse the XML data in Java is to use the JDOM library. One can download it and import the JDOM library in your project. You can get help from Google. If still, required help post your problem in the forum. I will try to give you the solution. For Scala, Scala has the inbuilt library for XML parsing. Scala-xml_2.11-1.0.2 jar (please check them for new version if available).</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-machine-learning-library-in-spark">Explain Machine Learning library in Spark.<a class="hash-link" href="#explain-machine-learning-library-in-spark" title="Direct link to heading">​</a></h4><p>It is a scalable machine learning library. It delivers both blazing speed (up to 100x faster than MapReduce) and high-quality algorithms (e.g., multiple iterations to increase accuracy). We can use this library in Java, Scala, and Python as part of Spark applications so that you can include it incomplete workflows. There are many tools, which are provided by MLlib. Such as-
ML Algorithms: Common learning algorithms such as classification, regression, clustering, and collaborative filtering.
Featurization: Feature extraction, transformation, dimensionality reduction, and selection.
Pipelines: Tools for constructing, evaluating, and tuning ML Pipelines.
Persistence: Saving and load algorithms, models, and Pipelines.
Utilities: Linear algebra, statistics, data handling, etc.
For detailed insights, follow link: Apache Spark MLlib (Machine Learning Library)</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="list-various-commonly-used-machine-learning-algorithm">List various commonly used Machine Learning Algorithm.<a class="hash-link" href="#list-various-commonly-used-machine-learning-algorithm" title="Direct link to heading">​</a></h4><p>Basically, there are three types of Machine Learning Algorithms :
(1) Supervised Learning Algorithm
(2) Unsupervised Learning Algorithm
(3) Reinforcement Learning Algorithm
Most commonly used Machine Learning Algorithm is as follows :
Linear Regression
Logistic Regression
Decision Tree
K-Means
KNN
SVM
Random Forest
Naïve Bayes
Dimensionality Reduction Algorithm
Gradient Boost and Adaboost
For what is MLlib see Apache Spark Ecosystem</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-the-parquet-file-format-in-apache-spark-when-is-it-the-best-to-choose-this">Explain the Parquet File format in Apache Spark. When is it the best to choose this?<a class="hash-link" href="#explain-the-parquet-file-format-in-apache-spark-when-is-it-the-best-to-choose-this" title="Direct link to heading">​</a></h4><p>Parquet is the columnar information illustration that is that the best choice for storing long run massive information for analytics functions. It will perform each scan and write operations with Parquet file. It could be a columnar information storage format.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-lineage-graph-1">What is Lineage Graph?<a class="hash-link" href="#what-is-lineage-graph-1" title="Direct link to heading">​</a></h4><p>The RDDs in Spark, depend on one or more other RDDs. The representation of dependencies in between RDDs is known as the lineage graph. Lineage graph information is used to compute each RDD on demand, so that whenever a part of persistent RDD is lost, the data that is lost can be recovered using the lineage graph information.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-trigger-automatic-cleanups-in-spark-to-handle-accumulated-metadata">How can you Trigger Automatic Cleanups in Spark to Handle Accumulated Metadata?<a class="hash-link" href="#how-can-you-trigger-automatic-cleanups-in-spark-to-handle-accumulated-metadata" title="Direct link to heading">​</a></h4><p>You can trigger the clean-ups by setting the parameter spark.cleaner.ttl or by dividing the long running jobs into different batches and writing the intermediary results to the disk.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-benefits-of-using-spark-with-apache-mesos">What are the benefits of using Spark With Apache Mesos?<a class="hash-link" href="#what-are-the-benefits-of-using-spark-with-apache-mesos" title="Direct link to heading">​</a></h4><p>It renders scalable partitioning among various Spark instances and dynamic partitioning between Spark and other big data frameworks.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-significance-of-sliding-window-operation">What is the Significance of Sliding Window Operation?<a class="hash-link" href="#what-is-the-significance-of-sliding-window-operation" title="Direct link to heading">​</a></h4><p>Sliding Window controls transmission of data packets between various computer networks. Spark Streaming library provides windowed computations where the transformations on RDDs are applied over a sliding window of data. Whenever the window slides, the RDDs that fall within the particular window are combined and operated upon to produce new RDDs of the windowed DStream.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="when-running-spark-applications-is-it-necessary-to-install-spark-on-all-nodes-of-yarn-cluster">When running Spark Applications is it necessary to install Spark on all Nodes of Yarn Cluster?<a class="hash-link" href="#when-running-spark-applications-is-it-necessary-to-install-spark-on-all-nodes-of-yarn-cluster" title="Direct link to heading">​</a></h4><p>Spark need not be installed when running a job under YARN or Mesos because Spark can execute on top of YARN or Mesos clusters without affecting any change to the cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-catalyst-framework">What is Catalyst Framework?<a class="hash-link" href="#what-is-catalyst-framework" title="Direct link to heading">​</a></h4><p>Catalyst framework is a new optimization framework present in Spark SQL. It allows Spark to automatically transform SQL queries by adding new optimizations to build a faster processing system.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-spark-library-allows-reliable-file-sharing-at-memory-speed-across-different-cluster-frameworks">Which Spark Library allows reliable File Sharing at Memory Speed across different cluster frameworks?<a class="hash-link" href="#which-spark-library-allows-reliable-file-sharing-at-memory-speed-across-different-cluster-frameworks" title="Direct link to heading">​</a></h4><p>Tachyon</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-is-blinkdb-used">Why is Blinkdb used?<a class="hash-link" href="#why-is-blinkdb-used" title="Direct link to heading">​</a></h4><p>BlinkDB is a query engine for executing interactive SQL queries on huge volumes of data and renders query results marked with meaningful error bars. BlinkDB helps users balance query accuracy with response time.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-compare-hadoop-and-spark-in-terms-of-ease-of-use">How can you compare Hadoop and Spark in terms of ease of use?<a class="hash-link" href="#how-can-you-compare-hadoop-and-spark-in-terms-of-ease-of-use" title="Direct link to heading">​</a></h4><p>Hadoop MapReduce requires programming in Java which is difficult, though Pig and Hive make it considerably easier. Learning Pig and Hive syntax takes time. Spark has interactive APIs for different languages like Java, Python or Scala and also includes Shark i.e. Spark SQL for SQL lovers - making it comparatively easier to use than Hadoop.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-common-mistakes-developers-make-when-running-spark-applications">What are the common mistakes developers make when running Spark Applications?<a class="hash-link" href="#what-are-the-common-mistakes-developers-make-when-running-spark-applications" title="Direct link to heading">​</a></h4><p>Developers often make the mistake of:-
Hitting the web service several times by using multiple clusters.
Run everything on the local node instead of distributing it.
Developers need to be careful with this, as Spark makes use of memory for processing.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-advantage-of-a-parquet-file-1">What is the Advantage of a Parquet File?<a class="hash-link" href="#what-is-the-advantage-of-a-parquet-file-1" title="Direct link to heading">​</a></h4><p>Parquet file is a columnar format file that helps:
Limit I/O operations
Consumes less space
Fetches only required columns.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-various-data-sources-available-in-sparksql">What are the various Data Sources available in Sparksql?<a class="hash-link" href="#what-are-the-various-data-sources-available-in-sparksql" title="Direct link to heading">​</a></h4><p>Parquet file
JSON Datasets
Hive tables</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-key-features-of-apache-spark-that-you-like">What are the Key Features of Apache Spark that you like?<a class="hash-link" href="#what-are-the-key-features-of-apache-spark-that-you-like" title="Direct link to heading">​</a></h4><p>Spark provides advanced analytic options like graph algorithms, machine learning, streaming data, etc
It has built-in APIs in multiple languages like Java, Scala, Python and R
It has good performance gains, as it helps run an application in the Hadoop cluster ten times faster on disk and 100 times faster in memory.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-pair-rdd-1">What do you understand by Pair Rdd?<a class="hash-link" href="#what-do-you-understand-by-pair-rdd-1" title="Direct link to heading">​</a></h4><p>Special operations can be performed on RDDs in Spark using key/value pairs and such RDDs are referred to as Pair RDDs. Pair RDDs allow users to access each key in parallel. They have a reduceByKey () method that collects data based on each key and a join () method that combines different RDDs together, based on the elements having the same key.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-about-different-types-of-transformations-on-dstreams">Explain about different Types of Transformations on Dstreams?<a class="hash-link" href="#explain-about-different-types-of-transformations-on-dstreams" title="Direct link to heading">​</a></h4><p>Stateless Transformations:- Processing of the batch does not depend on the output of the previous batch.
Examples: map (), reduceByKey (), filter ().
Stateful Transformations:- Processing of the batch depends on the intermediary results of the previous batch.
Examples: Transformations that depend on sliding windows.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-about-popular-use-cases-of-apache-spark">Explain about popular use cases of Apache Spark?<a class="hash-link" href="#explain-about-popular-use-cases-of-apache-spark" title="Direct link to heading">​</a></h4><p>Apache Spark is mainly used for:
Iterative machine learning.
Interactive data analytics and processing.
Stream processing
Sensor data processing</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-apache-spark-a-good-fit-for-reinforcement-learning">Is Apache Spark a good fit for reinforcement Learning?<a class="hash-link" href="#is-apache-spark-a-good-fit-for-reinforcement-learning" title="Direct link to heading">​</a></h4><p>No. Apache Spark works well only for simple machine learning algorithms like clustering, regression, classification.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-spark-core">What is Spark Core?<a class="hash-link" href="#what-is-spark-core" title="Direct link to heading">​</a></h4><p>It has all the basic functionalities of Spark, like - memory management, fault recovery, interacting with storage systems, scheduling tasks, etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-remove-the-elements-with-a-key-present-in-any-other-rdd">How can you remove the elements with a Key present in any other Rdd?<a class="hash-link" href="#how-can-you-remove-the-elements-with-a-key-present-in-any-other-rdd" title="Direct link to heading">​</a></h4><p>Use the subtractByKey () function.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-persist-and-cache">What is the difference between Persist and Cache?<a class="hash-link" href="#what-is-the-difference-between-persist-and-cache" title="Direct link to heading">​</a></h4><p>persist () allows the user to specify the storage level where as cache () uses the default storage level.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-spark-handles-monitoring-and-logging-in-standalone-mode">How Spark handles Monitoring and Logging in Standalone Mode?<a class="hash-link" href="#how-spark-handles-monitoring-and-logging-in-standalone-mode" title="Direct link to heading">​</a></h4><p>Spark has a web based user interface for monitoring the cluster in standalone mode that shows the cluster and job statistics. The log output for each job is written to the work directory of the slave nodes.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="does-apache-spark-provide-check-pointing">Does Apache Spark provide check pointing?<a class="hash-link" href="#does-apache-spark-provide-check-pointing" title="Direct link to heading">​</a></h4><p>Lineage graphs are always useful to recover RDDs from a failure but this is generally time consuming if the RDDs have long lineage chains. Spark has an API for check pointing i.e. a REPLICATE flag to persist. However, the decision on which data to checkpoint - is decided by the user. Checkpoints are useful when the lineage graphs are long and have wide dependencies.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-launch-spark-jobs-inside-hadoop-mapreduce">How can you launch Spark Jobs inside Hadoop Mapreduce?<a class="hash-link" href="#how-can-you-launch-spark-jobs-inside-hadoop-mapreduce" title="Direct link to heading">​</a></h4><p>Using SIMR (Spark in MapReduce) users can run any spark job inside MapReduce without requiring any admin rights.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-achieve-high-availability-in-apache-spark">How can you achieve High Availability in Apache Spark?<a class="hash-link" href="#how-can-you-achieve-high-availability-in-apache-spark" title="Direct link to heading">​</a></h4><p>Implementing single node recovery with local file system
Using StandBy Masters with Apache ZooKeeper.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="hadoop-uses-replication-to-achieve-fault-tolerance-and-how-is-this-achieved-in-apache-spark">Hadoop uses Replication to achieve Fault Tolerance and how is this achieved in Apache Spark?<a class="hash-link" href="#hadoop-uses-replication-to-achieve-fault-tolerance-and-how-is-this-achieved-in-apache-spark" title="Direct link to heading">​</a></h4><p>Data storage model in Apache Spark is based on RDDs. RDDs help achieve fault tolerance through lineage. RDD always has the information on how to build from other datasets. If any partition of a RDD is lost due to failure, lineage helps build only that particular lost partition.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explain-about-core-components-of-a-distributed-spark-application">Explain about Core Components of a distributed Spark Application?<a class="hash-link" href="#explain-about-core-components-of-a-distributed-spark-application" title="Direct link to heading">​</a></h4><p>Driver: The process that runs the main () method of the program to create RDDs and perform transformations and actions on them.
Executor: The worker processes that run the individual tasks of a Spark job.
Cluster Manager: A pluggable component in Spark, to launch Executors and Drivers. The cluster manager allows Spark to run on top of other external managers like Apache Mesos or YARN.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-lazy-evaluation">What do you understand by Lazy Evaluation?<a class="hash-link" href="#what-do-you-understand-by-lazy-evaluation" title="Direct link to heading">​</a></h4><p>Spark is intellectual in the manner in which it operates on data. When you tell Spark to operate on a given dataset, it heeds the instructions and makes a note of it, so that it does not forget - but it does nothing, unless asked for the final result.
When a transformation like map () is called on a RDD-the operation is not performed immediately. Transformations in Spark are not evaluated till you perform an action. This helps optimize the overall data processing workflow.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="define-a-worker-node">Define a Worker Node?<a class="hash-link" href="#define-a-worker-node" title="Direct link to heading">​</a></h4><p>A node that can run the Spark application code in a cluster can be called as a worker node. A worker node can have more than one worker which is configured by setting the SPARK<em> WORKER_INSTANCES property in the spark-env.sh file. Only one worker is started if the SPARK</em> WORKER_INSTANCES property is not defined.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-schemardd-1">What do you understand by Schemardd?<a class="hash-link" href="#what-do-you-understand-by-schemardd-1" title="Direct link to heading">​</a></h4><p>An RDD that consists of row objects (wrappers around basic string or integer arrays) with schema information about the type of data in each column.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-disadvantages-of-using-apache-spark-over-hadoop-mapreduce">What are the disadvantages of using Apache Spark over Hadoop Mapreduce?<a class="hash-link" href="#what-are-the-disadvantages-of-using-apache-spark-over-hadoop-mapreduce" title="Direct link to heading">​</a></h4><p>Apache spark does not scale well for compute intensive jobs and consumes large number of system resources. Apache Spark’s in-memory capability at times comes a major roadblock for cost efficient processing of big data. Also, Spark does have its own file management system and hence needs to be integrated with other cloud based data platforms or apache hadoop.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-necessary-to-install-spark-on-all-nodes-of-yarn-cluster-while-running-apache-spark-on-yarn">Is it necessary to install Spark on all Nodes of Yarn Cluster while running Apache Spark on Yarn?<a class="hash-link" href="#is-it-necessary-to-install-spark-on-all-nodes-of-yarn-cluster-while-running-apache-spark-on-yarn" title="Direct link to heading">​</a></h4><p>No , it is not necessary because Apache Spark runs on top of YARN.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-executor-memory-in-spark-application">What do you understand by Executor Memory in Spark Application?<a class="hash-link" href="#what-do-you-understand-by-executor-memory-in-spark-application" title="Direct link to heading">​</a></h4><p>Every spark application has same fixed heap size and fixed number of cores for a spark executor. The heap size is what referred to as the Spark executor memory which is controlled with the spark.executor.memory property of the –executor-memory flag.
Every spark application will have one executor on each worker node. The executor memory is basically a measure on how much memory of the worker node will the application utilize.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-does-the-spark-engine-do">What does the Spark Engine do?<a class="hash-link" href="#what-does-the-spark-engine-do" title="Direct link to heading">​</a></h4><p>Spark engine schedules, distributes and monitors the data application across the spark cluster.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-makes-apache-spark-good-at-low-latency-workloads-like-graph-processing-and-machine-learning">What makes Apache Spark good at Low latency Workloads like Graph Processing and Machine Learning?<a class="hash-link" href="#what-makes-apache-spark-good-at-low-latency-workloads-like-graph-processing-and-machine-learning" title="Direct link to heading">​</a></h4><p>Apache Spark stores data in-memory for faster model building and training. Machine learning algorithms require multiple iterations to generate a resulting optimal model and similarly graph algorithms traverse all the nodes and edges.
These low latency workloads that need multiple iterations can lead to increased performance. Less disk access and  controlled network traffic make a huge difference when there is lots of data to be processed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-dstream-in-apache-spark">What is Dstream in Apache Spark?<a class="hash-link" href="#what-is-dstream-in-apache-spark" title="Direct link to heading">​</a></h4><p>Dstream stands for Discretized Stream. It is a sequence of Resilient Distributed Database (RDD) representing a continuous stream of data. There are several ways to create Dstream from various sources like HDFS, Apache Flume, Apache Kafka, etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-yarn">What do you understand by YARN?<a class="hash-link" href="#what-do-you-understand-by-yarn" title="Direct link to heading">​</a></h4><p>Just like in Hadoop, YARN is one of the key features in Apache Spark, which is used to provide a central and resource management platform to deliver scalable operations across the cluster. Spark can run on YARN, as the same way Hadoop Map Reduce can run on YARN.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-necessary-to-install-spark-on-all-nodes-of-the-yarn-cluster">Is it necessary to install Spark on all nodes of the YARN cluster?<a class="hash-link" href="#is-it-necessary-to-install-spark-on-all-nodes-of-the-yarn-cluster" title="Direct link to heading">​</a></h4><p>No. It doesn&#x27;t seem necessary to install Spark on all YARN cluster nodes because Spark runs on top of the YARN. Apache Spark runs independently from its installation. Spark provides some options to use YARN when dispatching jobs to the cluster, rather than its built-in manager or Mesos. Besides this, there are also some configurations to run YARN, such as master, deploy-mode, driver-memory, executor-memory, executor-cores, and queue.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-different-data-sources-available-in-sparksql">What are the different data sources available in SparkSQL?<a class="hash-link" href="#what-are-the-different-data-sources-available-in-sparksql" title="Direct link to heading">​</a></h4><p>There are the following three data sources available in SparkSQL:
JSON Datasets
Hive tables
Parquet file</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-are-some-important-internal-daemons-used-in-apache-spark">Which are some important internal daemons used in Apache Spark?<a class="hash-link" href="#which-are-some-important-internal-daemons-used-in-apache-spark" title="Direct link to heading">​</a></h4><p>Following are the important internal daemons used in Spark:
Blockmanager
Memestore
DAGscheduler
Driver
Worker
Executor
Tasks etc.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-method-to-create-a-data-frame-in-apache-spark">What is the method to create a Data frame in Apache Spark?<a class="hash-link" href="#what-is-the-method-to-create-a-data-frame-in-apache-spark" title="Direct link to heading">​</a></h4><p>In Apache Spark, we can create a data frame using Tables in Hive and Structured data files.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-accumulators-in-apache-spark">What do you understand by accumulators in Apache Spark?<a class="hash-link" href="#what-do-you-understand-by-accumulators-in-apache-spark" title="Direct link to heading">​</a></h4><p>Accumulators are the write-only variables that are initialized once and sent to the workers. Then, these workers update based on the logic written, which will send back to the driver.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-default-level-of-parallelism-in-apache-spark">What is the default level of parallelism in Apache Spark?<a class="hash-link" href="#what-is-the-default-level-of-parallelism-in-apache-spark" title="Direct link to heading">​</a></h4><p>If it is not specified, then the number of partitions is called the default level of parallelism in Apache Spark.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-companies-are-using-spark-streaming-services">Which companies are using Spark streaming services?<a class="hash-link" href="#which-companies-are-using-spark-streaming-services" title="Direct link to heading">​</a></h4><p>The three most famous companies using Spark Streaming services are:
Uber
Netflix
Pinterest</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-possible-to-use-spark-to-access-and-analyze-data-stored-in-cassandra-databases">Is it possible to use Spark to access and analyze data stored in Cassandra databases?<a class="hash-link" href="#is-it-possible-to-use-spark-to-access-and-analyze-data-stored-in-cassandra-databases" title="Direct link to heading">​</a></h4><p>Yes, it is possible to use Spark to access and analyze Cassandra databases&#x27; data by using Cassandra Connector.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-we-run-apache-spark-on-apache-mesos">Can we run Apache Spark on Apache Mesos?<a class="hash-link" href="#can-we-run-apache-spark-on-apache-mesos" title="Direct link to heading">​</a></h4><p>Yes, we can run Apache Spark on the hardware clusters managed by Mesos.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-spark-sql">What do you understand by Spark SQL?<a class="hash-link" href="#what-do-you-understand-by-spark-sql" title="Direct link to heading">​</a></h4><p>Spark SQL is a module for structured data processing, which provides the advantage of SQL queries running on that database.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-connect-spark-to-apache-mesos">How can you connect Spark to Apache Mesos?<a class="hash-link" href="#how-can-you-connect-spark-to-apache-mesos" title="Direct link to heading">​</a></h4><p>Follow the steps given below to connect Spark to Apache Mesos:
Configure the spark driver program to connect to Mesos.
Set a path location for the Spark binary package that it can be accessible by Mesos.
Install Apache Spark in the same location as that of Apache Mesos and configure the property spark.mesos.executor.home to point to the location where it is installed.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-best-way-to-minimize-data-transfers-when-working-with-spark">What is the best way to minimize data transfers when working with Spark?<a class="hash-link" href="#what-is-the-best-way-to-minimize-data-transfers-when-working-with-spark" title="Direct link to heading">​</a></h4><p>To write a fast and reliable Spark program, we have to minimize data transfers and avoid shuffling. There are various ways to minimize data transfers while working with Apache Spark. These are:
Using Broadcast Variable- Broadcast variables enhance the efficiency of joins between small and large RDDs.
Using Accumulators- Accumulators are used to updating the values of variables in parallel while executing.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-lazy-evaluation-in-apache-spark">What do you understand by lazy evaluation in Apache Spark?<a class="hash-link" href="#what-do-you-understand-by-lazy-evaluation-in-apache-spark" title="Direct link to heading">​</a></h4><p>As the name specifies, lazy evaluation in Apache Spark means that the execution will not start until an action is triggered. In Spark, the lazy evaluation comes into action when Spark transformations occur. Transformations are lazy. When a transformation such as a map() is called on an RDD, it is not performed instantly. Transformations in Spark are not evaluated until you perform an action, which aids in optimizing the overall data processing workflow, known as lazy evaluation. So we can say that in lazy evaluation, data is not loaded until it is necessary.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-spark-driver">What do you understand by Spark Driver?<a class="hash-link" href="#what-do-you-understand-by-spark-driver" title="Direct link to heading">​</a></h4><p>Spark Driver is the program that runs on the master node of the machine and is used to declare transformations and actions on data RDDs.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-parquet-file-in-apache-spark">What is the Parquet file in Apache Spark?<a class="hash-link" href="#what-is-the-parquet-file-in-apache-spark" title="Direct link to heading">​</a></h4><p>Parquet is a column format file supported by many data processing systems. Spark SQL facilitates us to perform both read and write operations with the Parquet file.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-way-to-store-the-data-in-apache-spark">What is the way to store the data in Apache Spark?<a class="hash-link" href="#what-is-the-way-to-store-the-data-in-apache-spark" title="Direct link to heading">​</a></h4><p>Apache Spark is an open-source analytics and processing engine for large-scale data processing, but it does not have any storage engine. It can retrieve data from another storage engine like HDFS, S3.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-is-it-possible-to-implement-machine-learning-in-apache-spark">How is it possible to implement machine learning in Apache Spark?<a class="hash-link" href="#how-is-it-possible-to-implement-machine-learning-in-apache-spark" title="Direct link to heading">​</a></h4><p>Apache Spark itself provides a versatile machine learning library called MLif. By using this library, we can implement machine learning in Spark.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-some-disadvantages-or-demerits-of-using-apache-spark">What are some disadvantages or demerits of using Apache Spark?<a class="hash-link" href="#what-are-some-disadvantages-or-demerits-of-using-apache-spark" title="Direct link to heading">​</a></h4><p>Following is the list of some disadvantages or demerits of using Apache Spark:
Apache Spark requires more storage space than Hadoop and MapReduce, so that it may create some problems.
Apache Spark consumes a huge amount of data as compared to Hadoop.
Apache Spark requires more attentiveness because developers need to be careful while running their applications in Spark.
Spark runs on multiple clusters on different nodes instead of running everything on a single node. So, the work is distributed over multiple clusters.
The &quot;in-memory&quot; capability of Apache Spark makes it a more costly way for processing big data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-file-system-api-in-apache-spark">What is the use of File system API in Apache Spark?<a class="hash-link" href="#what-is-the-use-of-file-system-api-in-apache-spark" title="Direct link to heading">​</a></h4><p>File system API is used to read data from various storage devices such as HDFS, S3 or Local Files.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-the-tasks-of-a-spark-engine">What are the tasks of a Spark Engine?<a class="hash-link" href="#what-are-the-tasks-of-a-spark-engine" title="Direct link to heading">​</a></h4><p>The main task of a Spark Engine is handling the process of scheduling, distributing and monitoring the data application across the clusters.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-apache-sparkcontext">What is the use of Apache SparkContext?<a class="hash-link" href="#what-is-the-use-of-apache-sparkcontext" title="Direct link to heading">​</a></h4><p>The SparkContent is the entry point to Apache Spark. SparkContext facilitates users to create RDDs, which provide various ways of churning data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-possible-to-do-real-time-processing-with-sparksql">Is it possible to do real-time processing with SparkSQL?<a class="hash-link" href="#is-it-possible-to-do-real-time-processing-with-sparksql" title="Direct link to heading">​</a></h4><p>In SparkSQL, real-time data processing is not possible directly. We can register the existing RDD as a SQL table and trigger the SQL queries on priority.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-akka-in-apache-spark">What is the use of Akka in Apache Spark?<a class="hash-link" href="#what-is-the-use-of-akka-in-apache-spark" title="Direct link to heading">​</a></h4><p>Akka is used for scheduling in Apache Spark. Spark also uses Akka for messaging between the workers and masters.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-do-you-understand-by-spark-map-transformation">What do you understand by Spark map() Transformation?<a class="hash-link" href="#what-do-you-understand-by-spark-map-transformation" title="Direct link to heading">​</a></h4><p>Spark map() is a transformation operation used to apply the Transformation on every element of RDD, DataFrame, and Dataset and finally returns a new RDD/Dataset, respectively.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-advantage-of-using-the-parquet-file">What is the advantage of using the Parquet file?<a class="hash-link" href="#what-is-the-advantage-of-using-the-parquet-file" title="Direct link to heading">​</a></h4><p>In Apache Spark, the Parquet file is used to perform both read and write operations. Following is the list of some advantages of having a Parquet file:</p><p>Parquet file facilitates users to fetch specific columns for access.
It consumes less space.
It follows the type-specific encoding.
It supports limited I/O operations.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-difference-between-persist-and-cache-functions-in-apache-spark">What is the difference between persist() and cache() functions in Apache Spark?<a class="hash-link" href="#what-is-the-difference-between-persist-and-cache-functions-in-apache-spark" title="Direct link to heading">​</a></h4><p>In Apache Spark, the persist() function is used to allow the user to specify the storage level, whereas the cache() function uses the default storage level.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="which-spark-libraries-allow-reliable-file-sharing-at-memory-speed-across-different-cluster-frameworks">Which Spark libraries allow reliable file sharing at memory speed across different cluster frameworks?<a class="hash-link" href="#which-spark-libraries-allow-reliable-file-sharing-at-memory-speed-across-different-cluster-frameworks" title="Direct link to heading">​</a></h4><p>Tachyon is the Apache Spark library&#x27;s name, which is used for reliable file sharing at memory speed across various cluster frameworks.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-shuffling-in-apache-spark-when-does-it-occur">What is shuffling in Apache Spark? When does it occur?<a class="hash-link" href="#what-is-shuffling-in-apache-spark-when-does-it-occur" title="Direct link to heading">​</a></h4><p>In Apache Spark, shuffling is the process of redistributing data across partitions that may lead to data movement across the executors. The implementation of shuffle operation is entirely different in Spark as compared to Hadoop.
Shuffling has two important compression parameters:
shuffle.compress: It is used to check whether the engine would compress shuffle outputs or not.
shuffle.spill.compress: It is used to decide whether to compress intermediate shuffle spill files or not.
Shuffling comes in the scene when we join two tables or perform byKey operations such as GroupByKey or ReduceByKey.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-lineage-in-spark">What is the lineage in Spark?<a class="hash-link" href="#what-is-the-lineage-in-spark" title="Direct link to heading">​</a></h4><p>In Apache Spark, when a transformation (map or filter etc.) is called, it is not executed by Spark immediately; instead, a lineage is created for each transformation. This lineage is used to keep track of what all transformations have to be applied on that RDD. It also traces the location from where it has to read the data.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-you-trigger-automatic-clean-ups-in-spark-to-handle-accumulated-metadata">How can you trigger automatic clean-ups in Spark to handle accumulated metadata?<a class="hash-link" href="#how-can-you-trigger-automatic-clean-ups-in-spark-to-handle-accumulated-metadata" title="Direct link to heading">​</a></h4><p>You can trigger the clean-ups by setting the parameter Spark.cleaner.ttl or dividing the long-running jobs into different batches and writing the intermediary results to the disk.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="is-it-possible-to-launch-spark-jobs-inside-hadoop-mapreduce">Is it possible to launch Spark jobs inside Hadoop MapReduce?<a class="hash-link" href="#is-it-possible-to-launch-spark-jobs-inside-hadoop-mapreduce" title="Direct link to heading">​</a></h4><p>Yes, you can run all kinds of spark jobs inside MapReduce without the need to obtain the admin rights of that application.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-the-use-of-blinkdb-in-spark">What is the use of BlinkDB in Spark?<a class="hash-link" href="#what-is-the-use-of-blinkdb-in-spark" title="Direct link to heading">​</a></h4><p>BlinkDB is a query engine tool used to execute SQL queries on massive volumes of data and renders query results in the meaningful error bars.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="additional-resources">Additional Resources<a class="hash-link" href="#additional-resources" title="Direct link to heading">​</a></h2><ol><li><a href="https://moizs.medium.com/data-engineer-interview-amazon-2022-411664bbda25" target="_blank" rel="noopener noreferrer">Data Engineer Interview @ Amazon 2022</a></li><li><a href="https://www.guru99.com/data-engineer-interview-questions.html" target="_blank" rel="noopener noreferrer">Top 62 Data Engineer Interview Questions <!-- -->&amp;<!-- --> Answers in 2022</a></li><li><a href="https://www.interviewquery.com/p/data-engineer-interview-questions" target="_blank" rel="noopener noreferrer">Top 100 Data Engineer Interview Question 2022</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/facebook-data-engineer-interview-questions" target="_blank" rel="noopener noreferrer">Facebook Data Engineer Interview Questions</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/amazon-data-engineer-interview-questions" target="_blank" rel="noopener noreferrer">Amazon Data Engineer Interview Questions</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/uber-data-engineer-interview-questions" target="_blank" rel="noopener noreferrer">60+ Uber Data Engineer Interview Questions</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/square-data-engineer-interview-questions" target="_blank" rel="noopener noreferrer">Square Data Engineer Interview Questions</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/azure-data-engineer-interview-questions" target="_blank" rel="noopener noreferrer">Azure Data Engineer Interview Questions</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/snap-interview-questions" target="_blank" rel="noopener noreferrer">Top Snap Interview Questions and Answers for Product Managers, Data Engineers, and Software Engineers</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/kafka-interview-questions" target="_blank" rel="noopener noreferrer">Top Kafka Interview Questions and Answers You Should Prepare</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/docker-interview-questions" target="_blank" rel="noopener noreferrer">Top Docker Interview Questions and Answers for Your Interview Prep</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/rest-api-interview-questions" target="_blank" rel="noopener noreferrer">Top REST API Interview Questions</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/data-modeling-interview-questions" target="_blank" rel="noopener noreferrer">Top Data Modeling Interview Questions and Answers</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/pyspark-interview-questions" target="_blank" rel="noopener noreferrer">Top PySpark Interview Questions for Your Tech Interview</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/etl-interview-questions" target="_blank" rel="noopener noreferrer">Top ETL Interview Questions and Answers</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/hadoop-interview-questions" target="_blank" rel="noopener noreferrer">Top Hadoop Interview Questions to Practice for Your Tech Interview</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/sql-interview-questions-for-experienced" target="_blank" rel="noopener noreferrer">Top SQL Interview Questions for Experienced Professionals</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/sql-interview-questions-for-developers" target="_blank" rel="noopener noreferrer">Top SQL Interview Questions for Developers</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/google-database-interview-questions" target="_blank" rel="noopener noreferrer">Top Google Database Interview Questions for Your SQL Interview</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/relational-databases-interview-questions" target="_blank" rel="noopener noreferrer">Top Relational Databases Interview Questions and Answers You Should Learn</a></li><li><a href="https://www.interviewkickstart.com/interview-questions/cloud-engineer-interview-questions" target="_blank" rel="noopener noreferrer">Cloud Engineer Interview Questions - Basic, Intermediate, and Advanced</a></li><li><a href="https://www.interviewbit.com/data-engineer-interview-questions/" target="_blank" rel="noopener noreferrer">Data Engineer Interview Questions</a></li><li><a href="https://intellipaat.com/blog/interview-question/apache-spark-interview-questions/" target="_blank" rel="noopener noreferrer">Top 40 Apache Spark Interview Questions and Answers in 2022</a></li><li><a href="https://intellipaat.com/blog/interview-question/big-data-hadoop-interview-questions/" target="_blank" rel="noopener noreferrer">Top Hadoop Interview Questions and Answers</a></li><li><a href="https://intellipaat.com/blog/interview-question/kafka-interview-questions/" target="_blank" rel="noopener noreferrer">Top Kafka Interview Questions – Most Asked</a></li><li><a href="https://intellipaat.com/blog/interview-question/hdfs-interview-questions/" target="_blank" rel="noopener noreferrer">Top HDFS Interview Questions And Answers</a></li><li><a href="https://intellipaat.com/blog/interview-question/cassandra-interview-questions/" target="_blank" rel="noopener noreferrer">Top Cassandra Interview Questions and Answers</a></li><li><a href="https://intellipaat.com/blog/interview-question/pig-interview-questions/" target="_blank" rel="noopener noreferrer">Top PIG Interview Questions - Most Asked</a></li><li><a href="https://intellipaat.com/blog/interview-question/hive-interview-questions/" target="_blank" rel="noopener noreferrer">Top Hive Interview Questions – Most Asked</a></li><li><a href="https://intellipaat.com/blog/interview-question/sqoop-interview-questions/" target="_blank" rel="noopener noreferrer">Top Sqoop Interview Questions – Most Asked</a></li><li><a href="https://intellipaat.com/blog/interview-question/hdfs-interview-questions/" target="_blank" rel="noopener noreferrer">Top HDFS Interview Questions And Answers</a></li><li><a href="https://intellipaat.com/blog/interview-question/map-reduce-interview-questions/" target="_blank" rel="noopener noreferrer">Top Mapreduce Interview Questions And Answers</a></li><li><a href="https://github.com/OBenner/data-engineering-interview-questions" target="_blank" rel="noopener noreferrer">https://github.com/OBenner/data-engineering-interview-questions</a></li><li><a href="https://knowledgetree.notion.site/Spark-Interview-Questions-94ff173de85d4df6849b289665e8fff3" target="_blank" rel="noopener noreferrer">https://knowledgetree.notion.site/Spark-Interview-Questions-94ff173de85d4df6849b289665e8fff3</a></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="expert-tips">Expert Tips<a class="hash-link" href="#expert-tips" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tips-for-technical-round">Tips for Technical Round<a class="hash-link" href="#tips-for-technical-round" title="Direct link to heading">​</a></h2><p><strong>Coding</strong></p><p>Fortunately, online resources abound for preparing for a technical coding interview. Here’s the rough flow I would take:</p><ol><li>Get familiar with data structures. Focus on how to use lists and hashmaps (dictionaries) above all else and solve problems that require you to use and switch between them. Make sure you also understand trees and linked lists. You should understand when and why to use one data structure over another and the space and time complexity of basic operations on all of them (appending, inserting, searching).</li><li>Understand some of the basic algorithms such as binary search and merge sort. I recommend the book Grokking Algorithms by Aditya Y. Bhargava for a good introduction if you are not familiar with implementing them.</li><li>Work through problems on a platform like Leetcode or Hackerrank. I really recommend Leetcode, especially for refining your work down to one topic like a specific data structure or even to company-specific questions. Here is a link to their collection of “Easy” problems to work through, categorized by data structure: Leetcode Easy Problem Set. In my opinion, making the leap to Leetcode Premium is worth it to get full access to all of the questions plus solutions. They occasionally also offer month-long challenges that will pose a new question each day — take advantage of that! For most data engineering interviews, I think you can stick to “Easy” and “Medium” level problems. Focus on the data structures and understanding the complexity of your solutions. Don’t worry too much about delving into more advanced software engineering topics or “Hard” problems unless you anticipate that the role you are applying for will require it.</li></ol><p>In Python, you should be comfortable with standard Python and supplemental libraries like Matplotlib, Pandas, and NumPy, know what’s available, and understand when it’s appropriate to use each library. Tip: Don’t fake it. If you don’t have much experience, be honest. You can also describe a related skill or talk about your comfort level in quickly picking up new Python skills (with an example).</p><p><strong>SQL</strong></p><p>Ah, SQL. Seems so deceptively simple. Well, SQL has tripped me up a LOT in the past. I thought I knew it because I could do some INSERTS and SELECTS and knew how to do a basic JOIN or two. How naive I was… don’t underestimate SQL! That being said, it is not easy to find resources for truly tricky SQL problems to work through. Here are some I like:</p><ol><li>Leetcode (again!). Go the Database category and work your way through all of the available problems. Remember how I said that for coding you could focus on Easy and Medium but not too worry too much about Hard? Well, not for SQL. You should aim to be able to solve problems across all three difficulties. As a caveat, I don’t find the Leetcode platform as good for SQL as I find it for coding problems; sometimes the questions are oddly categorized and/or poorly defined. Sort by Acceptance rate to get a better sense of the problem’s difficulty, and keep an eye on the problem’s “thumbs up/down” user rating: some of the poorly rated questions are not very clear and can lead to confusion.</li><li>Hackerrank. I actually think Hackerrank’s SQL problem sets are top notch (and you’ll get sick of reading about Weather Observation stations!). Work your way through them, focusing on diversifying the skills you work on (joins, window functions, CASE WHEN statements) and less on cranking out the problems that are just multiple complicated WHERE filters.</li></ol><p><strong>Database Design</strong></p><p>Many companies will expect you to be able to design a proper data warehouse given a business use case. Luckily there are quite a few resources available out there for brushing up on these skills. While a lot of them are more textbook-like, try to push yourself to actually work through some real-life use cases by designing a data warehouse for an online store, a peer-to-peer marketplace, or a rideshare application. Sketch out the schema on paper or a whiteboard and build up a process for starting a new problem or design. Here are some books and online resources for learning about data warehouse design:</p><ol><li>The Data Warehouse Toolkit: The Complete Guide to Dimensional Modelling. This is oft-cited and a great introduction to a lot of the foundational concepts</li><li>Star Schema:The Complete Reference by Christopher Adamson. This covers dimensional design and data warehouse architectures as well as touching on ETL principles.</li><li>I have seen the Udemy course Data Warehouse Concepts recommended, but have not taken it myself.</li></ol><p><strong>Data Architecture and Big Data Technologies</strong></p><p>Some companies will expect you to have greater experience or familiarity in big data technologies (think Hadoop, Spark, and event processing technologies like Kafka) than others will. There are many books available that cover these technologies in-depth, but unfortunately due to the rate of change of the industry they tend to become out of date quite quickly. Nothing really beats experience here, but the following are some good resources for getting started and/or sharpening up your skills in preparation for an interview:</p><ol><li>Designing Data-Intensive Applications by Martin Kleppmann is a great book that covers distributed systems thoroughly from both a theoretical and practical standpoint, and discusses a lot of the concepts that underpin Big Data tech like Hadoop and Kafka. (Shameless plug for the review I wrote last summer which goes into more detail on this particular book).</li><li>Try playing with Spark with Databricks Community Edition</li><li>Read some of the official documentation for things like Hadoop, Spark, Kafka, Hive. Some companies like to ask questions to test your understanding of the underlying concepts of big data frameworks and technologies, and the official documentation often gives the most succinct and correct explanation as to the why and how of each one. I think it’s useful to think about when you might want to use one technology over another and the tradeoffs that you make when you decide to, say, move to a Hadoop cluster over a single powerful machine. Be able to discuss a good use case for Spark or Kafka, for example. This is somewhat esoteric, but unfortunately there’s not as neat a template for these kinds of questions as there is for the algorithmic or SQL kind.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="soft-skills">Soft Skills<a class="hash-link" href="#soft-skills" title="Direct link to heading">​</a></h2><p>Don’t neglect the soft skills portion of the interview, either! Good communication and problem solving skills go a long way.</p><ol><li>One of the most important things you can do to prepare is to talk your way through the problems you are solving. It is really easy to get focused on a coding or schema design problem and go silent as you think; but unfortunately, that makes you completely inscrutable to the interviewer. Talking your way through the thought process is harder than it sounds, and it doesn’t come naturally; I really recommend explicitly practicing this as you work your way through algorithmic and design problems.</li><li>Write your code on a whiteboard rather than in an IDE or on paper. It sounds odd, but you will likely be asked to solve a problem on a whiteboard during your on-site interview, or even type into a blank word processor during a video call. If you’ve only practiced in IDEs that provide syntax support and familiar formatting, this will seem unnatural and can throw you off. Definitely explicitly practice it so it feels comfortable when it comes time for the interview.</li><li>Use the platform Pramp to work through real practice interviews with real people. This tool helped me a lot; like I mentioned in point 1, I found it very difficult to talk my way through problems, and when I hit an obstacle I would often freeze and go silent or into an excruciating “uh” mode where no prompting from the interviewer could help. Pramp is ostensibly low-pressure, because you are not actually interviewing; but it feels just as high-pressure, because you are working through problems in front of a stranger. It also puts you in the position of the interviewer, too, which I found really useful for expanding my understanding of problems.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quiz-1">Quiz 1<a class="hash-link" href="#quiz-1" title="Direct link to heading">​</a></h2><ol><li>What type of workload is an OLAP model?<ol><li>Analytical workload</li><li>Transactional workload</li><li>Relational database</li></ol></li><li>How is data in a relational table organized?<ol><li>Rows and columns</li><li>Header and footer</li><li>Pages and paragraphs</li><li>Connections and arrows</li></ol></li><li>Which of the following is an example of unstructured data?<ol><li>Audio and video files</li><li>An <strong>Employee</strong> table with <strong>EmployeeID</strong>, <strong>EmployeeName</strong>, and <strong>EmployeeDesignation</strong> columns</li><li>A table within a relational database</li><li>A stored procedure in a database</li></ol></li><li>What type of cloud service is a database deployed in a virtual machine?<ol><li>PaaS</li><li>IaaS</li><li>SaaS</li><li>DaaS</li></ol></li><li>You&#x27;ve gathered data in a variety of formats from a variety of sources. Now, you must convert the data into a single, consistent format. Which of the following processes would you use?<ol><li>Data ingestion</li><li>Data modeling</li><li>Data storage</li><li>Dataset</li></ol></li><li>The process of transforming and translating raw data into a more useable format for analysis is known as <strong><strong>____</strong></strong>:<ol><li>Data cleaning</li><li>Data ingestion</li><li>Data modeling</li><li>Data analysis</li></ol></li><li>Choose the best sequence for putting the ELT process into action on Azure:<ol><li>Prepare the data for loading by extracting the source data into text files. Use PolyBase or the <strong>COPY</strong> command to load the data into staging tables. Transform the data and insert it into production tables by storing it in Azure Blob storage or Azure Data Lake Storage.</li><li>Extract the data from the source into text files. Place the information in Azure Data Lake Storage or Azure Blob storage. Make sure the data is ready to be loaded. Using PolyBase or the COPY command, load the data into staging tables, transform the data, then insert the data into production tables<em>.</em></li><li>Extract the data from the source into text files. Place the data in Azure Blob storage or Azure Data Lake Storage, as appropriate. Make sure the data is ready to be loaded. Use PolyBase or the <strong>COPY</strong> command to load the data into staging tables. Fill in the blanks in the production tables with the data and then transform the information.</li><li>Prepare the data for loading by extracting the source data into text files. Use PolyBase or the <strong>COPY</strong> command to load the data into staging tables. Place the data in Azure Blob storage or Azure Data Lake Storage, as appropriate. Fill in the blanks in the production tables with the data and then transform the information.</li></ol></li><li>Consider the following statements:<ul><li>S1: Make dashboards available to others, particularly those who are on the go</li><li>S2: Use Power BI mobile apps to view and interact with shared dashboards and results</li><li>S3: Import data and produce a report in Power BI Desktop</li><li>S4: Upload to the Power BI service, where you can create new dashboards and visualizations</li></ul></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="answer-key">Answer key<a class="hash-link" href="#answer-key" title="Direct link to heading">​</a></h3><p>1-A 2-A 3-A 4-B 5-A 6-D 7-B 8-C</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-typical-data-engineering-interview-process" class="table-of-contents__link toc-highlight">The typical Data Engineering interview process</a></li><li><a href="#phone-screen" class="table-of-contents__link toc-highlight">Phone Screen</a><ul><li><a href="#round-1-hr" class="table-of-contents__link toc-highlight">Round 1: HR</a></li><li><a href="#round-2-technical" class="table-of-contents__link toc-highlight">Round 2: Technical</a></li></ul></li><li><a href="#take-home-exam" class="table-of-contents__link toc-highlight">Take-Home Exam</a><ul><li><a href="#timed-hackerrank" class="table-of-contents__link toc-highlight">Timed HackerRank</a></li><li><a href="#coding-challenge" class="table-of-contents__link toc-highlight">Coding challenge</a></li></ul></li><li><a href="#on-site-interview" class="table-of-contents__link toc-highlight">On-Site Interview</a><ul><li><a href="#round-1-problem-solving---sql-and-programming" class="table-of-contents__link toc-highlight">Round 1: Problem Solving - SQL and Programming</a></li><li><a href="#round-2-resume-discussion" class="table-of-contents__link toc-highlight">Round 2: Resume Discussion</a></li><li><a href="#round-3-system-design" class="table-of-contents__link toc-highlight">Round 3: System Design</a></li><li><a href="#round-4-cultural-fit" class="table-of-contents__link toc-highlight">Round 4: Cultural Fit</a></li></ul></li><li><a href="#sql-questions" class="table-of-contents__link toc-highlight">SQL Questions</a></li><li><a href="#python-questions" class="table-of-contents__link toc-highlight">Python Questions</a></li><li><a href="#data-modeling-questions" class="table-of-contents__link toc-highlight">Data Modeling Questions</a></li><li><a href="#data-warehousing-questions" class="table-of-contents__link toc-highlight">Data Warehousing Questions</a></li><li><a href="#big-data--cloud-questions" class="table-of-contents__link toc-highlight">Big Data &amp; Cloud Questions</a></li><li><a href="#data-pipelining-questions" class="table-of-contents__link toc-highlight">Data Pipelining Questions</a></li><li><a href="#project-related-questions" class="table-of-contents__link toc-highlight">Project-related Questions</a></li><li><a href="#software-engineering-questions" class="table-of-contents__link toc-highlight">Software Engineering Questions</a></li><li><a href="#airflow" class="table-of-contents__link toc-highlight">Airflow</a></li><li><a href="#aws" class="table-of-contents__link toc-highlight">AWS</a></li><li><a href="#cassandra" class="table-of-contents__link toc-highlight">Cassandra</a></li><li><a href="#apache-kafka" class="table-of-contents__link toc-highlight">Apache Kafka</a></li><li><a href="#snowflake" class="table-of-contents__link toc-highlight">Snowflake</a></li><li><a href="#apache-spark" class="table-of-contents__link toc-highlight">Apache Spark</a></li><li><a href="#additional-resources" class="table-of-contents__link toc-highlight">Additional Resources</a></li><li><a href="#expert-tips" class="table-of-contents__link toc-highlight">Expert Tips</a></li><li><a href="#tips-for-technical-round" class="table-of-contents__link toc-highlight">Tips for Technical Round</a></li><li><a href="#soft-skills" class="table-of-contents__link toc-highlight">Soft Skills</a></li><li><a href="#quiz-1" class="table-of-contents__link toc-highlight">Quiz 1</a><ul><li><a href="#answer-key" class="table-of-contents__link toc-highlight">Answer key</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.251db5a0.js"></script>
<script src="/assets/js/main.1462881d.js"></script>
</body>
</html>