{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Data Ingestion Pipelines Using Azure Data Factory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure Data Factory is the bread and butter for a data engineer and understanding its fundamentals is extremely essential in building efficient pipelines. By the end of the lab, you will know how to provision a data factory account, copy data from an Azure SQL database to a data lake using copy activity, use control flow activities, move data from SQL Server to a data lake, and choose options to trigger a data factory pipeline.\n",
    "\n",
    "In this lab, we’ll cover the following recipes:\n",
    "\n",
    "* Provisioning Azure Data Factory\n",
    "* Copying files to a database from a data lake using a control flow and copy activity\n",
    "* Triggering a pipeline in Azure Data Factory\n",
    "* Copying data from a SQL Server virtual machine to a data lake using the Copy data wizard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 1 - Provisioning Azure Data Factory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started with Azure Data Factory, you need to run an Azure Data Factory account. An Azure Data Factory account is comprised of the following key components:\n",
    "\n",
    "-   **Linked services**: A component that maintains the connection credentials to data sources. An example of this is a connection to a SQL database/text file.\n",
    "-   **Dataset**: The data that's obtained after connecting to the data source using a linked service. An example of this is a group of tables or files connected via a linked service.\n",
    "-   **Activity**: A task that will process the dataset. An example of this is a copy activity that moves the data from a flat file to a database.\n",
    "-   **Data flow**: These are specific tasks that perform data transformations on datasets. An example of this is pivoting or sorting a dataset while it is being moved from source to destination. This can be done by a data flow transformation task.\n",
    "-   **Integration runtime**: This is the Azure Data Factory engine that works behind the scenes and provides the compute and resources to run the activities or tasks.\n",
    "-   **Pipeline**: A single entity that combines all the aforementioned components to connect, process, transform, and ingest the data to the destination. A single pipeline may contain multiple linked services, datasets, activities, and data flows.\n",
    "\n",
    "In this recipe, we will be provisioning an Azure Data Factory using the Azure portal. Follow these steps:\n",
    "\n",
    "1. Log in to portal.azure.com, click on Create a resource, and search for Data Factory. Select Data Factory and click on Create. Provide the data factory name, the resource group name, and location.\n",
    "1. Click on Next: Git configuration. Git configuration allows you to configure integration with Azure DevOps or GitHub. Git integration helps you save data factory pipelines as Azure Resource Manager (ARM) templates and lets you perform continuous integration and continuous deployment (CI/CD). For this recipe, we will choose Configure Git later.\n",
    "1. As for the remaining tabs (Networking/Advanced/Tags), they can remain as is. Click Review + create to create the data factory.\n",
    "\n",
    "How it works…\n",
    "\n",
    "It is fairly straightforward to create a data factory instance. The data factory that we created in this recipe will be used to hold several datasets, pipelines, and data sources that are to be created in the following recipes in this lab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 2 - Copying files to a database from a data lake using a control flow and copy activity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, we will be building a pipeline that will copy a group of files in blob storage to Azure SQL Database, but only if the filenames contain today's date as a suffix, by following these steps:\n",
    "\n",
    "1.  Get the list of files to be copied using the **Get Metadata** activity in the data factory.\n",
    "2.  Use the **Filter** activity to filter the file whose suffix is the current date.\n",
    "3.  Use the **ForEach** activity to loop through the files.\n",
    "4.  Use the **Copy** activity to load the file into Azure SQL Database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following command to create a container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "$storageaccountname=\"sparshstorage1\"\n",
    "$resourcegroup=\"sparsh-resource-1\"\n",
    "$containername=\"dataloading\"\n",
    "\n",
    "$storagecontext = (Get-AzStorageAccount -ResourceGroupName $resourcegroup -Name $storageaccountname).Context;\n",
    "\n",
    "New-AzStorageContainer -Name $containername -Context $storagecontext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following script to create a SQL Server database in the same resource group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "$serverName = \"sparshdeadfsql\"\n",
    "$adminSqlLogin = \"sqladmin\"\n",
    "$password = \"MyPass123\"\n",
    "$startIp = \"0.0.0.0\"\n",
    "$endIp = \"255.255.255.255\"\n",
    "$databasename = \"sample\"\n",
    "\n",
    "$server = New-AzSqlServer -ResourceGroupName $resourcegroup -ServerName $serverName -Location \"Eastus\" -SqlAdministratorCredentials $(New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $adminSqlLogin, $(ConvertTo-SecureString -String $password -AsPlainText -Force))\n",
    "\n",
    "$serverFirewallRule = New-AzSqlServerFirewallRule -ResourceGroupName $resourcegroup -ServerName $serverName -FirewallRuleName \"AllowedIPs\" -StartIpAddress $startIp -EndIpAddress $endIp\n",
    "\n",
    "$database = New-AzSqlDatabase -ResourceGroupName $resourcegroup -ServerName $serverName -DatabaseName $databaseName -RequestedServiceObjectiveName \"S0\"\n",
    "\n",
    "$database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see output similar to:\n",
    "\n",
    "```\n",
    "PS /> $serverName = \"sparshdeadfsql\"                     PS /> $adminSqlLogin = \"sqladmin\"\n",
    "PS /> $password = \"MyPass123\"\n",
    "PS /> $startIp = \"0.0.0.0\"\n",
    "PS /> $endIp = \"255.255.255.255\"\n",
    "PS /> $databasename = \"sample\"\n",
    "PS /> \n",
    "PS /> $server = New-AzSqlServer -ResourceGroupName $resourcegroup -ServerName $serverName -Location \"Eastus\" -SqlAdministratorCredentials $(New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList $adminSqlLogin, $(ConvertTo-SecureString -String $password -AsPlainText -Force))\n",
    "PS /> \n",
    "PS /> $serverFirewallRule = New-AzSqlServerFirewallRule -ResourceGroupName $resourcegroup -ServerName $serverName -FirewallRuleName \"AllowedIPs\" -StartIpAddress $startIp -EndIpAddress $endIp\n",
    "PS /> \n",
    "PS /> $database = New-AzSqlDatabase -ResourceGroupName $resourcegroup -ServerName $serverName -DatabaseName $databaseName -RequestedServiceObjectiveName \"S0\"\n",
    "WARNING: Upcoming breaking changes in the cmdlet 'New-AzSqlDatabase' :\n",
    "\n",
    "- The output type 'Microsoft.Azure.Commands.Sql.Database.Model.AzureSqlDatabaseModel' is changing\n",
    "- The following properties in the output type are being deprecated : 'BackupStorageRedundancy'\n",
    "- The following properties are being added to the output type : 'CurrentBackupStorageRedundancy' 'RequestedBackupStorageRedundancy'\n",
    "- The change is expected to take effect from the version : '3.0.0'\n",
    "Note : Go to https://aka.ms/azps-changewarnings for steps to suppress this breaking change warning, and other information on breaking changes in Azure PowerShell.\n",
    "PS /> \n",
    "PS /> $database\n",
    "\n",
    "ResourceGroupName                : sparsh-resource-1\n",
    "ServerName                       : sparshdeadfsql\n",
    "DatabaseName                     : sample\n",
    "Location                         : eastus\n",
    "DatabaseId                       : 2a584e8b-b1d3-38df5dc0eedb\n",
    "Edition                          : Standard\n",
    "CollationName                    : SQL_Latin1_General_CP1_CI_AS\n",
    "CatalogCollation                 : \n",
    "MaxSizeBytes                     : 268435456000\n",
    "Status                           : Online\n",
    "CreationDate                     : 2/10/2023 8:25:22 PM\n",
    "CurrentServiceObjectiveId        : 00000000-0000-0000-0000-000000000000\n",
    "CurrentServiceObjectiveName      : S0\n",
    "RequestedServiceObjectiveName    : S0\n",
    "RequestedServiceObjectiveId      : \n",
    "ElasticPoolName                  : \n",
    "EarliestRestoreDate              : \n",
    "Tags                             : \n",
    "ResourceId                       : /subscriptions/044e679e-4bef-add9/resourceGroups/sparsh-resource-1/providers/Microsoft.Sql/servers/sparshdeadfsql/data\n",
    "                                   bases/sample\n",
    "CreateMode                       : \n",
    "ReadScale                        : Disabled\n",
    "ZoneRedundant                    : False\n",
    "Capacity                         : 10\n",
    "Family                           : \n",
    "SkuName                          : Standard\n",
    "LicenseType                      : \n",
    "AutoPauseDelayInMinutes          : \n",
    "MinimumCapacity                  : \n",
    "ReadReplicaCount                 : \n",
    "HighAvailabilityReplicaCount     : \n",
    "CurrentBackupStorageRedundancy   : Geo\n",
    "RequestedBackupStorageRedundancy : Geo\n",
    "SecondaryType                    : \n",
    "MaintenanceConfigurationId       : /subscriptions/044e679e-431e-ccfcdb3/providers/Microsoft.Maintenance/publicMaintenanceConfigurations/SQL_Default\n",
    "EnableLedger                     : False\n",
    "PreferredEnclaveType             : Default\n",
    "PausedDate                       : \n",
    "ResumedDate                      : \n",
    "\n",
    "PS /> \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data files into the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "import pandas as pd\n",
    " \n",
    "for i in range(3):\n",
    "    now = datetime.now(timezone.utc) - timedelta(days=i)\n",
    "    dt_string = now.strftime(\"%Y%m%d\")\n",
    "    df = pd.DataFrame()\n",
    "    product = ['PC', 'keyboard', 'cable', 'camera', 'mobile']\n",
    "    cost = [1000, 20, 1, 50, 100]\n",
    "    quantity = [5, 20, 1000, 50, 200]\n",
    "    location = ['Singapore', 'Dubai', 'Singapore', 'Delhi', 'HongKong']\n",
    "    df = pd.DataFrame({'product': product, 'cost': cost, 'quantity': quantity, 'location': location})\n",
    "    df['order_dt'] = dt_string\n",
    "    filepath = f\"./data/orderdtls-{dt_string}.csv\"\n",
    "    df[['order_dt', 'product', 'cost', 'quantity', 'location']].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "$files = Get-ChildItem -Path \".\\data\";\n",
    "\n",
    "foreach($file in $files){\n",
    "\n",
    "Set-AzStorageBlobContent -File $file.FullName -Context $storagecontext -Blob $file.BaseName.csv -Container $containername -Force\n",
    "\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To copy the files in blob storage that have the current date as the suffix into Azure SQL Database, we will do the following:\n",
    "\n",
    "1. Create linked services to connect the blob storage and Azure SQL Database.\n",
    "1. Add the Get Metadata activity to get a list of files.\n",
    "1. Add the Filter activity to filter the files with the current date as the suffix.\n",
    "1. Add the Copy activity to copy the files that have been filtered to Azure SQL Database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linked service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s create two connections (linked services) – one for Azure SQL Database and another for blob storage:\n",
    "\n",
    "1. In the Azure portal, open the data factory that we provisioned. Click on Open Azure Data Factory Studio. Once the Azure Data Factory Studio opens, click on the Manage button.\n",
    "1. Click on Linked Services, then + New, and search for data under Data store. Select `Azure Data Lake Storage Gen2` and click Continue.\n",
    "1. Create a connection to the storage account.\n",
    "1. Similarly, create a linked service for `Azure SQL Database`. Set User name to `sqladmin` and Password to `MyPass123`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Get Metadata activity to get filenames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to get the list of files in the container. We’ll do this using the Get Metadata activity. Follow these steps:\n",
    "\n",
    "1. Create a new pipeline by clicking on the Author icon (the pencil-shaped icon on the left), then the + button, and then Pipeline.\n",
    "1. Under Activities, search and drag the `Get Metadata` activity onto the pipeline.\n",
    "1. Set the name of the activity to `GetFilename`.\n",
    "1. Under Dataset, click on the + New button to add a new dataset.\n",
    "1. Select `Azure Data Lake Storage Gen2` and select CSV as the file type. Name the dataset `OrderdtlsCSV`. Set Linked service to DataLoading, which we created earlier. Under File path, select the dataloading container. Then, check the First row as header box.\n",
    "1. Under Field list, click on the + New button and select `Child items`.\n",
    "1. Hit the Debug button at the top and check the output. If no errors have been reported and the output shows the filenames, then we have configured the Get Metadata task correctly and we can proceed to the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the current date files using the Filter activity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to filter the list of files that are returned by the Get Metadata activity for the files with the current date as the suffix. Let’s add a Filter activity to the pipeline:\n",
    "\n",
    "1. Search for filter in the Activities tab and drag and drop the activity onto the pipeline.\n",
    "1. Connect the Get Metadata activity and the Filter activity.\n",
    "1. Name the Filter activity `FilterTodaysDate`. Move to the Settings tab of the Filter activity.\n",
    "1. For Items, click on the textbox and then click on `Add dynamic content` (Alt + Shift + D).\n",
    "1. Paste `@activity('GetFileName').output.childitems` into the Items field. This will retrieve the output array that was returned by the Get Metadata activity.\n",
    "1. Similarly, add `@endswith(item().name,concat('-',formatDateTime(utcnow(),'yyyMMdd'),'.csv'))` for Condition.\n",
    "1. Hit the Debug button to test this. Ensure that the `FilterTodaysDate` activity has been completed successfully and shows the filename with the current date as the suffix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a ForEach activity to loop through the files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a ForEach activity to iterate through the files that are returned by the Filter activity. Follow these steps:\n",
    "\n",
    "1. Search for `ForEach` under Activities and add the activity to the pipeline. Link it to the FilterTodaysDate activity.\n",
    "1. Go to the Settings tab of the ForEach activity. For Items, click on Add dynamic content (Alt + Shift + D).\n",
    "1. Under Activity Outputs, click on the `FilterTodaysDate` activity’s output. This will automatically add `@activity('FilterTodaysDate').output`.\n",
    "1. Append `.value` and set Items to `@activity('FilterTodaysDate').output.value`. This will pass the filenames from the FilterTodaysDate activity to the ForEach activity.\n",
    "1. Go to the Activities (0) tab and click on the pencil button."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Copy data activity to ingest files to Azure SQL Database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will ingest files from the data lake to Azure SQL Database. We can do this by passing the files listed by the ForEach activity to the Copy activity. Follow these steps:\n",
    "\n",
    "1. Search for Copy Data under Activities and add it to the pipeline.\n",
    "1. Name the Copy data activity `CopyOrderDtltoSQL`.\n",
    "1. Go to the Source tab. Select OrderdtlsCSV as the dataset since we need to copy CSV files to Azure SQL Database.\n",
    "1. Under File path type, select Wildcard file path. In the last textbox, type `@item().name`.\n",
    "1. Move to the Sink tab. Press + New to add the dataset.\n",
    "1. Search for SQL and select `Azure SQL Database`.\n",
    "1. Select `SQLDB` as the linked service as we had created the connection to Azure SQL Database initially. Name the dataset as `OrderdtlSQL`. Click on the Edit checkbox below Table name. Provide table name as `dbo.orderdtls`. Select None option under Import schema. Press OK.\n",
    "1. In the Sink tab, copy and paste the following script into the Pre-copy script field:\n",
    "    ```\n",
    "    if not exists ( Select * from sys.objects where name like 'orderdtls')\n",
    "\n",
    "    Create table dbo.orderdtls(order_dt varchar(30),product varchar(100),cost int, quantity int, location varchar(100))\n",
    "    ```\n",
    "    This will create a table called orderdtls for the first time and append rows to the same table in subsequent runs. All the other options can be left as is. Click on pipeline1 to go back to the pipeline.\n",
    "1. Hit the Debug button to test all activities. The activities will complete.\n",
    "1. Rename the pipeline `ControlFlowActivities` and hit the Publish button to save the pipeline. You can verify the result by querying Azure SQL Database from the Azure portal via Query editor.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/62965911/218207655-a53ff18b-58f7-47de-8e2c-c14002ef4da3.png)\n",
    "\n",
    "How it works…\n",
    "\n",
    "In this recipe, we performed four key steps to move the data from blob storage to Azure SQL Database:\n",
    "\n",
    "- We got the list of files in the container using the Get Metadata task.\n",
    "- We filtered for files while using the current date as a prefix using the Filter task.\n",
    "- We iterated through all the current date files using the ForEach task.\n",
    "- We ingested the file’s content in a SQL table using the Copy task.\n",
    "\n",
    "By using various control activities, we can successfully transfer data from files to a database. Transferring files to a database is a common scenario in ETL workloads; you can use the preceding framework and customize the data transfer based on your requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 3 - Triggering a pipeline in Azure Data Factory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Azure Data Factory pipeline can be triggered manually, scheduled, or triggered by an event. In this recipe, we’ll configure an event-based trigger to run the pipeline that we created in the previous recipe whenever a new file is uploaded to the Data Lake Store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the trigger, follow these steps:\n",
    "\n",
    "- The event trigger requires the eventgrid resource to be registered in the subscription. To do that, execute the following PowerShell command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "Register-AzResourceProvider -ProviderNamespace Microsoft. EventGrid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the Azure portal, under All resources, open the data factory that you created in the Provisioning Azure Data Factory recipe. On the data factory overview page, select Open Azure Data Factory Studio. Click on the Author button on the left. Expand Pipeline and click on ControlFlowActivities, which was created in the previous recipe\n",
    "- Select Add trigger and then select New/Edit.\n",
    "- In the Add triggers window, select Choose trigger and select New.\n",
    "- In the New trigger window, set Name as NewFileTrigger. Set the event’s Type to Storage event. Use the Storage account name and Container name properties you created earlier. Under Event, select blob Created. This will ensure that any time a file is uploaded to the dataloading container, the pipeline will be triggered.\n",
    "- Click Continue to create the trigger.\n",
    "- In the Data preview window, all the files in the dataloading container will be listed.\n",
    "- Click Continue.\n",
    "- In the New trigger window, we can specify the parameter values, if any, that are required by the pipeline to run.\n",
    "- Click OK to create the trigger.\n",
    "- The trigger will be created. Click Publish all to save and apply these changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the trigger in action, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame()\n",
    "product = ['PC', 'keyboard', 'cable', 'camera', 'mobile']\n",
    "cost = [1000, 20, 1, 50, 100]\n",
    "quantity = [5, 20, 1000, 50, 200]\n",
    "location = ['Singapore', 'Dubai', 'Singapore', 'Delhi', 'HongKong']\n",
    "df = pd.DataFrame({'product': product, 'cost': cost, 'quantity': quantity, 'location': location})\n",
    "df['order_dt'] = dt_string\n",
    "filepath = \"./data/orderdtls-Trigger.csv\"\n",
    "df[['order_dt', 'product', 'cost', 'quantity', 'location']].to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "Set-AzStorageBlobContent -File \".\\data\\orderdtls-Trigger.csv\" -Context $storagecontext -Blob orderdtls-Trigger.csv -Container $containername"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once the file has been uploaded, NewFileTrigger will trigger the ControlFlowActivities pipeline.\n",
    "- To check the trigger and pipeline execution, open the Monitor window.\n",
    "- You will see that the ControlFlowActivities pipeline was executed and that it was triggered by NewFileTrigger. This proves that the execution was triggered by the file being uploaded.\n",
    "\n",
    "How it works…\n",
    "\n",
    "Azure Event Grid, which we registered at the subscription level using the Register-AzureResourceProvider PowerShell command, helps track and trigger the pipeline when a file is uploaded to the data lake container. The storage event-based trigger makes it a powerful feature in data engineering projects, since pipelines need to be triggered when a file is uploaded by another batch process or other similar scenarios. You can add conditions such as blob prefix/suffix filters or parameters to trigger the pipeline, but only when specific files are loaded/deleted or when granular conditions must be met."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe 4 - Copying data from a SQL Server virtual machine to a data lake using the Copy data wizard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common scenario in data engineering projects is where you need to ingest data from a relational database engine such as SQL Server, Oracle, or MySQL to a data lake. This recipe will show you how to ingest data from SQL Server, which has been installed in an Azure VM, to an Azure Data Lake. This method will work in on-premises SQL Server to Azure Data Lake instances too, but you will need to install an integration runtime. In this recipe, we will focus on copying data from SQL Server in an Azure VM to a data lake. We will be using the user-friendly Copy data wizard to transfer the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provision the SQL Server VM by doing the following:\n",
    "\n",
    "1. Log in to portal.azure.com.\n",
    "1. Click on Create a Resource.\n",
    "1. Search for SQL Server.\n",
    "1. Select SQL Server 2019 on Windows Server 2019. Pick the Free SQL Server License: SQL 2019 Developer on Windows Server 2019 option.\n",
    "1. Set Resource group to sparshADEADF and Virtual machine name to SQLVM. Then, set Availability options to No infrastructure redundancy required. After that, ensure that Username is set to sqladmin and that Password is set to `MyPass123`. Leave Select inbound ports as is to allow the (RDP) 3389 port since it is allowed by default.\n",
    "1. Click on the SQL Server settings tab and set SQL connectivity to Public (Internet). For SQL Authentication, choose Enable.\n",
    "1. Click on Review + create. It will take around 15 minutes to create the VM. Once the VM has been created, go to the VM’s overview page in the Azure portal, get the Public IP Address information, and perform a remote desktop connection to the VM. Log in using your user ID and password – that is, sqladmin/MyPass123.\n",
    "1. Open Windows PowerShell in the SQL VM and run the following commands. These commands will create a folder and download the adventureworks backup file into the folder:\n",
    "    ```\n",
    "    New-Item -Path c:\\temp -ItemType directory\n",
    "\n",
    "    cd c:\\temp\n",
    "\n",
    "    Invoke-WebRequest \"https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorksLT2019.bak\"  -OutFile \"AdventureWorksLT2019.bak\"\n",
    "    ```\n",
    "1. Open Command Prompt in the SQL VM and type the following: ```Sqlcmd -e```\n",
    "1. Paste the following command in Command Prompt to restore the database to SQL Server:\n",
    "    ```\n",
    "    RESTORE DATABASE [AdventureWorksLT2019] FROM  DISK = N'c:\\temp\\AdventureWorksLT2019.bak' WITH  FILE = 1,  MOVE N'AdventureWorksLT2012_Data' TO N'F:\\data\\AdventureWorksLT2012.mdf',  MOVE N'AdventureWorksLT2012_Log' TO N'F:\\log\\AdventureWorksLT2012_log.ldf',  NOUNLOAD,  STATS = 5\n",
    "\n",
    "    GO\n",
    "    ```\n",
    "1. Hit Enter on Command Prompt. This will restore the database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the database has been restored, let’s copy the data from the database into our data lake using the Copy data wizard in the data factory. Follow these steps:\n",
    "\n",
    "1. Log in to portal.azure.com. Go to the data factory that you created earlier. Open Azure Data Factory Studio. Then, click on the Ingest button on the home page.\n",
    "1. Select Built-in copy task and choose the Run once now option.\n",
    "1. Set Source type to SQL server and click + New connection.\n",
    "1. Provide the SQL VM’s public IP address under Server name. Set Database name as AdventureWorksLT2019 and pick SQL authentication under Authentication type. Finally, set User name as sqladmin and Password as MyPass123. Then, click Create.\n",
    "1. Select a few tables you want to copy to the data lake.\n",
    "1. Hit Next twice to go to the Destination data store configuration screen. Select `Azure Data Lake Storage Gen2` under Target type and click + New connection.\n",
    "1. Pick the storage account you created earlier and click Create to create the connection.\n",
    "1. Under Folder path, click on Browse. Select the dataloading container you created earlier. Then, click OK.\n",
    "1. Pick a File format. Check the Add header to file box to ensure that the column names are shown. Then, click Next.\n",
    "1. Set Task name as `CopySQLVMtoADL` and click Next.\n",
    "1. Review your configuration on the Summary page and click Next. This will create the pipelines automatically and execute them. Click Finish.\n",
    "1. Verify that the files have been created by checking the blob storage container in the Azure portal.\n",
    "\n",
    "![Figure_3](https://user-images.githubusercontent.com/62965911/218211302-64d1e918-852c-4ec3-8c00-e9b52b2e717f.jpg)\n",
    "\n",
    "How it works…\n",
    "\n",
    "Go to Azure Data Factory Studio and click on the Author button on the left. Expand Pipeline and notice that a new pipeline, CopySQLVMtoADL, has been created. Click on it. You will see that there’s a ForEach activity in the pipeline. The ForEach activity is used to iterate through each table that needs to be copied.\n",
    "\n",
    "![Figure_3 51](https://user-images.githubusercontent.com/62965911/218211682-5c195007-6d39-4210-b1e2-67eb7acac6a9.jpg)\n",
    "\n",
    "Click on Activities. You will notice a Copy data task whose source is SQL Server and the destination is Azure Data Lake Storage Gen2. The Copy data task will copy one table at a time from SQL Server to the data lake.\n",
    "\n",
    "You will also notice that the source table name and destination filename come from the ForEach activity (item().source.table /item().destination.fileName). The Copy data wizard has automatically created the pipeline with the relevant activities to move the data.\n",
    "\n",
    "You can easily customize the pipeline or schedule it based on your needs to transfer data periodically. Copy data wizard saves so much time when it comes to configuring the datasets and the ForEach activity, which makes it easy to get started with data movement tasks.\n",
    "\n",
    "Ensure that you delete the sparshADEADF resource group once you have finished since you will incur Azure consumption costs otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
