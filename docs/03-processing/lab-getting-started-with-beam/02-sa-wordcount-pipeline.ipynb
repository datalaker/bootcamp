{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline2.py\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the pipeline\n",
    "\n",
    "In this example, the code first creates a `PipelineOptions` object. This object lets us set various options for our pipeline, such as the pipeline runner that will execute our pipeline and any runner-specific configuration required by the chosen runner. In this example we set these options programmatically, but more often, command-line arguments are used to set `PipelineOptions`.\n",
    "\n",
    "You can specify a runner for executing your pipeline, such as the `DataflowRunner` or `SparkRunner`. If you omit specifying a runner, as in this example, your pipeline executes locally using the `DirectRunner`. In the next sections, we will specify the pipeline's runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "def main(argv=None, save_main_session=True):\n",
    "  \"\"\"Main entry point; defines and runs the wordcount pipeline.\"\"\"\n",
    "\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--input',\n",
    "      dest='input',\n",
    "      help='Input file to process.')\n",
    "  parser.add_argument(\n",
    "      '--output',\n",
    "      dest='output',\n",
    "      help='Output file to write results to.')\n",
    "\n",
    "  known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "  # We use the save_main_session option because one or more DoFn's in this\n",
    "  # workflow rely on global context (e.g., a module imported at module level).\n",
    "  pipeline_options = PipelineOptions(pipeline_args)\n",
    "  pipeline_options.view_as(SetupOptions).save_main_session = save_main_session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a Pipeline object with the options we’ve just constructed. The Pipeline object builds up the graph of transformations to be executed, associated with that particular pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "  with beam.Pipeline(options=pipeline_options) as p:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying pipeline transforms\n",
    "\n",
    "The MinimalWordCount pipeline contains several transforms to read data into the pipeline, manipulate or otherwise transform the data, and write out the results. Transforms can consist of an individual operation, or can contain multiple nested transforms (which is a [composite transform](https://beam.apache.org/documentation/programming-guide#composite-transforms)).\n",
    "\n",
    "Each transform takes some kind of input data and produces some output data. The input and output data is often represented by the SDK class `PCollection`. `PCollection` is a special class, provided by the Beam SDK, that you can use to represent a dataset of virtually any size, including unbounded datasets.\n",
    "\n",
    "![](./img/pipeline2.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MinimalWordCount pipeline contains five transforms:\n",
    "\n",
    "1.  A text file `Read` transform is applied to the `Pipeline` object itself, and produces a `PCollection` as output. Each element in the output `PCollection` represents one line of text from the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "    # Read the text file[pattern] into a PCollection.\n",
    "    lines = p | ReadFromText(known_args.input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  This transform splits the lines in `PCollection<String>`, where each element is an individual word in Shakespeare's collected texts. As an alternative, it would have been possible to use a [ParDo](https://beam.apache.org/documentation/programming-guide/#pardo) transform that invokes a `DoFn` (defined in-line as an anonymous class) on each element that tokenizes the text lines into individual words. The input for this transform is the `PCollection` of text lines generated by the previous `TextIO.Read` transform. The `ParDo` transform outputs a new `PCollection`, where each element represents an individual word in the text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  The SDK-provided `Count` transform is a generic transform that takes a `PCollection` of any type, and returns a `PCollection` of key/value pairs. Each key represents a unique element from the input collection, and each value represents the number of times that key appeared in the input collection. In this pipeline, the input for `Count` is the `PCollection` of individual words generated by the previous `ParDo`, and the output is a `PCollection` of key/value pairs where each key represents a unique word in the text and the associated value is the occurrence count for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "    # Count the occurrences of each word.\n",
    "    counts = (\n",
    "        lines\n",
    "        | 'Split' >> (\n",
    "            beam.FlatMap(\n",
    "                lambda x: re.findall(r'[A-Za-z\\']+', x)).with_output_types(str))\n",
    "        | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
    "        | 'GroupAndSum' >> beam.CombinePerKey(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  The next transform formats each of the key/value pairs of unique words and occurrence counts into a printable string suitable for writing to an output file. The map transform is a higher-level composite transform that encapsulates a simple `ParDo`. For each element in the input `PCollection`, the map transform applies a function that produces exactly one output element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "    # Format the counts into a PCollection of strings.\n",
    "    def format_result(word_count):\n",
    "      (word, count) = word_count\n",
    "      return '%s: %s' % (word, count)\n",
    "\n",
    "    output = counts | 'Format' >> beam.Map(format_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  A text file write transform. This transform takes the final `PCollection` of formatted Strings as input and writes each element to an output text file. Each element in the input `PCollection` represents one line of text in the resulting output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "    # Write the output using a \"Write\" transform that has side effects.\n",
    "    # pylint: disable=expression-not-assigned\n",
    "    output | WriteToText(known_args.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `Write` transform produces a trivial result value of type `PDone`, which in this case is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline2.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  logging.getLogger().setLevel(logging.INFO)\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "INFO:root:Default Python SDK image for environment is apache/beam_python3.9_sdk:2.38.0\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function annotate_downstream_side_inputs at 0x7ff4e16233a0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function fix_side_input_pcoll_coders at 0x7ff4e16234c0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function pack_combiners at 0x7ff4e16239d0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function lift_combiners at 0x7ff4e1623a60> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_sdf at 0x7ff4e1623c10> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function expand_gbk at 0x7ff4e1623ca0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sink_flattens at 0x7ff4e1623dc0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function greedily_fuse at 0x7ff4e1623e50> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function read_to_impulse at 0x7ff4e1623ee0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function impulse_to_input at 0x7ff4e1623f70> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function sort_stages at 0x7ff4e16241f0> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function setup_timer_mapping at 0x7ff4e1624160> ====================\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.translations:==================== <function populate_data_channel_coders at 0x7ff4e1624280> ====================\n",
      "INFO:apache_beam.runners.worker.statecache:Creating state cache with size 100\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.worker_handlers:Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7ff4e16e1430> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, b'')\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((((ref_AppliedPTransform_ReadFromText-Read-Impulse_4)+(ref_AppliedPTransform_ReadFromText-Read-Map-lambda-at-iobase-py-898-_5))+(ReadFromText/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(ReadFromText/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_2_split/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_PCollection_PCollection_2_split/Read)+(ReadFromText/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_Split_8))+(ref_AppliedPTransform_PairWithOne_9))+(GroupAndSum/Precombine))+(GroupAndSum/Group/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((ref_AppliedPTransform_WriteToText-Write-WriteImpl-DoOnce-Impulse_19)+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-DoOnce-FlatMap-lambda-at-core-py-3229-_20))+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-DoOnce-Map-decode-_22))+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-InitializeWrite_23))+(ref_PCollection_PCollection_11/Write))+(ref_PCollection_PCollection_12/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (((((((GroupAndSum/Group/Read)+(GroupAndSum/Merge))+(GroupAndSum/ExtractOutputs))+(ref_AppliedPTransform_Format_14))+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-WindowInto-WindowIntoFn-_24))+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-WriteBundles_25))+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-Pair_26))+(WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-Extract_28))+(ref_PCollection_PCollection_17/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running ((ref_PCollection_PCollection_11/Read)+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-PreFinalize_29))+(ref_PCollection_PCollection_18/Write)\n",
      "INFO:apache_beam.runners.portability.fn_api_runner.fn_runner:Running (ref_PCollection_PCollection_11/Read)+(ref_AppliedPTransform_WriteToText-Write-WriteImpl-FinalizeWrite_30)\n",
      "INFO:apache_beam.io.filebasedsink:Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
      "INFO:apache_beam.io.filebasedsink:Renamed 1 shards in 0.04 seconds.\n"
     ]
    }
   ],
   "source": [
    "!python src/pipeline2.py --input data/kinglear.txt --output output/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING: 243\n",
      "LEAR: 236\n",
      "DRAMATIS: 1\n",
      "PERSONAE: 1\n",
      "king: 65\n",
      "of: 447\n",
      "Britain: 2\n",
      "OF: 15\n",
      "FRANCE: 10\n",
      "DUKE: 3\n"
     ]
    }
   ],
   "source": [
    "!head output/wordcount-00000-of-00001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KING', 243)\n",
      "('LEAR', 236)\n",
      "('DRAMATIS', 1)\n",
      "('PERSONAE', 1)\n",
      "('king', 65)\n",
      "('of', 447)\n",
      "('Britain', 2)\n",
      "('OF', 15)\n",
      "('FRANCE', 10)\n",
      "('DUKE', 3)\n",
      "('BURGUNDY', 8)\n",
      "('CORNWALL', 63)\n",
      "('ALBANY', 67)\n",
      "('EARL', 2)\n",
      "('KENT', 156)\n",
      "('GLOUCESTER', 141)\n",
      "('EDGAR', 126)\n",
      "('son', 29)\n",
      "('to', 438)\n",
      "('Gloucester', 26)\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import re\n",
    "\n",
    "inputs_pattern = 'data/kinglear*'\n",
    "outputs_prefix = 'output/pipe2'\n",
    "\n",
    "# Running locally in the DirectRunner.\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
    "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
    "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
    "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
    "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
    "  )\n",
    "\n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "!head -n 20 {outputs_prefix}-00000-of-*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count with comments\n",
    "\n",
    "Below is mostly the same code as above, but with comments explaining every line in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KING', 243)\n",
      "('LEAR', 236)\n",
      "('DRAMATIS', 1)\n",
      "('PERSONAE', 1)\n",
      "('king', 65)\n",
      "('of', 447)\n",
      "('Britain', 2)\n",
      "('OF', 15)\n",
      "('FRANCE', 10)\n",
      "('DUKE', 3)\n",
      "('BURGUNDY', 8)\n",
      "('CORNWALL', 63)\n",
      "('ALBANY', 67)\n",
      "('EARL', 2)\n",
      "('KENT', 156)\n",
      "('GLOUCESTER', 141)\n",
      "('EDGAR', 126)\n",
      "('son', 29)\n",
      "('to', 438)\n",
      "('Gloucester', 26)\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "import re\n",
    "\n",
    "inputs_pattern = 'data/kinglear*'\n",
    "outputs_prefix = 'output/pipe2'\n",
    "\n",
    "# Running locally in the DirectRunner.\n",
    "with beam.Pipeline() as pipeline:\n",
    "  # Store the word counts in a PCollection.\n",
    "  # Each element is a tuple of (word, count) of types (str, int).\n",
    "  word_counts = (\n",
    "      # The input PCollection is an empty pipeline.\n",
    "      pipeline\n",
    "\n",
    "      # Read lines from a text file.\n",
    "      | 'Read lines' >> beam.io.ReadFromText(inputs_pattern)\n",
    "      # Element type: str - text line\n",
    "\n",
    "      # Use a regular expression to iterate over all words in the line.\n",
    "      # FlatMap will yield an element for every element in an iterable.\n",
    "      | 'Find words' >> beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\n",
    "      # Element type: str - word\n",
    "\n",
    "      # Create key-value pairs where the value is 1, this way we can group by\n",
    "      # the same word while adding those 1s and get the counts for every word.\n",
    "      | 'Pair words with 1' >> beam.Map(lambda word: (word, 1))\n",
    "      # Element type: (str, int) - key: word, value: 1\n",
    "\n",
    "      # Group by key while combining the value using the sum() function.\n",
    "      | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "      # Element type: (str, int) - key: word, value: counts\n",
    "  )\n",
    "\n",
    "  # We can process a PCollection through other pipelines too.\n",
    "  (\n",
    "      # The input PCollection is the word_counts created from the previous step.\n",
    "      word_counts\n",
    "\n",
    "      # Format the results into a string so we can write them to a file.\n",
    "      | 'Format results' >> beam.Map(lambda word_count: str(word_count))\n",
    "      # Element type: str - text line\n",
    "\n",
    "      # Finally, write the results to a file.\n",
    "      | 'Write results' >> beam.io.WriteToText(outputs_prefix)\n",
    "  )\n",
    "\n",
    "# Sample the first 20 results, remember there are no ordering guarantees.\n",
    "!head -n 20 {outputs_prefix}-00000-of-*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
