{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR Serverless Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws cloudformation create-stack --stack-name EMRLabv1 \\\n",
    "            --template-url https://wysde-assets.s3.us-east-1.amazonaws.com/cfn/emr-serverless-cfn-v2.json \\\n",
    "            --capabilities CAPABILITY_NAMED_IAM CAPABILITY_AUTO_EXPAND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CloudFormation stack will roughly take 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Spark jobs to EMR Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Submit Spark jobs from EMR Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we submit a Job to EMR Serverless we need to create an Application. You can create one or more applications that use open-source analytics frameworks. To create an application, you specify the open-source framework that you want to use (for example, Apache Spark or Apache Hive), the Amazon EMR release for the open-source framework version (for example, Amazon EMR release 6.4, which corresponds to Apache Spark 3.1.2), and a name for your application\n",
    "\n",
    "1. In AWS Console, under services search for EMR. In the EMR console select EMR Serverless, or alternatively, go to the EMR Serverless Console\n",
    "2. Click Create and launch EMR Studio.\n",
    "3. To create an EMR Serverless application, choose Create application\n",
    "4. Give a name to your application. In the rest of this lab we are going to use **my-serverless-application** as the name of our application. Choose Spark as the Type and emr-6.7.0 as the release version. Choose default settings. Click on Create application\n",
    "5. You should see the Status Created.\n",
    "6. Now you are ready to submit a job. To do this, choose Submit job.\n",
    "7. In the Submit job screen enter the details as below:\n",
    "    | Name                   | word\\_count                                                                                                                                           |\n",
    "    | ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "    | Runtime role           | EMRServerlessS3RuntimeRole                                                                                                                            |\n",
    "    | Script location S3 URI | s3://wysde-assets/src/py/emr/wordcount.py (This is not a public path, so you might get access denied error in emr)                                                                |\n",
    "    | Script arguments       | \\[“s3://YOUR\\_BUCKET/wordcount\\_output/\"\\] Replace YOUR\\_BUCKET with the S3 bucket name that you noted from the Cloudformation Stack Outputs Section. |\n",
    "8. You can see that the Application status shows as Starting and the Job run status shows as Pending. First time job submissions take a little longer since the application needs to be prepared and started. Furthermore, capacity needs to be provisioned before the application becomes ready to accept jobs. If the application is already in Started status, jobs will start executing as and when submitted.\n",
    "9. Once the job has been submitted the Run status shows Success.\n",
    "10. You can now verify that the job has written its output in the s3 path that we provided as an argument when submitting the job. You can go to the s3 path and see csv files successfully created by the EMR Serverless application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 - Submit Spark jobs from CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In AWS Console, under services search for Cloud9 and choose Cloud9 or alternatively go to Cloud9 Console \n",
    "2. The workshop cloudformation template should have already created a Cloud9 environment Cloud9 for EMRServerless Workshop\n",
    "3. Choose Open IDE to launch the Cloud9 IDE.\n",
    "4. Open a new terminal tab in the main work area\n",
    "5. Go to Cloudformation  and click on the stack emrserverless-workshop or look for the description EMR Serverless workshop\n",
    "6. Go to outputs tab\n",
    "7. We will now export the output values in Cloud9 terminal:\n",
    "   1. export JOB_ROLE_ARN=<<EMRServerlessS3RuntimeRoleARN>>\n",
    "   2. export S3_BUCKET=s3://<<YOUR_BUCKET>>\n",
    "8. We will now export the application id of our EMR Serverless Application to cloud9 terminal.\n",
    "9. Go to EMR Serverless console and click on application and copy the application id and use in the terminal as below\n",
    "   1.  export APPLICATION_ID=<<application_id>>\n",
    "10. Submit the job using the following command:\n",
    "    ```\n",
    "    aws emr-serverless start-job-run --application-id ${APPLICATION_ID} --execution-role-arn ${JOB_ROLE_ARN} --name \"Spark-WordCount-CLI\" --job-driver '{\n",
    "            \"sparkSubmit\": {\n",
    "                \"entryPoint\": \"s3://wysde-assets/src/py/emr/wordcount.py\",\n",
    "                \"entryPointArguments\": [\n",
    "            \"'\"$S3_BUCKET\"'/wordcount_output_cli/\"\n",
    "            ],\n",
    "                \"sparkSubmitParameters\": \"--conf spark.executor.cores=1 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.memory=4g --conf spark.executor.instances=1 --conf spark.hadoop.hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\"\n",
    "            }\n",
    "        }'\n",
    "    ```\n",
    "11. If you get aws cli \"command not found error\", make sure your aws cli is of latest version.\n",
    "11. Check the status on the EMR console.\n",
    "12. Navigate to your S3 bucket and check the output under wordcount_output folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Spark job output data using Athena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us verify that the word_count output written by our job has the correct data. In order to do this we need to crawl the job output data and store the schema in Glue Data Catalog.\n",
    "\n",
    "1. In AWS Console, under services search for Glue and select AWS Glue or alternatively, go to Glue Console.\n",
    "2. In the Glue console click Crawlers\n",
    "3. Click Add crawler\n",
    "4. Give a name for the Glue crawler. In this lab we use word-count-output-crawler as the Crawler name. Click Next.\n",
    "5. In the Add a data store screen provide s3://$S3_BUCKET/wordcount_output as the path. Make sure you replace $S3_BUCKET with your S3 bucket name. This is the data written as output by EMR Serverless. We are going to crawl this data. Click Next.\n",
    "6. In order to be able to crawl, the Glue crawler job needs the permissions to read the S3 bucket and create a Glue database and table. For this, we need an IAM service role for Glue with the right permissions. Choose Create an IAM role and provide emrserverless as the suffix for the role name. Click Next.\n",
    "7. In the the crawler’s output screen, choose Add database. Give emrserverlessdb as the Database name.\n",
    "8. Choose Run on demand as the Frequency for the crawler. Click Create.\n",
    "9. The new Glue database shows up. Click Next. Review the choices you have made and click Next again.\n",
    "10. We have created the Crawler, but we are not not running yet. Click Run it now to start running it.\n",
    "11. After less than a minute, you should get a message on the console that 1 table got created.\n",
    "12. In the left Navigation menu, select Tables to see that a new table called wordcount_output got created.\n",
    "13. Execute the following query \"SELECT * FROM wordcount_output limit 10;\".\n",
    "14. Now we can see the wordcount data generated by the EMR Serverless job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Hive jobs from EMR Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On EMR Serverless Console click on Create application Enter name as hive-serverless, Select Type as Hive and latest release version.\n",
    "2. Under Application setup options, Choose default settings and click Create application. You will notice the application is created and shows Starting.\n",
    "3. Once the application is started, we can submit Hive jobs to it. Click on Submit job\n",
    "4. Enter the details as below:\n",
    "  ```\n",
    "  Name\tHive-Serverless-Console\n",
    "  Runtime role\tEMRServerlessS3RuntimeRole\n",
    "  Initialization Script Location S3 URI\ts3://wysde-assets/src/sql/create_taxi_trip.sql\n",
    "  Script location S3 URI\ts3://wysde-assets/src/sql/count.sql\n",
    "  ```\n",
    "5. As part of the initialization script we are creating a hive table for NewYork Taxi Trip details:\n",
    "   ```sql\n",
    "  CREATE EXTERNAL TABLE if not exists `nytaxitrip`(\n",
    "    `vendorid` bigint, \n",
    "    `tpep_pickup_datetime` string, \n",
    "    `tpep_dropoff_datetime` string, \n",
    "    `passenger_count` bigint, \n",
    "    `trip_distance` double, \n",
    "    `ratecodeid` bigint, \n",
    "    `store_and_fwd_flag` string, \n",
    "    `pulocationid` bigint, \n",
    "    `dolocationid` bigint, \n",
    "    `payment_type` bigint, \n",
    "    `fare_amount` double, \n",
    "    `extra` double, \n",
    "    `mta_tax` double, \n",
    "    `tip_amount` double, \n",
    "    `tolls_amount` double, \n",
    "    `improvement_surcharge` double, \n",
    "    `total_amount` double, \n",
    "    `congestion_surcharge` string)\n",
    "  ROW FORMAT DELIMITED \n",
    "    FIELDS TERMINATED BY ',' \n",
    "  STORED AS INPUTFORMAT \n",
    "    'org.apache.hadoop.mapred.TextInputFormat' \n",
    "  OUTPUTFORMAT \n",
    "    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "  LOCATION\n",
    "  's3://wysde-datasets/tripdata/';\n",
    "   ```\n",
    "6. Once the table is created by Hive, we want to run a count query to see the number of records in this table:\n",
    "  ```sql\n",
    "  SET hive.cli.print.header=true;\n",
    "  SET hive.query.name=TaxiTrips;\n",
    "\n",
    "  Select count(*) as count from nytaxitrip;\n",
    "  ```\n",
    "7. Under Job configuration update the DOC-EXAMPLE_BUCKET with your S3 bucket name.\n",
    "8. Under Additional settings select Upload logs to your Amazon S3 bucket and add your s3 bucket path - <<S3_Bucket/logs>>.\n",
    "9. Click Submit job\n",
    "10. Once the job is completed successfully, go over to Athena console to verify if the table got created and query it to check the data.\n",
    "11. Click on the three dots and click Preview table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration using MWAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon EMR Serverless supports following airflow operators\n",
    "\n",
    "**EmrServerlessCreateApplicationOperator** - Operator to create Serverless EMR Application with following parameter\n",
    "\n",
    "- release_label: The EMR release version associated with the application.\n",
    "- job_type: The type of application you want to start, such as Spark or Hive.\n",
    "- wait_for_completion: If true, wait for the Application to start before returning. Default to True\n",
    "- client_request_token: The client idempotency token of the application to create. Its value must be unique for each request.\n",
    "- aws_conn_id: AWS connection to use\n",
    "\n",
    "**EmrServerlessStartJobOperator** - Operator to start EMR Serverless job with following parameter\n",
    "\n",
    "- application_id: ID of the EMR Serverless application to start.\n",
    "- execution_role_arn: ARN of role to perform action.\n",
    "- job_driver: Driver that the job runs on.\n",
    "- configuration_overrides: Configuration specifications to override existing configurations.\n",
    "- client_request_token: The client idempotency token of the application to create.\n",
    "- wait_for_completion: If true, waits for the job to start before returning. Defaults to True.\n",
    "- aws_conn_id: AWS connection to use\n",
    "- config: extra configuration for the job.\n",
    "\n",
    "**EmrServerlessDeleteApplicationOperator** - Operator to delete EMR Serverless application\n",
    "- application_id: ID of the EMR Serverless application to delete.\n",
    "- wait_for_completion: If true, wait for the Application to start before returning. Default to True\n",
    "- aws_conn_id: AWS connection to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWAA Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before scheduling EMR Serverless job using Amazon MWAA, you need to create the required AWS resources. To do this, we provide AWS CloudFormation template to create a stack that contains the resources. When you create the stack, AWS creates following resources in your account:\n",
    "\n",
    "- Setup the VPC network for the Amazon MWAA environment, deploying the following resources:\n",
    "  - A VPC with a pair of public and private subnets spread across two Availability Zones.\n",
    "  - An internet gateway, with a default route on the public subnets.\n",
    "  - A pair of NAT gateways (one in each Availability Zone), and default routes for them in the private subnets.\n",
    "  - Amazon S3 gateway VPC endpoints and EMR interface VPC endpoints in the private subnets in two Availability Zones.\n",
    "  - A security group to be used by the Amazon MWAA environment that only allows local inbound traffic and all outbound traffic.\n",
    "- Setup an Amazon MWAA execution IAM role with permissions to\n",
    "  - Launch an EMR cluster,\n",
    "  - Access EMR Studio,\n",
    "  - Access to the required s3-buckets,\n",
    "  - Access to data catalog and monitoring tools.\n",
    "- An Amazon Simple Storage Service (Amazon S3) bucket that meets the following Amazon MWAA requirements:\n",
    "  - The bucket in the same AWS Region where you create the MWAA environment.\n",
    "  - Globally unique bucket name,\n",
    "  - Bucket versioning is enabled\n",
    "- A folder named dags created in the same bucket to store DAGs.\n",
    "- EMR Serverless application with the following attributes\n",
    "  - Amazon EMR release version 6.6.6\n",
    "  - Apache Spark runtime to use within the application\n",
    "- Amazon MWAA environment we setup Airflow version 2.2.2, with mw1.small environment class and maximum worker count as 1.\n",
    "  - For monitoring, we choose to publish environment performance to CloudWatch Metrics.\n",
    "  - For Airflow logging configuration, we choose to send only the task logs and use log level INFO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws cloudformation create-stack --stack-name EMR-Serverless-Orchestration \\\n",
    "            --template-url https://wysde-assets.s3.us-east-1.amazonaws.com/cfn/mwaa_emr.yml \\\n",
    "            --capabilities CAPABILITY_NAMED_IAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CloudFormation stack will roughly take 30-40 minutes to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting Spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will setup the Managed Apache Airflow environment to handle EMR Serverless jobs. Further we will upload an airflow dag for Spark job and run it in the Amazon MWAA environment\n",
    "\n",
    "1. Go to the Stack Output tab and copy the Value for the S3Bucket Key\n",
    "2. Upload the requirements.txt file to S3 bucket\n",
    "4. Navigate to the Managed Apache Airflow Console \n",
    "5. You should be able to see your MWAA environment\n",
    "6. Click on the mwaa-emr-serverless environment and then in the window that opens click on Edit button\n",
    "7. In the Edit window go to the section DAG code in Amazon S3 and look for Requirements file - optional and then click on Browse S3 button next to it.\n",
    "8. Select the requirements.txt file and click on Choose.\n",
    "9. Keep rest as default and click on Next button\n",
    "10. In the next window Configure advanced settings leave everything as default and click on Next button at the bottom of the screen\n",
    "11. In the Review and Save window click on Save\n",
    "12. Go back to Airflow environments and wait for the status to change from Updating to Available\n",
    "13. The update will roughly take 8-10 minutes to complete.\n",
    "14. Go to the Stack Output tab and copy the Values for the ApplicationID, JobRoleArn, S3Bucket Keys.\n",
    "15. Open the file using text editor and validate the contents of the dag. Replace the values for ApplicationID, JobRoleArn and S3Bucket with the values you copied from CloudFormation stack above into the code in appropriate section.\n",
    "16. Upload the python script to the dags folder in the S3 bucket\n",
    "17. Navigate to the S3 console \n",
    "18. Search for s3 bucket with the Value you copied from the Output section of the CloudFormation stack for S3Bucket Key\n",
    "19. Move to dags folder inside the s3 bucket and then Upload the example_emr_serverless.py script\n",
    "20. Trigger the jobs in Apache Airflow UI\n",
    "21. Navigate to the Managed Apache Airflow Console \n",
    "22. Click on Open Airflow UI link for the Managed Apache Airflow environment mwaa-emr-serverless\n",
    "23. You should see the DAG example_emr_serverless_job in Apache Airflow UI. It may take 2-3 minutes to show up the dag in the Airflow UI.\n",
    "24. Click on play button for dag example_emr_serverless_job in the Actions column and then Trigger Dag to schedule the spark job on EMR Serverless\n",
    "25. Once the status changes from light green to dark green the job is completed.\n",
    "26. Click on the dag example_emr_serverless_job and then in the next window click on the dark green square icon next to start_job\n",
    "27. Click on the Logs button in the popup window \n",
    "28. You will able to see the logs for the spark job. Notice that final state for the job is success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchestrate from locally running Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of MWAA, it is also possible to orchestrate the EMR Serverless Job fom the locally running Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactional Data Lake with Apache Hudi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Upload all the files of src/hudi-*.py to Amazon S3 bucket created by cloud formation template.\n",
    "2. Writing to Hudi Table - Apache Hudi provides 2 types of storage formats for creating a hudi table - Copy on Write and Merge on Read. Refer [this](https://hudi.apache.org/docs/next/faq#what-is-the-difference-between-copy-on-write-cow-vs-merge-on-read-mor-storage-types) to understand which table type is suited for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hudi Copy On Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Click on Submit Job under your spark EMR serverless application\n",
    "2. Provide sample name like hudi_copy_on_write for job name and select EMRServerlessS3RuntimeRole under Runtime role. Browse to S3 folder where you have saved all scripts from prerequisite section and chose hudi-cow.py. The script needs S3 bucket name as argument. This bucket will be used by hudi to write your data. You can provide the S3 bucket name created along with cloudformation template: [\"emrserverless-workshop-<account_id>\"]\n",
    "3. Under spark properties select “Edit in text” and paste below spark configurations.\n",
    "   ```\n",
    "   --conf spark.jars=/usr/lib/hudi/hudi-spark-bundle.jar --conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
    "   ```\n",
    "4. Check the Run status to identify status of job completion. \n",
    "5. Now that our hudi copy on write table is created. Lets verify the count of records from Amazon Athena. Run below query on Athena query editor. Note - EMR serverless currently does not support interactive analysis. So you will have to use other platforms like EMR on EC2 or Athena to query hudi tables.\n",
    "    ```sql\n",
    "    select count(*) from default.hudi_trips_cow\n",
    "    ```\n",
    "6. Now that we have verified our hudi table. Lets perform an upsert operation. But first, lets verify the records we want to modify by running below query on athena:\n",
    "    ```sql\n",
    "    select trip_id, route_id, tstamp, destination from default.hudi_trips_cow where trip_id between 999996 and 1000013\n",
    "    ```\n",
    "7. Lets submit another job that will change the destination and timestamp for trip IDs 1000000 to 1000010. Browse the S3 folder where you have uploaded all files from pre requisite section and chose hudi-upsert-cow.py. Similar to in above section, provide S3 bucket name where hudi table is created as script argument: [\"emrserverless-workshop-<account_id>\"]. Make sure Spark properties are provided. \n",
    "8. Once the job is completed. Verify the updated rows by running below query again on athena. You will notice that destination column for trip_id 1000000 to 1000010 is updated to Boston.\n",
    "    ```sql\n",
    "    select trip_id, route_id, tstamp, destination from default.hudi_trips_cow where trip_id between 999996 and 1000013\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hudi Merge on Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Provide sample name like hudi_merge_on_read for job name and select EMRServerlessS3RuntimeRole under Runtime role. Browse to S3 folder where you have saved all scripts from prerequisite section and chose hudi-mor.py. The script needs S3 bucket name as argument. This bucket will be used by hudi to write your data. You can provide the S3 bucket name created along with cloudformation template: [\"emrserverless-workshop-<account_id>\"]. Make sure spark properties are correctly provided.\n",
    "2. Once the job is complete. It will create two table query types referenced as _ro(Read Optimized) and _rt(Real Time). Run below queries on Athena to validate data in these tables.\n",
    "    ```sql\n",
    "    select count(*) from default.hudi_trips_mor_ro\n",
    "    select count(*) from default.hudi_trips_mor_rt\n",
    "    ```\n",
    "3. Now lets update some records and see how these Read Optimized and Real Time query types behave. Similar to above section on Hudi Copy on Write, we will update the records with trip IDs 1000000 to 1000010. So lets verify records in both query types. Both will have identical data.\n",
    "    ```sql\n",
    "    select trip_id, route_id, tstamp, destination from default.hudi_trips_mor_ro where trip_id between 999996 and 1000013\n",
    "    select trip_id, route_id, tstamp, destination from default.hudi_trips_mor_rt where trip_id between 999996 and 1000013\n",
    "    ```\n",
    "4. Lets submit a job that will update destination for trip IDs 1000000 to 1000010. Browse S3 folder where you have saved all scripts from prerequisite section and chose hudi-upsert-mor.py. Provide hudi table location as script argument and make sure spark properties are correctly provided. \n",
    "5. Once the job is complete. Run below query on Athena. You will notice that hudi_trips_mor_rt has an updated Boston destination for trip_id 1000000 to 1000010.\n",
    "    ```sql\n",
    "    select trip_id, route_id, tstamp, destination from default.hudi_trips_mor_rt where trip_id between 999996 and 1000013\n",
    "    ```\n",
    "6. Lets run the same query against hudi_trips_mor_ro and we will notice that it still has older records. This is because read optimized only reads data upto last compaction. Since in our case data is not yet compacted, latest updated records are not yet reflected. This is a tradeoff between performance and data freshness. Running queries against read optimized will be faster as against real time, which always provides fresh snapshot of data. Notice the time difference of 1.17s against 6.127s as well as difference in data scanned of 2.25MB against 21.07MB\n",
    "    ```sql\n",
    "    select trip_id, route_id, tstamp, destination from default.hudi_trips_mor_ro where trip_id between 999996 and 1000013\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps below for clean up:\n",
    "\n",
    "1. Delete all the Spark/Hive applications that you created for this lab from EMR Studio/Serverless Console\n",
    "2. Delete the S3 bucket emrserverless-workshop-<<account-id>> created as part of the workshop. Click here to know more about how to delete the Amazon S3 bucket \n",
    "3. Open up the CloudFormation console  and select the EMRServerless stack and click on Delete button to terminate the stack."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('env-spacy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
