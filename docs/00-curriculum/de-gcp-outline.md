# Curriculum - Data Engineering Training (GCP)

**Estimated Time:** 55 hours

- Getting Started

  - Creating an account
  - Creating your first GCP project
  - Using GCP Cloud Shell
  - A quick overview of GCP services for data engineering
- Developer Foundations

  - VS Code
    - Download and Install vscode
    - Understand vscode features
    - Install extensions in vscode
  - Anaconda (Python)
    - Download and Install Anaconda
    - Create virtual environment in anaconda
    - Create jupyter notebook in vscode and connect to venv
  - Github
    - Create github account
    - Install git cli
    - Create git repo and add students as collaborator
    - Connect local workspace to git repo
    - Learn git commands
  - Bash
    - Learn bash commands
- Data Engineering Basics

  - What is Data Engineering?
  - Role of Data Engineering in Organizations
  - Skills required to become a Data Engineer
  - Data engineer versus data scientist
  - What is data lake and data warehouse?
  - What is medallion architecture?
  - What is EL, ETL and ELT?
  - What are the benefits of cloud computing?
  - Understanding what the cloud is
  - The difference between the cloud and non-cloud era
  - The on-demand nature of the cloud
  - OLTP vs OLAP technologies
- Programming

  - SQL
    - Query Types
    - Query Engines
    - SQL Basic
      - SELECT, LIMIT, WHERE
      - Comparison and Logical Operators
      - ORDER BY
    - SQL Intermediate
      - Aggregation Functions and GROUP BY
      - CASE
      - JOINS
    - SQL Advanced
      - Dates
      - Texts
      - Subqueries
      - Window Functions
    - Query Optimizations
  - Python
    - Lists and dictionaries
    - For loops and while loops
    - Functions and Inline functions
    - Pandas Dataframes
    - `requests` library
    - `psycopg2` and `sqlalchemy` library
    - Building Functions in Python
    - Read/Write and Manipulate Data using Pandas
    - Data Format Conversion - CSV to Parquet, JSON to CSV/Parquet
    - Pulling Data from APIs using requests library
    - Connect to Postgres and Redshift from Python
    - Load and Read the data from Postgres using Python
  - PySpark
    - Spark and Hadoop Fundamentals
    - Databricks
    - Spark UDFs
    - Spark Dataframe API
    - Create Databricks Account
    - Create Spark Cluster in Databricks
    - M&M Analysis
    - Movielens and Song Analysis
    - San Francisco Fire Department Analysis
    - Data Transformation with PySpark
    - Connect AWS to PySpark
    - ETL Pipeline with AWS S3 and PySpark
  - Scala
    - Introduction to Scala Programming
    - Transform complex data types
    - Extract and Load Process with Spark Scala, S3 and Postgres
- Data Modeling

  - SQL Data Modeling
    - Music Data Modeling with Postgres
    - Healthcare Data Modeling with Postgres
    - Building Data Model in Snowflake
  - NoSQL Data Modeling
    - Music Data Modeling with Cassandra
- Selecting Appropriate Storage Technologies

  - From Business Requirements to Storage Systems
  - Technical Aspects of Data: Volume, Velocity, Variation, Access, and Security
  - Types of Structure: Structured, Semi-Structured, and Unstructured
  - Schema Design Considerations
- Building and Operationalizing Storage Systems

  - Cloud SQL
  - Cloud Spanner
  - Cloud Bigtable
  - Cloud Firestore
  - BigQuery
  - Cloud Memorystore
  - Cloud Storage
  - Unmanaged Databases
- Designing a Data Processing Solution

  - Designing Infrastructure
  - Designing for Distributed Processing
  - Migrating a Data Warehouse
- BigQuery Data Warehouse

  - Introduction to BigQuery
    - Introduction to Google Cloud Storage and BigQuery
    - BigQuery data location
    - Introduction to the BigQuery console
    - Creating a dataset in BigQuery using the console
    - Loading a local CSV file into the BigQuery table
    - Using public data in BigQuery
    - Data types in BigQuery compared to other databases
    - Timestamp data in BigQuery compared to other databases
  - Lab - Building a Data Warehouse in BigQuery
    - Preparing the prerequisites before developing our data warehouse
    - Step 1: Access your Cloud shell
    - Step 2: Check the current setup using the command line
    - Step 3: The gcloud init command
    - Step 4: Download example data from Git
    - Step 5: Upload data to GCS from Git
    - Practicing developing a data warehouse
    - Data warehouse in BigQuery – Requirements for scenario 1
    - Steps and planning for handling scenario 1
    - Data warehouse in BigQuery – Requirements for scenario 2
    - Steps and planning for handling scenario 2
    - Exercise – Scenario 3
- Snowflake Data Warehousing

  - Loading Data into Snowflake
  - Queries in Snowflake
- Orchestration Layer

  - Data Pipelines (ETL/ELT)
  - Apache Airflow
    - Common features
      - UI
      - Operators
      - Variables
      - Plugins
      - Schedules
    - Install Airflow in your PC
    - First DAG/Pipeline - executing Bash commands
    - CSV to JSON ETL Pipeline
  - AWS SNS and SES for Notifications
- Lab - ACLED Data Pipeline
- Batch Data Processing

  - Spark and Hadoop
  - Spark Jobs
  - Big Data Processing
  - Clusters
  - Horizontal and Vertical Scaling
  - Processing data using dbt in Snowflake
  - Processing data with Databricks
- Data Lakes and Lakehouses

  - Delta, Iceberg and Hudi
  - Working with Delta lake in Databricks
- Designing Databases for Reliability, Scalability, and Availability

  - Designing Cloud Bigtable Databases for Scalability and Reliability
  - Designing Cloud Spanner Databases for Scalability and Reliability
  - Designing BigQuery Databases for Data Warehousing
- Designing Data Pipelines

  - Overview of Data Pipelines
  - GCP Pipeline Components
  - Migrating Hadoop and Spark to GCP
- Cloud Composer

  - Introduction to Cloud Composer
  - Understanding the working of Airflow
  - Provisioning Cloud Composer in a GCP project
  - Lab - Building Orchestration for Batch Data Loading Using Cloud Composer
    - Level 1 DAG – Creating dummy workflows
    - Level 2 DAG – Scheduling a pipeline from Cloud SQL to GCS and BigQuery datasets
    - Level 3 DAG – Parameterized variables
    - Level 4 DAG – Guaranteeing task idempotency in Cloud Composer
    - Level 5 DAG – Handling late data using a sensor
- Dataproc

  - Introduction to Dataproc
    - A brief history of the data lake and Hadoop ecosystem
    - A deeper look into Hadoop components
    - How much Hadoop-related knowledge do you need on GCP?
    - Introducing the Spark RDD and the DataFrame concept
    - Introducing the data lake concept
    - Hadoop and Dataproc positioning on GCP
  - Lab - Building a Data Lake Using Dataproc
    - Creating a Dataproc cluster on GCP
    - Using Cloud Storage as an underlying Dataproc file system
    - Exercise: Creating and running jobs on a Dataproc cluster
    - Preparing log data in GCS and HDFS
    - Developing Spark ETL from HDFS to HDFS
    - Developing Spark ETL from GCS to GCS
    - Developing Spark ETL from GCS to BigQuery
    - Understanding the concept of the ephemeral cluster
    - Practicing using a workflow template on Dataproc
    - Building an ephemeral cluster using Dataproc and Cloud Composer
