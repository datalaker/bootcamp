# Curriculum - Data Engineering Training (Azure)

**Estimated Time:** 55 hours

- Getting Started
  - Create Azure Account
  - Install Azure Powershell
  - Install AzCopy
  - Install Azure CLI
  - A quick overview of Azure services for data engineering
- Developer Foundations
  - VS Code
    - Download and Install vscode
    - Understand vscode features
    - Install extensions in vscode
  - Anaconda (Python)
    - Download and Install Anaconda
    - Create virtual environment in anaconda
    - Create jupyter notebook in vscode and connect to venv
  - Github
    - Create github account
    - Install git cli
    - Create git repo and add students as collaborator
    - Connect local workspace to git repo
    - Learn git commands
  - Bash
    - Learn bash commands
- Data Engineering Basics
  - What is Data Engineering?
  - Role of Data Engineering in Organizations
  - Skills required to become a Data Engineer
  - Data engineer versus data scientist
  - What is data lake and data warehouse?
  - What is medallion architecture?
  - What is EL, ETL and ELT?
  - What are the benefits of cloud computing?
  - Understanding what the cloud is
  - The difference between the cloud and non-cloud era
  - The on-demand nature of the cloud
  - OLTP vs OLAP technologies
- Programming
  - SQL
    - Query Types
    - Query Engines
    - SQL Basic
      - SELECT, LIMIT, WHERE
      - Comparison and Logical Operators
      - ORDER BY
    - SQL Intermediate
      - Aggregation Functions and GROUP BY
      - CASE
      - JOINS
    - SQL Advanced
      - Dates
      - Texts
      - Subqueries
      - Window Functions
    - Query Optimizations
  - Python
    - Lists and dictionaries
    - For loops and while loops
    - Functions and Inline functions
    - Pandas Dataframes
    - `requests` library
    - `psycopg2` and `sqlalchemy` library
    - Building Functions in Python
    - Read/Write and Manipulate Data using Pandas
    - Data Format Conversion - CSV to Parquet, JSON to CSV/Parquet
    - Pulling Data from APIs using requests library
    - Connect to Postgres and Redshift from Python
    - Load and Read the data from Postgres using Python
  - PySpark
    - Spark and Hadoop Fundamentals
    - Databricks
    - Spark UDFs
    - Spark Dataframe API
    - Create Databricks Account
    - Create Spark Cluster in Databricks
    - M&M Analysis
    - Movielens and Song Analysis
    - San Francisco Fire Department Analysis
    - Data Transformation with PySpark
    - Connect AWS to PySpark
    - ETL Pipeline with AWS S3 and PySpark
  - Scala
    - Introduction to Scala Programming
    - Transform complex data types
    - Extract and Load Process with Spark Scala, S3 and Postgres
- Data Modeling
  - SQL Data Modeling
    - Music Data Modeling with Postgres
    - Healthcare Data Modeling with Postgres
    - Building Data Model in Snowflake
  - NoSQL Data Modeling
    - Music Data Modeling with Cassandra
- Azure Data Lake
  - Creating and Managing Data in Azure Data Lake
    - Provisioning an Azure storage account using the Azure portal
    - Provisioning an Azure storage account using PowerShell
    - Creating containers and uploading files to Azure Blob storage using PowerShell
    - Managing blobs in Azure Storage using PowerShell
      - Copying blobs between containers
      - Listing blobs in an Azure storage container
      - Modifying a blob access tier
      - Downloading a blob
      - Deleting a blob
    - Configuring blob lifecycle management for blob objects using the Azure portal
  - Securing and Monitoring Data in Azure Data Lake
    - Configuring a firewall for an Azure Data Lake account using the Azure portal
    - Configuring virtual networks for an Azure Data Lake account using the Azure portal
    - Configuring encryption using Azure Key Vault for Azure Data Lake
    - Creating an alert to monitor an Azure storage account
- Azure Data Factory
  - Building Data Ingestion Pipelines Using Azure Data Factory
    - Provisioning Azure Data Factory
    - Copying files to a database from a data lake using a control flow and copy activity
    - Triggering a pipeline in Azure Data Factory
    - Copying data from a SQL Server virtual machine to a data lake using the Copy data wizard
  - Incremental Data Loading using Azure Data Factory
    - Using Watermarking
    - Using File Timestamps
    - Using File partitions and folder structures
  - Develop Batch Processing Solution
    - Data Ingestion using Data Flow
    - Data Transformation using Azure Databricks
    - Data Serving using PolyBase
    - Data Pipeline using Azure Data Factory Pipeline
    - End to end data processing with Azure Batch
- Azure SQL Database
  - Configuring and Securing Azure SQL Database
    - Provisioning and connecting to an Azure SQL database using PowerShell
    - Implementing an Azure SQL Database elastic pool using PowerShell
- Azure Databricks
  - Processing Data Using Azure Databricks
    - Configuring the Azure Databricks environment
    - Integrate Databricks with Azure Key Vault
    - Mounting an Azure Data Lake container in Databricks
    - Processing data using notebooks
    - Scheduling notebooks using job clusters
    - Working with Delta Lake tables
- Azure Synapse Analytics
  - Processing Data Using Azure Synapse Analytics
    * Provisioning an Azure Synapse Analytics workspace
    * Analyzing data using serverless SQL pool
    * Provisioning and configuring Spark pools
    * Processing data using Spark pools and a lake database
    * Querying the data in a lake database from serverless SQL pool
    * Scheduling notebooks to process data incrementally
  - Transforming Data Using Azure Synapse Dataflows
    - Copying data using a Synapse data flow
    - Performing data transformation using activities such as join, sort, and filter
    - Monitoring data flows and pipelines
    - Configuring partitions to optimize data flows
    - Parameterizing mapping data flows
    - Handling schema changes dynamically in data flows using schema drift
  - Implementing the Serving Layer Star Schema
    - Delivering data in a relational star schema
    - Implementing a dimensional hierarchy
    - Delivering data in Parquet files
    - Maintaining metadata
- Snowflake Data Warehousing
  - Loading Data into Snowflake
  - Queries in Snowflake
- Data Lakes and Lakehouses
  - Delta, Iceberg and Hudi
  - Working with Delta lake in Databricks
- Azure Data Lake
  - Creating and Managing Data in Azure Data Lake
    - Provisioning an Azure storage account using the Azure portal
    - Provisioning an Azure storage account using PowerShell
    - Creating containers and uploading files to Azure Blob storage using PowerShell
    - Managing blobs in Azure Storage using PowerShell
      - Copying blobs between containers
      - Listing blobs in an Azure storage container
      - Modifying a blob access tier
      - Downloading a blob
      - Deleting a blob
    - Configuring blob lifecycle management for blob objects using the Azure portal
  - Securing and Monitoring Data in Azure Data Lake
    - Configuring a firewall for an Azure Data Lake account using the Azure portal
    - Configuring virtual networks for an Azure Data Lake account using the Azure portal
    - Configuring encryption using Azure Key Vault for Azure Data Lake
    - Creating an alert to monitor an Azure storage account
- Azure Data Factory
  - Building Data Ingestion Pipelines Using Azure Data Factory
    - Provisioning Azure Data Factory
    - Copying files to a database from a data lake using a control flow and copy activity
    - Triggering a pipeline in Azure Data Factory
    - Copying data from a SQL Server virtual machine to a data lake using the Copy data wizard
  - Incremental Data Loading using Azure Data Factory
    - Using Watermarking
    - Using File Timestamps
    - Using File partitions and folder structures
  - Develop Batch Processing Solution
    - Data Ingestion using Data Flow
    - Data Transformation using Azure Databricks
    - Data Serving using PolyBase
    - Data Pipeline using Azure Data Factory Pipeline
    - End to end data processing with Azure Batch
- Azure SQL Database
  - Configuring and Securing Azure SQL Database
    - Provisioning and connecting to an Azure SQL database using PowerShell
    - Implementing an Azure SQL Database elastic pool using PowerShell
- Batch Data Processing- Spark and Hadoop
  - Spark Jobs
  - Big Data Processing
  - Clusters
  - Horizontal and Vertical Scaling
  - Processing data using dbt in Snowflake
  - Processing data with Databricks
- Azure Databricks
  - Processing Data Using Azure Databricks
    - Configuring the Azure Databricks environment
    - Integrate Databricks with Azure Key Vault
    - Mounting an Azure Data Lake container in Databricks
    - Processing data using notebooks
    - Scheduling notebooks using job clusters
    - Working with Delta Lake tables
- Azure Synapse Analytics
  - Processing Data Using Azure Synapse Analytics
    * Provisioning an Azure Synapse Analytics workspace
    * Analyzing data using serverless SQL pool
    * Provisioning and configuring Spark pools
    * Processing data using Spark pools and a lake database
    * Querying the data in a lake database from serverless SQL pool
    * Scheduling notebooks to process data incrementally
  - Transforming Data Using Azure Synapse Dataflows
    - Copying data using a Synapse data flow
    - Performing data transformation using activities such as join, sort, and filter
    - Monitoring data flows and pipelines
    - Configuring partitions to optimize data flows
    - Parameterizing mapping data flows
    - Handling schema changes dynamically in data flows using schema drift
  - Implementing the Serving Layer Star Schema
    - Delivering data in a relational star schema
    - Implementing a dimensional hierarchy
    - Delivering data in Parquet files
    - Maintaining metadata
- Orchestration Layer
  - Data Pipelines (ETL/ELT)
  - Apache Airflow
    - Common features
      - UI
      - Operators
      - Variables
      - Plugins
      - Schedules
    - Install Airflow in your PC
    - First DAG/Pipeline - executing Bash commands
    - CSV to JSON ETL Pipeline
  - AWS SNS and SES for Notifications
- Capstone Project - ACLED Data Pipeline
