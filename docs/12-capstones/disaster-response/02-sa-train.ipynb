{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "# nltk.download(['punkt', 'stopwords', 'wordnet'])\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Loading data from database and split it into messages (X) and categories (Y)\n",
    "    INPUT\n",
    "        database_filepath is the file path of the database to process\n",
    "    OUTPUT\n",
    "        X is a numpy array of all messages only (strings)\n",
    "        Y is a numpy array of all other categories relates to these messages (except 'original' messages -Fr, UK, US ...-\n",
    "          in column #2, and except 'genre' in column #3\n",
    "        category_names is names for Y categories\n",
    "    '''\n",
    "    \n",
    "    engine = create_engine('sqlite:///{}'.format(database_filepath))\n",
    "    df = pd.read_sql_table(table_name='DisasterResponse', con=engine)  # load the Disaster Response table cleaned\n",
    "    X = df[\"message\"].values  # get column of messages\n",
    "    Y = np.asarray(df[df.columns[4:]].values) # get categories columns\n",
    "    Y = Y.astype(int)  # convert all Y values to int  # help from https://knowledge.udacity.com/questions/59448\n",
    "    category_names = df.columns[4:]\n",
    "    return X, Y, category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        text tokenization\n",
    "    INPUT\n",
    "        text is a list of messages (strings)\n",
    "    OUTPUT\n",
    "        clean_tokens is XXXX as result of the tokenization, which includes\n",
    "         - normalizing the text, i.e. keep only spaces, letters and figures\n",
    "         - Tokenization itself\n",
    "         - Stop words (removal of all not english words; i.e. the X columns is already a conversion of\n",
    "           messages from various languages (Fr, En ...) to english, thus no need to consider another language\n",
    "         - Lemmatization\n",
    "           including a change for Lower casing + remove leading and trailing spaces\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Normalize, remove all that is not letters and figures\n",
    "    pure_text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    \n",
    "    # split sentences into word\n",
    "    tokens = word_tokenize(pure_text)\n",
    "    \n",
    "    # remove stop words from the sentences\n",
    "    stop_word = [word for word in tokens if word not in STOPWORDS]\n",
    "    \n",
    "    # normalize words keeping the source word according to the context\n",
    "    #  and get only lower characters and remove leading and trailing characters\n",
    "    clean_tokens = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in stop_word:\n",
    "        lemmatized = lemmatizer.lemmatize(word).lower().strip()\n",
    "        clean_tokens.append(lemmatized)    \n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Build a model through pipeline with RandomForestClassifier and GridSearchCV\n",
    "    INPUT\n",
    "        nil\n",
    "    OUTPUT\n",
    "        pipeline is the Pipeline defined for classification of data\n",
    "    '''\n",
    "\n",
    "    # Build the pipeline\n",
    "    # - using MultiOutputClassifer link and refering to https://knowledge.udacity.com/questions/158218\n",
    "    \n",
    "    parameters = {\n",
    "        'clf__estimator__n_estimators': [20]\n",
    "    }\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer()),        \n",
    "            ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])\n",
    "\n",
    "    model = GridSearchCV(pipeline, param_grid=parameters, n_jobs= -1)\n",
    "    \n",
    "    # MultiOutputClassifier (fit one classifier per target):\n",
    "    #  n_estimators = 10\n",
    "    #  n = -1  means using all available processes / threads\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_value(values, index):\n",
    "    try:\n",
    "        value = values[index]\n",
    "    except:\n",
    "        value = np.nan\n",
    "    return value\n",
    "\n",
    "\n",
    "def store_float_value(values, index):\n",
    "    try:\n",
    "        value = float(values[index])\n",
    "    except:\n",
    "        value = np.nan\n",
    "    return value\n",
    "\n",
    "def get_scores_dict(report_txt, titles):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Transform a text of report into a global dictionary\n",
    "    INPUT\n",
    "        report_txt is report under text format\n",
    "        titles is the list of names for every results table\n",
    "    OUTPUT\n",
    "        dico is a dictionary of titles with related result per values 0, 1, total and related scores\n",
    "        {title_1: {'0': {'precision': x, 'recall: x', 'f1-score': x, 'support': x}, '1': {...}, 'total': {...} }, title_2: ...}\n",
    "    \n",
    "    A line of the report contains:\n",
    "       report A      precision  recall  f1-score  support\n",
    "               0        x          x        x        x\n",
    "               1        x          x        x        x\n",
    "               total    x          x        x        x\n",
    "    Sometimes, 0 or 1 my be not present\n",
    "    '''\n",
    "    \n",
    "    lst = []  # initiate a list of list i.e. a list (lst) containing all words of every table contained in a related list (mots)\n",
    "    \n",
    "    # Read line by line of the report; one table result is contained on a line\n",
    "    for tab in report_txt:\n",
    "        # create a table with all elements, usung space as separator\n",
    "        li = tab.split(' ')\n",
    "        mots = []  # initiate a list of word and figure that will be extract from the line\n",
    "        # Read the list of word read on the line\n",
    "        for i, elt in enumerate(li):\n",
    "            if elt != '':  # when word contains characters\n",
    "                if i == 0:            # when this is the first word\n",
    "                    mots.append(elt)  # - store this word in the list mots\n",
    "                elif '\\n' in elt:     # when the word contains a line break, indicating the end a of sub-line,\n",
    "                                      #  it's time to change of sub-list\n",
    "                    elt_f = elt.split('\\n')[0]  # - get the first part of the word \n",
    "                    mots.append(elt_f)          # - and store it in the last list\n",
    "                    lst.append(mots)            # - and store this list mtos into the list lst, before initiating a new list mots\n",
    "                    elt_s = elt.split('\\n')[1].replace('\\n','')  # - get the second part of the word\n",
    "                    if elt_s != '':             # - when the second of the word contains characters\n",
    "                        mots = [elt_s]          # - store this new word into the new list mots\n",
    "                    else:                       # - otherwise\n",
    "                        mots = []               # - initiate a empty list mots\n",
    "                else:\n",
    "                    mots.append(elt)   # otherwise (not the first word, not the last word of a sub-line), then store the word\n",
    "                                       #  into the list mots\n",
    "        lst.append(mots)  # Finally, store every mots (containing words of every sub-line)\n",
    "    \n",
    "    # Now that we have list and sub-list of words of the report\n",
    "    #  we gonna get values per label\n",
    "    dico = dict()\n",
    "    start_cycle = -1\n",
    "    di = dict()\n",
    "    k = -1\n",
    "    # Read every sub-list of words of the list\n",
    "    for i, t in enumerate(lst):\n",
    "        label = ''        \n",
    "        values = t[-4:]  # Get last four values of every line\n",
    "        if len(values) > 0:  # if there are values\n",
    "            if values[0] == 'precision': # detect the word 'precision' as starter\n",
    "                k += 1  # iterate k as index to join title and related data\n",
    "                start_cycle = i  # store i\n",
    "                if len(di) > 0:          # if we fulfill di (dict) during previous lines\n",
    "                    dico[titles[k]] = di #  then we store it in dico \n",
    "                di = dict()              # any way, at thi stage of the starter, we initiate dict di\n",
    "                                         # di is a dict to store every result values \n",
    "            else:  # when we are in another line that the starter\n",
    "                if i > start_cycle: \n",
    "                    label = label.join(t[:(len(t)-4)]).replace('/', '') # we get the label of this line\n",
    "                    # and then we get score values for every category of result, storing that in a dictionary d\n",
    "                    d = dict()  # d for storing scores of a label\n",
    "                    d['precision'] = store_value(values, 0)\n",
    "                    d['recall'] = store_value(values, 1)\n",
    "                    d['f1-score'] = store_value(values, 2)\n",
    "                    d['support'] = store_value(values, 3)\n",
    "                    di[label] = d  # when we get all scores, we store it into dictionary di, di = scores for all labels of a table\n",
    "\n",
    "    return dico\n",
    "\n",
    "\n",
    "def get_feature_values(dico, feature):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Reading a dictionary of result (from get_scores), we focus on a feature of results to get these values\n",
    "    INPUT\n",
    "        dico is a dictionary of results, multi tables, multi-lines with result features for every line\n",
    "    OUTPUT\n",
    "        dico_feat is a dictionary of the result-score for the selected result-feature\n",
    "        {title_1: [0_score, 1_score, total_score], ...}, nan is no value\n",
    "    '''\n",
    "\n",
    "    dico_feat = dict()\n",
    "    # Read every table of result\n",
    "    for i, j in dico.items():\n",
    "        k0, k1, k2 = np.nan, np.nan, np.nan  # initiate scores for the selected result-feature\n",
    "        \n",
    "        # Read every label and relates scores of a table\n",
    "        for k, m in j.items():\n",
    "            #  get score for every label\n",
    "            if k == '0':\n",
    "                k0 = store_float_value(m, feature)\n",
    "            elif k == '1':\n",
    "                k1 = store_float_value(m, feature)\n",
    "            else:\n",
    "                k2 = store_float_value(m, feature)\n",
    "        # store scores into a dictionary\n",
    "        dico_feat[i] = [k0, k1, k2]\n",
    "    return dico_feat\n",
    "\n",
    "\n",
    "def get_list_value(lst, pos):\n",
    "    try:\n",
    "        r = lst[pos]\n",
    "    except:\n",
    "        r = np.nan\n",
    "    return r\n",
    "\n",
    "\n",
    "def dict_to_df(dico):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        transform a dictionary of scores (from get_feature_values) into a dataframe\n",
    "    INPUT\n",
    "        dico is a dictionary of the score per title\n",
    "              under format {title_1: [0_score, 1_score, total_score], ...}, nan is no value\n",
    "    OUTPUT\n",
    "        df is a dataframe with these scores, under format\n",
    "        columns = items, '0', '1', 'total' ; rows = titles ; values = score-values\n",
    "    '''\n",
    "\n",
    "    col = ['items', '0', '1', 'total']\n",
    "    items, zero, un, total = [], [], [], []\n",
    "    for i, j in dico.items():\n",
    "        items.append(i)\n",
    "        # It's possible not to have a value for 0, 1 or total, so we have to manage it by proposing an alternate solution\n",
    "        zero.append(get_list_value(j, 0))\n",
    "        un.append(get_list_value(j, 1))\n",
    "        total.append(get_list_value(j, 2))\n",
    "\n",
    "    d = {'items': items, '0': zero, '1': un, 'total':total}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df[['0', '1', 'total']].astype(float)  # change type values to float\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_f1_score(report, titles, result_feature='f1-score'):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Transform a report of scoresinto a dataframe of the selected result-feature (here: f1-score)\n",
    "    INPUT\n",
    "        report is the report of scores (string)\n",
    "        titles is the list of names of all parameters\n",
    "        result_feature is the name of the result-feature that we want to get ; f1-score per default\n",
    "    OUTPUT\n",
    "        df_feature is a dataframe with result-scores of the selected result-feature for all labels and parameters\n",
    "    '''\n",
    "    dico = get_scores_dict(report, titles)  # Transform a text of report into a global dictionary\n",
    "    dico_feature = get_feature_values(dico, result_feature)  # Reading a dictionary of result (from get_scores),\n",
    "                                                             #  we focus on a feature of results to get these values\n",
    "    df_feature = dict_to_df(dico_feature)  # transform a dictionary of scores (from get_feature_values) into a dataframe\n",
    "    \n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        evaluate the model on test data\n",
    "    INPUT\n",
    "        model is the trained model to evaluate\n",
    "        X_test is the test input data set \n",
    "        Y_test is the test output data set\n",
    "        category_names is the list of names of all categories\n",
    "    OUTPUT\n",
    "        nil\n",
    "    '''    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    # F1 score with MultiLabelBinarizer and f1_score\n",
    "    mlb = MultiLabelBinarizer().fit(Y_test)\n",
    "    f1_score_mlb = f1_score(mlb.transform(Y_test),\n",
    "         mlb.transform(Y_pred),\n",
    "         average='macro')\n",
    "    print(' -> f1 score mlb:', f1_score_mlb)\n",
    "        \n",
    "    # F1 score with a classification report\n",
    "    # Then using the classificatio_report mentioned for the exercice:\n",
    "    # refering to https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "    scores_class_report = []\n",
    "    # Run over all categories to get their own individual score\n",
    "    for i in range(len(category_names)):\n",
    "        # get a report, i.e. a text, with classification results, including scores for 0, 1 and total\n",
    "        report = classification_report(np.array(Y_test)[i], np.array(Y_pred)[i], zero_division=1)\n",
    "        # Warning may appear: it may be avoid with zero_division=0 & output_dict=True\n",
    "        #  but they are available only with New in version 0.20. while we work on version 0.19!\n",
    "        #  Refer to the import of Warnings and related processing\n",
    "        #                     report_score = fscore(y_test, y_pred, average='weighted')  # TEST !\n",
    "        #                     print('report score:\\n', report_score)  # TEST !\n",
    "        scores_class_report.append(report)  # We store all reports for a further processing\n",
    "    # Now we have reports of classification for every categories\n",
    "    #  we gonna get only score values, for 0, 1 and total for every category and resume that in a dataframe \n",
    "    df_scores_class_report = get_f1_score(scores_class_report, category_names)\n",
    "    # then I can compute one score for all categories by computing the 'total' score over all categories.\n",
    "    f1_score_cr = df_scores_class_report[\"total\"].mean(axis=0)\n",
    "    print(' -> f1 score cr:', f1_score_cr)  \n",
    "    \n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    pickle.dump(model, open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath of the disaster messages database\n",
    "database_filepath = \"./data/DisasterResponse.db\"\n",
    "\n",
    "# filepath of the pickle file to save the model\n",
    "model_filepath = \"model/classifier.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    DATABASE: ./data/DisasterResponse.db\n",
      "Building model...\n",
      "Training model...\n",
      "Evaluating model...\n",
      " -> f1 score mlb: 0.7722753223426776\n",
      " -> f1 score cr: 0.9522857142857144\n",
      "Saving model...\n",
      "    MODEL: model/classifier.pkl\n",
      "Trained model saved!\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "X, Y, category_names = load_data(database_filepath)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "print('Building model...')\n",
    "model = build_model()\n",
    "\n",
    "print('Training model...')\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "print('Evaluating model...')\n",
    "evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "save_model(model, model_filepath)\n",
    "\n",
    "print('Trained model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sparsh A.\n",
      "\n",
      "Last updated: 2022-12-18 19:10:21\n",
      "\n",
      "Compiler    : Clang 10.0.0 \n",
      "OS          : Darwin\n",
      "Release     : 21.6.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n",
      "re     : 2.2.1\n",
      "sklearn: 1.0.2\n",
      "sys    : 3.9.7 (default, Sep 16 2021, 08:50:36) \n",
      "[Clang 10.0.0 ]\n",
      "pandas : 1.3.5\n",
      "nltk   : 3.6.5\n",
      "numpy  : 1.21.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run ~/postrun.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
