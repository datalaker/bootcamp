{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import nltk\n",
    "# nltk.download(['stopwords', 'wordnet'])\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        load content of the file defined by filepath and convert it to dataframe\n",
    "    INPUT\n",
    "        filepath is the filepath of the file to load\n",
    "    OUTPUT\n",
    "        df is the dataframe of the file's content\n",
    "    '''\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df1, df2):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Merge both dataframes messages and categories by their 'id' column\n",
    "    INPUT\n",
    "        df1 is the first dataframe that will be completed with the second\n",
    "        df2 is the second dataframe to complete the first one\n",
    "    OUTPUT\n",
    "        df1 is the merge of both dataframes, sorted by the id values\n",
    "    '''\n",
    "    df = pd.merge(df1, df2, how='outer', on=['id'])\n",
    "    df.sort_values(['id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_data(df):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        add dummy data to df removing the source of dummy\n",
    "    INPUT\n",
    "        df is the initial dataframe to work on\n",
    "    OUTPUT\n",
    "        df_dummy is the dataframe as copy of df but with dummy columns, removing the source of dummy\n",
    "    '''       \n",
    "    # create a dataframe of the 36 individual category columns\n",
    "    categories = df.categories.str.split(pat=\";\", expand=True)\n",
    "    \n",
    "    # select the first row of the categories dataframe\n",
    "    row = categories.head(1)  # categories.values[:1][0]\n",
    "\n",
    "    # use this row to extract a list of new column names for categories.\n",
    "    # one way is to apply a lambda function that takes everything \n",
    "    # up to the second to last character of each string with slicing\n",
    "    category_colnames = row.apply(lambda name: name.str.split('-')[0][0])\n",
    "    \n",
    "    # rename the columns of `categories`\n",
    "    categories.columns = category_colnames\n",
    "    \n",
    "    # Convert category values to just numbers 0 or 1.\n",
    "    for column in categories:\n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].apply(lambda name: name[-1:])        \n",
    "        # convert column from string to numeric\n",
    "        # categories[column] = categories[column].astype(int)  # is replaced by\n",
    "        categories[column] = pd.get_dummies(categories[column])\n",
    "    \n",
    "    # Replace categories column in df with new category columns\n",
    "    # by dropping the original categories column from `df`\n",
    "    df.drop(columns='categories', inplace=True)\n",
    "    \n",
    "    # concatenate the original dataframe with the new `categories` dataframe\n",
    "    df_dummy = pd.concat([df, categories], axis=1, join='outer')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    # - check number of duplicates\n",
    "    duplicates = df_dummy.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df_dummy.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "    return df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        clean dataframe from empty data\n",
    "    INPUT\n",
    "        df is the dataframe to clean\n",
    "    OUTPUT\n",
    "        df_clean is the cleaned dataframe\n",
    "    '''   \n",
    "    # Clean the dataframe\n",
    "    df_clean = df.dropna(how='any')  # remove rows with an empty field \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, database_filepath):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        save a dataframe into database file\n",
    "    INPUT\n",
    "        df is the dataframe to save\n",
    "        database_filepath if the filepath of the database, i.e. target of the save\n",
    "    OUTPUT\n",
    "        nil\n",
    "    '''\n",
    "    table_name = 'DisasterResponse'\n",
    "    # conn = sqlite3.connect(database_filepath)\n",
    "    # df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    engine = create_engine('sqlite:///{}'.format(database_filepath))\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths of the messages\n",
    "messages_filepath = \"./data/disaster_messages.csv\"\n",
    "\n",
    "# filepaths of the categories\n",
    "categories_filepath = \"./data/disaster_categories.csv\"\n",
    "\n",
    "# filepath of the database to save the cleaned data\n",
    "database_filepath = \"./data/DisasterResponse.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data...\\n    MESSAGES: {}\\n    CATEGORIES: {}'\n",
    "        .format(messages_filepath, categories_filepath))\n",
    "df_msg = load_data(messages_filepath)\n",
    "df_cat = load_data(categories_filepath)\n",
    "\n",
    "print('Data Merge...')\n",
    "df = merge_data(df_msg, df_cat)\n",
    "\n",
    "print('Data Merge...')\n",
    "df = dummy_data(df)\n",
    "\n",
    "print('Cleaning data...')\n",
    "df = clean_data(df)\n",
    "\n",
    "print('Saving data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "save_data(df, database_filepath)\n",
    "\n",
    "print('Cleaned data saved to database!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
