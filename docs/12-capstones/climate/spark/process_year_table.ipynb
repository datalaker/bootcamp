{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, IntegerType, DateType, StructField, StringType, TimestampType\n",
    "import logging, traceback\n",
    "import requests\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Used when submitting job to spark master with parameters\n",
    "start_year = int(sys.argv[1])\n",
    "end_year = int(sys.argv[2])\n",
    "\"\"\"\n",
    "start_year = 2022\n",
    "end_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4a07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ingestion to local (used when developing)\n",
    "URL_PREFIX = 'https://noaa-ghcn-pds.s3.amazonaws.com'\n",
    "TEMP_STORAGE_PATH = '/home/marcos/ghcn-d/spark/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c38f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For local spark master\n",
    "conf = pyspark.SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"gcs-connector-hadoop3-latest.jar, spark-bigquery-with-dependencies_2.12-0.24.2.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/home/marcos/.google/credentials/google_credentials.json\") \\\n",
    "    .set(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .set(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccddb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Cloud Storage bucket for temporary BigQuery export data used\n",
    "# by the connector.\n",
    "bucket = \"ghcnd_raw\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used only when developing with local spark master\n",
    "def download_file(url, local_file_path):\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(year, mode, df_stations, df_countries):\n",
    "\n",
    "  \"\"\"\n",
    "  # For developing process read directly from origin\n",
    "  csv_file_name = f'/{year}.csv'\n",
    "  dataset_url = URL_PREFIX + '/csv' + csv_file_name\n",
    "  csv_file_path = TEMP_STORAGE_PATH + csv_file_name\n",
    "\n",
    "  download_file(dataset_url, csv_file_path)    \n",
    "\n",
    "  schema = StructType([\n",
    "      StructField(\"id\", StringType(), True),\n",
    "      StructField(\"date\", IntegerType(), True),\n",
    "      StructField(\"element\", StringType(), True),   \n",
    "      StructField(\"value\", IntegerType(), True),   \n",
    "      StructField(\"m_flag\", StringType(), True),   \n",
    "      StructField(\"q_flag\", StringType(), True),   \n",
    "      StructField(\"s_flag\", StringType(), True),\n",
    "      StructField(\"obs_time\",IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "  df = spark.read \\\n",
    "    .options(header=False)\n",
    "    .schema(schema)\n",
    "    .csv(csv_file_path)\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # Option, read from BQ\n",
    "  df = spark.read.format('bigquery') \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','ghcnd') \\\n",
    "    .option('table',f'{year}').load()\n",
    "\n",
    "\n",
    "  # Option, read from GCS\n",
    "  #df = spark.read.parquet(f'gs://ghcnd_raw/{year}.parquet')\n",
    "\n",
    "  print(f'processing year {year}...')\n",
    "  # Only used when reading from csv in order to convert to date. \n",
    "  # If reading from BQ, this is already done\n",
    "  # df = df.withColumn(\"date\", F.to_date(df.date.cast(\"string\"), \"yyyyMMdd\"))\n",
    "\n",
    "  df = df \\\n",
    "    .drop(\"q_flag\") \\\n",
    "    .withColumn(\"tmax\", \n",
    "          F.when(df.element == \"TMAX\", \n",
    "              F.when(df.value > 700, None).otherwise(\n",
    "                  F.when(df.value < -700, None). otherwise(\n",
    "                      df.value.cast(\"double\")/10)\n",
    "                  )\n",
    "          ).otherwise(\"None\")\n",
    "      ) \\\n",
    "      .withColumn(\"tmin\", \n",
    "          F.when(df.element == \"TMIN\", \n",
    "              F.when(df.value > 700, None).otherwise(\n",
    "                  F.when(df.value < -700, None). otherwise(\n",
    "                      df.value.cast(\"double\")/10)\n",
    "                  )\n",
    "          ).otherwise(\"None\")\n",
    "      ) \\\n",
    "      .withColumn(\"prcp\", F.when(df.element == \"PRCP\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "      .withColumn(\"snow\", F.when(df.element == \"SNOW\", df.value.cast(\"double\")).otherwise(None)) \\\n",
    "      .withColumn(\"snwd\", F.when(df.element == \"SNWD\", df.value.cast(\"double\")).otherwise(None))\n",
    "\n",
    "  df_daily = df \\\n",
    "      .groupBy(\"id\", \"date\").agg( \n",
    "          F.avg(\"tmax\"),\n",
    "          F.avg(\"tmin\"),\n",
    "          F.avg(\"prcp\"),\n",
    "          F.avg(\"snow\"),\n",
    "          F.avg(\"snwd\"),\n",
    "          F.first(\"m_flag\"),\n",
    "          F.first(\"s_flag\")\n",
    "      ) \\\n",
    "      .join(df_stations, df.id == df_stations.station_id, \"inner\") \\\n",
    "      .join(df_countries, df_stations.country_code == df_countries.code, \"inner\") \\\n",
    "      .drop ('station_id', 'code') \\\n",
    "      .toDF('id','date','tmax','tmin','prcp','snow','snwd','m_flag','s_flag','latitude','longitude','elevation','station_name','country_code','country_name') \n",
    "\n",
    "  # Note: toDF after joins, otherwise join will raise error\n",
    "  # Note: toDF since BQ does not allow field names with () and average generates these kind of names avg(tmax)\n",
    "\n",
    "  df_yearly =  df \\\n",
    "    .withColumn(\"date\", F.trunc(\"date\", \"year\")) \\\n",
    "    .groupBy(\"id\", \"date\").agg( \n",
    "      F.avg(\"tmax\"),\n",
    "      F.avg(\"tmin\"),\n",
    "      F.avg(\"prcp\"),\n",
    "      F.avg(\"snow\"),\n",
    "      F.avg(\"snwd\"),\n",
    "      F.first(\"m_flag\"),\n",
    "      F.first(\"s_flag\")\n",
    "    ) \\\n",
    "    .join(df_stations, df.id == df_stations.station_id, \"inner\") \\\n",
    "    .join(df_countries, df_stations.country_code == df_countries.code, \"inner\") \\\n",
    "    .drop ('station_id', 'code') \\\n",
    "    .toDF('id','date','tmax','tmin','prcp','snow','snwd','m_flag','s_flag','latitude','longitude','elevation','station_name','country_code','country_name') \\\n",
    "\n",
    "  # For some reason, partition by date does not work after F.year(\"date\"). This has to be fixed\n",
    "  # Also, partition is needed for clustering\n",
    "  df_yearly.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(mode) \\\n",
    "    .option(\"clusteredFields\", \"date, country_code\") \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','production') \\\n",
    "    .option('table','fact_observations_spark_yearly') \\\n",
    "    .save()\n",
    "    \n",
    "  \n",
    "  df_daily.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(mode) \\\n",
    "    .option(\"partitionField\", \"date\") \\\n",
    "    .option(\"partitionType\", \"YEAR\") \\\n",
    "    .option(\"clusteredFields\", \"country_code\") \\\n",
    "    .option('project','ghcn-d') \\\n",
    "    .option('dataset','production') \\\n",
    "    .option('table','fact_observations_spark') \\\n",
    "    .save()\n",
    "  \n",
    "\n",
    "\n",
    "  print(f'process {year} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275b2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use if needed to read from BigQuery instead of GCS\n",
    "df_stations = spark.read.format('bigquery') \\\n",
    "  .option('project','ghcn-d') \\\n",
    "  .option('dataset','ghcnd') \\\n",
    "  .option('table', 'stations').load() \\\n",
    "  .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id') \\\n",
    "  .withColumnRenamed('name', 'station_name') \\\n",
    "  .withColumnRenamed('id', 'station_id') \\\n",
    "  .withColumn('country_code', F.substring('station_id', 0, 2))\n",
    "\n",
    "df_countries = spark.read.format('bigquery') \\\n",
    "  .option('project','ghcn-d') \\\n",
    "  .option('dataset','ghcnd') \\\n",
    "  .option('table', 'countries').load() \\\n",
    "  .withColumnRenamed('name', 'country_name')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8305e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_stations = spark.read.parquet('gs://ghcnd_raw/ghcnd-stations.parquet') \\\n",
    "  .drop('state', 'gsn_flag', 'hcn_crn_flag', 'wmo_id') \\\n",
    "  .withColumnRenamed('name', 'station_name') \\\n",
    "  .withColumnRenamed('id', 'station_id') \\\n",
    "  .withColumn('country_code', F.substring('station_id', 0, 2))\n",
    "\n",
    "df_countries = spark.read.parquet('gs://ghcnd_raw/ghcnd-countries.parquet') \\\n",
    "  .withColumnRenamed('name', 'country_name')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c589e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year+1):\n",
    "  if year == start_year:\n",
    "    process_year(year, 'overwrite', df_stations, df_countries)\n",
    "  else:\n",
    "    process_year(year, 'append', df_stations, df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
