{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d007811b-0cb6-4223-a7e8-650f85de28a0",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83dfd21-74e5-4001-b7cb-7dc1f4683bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 freeze | grep -E 'boto3|s3fs|pandas==' | grep -v 'geo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac210e58-3c50-48ed-8b5b-9e6f1faabefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c1e86-f58c-495b-b0ef-750bccb5541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "import boto3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd167a5-a2bc-4657-8f7f-4383cb4c8958",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f15c05-6c18-4f1e-8caa-0b71f33e1276",
   "metadata": {},
   "source": [
    "This notebook will combine all the streamed tweets per hour into a single `parquet` file that can be used in data processing to prepare the data for use in machine learning model development. Such a file will be created for every hour of every day that tweets were streamed.\n",
    "\n",
    "Before exporting to a `.parquet` file, the combined hourly streamed tweets, including all collected and extracted metadata will be stored in a `pandas` DataFrame. Since tweets will be combined at the hourly level, the combined hourly `DataFrame`s **for this choices of subject** are small enough to fit in to single-node memory and so memory-bound tools such as `pandas` can be used to hold this hourly data before it is exported to the `.parquet` format.\n",
    "\n",
    "**Pre-Requisites**\n",
    "1. In order to access data stored on Amazon S3, this data preparation notebook must be provided with the following environment variables\n",
    "   - `AWS_S3_BUCKET_NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bb70a-2362-40f9-a7ee-730abb764008",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b93f73-b477-4d45-81d0-5f1f5bf39026",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# raw data on S3\n",
    "path_to_folder = \"/datasets/twitter/kinesis-demo/\"\n",
    "years_wanted = [2021] + [2022]\n",
    "\n",
    "# processed data\n",
    "processed_data_dir = \"data/processed\"  # (S3) \"datasets/twitter/kinesis-demo/processed\" or (locally) \"data/processed\"\n",
    "proc_zip_fname = \"combined_data.zip\"\n",
    "\n",
    "# List of headers for all streamed twitter attributes\n",
    "headers = [\n",
    "    \"id\",\n",
    "    \"geo\",\n",
    "    \"coordinates\",\n",
    "    \"place\",\n",
    "    \"contributors\",\n",
    "    \"is_quote_status\",\n",
    "    \"quote_count\",\n",
    "    \"reply_count\",\n",
    "    \"retweet_count\",\n",
    "    \"favorite_count\",\n",
    "    \"favorited\",\n",
    "    \"retweeted\",\n",
    "    \"created_at\",\n",
    "    \"source\",\n",
    "    \"in_reply_to_user_id\",\n",
    "    \"in_reply_to_screen_name\",\n",
    "    \"source_text\",\n",
    "    \"place_id\",\n",
    "    \"place_url\",\n",
    "    \"place_place_type\",\n",
    "    \"place_name\",\n",
    "    \"place_full_name\",\n",
    "    \"place_country_code\",\n",
    "    \"place_country\",\n",
    "    \"place_bounding_box_type\",\n",
    "    \"place_bounding_box_coordinates\",\n",
    "    \"place_attributes\",\n",
    "    \"coords_type\",\n",
    "    \"coords_lon\",\n",
    "    \"coords_lat\",\n",
    "    \"geo_type\",\n",
    "    \"geo_lon\",\n",
    "    \"geo_lat\",\n",
    "    \"user_name\",\n",
    "    \"user_screen_name\",\n",
    "    \"user_followers\",\n",
    "    \"user_friends\",\n",
    "    \"user_listed\",\n",
    "    \"user_favourites\",\n",
    "    \"user_statuses\",\n",
    "    \"user_protected\",\n",
    "    \"user_verified\",\n",
    "    \"user_contributors_enabled\",\n",
    "    \"user_joined\",\n",
    "    \"user_location\",\n",
    "    \"retweeted_tweet\",\n",
    "    \"tweet_text_urls\",\n",
    "    \"tweet_text_hashtags\",\n",
    "    \"tweet_text_usernames\",\n",
    "    \"num_urls_in_tweet_text\",\n",
    "    \"num_users_in_tweet_text\",\n",
    "    \"num_hashtags_in_tweet_text\",\n",
    "    \"text\",\n",
    "]\n",
    "\n",
    "# # keep tweets with text containing terms\n",
    "# case_sensitive_tweet_search_terms = [\"NASA\", \"ESA\", \"CSA\", \"Kepler\"]\n",
    "# tweet_search_terms = [\n",
    "#     \"aeronautics\",\n",
    "#     # \"webb\",  # picks up random astronomers unrelated to the mission\n",
    "#     \"goddard\",\n",
    "#     \"propulsion\",\n",
    "#     # \"telescope\",  # picks up topics not related to the mission\n",
    "#     \"exoplanet\",\n",
    "#     # 'launch',\n",
    "#     # 'astronomy',  # picks up random astronomers unrelated to the mission\n",
    "#     # 'astrophysics',  # picks up random astrophysicists unrelated to the mission\n",
    "#     # \"laboratory\",  # picks up random astronomers unrelated to the mission\n",
    "#     \"jwst\",\n",
    "#     # \"exploration\",  # picks up topics not related to space exploration\n",
    "#     # \" mission\",  # picks up topics not related to space exploration\n",
    "#     \"spacecraft\",\n",
    "# ]\n",
    "# joined_tweet_search_terms = [\n",
    "#     \"james webb\",\n",
    "#     \"space telescope\",\n",
    "#     \"webb space\",\n",
    "#     \"webb telescope\",\n",
    "#     \"jet propulsionlab\",\n",
    "#     \"canadian space agency\",\n",
    "#     \"european space agency\",\n",
    "#     \"national aeronautics\",\n",
    "#     \"shuttle launch\",\n",
    "#     \"space shuttle\",\n",
    "#     \"goddard space flightcenter\",\n",
    "#     \"johnson space center\",\n",
    "#     \"ames research center\",\n",
    "#     \"marshall space flightcenter\",\n",
    "#     \"glenn research center\",\n",
    "#     \"ball aerospace\",\n",
    "#     \"harris corporation\",\n",
    "#     \"space telescope science institute\",\n",
    "#     \"billochs\",\n",
    "#     \"johnmather\",\n",
    "#     \"northrop grumman\",\n",
    "#     \"lockheed martin\",\n",
    "#     \"stephen hawking\",\n",
    "#     \"dark matter\",\n",
    "#     \"dark energy\",\n",
    "#     \"hubble space\",\n",
    "#     \"hubble telescope\",\n",
    "# ]\n",
    "# # remove tweets with text containing terms\n",
    "# crypto_terms = [\n",
    "#     \"crypto\",\n",
    "#     # \"token\",\n",
    "#     \"koistarter\",\n",
    "#     \"daostarter\",\n",
    "#     \"decentralized\",\n",
    "#     # \"services\",\n",
    "#     # \"pancakeswap\",\n",
    "#     # \"eraxnft\",\n",
    "#     # \"browsing\",\n",
    "#     # \"kommunitas\",\n",
    "#     # \"hosting\",\n",
    "#     # \"internet\",\n",
    "#     # \"exipofficial\",\n",
    "#     # \"servers\",\n",
    "#     # \"wallet\",\n",
    "#     # \"liquidity\",\n",
    "#     # \"rewards\",\n",
    "#     # \"floki\",\n",
    "#     # \"10000000000000linkstelegram\",\n",
    "#     \"dogecoin\",\n",
    "#     \"czbinance\",\n",
    "#     # \"watch\",\n",
    "#     \"binance\",\n",
    "#     \"dogelonmars\",\n",
    "#     \"cryptocurrency\",\n",
    "#     # \"money\",\n",
    "#     # \"danheld\",\n",
    "#     # \"cybersecurity\",\n",
    "#     \"ethereum\",\n",
    "#     \"bitcrush\",\n",
    "#     \"vvsorigin\",\n",
    "# ]\n",
    "# video_games_terms = [\n",
    "#     # \"gamejoin\",\n",
    "#     \"arcade\",\n",
    "#     \"dreamcast\",\n",
    "#     \"sega\",\n",
    "#     \"xbox\",\n",
    "#     \"wii\",\n",
    "#     \"ps4\",\n",
    "# ]\n",
    "# non_english_terms = [\n",
    "#     \"webuye\",\n",
    "#     \"bungoma\",\n",
    "#     \"ethereum\",\n",
    "#     \"pay someone\",\n",
    "#     \"seungkwan\",\n",
    "#     \"woozi\",\n",
    "#     \"hoshi\",\n",
    "#     \"kasama\",\n",
    "#     \"nung\",\n",
    "#     \"lahat\",\n",
    "#     \"jinsoul\",\n",
    "#     \"brunisoul\",\n",
    "#     \"loona\",\n",
    "#     \"taas\",\n",
    "#     \"nung\",\n",
    "# ]\n",
    "# misc_unwanted_terms = [\n",
    "#     \"nft\",\n",
    "#     \"volcano detected\",\n",
    "#     \"block-2\",\n",
    "#     \"tanzanite\",\n",
    "#     \"gemstonecarat\",\n",
    "#     \"popescu\",\n",
    "#     \"breeding\",\n",
    "#     \"nairobi\",\n",
    "#     \"pay someone\",\n",
    "#     \"homeworkpay\",\n",
    "#     \"homework\",\n",
    "#     \"photocards\",\n",
    "#     \"essay\",\n",
    "#     # \"hbomax\",\n",
    "# ]\n",
    "# religious_terms = [\n",
    "#     \"scriptures\",\n",
    "#     \"methusealah\",\n",
    "#     \"testament\",\n",
    "#     \"yahweh\",\n",
    "#     \"god\",\n",
    "#     \"mullah\",\n",
    "#     \"allah\",\n",
    "#     \"clergy\",\n",
    "#     \"mercy\",\n",
    "#     \"morality\",\n",
    "#     \"muslims,\",\n",
    "#     \"hindus\",\n",
    "#     \"buddhist\",\n",
    "#     \"catholics\",\n",
    "#     \"christians\",\n",
    "#     \"atheist\",\n",
    "# ]\n",
    "# inappropriate_terms = [\n",
    "#     \"prostitution\",\n",
    "#     \"musembe\",\n",
    "#     \"mo-greene\",\n",
    "#     \"running scared2012\",\n",
    "#     \"running scared 2012\",\n",
    "#     \"massacres\",\n",
    "#     \"eric ephriam chavez\",\n",
    "#     \"drugs\",\n",
    "#     \"bin laden\",\n",
    "#     \"saddam\",\n",
    "#     \"perished\",\n",
    "#     \"whore\",\n",
    "#     \"nasty\",\n",
    "#     \"nazist\",\n",
    "#     \"antifa\",\n",
    "#     \"proud boys\",\n",
    "# ]\n",
    "# min_num_words_tweet = 10\n",
    "\n",
    "upload_to_s3 = True\n",
    "cleanup_local_files = True\n",
    "\n",
    "# columns for EDA\n",
    "vcols = [\n",
    "    \"is_quote_status\",\n",
    "    \"quote_count\",\n",
    "    \"reply_count\",\n",
    "    # \"retweet_count\",\n",
    "    \"favorite_count\",\n",
    "    \"favorited\",\n",
    "    \"retweeted\",\n",
    "    \"source_text\",\n",
    "    \"user_followers\",\n",
    "    \"user_friends\",\n",
    "    \"user_favourites\",\n",
    "    \"user_verified\",\n",
    "    \"retweeted_tweet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9031e2-64d5-4daa-86b4-32d6f825f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = os.getenv(\"AWS_S3_BUCKET_NAME\", \"sagemakertestwillz3s\")\n",
    "session = boto3.Session(profile_name=\"default\")\n",
    "s3_client = session.client(\"s3\")\n",
    "\n",
    "# joined_tweet_search_terms_no_spaces = [\n",
    "#     t.replace(\" \", \"\") for t in joined_tweet_search_terms\n",
    "# ]\n",
    "# unwanted_partial_strings_list = (\n",
    "#     crypto_terms\n",
    "#     + religious_terms\n",
    "#     + inappropriate_terms\n",
    "#     + video_games_terms\n",
    "#     + misc_unwanted_terms\n",
    "#     + non_english_terms\n",
    "# )\n",
    "dtypes_dict = {\n",
    "    \"id\": pd.StringDtype(),\n",
    "    \"geo\": pd.StringDtype(),\n",
    "    \"coordinates\": pd.StringDtype(),\n",
    "    \"place\": pd.StringDtype(),\n",
    "    \"contributors\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"is_quote_status\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"quote_count\": pd.Int32Dtype(),\n",
    "    \"reply_count\": pd.Int32Dtype(),\n",
    "    \"retweet_count\": pd.Int32Dtype(),\n",
    "    \"favorite_count\": pd.Int32Dtype(),\n",
    "    \"favorited\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"retweeted\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"source\": pd.StringDtype(),\n",
    "    \"in_reply_to_user_id\": pd.StringDtype(),\n",
    "    \"in_reply_to_screen_name\": pd.StringDtype(),\n",
    "    \"source_text\": pd.StringDtype(),\n",
    "    \"place_id\": pd.StringDtype(),\n",
    "    \"place_url\": pd.StringDtype(),\n",
    "    \"place_place_type\": pd.StringDtype(),\n",
    "    \"place_name\": pd.StringDtype(),\n",
    "    \"place_full_name\": pd.StringDtype(),\n",
    "    \"place_country_code\": pd.StringDtype(),\n",
    "    \"place_country\": pd.StringDtype(),\n",
    "    \"place_bounding_box_type\": pd.StringDtype(),\n",
    "    \"place_bounding_box_coordinates\": pd.StringDtype(),\n",
    "    \"place_attributes\": pd.StringDtype(),\n",
    "    \"coords_type\": pd.StringDtype(),\n",
    "    \"coords_lon\": pd.StringDtype(),\n",
    "    \"coords_lat\": pd.StringDtype(),\n",
    "    \"geo_type\": pd.StringDtype(),\n",
    "    \"geo_lon\": pd.StringDtype(),\n",
    "    \"geo_lat\": pd.StringDtype(),\n",
    "    \"user_name\": pd.StringDtype(),\n",
    "    \"user_screen_name\": pd.StringDtype(),\n",
    "    \"user_followers\": pd.Int32Dtype(),\n",
    "    \"user_friends\": pd.Int32Dtype(),\n",
    "    \"user_listed\": pd.Int32Dtype(),\n",
    "    \"user_favourites\": pd.Int32Dtype(),\n",
    "    \"user_statuses\": pd.Int32Dtype(),\n",
    "    \"user_protected\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_verified\": pd.StringDtype(),  # pd.BooleanDtype(),\n",
    "    \"user_contributors_enabled\": pd.StringDtype(),\n",
    "    \"user_location\": pd.StringDtype(),\n",
    "    \"retweeted_tweet\": pd.StringDtype(),\n",
    "    \"tweet_text_urls\": pd.StringDtype(),\n",
    "    \"tweet_text_hashtags\": pd.StringDtype(),\n",
    "    \"tweet_text_usernames\": pd.StringDtype(),\n",
    "    \"num_urls_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_users_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"num_hashtags_in_tweet_text\": pd.Int32Dtype(),\n",
    "    \"text\": pd.StringDtype(),\n",
    "    # \"contains_wanted_text\": pd.BooleanDtype(),\n",
    "    # \"contains_wanted_text_case_sensitive\": pd.BooleanDtype(),\n",
    "    # \"contains_multi_word_wanted_text\": pd.BooleanDtype(),\n",
    "    # \"contains_crypto_terms\": pd.BooleanDtype(),\n",
    "    # \"contains_religious_terms\": pd.BooleanDtype(),\n",
    "    # \"contains_inappropriate_terms\": pd.BooleanDtype(),\n",
    "    # \"contains_video_games_terms\": pd.BooleanDtype(),\n",
    "    # \"contains_misc_unwanted_terms\": pd.BooleanDtype(),\n",
    "    # \"contains_non_english_terms\": pd.BooleanDtype(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad80ef1-dbef-4fab-b163-17756a4cc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip_file(\n",
    "    file_search_pattern: str, processed_data_dir: str, proc_data_zip_fname: str\n",
    "):\n",
    "    \"\"\"Create zipped file.\"\"\"\n",
    "    os.chdir(processed_data_dir)\n",
    "    ZipFile = zipfile.ZipFile(proc_data_zip_fname, \"w\")\n",
    "    for f in glob(file_search_pattern):\n",
    "        ZipFile.write(f, compress_type=zipfile.ZIP_DEFLATED)\n",
    "    ZipFile.close()\n",
    "    os.chdir(\"../../\")\n",
    "\n",
    "\n",
    "def upload_file_to_s3(\n",
    "    aws_region: str,\n",
    "    processed_data_dir: str,\n",
    "    fname: str,\n",
    "    s3_bucket_name: str,\n",
    "    s3_key: str,\n",
    ") -> None:\n",
    "    \"\"\"Upload file to key in S3 bucket.\"\"\"\n",
    "    s3_resource = boto3.resource(\"s3\", region_name=aws_region)\n",
    "    s3_resource.meta.client.upload_file(\n",
    "        f\"{processed_data_dir}/{fname}\",\n",
    "        s3_bucket_name,\n",
    "        s3_key,\n",
    "    )\n",
    "\n",
    "\n",
    "def download_file_from_s3(\n",
    "    s3_bucket_name: str,\n",
    "    path_to_folder: str,\n",
    "    data_dir: str,\n",
    "    fname: str,\n",
    "    aws_region: str,\n",
    ") -> None:\n",
    "    \"\"\"Download file from .\"\"\"\n",
    "    dest_filepath = os.path.join(data_dir, fname)\n",
    "    s3_filepath_key = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name,\n",
    "        Delimiter=\"/\",\n",
    "        Prefix=f\"{path_to_folder[1:]}processed/\",\n",
    "    )[\"Contents\"][0][\"Key\"]\n",
    "    start = datetime.now()\n",
    "    print(\n",
    "        f\"Started downloading processed data zip file from {s3_filepath_key} to \"\n",
    "        f\"{dest_filepath} at {start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "    )\n",
    "    s3 = boto3.resource(\"s3\", region_name=aws_region)\n",
    "    s3.meta.client.download_file(\n",
    "        s3_bucket_name,\n",
    "        s3_filepath_key,\n",
    "        dest_filepath,\n",
    "    )\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    print(f\"Done downloading in {duration:.3f} seconds.\")\n",
    "\n",
    "\n",
    "def extract_zip_file(dest_filepath: str, data_dir: str) -> None:\n",
    "    \"\"\".\"\"\"\n",
    "    start = datetime.now()\n",
    "    print(\n",
    "        \"Started extracting filtered data parquet files from \"\n",
    "        f\"processed data zip file to {data_dir} at \"\n",
    "        f\"{start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "    )\n",
    "    zip_ref = zipfile.ZipFile(dest_filepath)\n",
    "    zip_ref.extractall(data_dir)\n",
    "    zip_ref.close()\n",
    "    duration = (datetime.now() - start).total_seconds()\n",
    "    print(f\"Done extracting in {duration:.3f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640fc02-43a4-4384-8e08-fc8c0a10170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_blank_tweets(df: pd.DataFrame, subset: List[str] = [\"text\"]) -> pd.DataFrame:\n",
    "    \"\"\"Drop tweets with no text.\"\"\"\n",
    "    df_no_nans = df.dropna(subset=subset)\n",
    "    num_rows_dropped = len(df) - len(df_no_nans)\n",
    "    print(f\"Dropped {num_rows_dropped:,} tweets from raw data\")\n",
    "    return df_no_nans\n",
    "\n",
    "\n",
    "# def get_raw_masks(\n",
    "#     df,\n",
    "#     tweet_search_terms_list: List[str],\n",
    "#     case_sensitive_tweet_search_terms_list: List[str],\n",
    "#     joined_tweet_search_terms_no_spaces_list: List[str],\n",
    "# ) -> List[pd.Series]:\n",
    "#     \"\"\"Get masks for tweets with types of wanted terms in text.\"\"\"\n",
    "#     lowercase_mask = (\n",
    "#         df[\"text\"].str.lower().str.contains(\"|\".join(tweet_search_terms_list))\n",
    "#     )\n",
    "#     case_mask = df[\"text\"].str.contains(\n",
    "#         \"|\".join(case_sensitive_tweet_search_terms_list)\n",
    "#     )\n",
    "#     joined_case_mask = (\n",
    "#         df[\"text\"]\n",
    "#         .str.lower()\n",
    "#         .str.replace(\" \", \"\")\n",
    "#         .str.contains(\"|\".join(joined_tweet_search_terms_no_spaces_list))\n",
    "#     )\n",
    "#     print(\"Created masks to filter raw data based on wanted text in tweets\")\n",
    "#     return [lowercase_mask, case_mask, joined_case_mask]\n",
    "\n",
    "\n",
    "# def add_search_term_boolean_columns(\n",
    "#     df: pd.DataFrame,\n",
    "#     lowercase_mask: pd.Series,\n",
    "#     case_mask: pd.Series,\n",
    "#     joined_case_mask: pd.Series,\n",
    "#     crypto_terms_list: List[str],\n",
    "#     religious_terms_list: List[str],\n",
    "#     inappropriate_terms_list: List[str],\n",
    "#     video_games_terms_list: List[str],\n",
    "#     misc_unwanted_terms_list: List[str],\n",
    "#     non_english_terms_list: List[str],\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Add boolean columns based on presence of wanted and unwanted terms in tweet text.\"\"\"\n",
    "#     df = (\n",
    "#         df.assign(contains_wanted_text=lowercase_mask)\n",
    "#         .assign(contains_wanted_text_case_sensitive=case_mask)\n",
    "#         .assign(contains_multi_word_wanted_text=joined_case_mask)\n",
    "#         .assign(\n",
    "#             contains_crypto_terms=df[\"text\"].str.contains(\"|\".join(crypto_terms_list))\n",
    "#         )\n",
    "#         .assign(\n",
    "#             contains_religious_terms=df[\"text\"].str.contains(\n",
    "#                 \"|\".join(religious_terms_list)\n",
    "#             )\n",
    "#         )\n",
    "#         .assign(\n",
    "#             contains_inappropriate_terms=df[\"text\"].str.contains(\n",
    "#                 \"|\".join(inappropriate_terms_list)\n",
    "#             )\n",
    "#         )\n",
    "#         .assign(\n",
    "#             contains_video_games_terms=df[\"text\"].str.contains(\n",
    "#                 \"|\".join(video_games_terms_list)\n",
    "#             )\n",
    "#         )\n",
    "#         .assign(\n",
    "#             contains_misc_unwanted_terms=df[\"text\"].str.contains(\n",
    "#                 \"|\".join(misc_unwanted_terms_list)\n",
    "#             )\n",
    "#         )\n",
    "#         .assign(\n",
    "#             contains_non_english_terms=df[\"text\"].str.contains(\n",
    "#                 \"|\".join(non_english_terms_list)\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "#     print(\"Created boolean columns to indicate presence of unwanted terms in tweets\")\n",
    "#     terms_str = []\n",
    "#     pcts_total = []\n",
    "#     for c in df.columns[df.columns.str.endswith(\"_terms\")]:\n",
    "#         pct_of_total = (df[c].sum() / len(df)) * 100\n",
    "#         term_type = c.replace(\"contains_\", \"\").replace(\"_terms\", \"\")\n",
    "#         term_str = f\"{term_type}={pct_of_total:.3f}\"\n",
    "#         terms_str.append(term_str)\n",
    "#         pcts_total.append(pct_of_total)\n",
    "#     term_str_full = \" | \".join(terms_str) + f\" | total unwanted={sum(pcts_total):.3f}\"\n",
    "#     print(term_str_full)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def apply_masks(\n",
    "#     df: pd.DataFrame,\n",
    "#     case_mask: pd.Series,\n",
    "#     lowercase_mask: pd.Series,\n",
    "#     joined_case_mask: pd.Series,\n",
    "#     unwanted_partial_strings_list: List[str],\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Apply masks for only keeping tweets based on wanted terms in text.\"\"\"\n",
    "#     df = df.loc[case_mask | lowercase_mask | joined_case_mask]\n",
    "#     unwanted_mask = df[\"text\"].str.contains(\"|\".join(unwanted_partial_strings_list))\n",
    "#     df = df.loc[~unwanted_mask]\n",
    "#     print(f\"Kept {len(df):,} tweets after filtering raw data with masks\")\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def filter_by_num_words_in_tweet(\n",
    "#     df: pd.DataFrame, min_num_tweet_words_wanted: int\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Filter tweets based on number of words in text.\"\"\"\n",
    "#     min_num_words_mask = (\n",
    "#         df[\"text\"].str.split(\" \").str.len() >= min_num_tweet_words_wanted\n",
    "#     )\n",
    "#     print(\n",
    "#         f\"Kept {len(df.loc[min_num_words_mask]):,} tweets with more than \"\n",
    "#         f\"approximately {min_num_tweet_words_wanted:,} words per tweet\"\n",
    "#     )\n",
    "#     df = df.loc[min_num_words_mask]\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def filter_tweets_based_on_content(\n",
    "#     df_raw: pd.DataFrame,\n",
    "#     # tweet_search_terms: List[str],\n",
    "#     # case_sensitive_tweet_search_terms: List[str],\n",
    "#     # joined_tweet_search_terms_no_spaces: List[str],\n",
    "#     # crypto_terms: List[str],\n",
    "#     # religious_terms: List[str],\n",
    "#     # inappropriate_terms: List[str],\n",
    "#     # video_games_terms: List[str],\n",
    "#     # misc_unwanted_terms: List[str],\n",
    "#     # non_english_terms: List[str],\n",
    "#     # min_num_words_tweet: int,\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"Filter tweets based on terms in text and approximate number in words.\"\"\"\n",
    "#     start = datetime.now()\n",
    "#     print(\n",
    "#         \"Filtering Tweets - Starting time = \"\n",
    "#         f\"{start.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}...\"\n",
    "#     )\n",
    "#     df = df_raw.pipe(drop_blank_tweets, subset=[\"text\"])\n",
    "#     # lowercase_mask, case_mask, joined_case_mask = get_raw_masks(\n",
    "#     #     df_raw,\n",
    "#     #     tweet_search_terms,\n",
    "#     #     case_sensitive_tweet_search_terms,\n",
    "#     #     joined_tweet_search_terms_no_spaces,\n",
    "#     # )\n",
    "#     # df = (\n",
    "#     #     df_raw.pipe(\n",
    "#     #         add_search_term_boolean_columns,\n",
    "#     #         lowercase_mask=lowercase_mask,\n",
    "#     #         case_mask=case_mask,\n",
    "#     #         joined_case_mask=joined_case_mask,\n",
    "#     #         crypto_terms_list=crypto_terms,\n",
    "#     #         religious_terms_list=religious_terms,\n",
    "#     #         inappropriate_terms_list=inappropriate_terms,\n",
    "#     #         video_games_terms_list=video_games_terms,\n",
    "#     #         misc_unwanted_terms_list=misc_unwanted_terms,\n",
    "#     #         non_english_terms_list=non_english_terms,\n",
    "#     #     )\n",
    "#     #     # .pipe(\n",
    "#     #     #     apply_masks,\n",
    "#     #     #     case_mask=case_mask,\n",
    "#     #     #     lowercase_mask=lowercase_mask,\n",
    "#     #     #     joined_case_mask=joined_case_mask,\n",
    "#     #     #     unwanted_partial_strings_list=unwanted_partial_strings_list,\n",
    "#     #     # )\n",
    "#     #     # .pipe(\n",
    "#     #     #     filter_by_num_words_in_tweet, min_num_tweet_words_wanted=min_num_words_tweet\n",
    "#     #     # )\n",
    "#     # )\n",
    "#     end = datetime.now()\n",
    "#     duration = (end - start).total_seconds()\n",
    "#     print(\n",
    "#         \"Done filtering at \"\n",
    "#         f\"{end.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]} ({duration:.3f} seconds).\"\n",
    "#     )\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345babe9-67fa-49ba-9dd5-8bab336752bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_folders_per_day(\n",
    "    s3_bucket_name: str,\n",
    "    path_to_folder: str,\n",
    "    years_wanted: List[int],\n",
    "    verbose: bool = False,\n",
    ") -> List[str]:\n",
    "    \"\"\"Get list of S3 hourly data folders, per day of streamed data.\"\"\"\n",
    "    list_of_hourly_dirs = []\n",
    "    for year in years_wanted:\n",
    "        monthly_prefixes = s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket_name,\n",
    "            Prefix=f\"{path_to_folder[1:]}{year}/\",\n",
    "            Delimiter=\"/\",\n",
    "        )[\"CommonPrefixes\"]\n",
    "        # print(monthly_prefixes)\n",
    "\n",
    "        for monthly_prefix in monthly_prefixes:\n",
    "            daily_prefixes = s3_client.list_objects_v2(\n",
    "                Bucket=s3_bucket_name,\n",
    "                Prefix=monthly_prefix[\"Prefix\"],\n",
    "                Delimiter=\"/\",\n",
    "            )[\"CommonPrefixes\"]\n",
    "            # print(monthly_prefix, daily_prefixes)\n",
    "\n",
    "            for daily_prefix in daily_prefixes:\n",
    "                hourly_prefixes = s3_client.list_objects_v2(\n",
    "                    Bucket=s3_bucket_name,\n",
    "                    Prefix=daily_prefix[\"Prefix\"],\n",
    "                    Delimiter=\"/\",\n",
    "                )[\"CommonPrefixes\"]\n",
    "                # print(\n",
    "                #     monthly_prefix,\n",
    "                #     # daily_prefixes,\n",
    "                #     hourly_prefixes,\n",
    "                # )\n",
    "                list_of_hourly_dirs.append(hourly_prefixes)\n",
    "    list_of_hourly_dirs_flat = [sl[\"Prefix\"] for l in list_of_hourly_dirs for sl in l]\n",
    "    print(f\"Found {len(list_of_hourly_dirs_flat):,} hourly folders\")\n",
    "    if verbose:\n",
    "        for hourly_dirs in list_of_hourly_dirs_flat:\n",
    "            print(hourly_dirs)\n",
    "    return list_of_hourly_dirs_flat\n",
    "\n",
    "\n",
    "def get_hourly_file_name(first_object_key: Dict) -> str:\n",
    "    \"\"\"Get name of hourly object on S3.\"\"\"\n",
    "    fname = (\n",
    "        first_object_key[\"Key\"]\n",
    "        .split(\"twitter_delivery_stream\")[0]\n",
    "        .split(\"kinesis-demo\")[1]\n",
    "        .replace(\"/\", \"\")\n",
    "    )\n",
    "    return fname\n",
    "\n",
    "\n",
    "def get_hourly_files_names(s3_bucket_name: str, list_of_hourly_dirs_flat: List[str]):\n",
    "    \"\"\"Get year, month, date and hour per hour for hourly S3 objects.\"\"\"\n",
    "    keys_yyyymmdd = [\n",
    "        get_hourly_file_name(\n",
    "            first_object_key=s3_client.list_objects_v2(\n",
    "                Bucket=s3_bucket_name, Prefix=list_of_hourly_dirs\n",
    "            )[\"Contents\"][0]\n",
    "        )\n",
    "        for list_of_hourly_dirs in list_of_hourly_dirs_flat\n",
    "    ]\n",
    "    file_names = pd.Series(keys_yyyymmdd)\n",
    "    assert file_names.sort_values().equals(file_names)\n",
    "    assert len(file_names) == len(list_of_hourly_dirs_flat)\n",
    "    return file_names\n",
    "\n",
    "\n",
    "def save_to_parquet(\n",
    "    df: pd.DataFrame, filepath: str, storage_options: Union[None, Dict[str, str]] = None\n",
    ") -> None:\n",
    "    \"\"\"Save DataFrame to .parquet.gzip file.\"\"\"\n",
    "    print(f\"Saving to parquet file {os.path.basename(filepath)}...\")\n",
    "    df.to_parquet(filepath, index=False, storage_options=storage_options)\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def read_parquet(\n",
    "    filepath: str,\n",
    "    columns: Union[None, List[str]] = None,\n",
    "    storage_options: Union[None, Dict[str, str]] = None,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read DataFrame from .parquet.gzip file.\"\"\"\n",
    "    df = pd.read_parquet(filepath, columns=columns, storage_options=storage_options)\n",
    "    if verbose:\n",
    "        print(f\"Read data from parquet file {os.path.basename(filepath)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_s3_bucket_file_list(s3_bucklet_name: str, processed_data_dir: str) -> List[str]:\n",
    "    \"\"\"Get list of all filepaths in folder in S3 bucket.\"\"\"\n",
    "    print(f\"Getting list of files in {processed_data_dir} in S3 bucket...\")\n",
    "    s3_processed_data_filepaths = [\n",
    "        f\"s3://{s3_bucket_name}/{c['Key']}\"\n",
    "        for c in s3_client.list_objects_v2(\n",
    "            Bucket=s3_bucket_name,\n",
    "            Delimiter=\"/\",\n",
    "            Prefix=f\"{processed_data_dir}/\",\n",
    "        )[\"Contents\"]\n",
    "    ]\n",
    "    print(\"Done.\")\n",
    "    return s3_processed_data_filepaths\n",
    "\n",
    "\n",
    "def convert_file_contents_to_df(file_contents_all_flat: List) -> pd.DataFrame:\n",
    "    \"\"\"Read contents of streamed file into single-row DataFrame.\"\"\"\n",
    "    nested_list_of_records = []\n",
    "    for file_body in file_contents_all_flat:\n",
    "        list_of_records = file_body.decode(\"utf-8\").split(\"\\n\")[:-1]\n",
    "        nested_list_of_records.append(list_of_records)\n",
    "    df = pd.DataFrame(\n",
    "        [record.split(\"\\t\")[:-1] for sl in nested_list_of_records for record in sl]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_column_headers(df: pd.DataFrame, headers: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Add column headers to DataFrame.\"\"\"\n",
    "    num_rows, num_cols = df.shape\n",
    "    print(f\"Raw Data contains {num_rows:,} rows and {num_cols:,} columns\")\n",
    "    # df_mismatched = df[~df.iloc[:, -2:].isna().all(axis=1)]\n",
    "    # mismatched_rows = len(df_mismatched)\n",
    "    # with pd.option_context(\"display.max_columns\", None):\n",
    "    #     display(df_mismatched.style.set_caption(f\"{mismatched_rows:,} Mismatched rows\"))\n",
    "    if num_cols == 54:\n",
    "        df = df.loc[df.iloc[:, -1:].isna().all(axis=1)].drop(columns=[53])\n",
    "    elif num_cols == 55:\n",
    "        df = df.loc[df.iloc[:, -2:].isna().all(axis=1)].drop(columns=[53, 54])\n",
    "    num_new_rows = len(df)\n",
    "    print(f\"Dropped {(num_rows - num_new_rows):,} mismatched rows out of {num_rows:,}\")\n",
    "    assert df.shape[1] == len(headers)\n",
    "    df.columns = headers\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_single_hour_files(\n",
    "    q: int,\n",
    "    list_of_hourly_dirs: List[str],\n",
    "    headers: List[str],\n",
    "    file_name: str,\n",
    "    len_flat_list_of_hourly_dirs: List[str],\n",
    "    # tweet_search_terms: List[str],\n",
    "    # case_sensitive_tweet_search_terms: List[str],\n",
    "    # joined_tweet_search_terms_no_spaces: List[str],\n",
    "    # crypto_terms: List[str],\n",
    "    # religious_terms: List[str],\n",
    "    # inappropriate_terms: List[str],\n",
    "    # video_games_terms: List[str],\n",
    "    # misc_unwanted_terms: List[str],\n",
    "    # non_english_terms: List[str],\n",
    "    # min_num_words_tweet: int,\n",
    "    s3_bucket_name: str,\n",
    "    processed_data_dir: str,\n",
    "    aws_region: str,\n",
    "    dtypes_dict: Dict,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Process single hour of file objects on S3.\"\"\"\n",
    "    # extract\n",
    "    objects_hourly_all = s3_client.list_objects_v2(\n",
    "        Bucket=s3_bucket_name, Prefix=list_of_hourly_dirs\n",
    "    )\n",
    "    file_contents_list = []\n",
    "    for k, file_obj_dict in enumerate(objects_hourly_all[\"Contents\"], 1):\n",
    "        file_body = s3_client.get_object(\n",
    "            Bucket=s3_bucket_name, Key=file_obj_dict.get(\"Key\")\n",
    "        )[\"Body\"].read()\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Dir {q}/{len_flat_list_of_hourly_dirs} - \"\n",
    "                f\"{list_of_hourly_dirs}, reading object \"\n",
    "                f\"{k}/{len(objects_hourly_all['Contents'])}\"\n",
    "            )\n",
    "        file_contents_list.append(file_body)\n",
    "    print(\n",
    "        f\"Dir {q}/{len_flat_list_of_hourly_dirs} - {list_of_hourly_dirs} contains \"\n",
    "        f\"{len(file_contents_list):,} file objects\"\n",
    "    )\n",
    "    # transform\n",
    "    df = (\n",
    "        convert_file_contents_to_df(file_contents_list)\n",
    "        .pipe(add_column_headers, headers=headers)\n",
    "        # convert to datetime\n",
    "        .assign(created_at=lambda df: pd.to_datetime(df[\"created_at\"]))\n",
    "        .assign(user_joined=lambda df: pd.to_datetime(df[\"user_joined\"]))\n",
    "        # drop blank tweets\n",
    "        .pipe(drop_blank_tweets, subset=[\"text\"])\n",
    "        # # filter based on text in tweet\n",
    "        # .pipe(\n",
    "        #     filter_tweets_based_on_content,\n",
    "        #     tweet_search_terms=tweet_search_terms,\n",
    "        #     case_sensitive_tweet_search_terms=case_sensitive_tweet_search_terms,\n",
    "        #     joined_tweet_search_terms_no_spaces=joined_tweet_search_terms_no_spaces,\n",
    "        #     crypto_terms=crypto_terms,\n",
    "        #     religious_terms=religious_terms,\n",
    "        #     inappropriate_terms=inappropriate_terms,\n",
    "        #     video_games_terms=video_games_terms,\n",
    "        #     misc_unwanted_terms=misc_unwanted_terms,\n",
    "        #     non_english_terms=non_english_terms,\n",
    "        #     # min_num_words_tweet=min_num_words_tweet,\n",
    "        # )\n",
    "        # change datatypes\n",
    "        .astype(dtypes_dict)\n",
    "    )\n",
    "    # load\n",
    "    if \"data/\" in processed_data_dir:\n",
    "        # save to .parquet.gzip\n",
    "        filepath = f\"{processed_data_dir}/{file_name}.parquet.gzip\"\n",
    "        storage_options = None\n",
    "    else:\n",
    "        filepath = (\n",
    "            f\"s3://{s3_bucket_name}/{processed_data_dir}/{file_name}.parquet.gzip\"\n",
    "        )\n",
    "        storage_options = {\n",
    "            \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "            \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        }\n",
    "    save_to_parquet(df, filepath, storage_options)\n",
    "\n",
    "\n",
    "def process_files_per_hour(\n",
    "    s3_bucket_name: str,\n",
    "    flat_list_of_hourly_dirs: List[str],\n",
    "    headers: List[str],\n",
    "    file_names: List[str],\n",
    "    # tweet_search_terms: List[str],\n",
    "    # case_sensitive_tweet_search_terms: List[str],\n",
    "    # joined_tweet_search_terms_no_spaces: List[str],\n",
    "    # crypto_terms: List[str],\n",
    "    # religious_terms: List[str],\n",
    "    # inappropriate_terms: List[str],\n",
    "    # video_games_terms: List[str],\n",
    "    # misc_unwanted_terms: List[str],\n",
    "    # non_english_terms: List[str],\n",
    "    # min_num_words_tweet: int,\n",
    "    processed_data_dir: str,\n",
    "    proc_zip_fname: str,\n",
    "    path_to_folder: str,\n",
    "    aws_region: str,\n",
    "    dtypes_dict: Dict,\n",
    "    cleanup_local_files: bool = False,\n",
    "    upload_to_s3: bool = False,\n",
    "    verbose: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Run ETL workflow to process hourly streamed tweets into separate parquet file.\"\"\"\n",
    "    for q, (file_name, list_of_hourly_dirs) in enumerate(\n",
    "        zip(file_names, flat_list_of_hourly_dirs), 1\n",
    "    ):\n",
    "        process_single_hour_files(\n",
    "            q,\n",
    "            list_of_hourly_dirs,\n",
    "            headers,\n",
    "            file_name,\n",
    "            len(flat_list_of_hourly_dirs),\n",
    "            # tweet_search_terms,\n",
    "            # case_sensitive_tweet_search_terms,\n",
    "            # joined_tweet_search_terms_no_spaces,\n",
    "            # crypto_terms,\n",
    "            # religious_terms,\n",
    "            # inappropriate_terms,\n",
    "            # video_games_terms,\n",
    "            # misc_unwanted_terms,\n",
    "            # non_english_terms,\n",
    "            # min_num_words_tweet,\n",
    "            s3_bucket_name,\n",
    "            processed_data_dir,\n",
    "            aws_region,\n",
    "            dtypes_dict,\n",
    "            verbose,\n",
    "        )\n",
    "        if q < len(flat_list_of_hourly_dirs):\n",
    "            print()\n",
    "\n",
    "    # (if saved locally) zip all processed data files, upload to S3, delete local files\n",
    "    if \"data/\" in processed_data_dir:\n",
    "        if upload_to_s3:\n",
    "            # create zip of all .parquet.gzip processed data files\n",
    "            curr_dir = os.getcwd()\n",
    "            create_zip_file(\"*.parquet.gzip\", processed_data_dir, proc_zip_fname)\n",
    "            # upload zip file to S3 bucket\n",
    "            try:\n",
    "                assert os.getcwd() == curr_dir\n",
    "                upload_file_to_s3(\n",
    "                    aws_region,\n",
    "                    processed_data_dir,\n",
    "                    proc_zip_fname,\n",
    "                    s3_bucket_name,\n",
    "                    f\"{path_to_folder[1:-1]}/processed/{proc_zip_fname}\",\n",
    "                )\n",
    "                print(\"\\nUploaded zipped file to S3 bucket\")\n",
    "            except AssertionError as e:\n",
    "                print(\n",
    "                    f\"\\n{str(e)}: Incorrect working directory. \"\n",
    "                    \"Did not upload zipped file to S3 bucket.\"\n",
    "                )\n",
    "\n",
    "        if cleanup_local_files:\n",
    "            # delete locally exported parquet files\n",
    "            list(\n",
    "                map(\n",
    "                    os.remove,\n",
    "                    [\n",
    "                        os.path.join(processed_data_dir, f\"{f}.parquet.gzip\")\n",
    "                        for f in file_names\n",
    "                    ],\n",
    "                )\n",
    "            )\n",
    "    print(\"Deleted local .parquet.gzip files with filtered data.\")\n",
    "    # delete local zip file\n",
    "    os.remove(os.path.join(processed_data_dir, proc_zip_fname))\n",
    "    print(\"Deleted local .zip file created from all filtered data files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82981b60-e640-464d-8096-dde63904324b",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aff34e-caf2-42d2-b9af-347762437868",
   "metadata": {},
   "source": [
    "Get a nested list of hourly S3 object prefixes per day for which tweets were streamed into the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e700ce-a45e-447f-9767-b824d3e9cd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "list_of_hourly_dirs_flat = get_hourly_folders_per_day(\n",
    "    s3_bucket_name, path_to_folder, years_wanted\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5e620-45e0-41e1-a2fa-273cd37bea1e",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. In this nested list, each sublist consists of hourly S3 object prefixes per day. Each prefix has the format `datasets/twitter/kinesis-demo/<yyyy>/<mm>/<dd>/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f452e8-0d72-4e78-9b77-28643fc073e1",
   "metadata": {},
   "source": [
    "Get a flat list of hourly object prefixes covering all the days of streamed tweets in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f1833-480f-494e-adac-81c9bb0f692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_names =  get_hourly_files_names(s3_bucket_name, list_of_hourly_dirs_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323cfb3-902a-4a6f-ab6a-562678e18359",
   "metadata": {},
   "source": [
    "## Combine Data Per Hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d04e99-a1af-44d0-b363-cd1989da1001",
   "metadata": {},
   "source": [
    "Run an ETL workflow to process hourly file objects in the S3 bucket\n",
    "- extract all hourly file objects into a single `pandas` `DataFrame`\n",
    "- process the hourly data\n",
    "  - convert to `datetime`, as required\n",
    "  - add column names\n",
    "  - drop rows in the data where the tweet does not contain text (i.e. drop blank tweets)\n",
    "- export the processed `DataFrame` to a `.parquet` file\n",
    "\n",
    "This ETL workflow will give a single `.parquet` file for every hour of every day on which tweets were streamed. All `.parquet` files are then\n",
    "- combined into a single `.zip` file, which is then\n",
    "  - uploaded to the `datasets/twitter/kinesis-demo/processed` prefix in the S3 bucket\n",
    "  - deleted locally\n",
    "- deleted locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e54ffc-53a1-49ca-b263-2ebfdd8afa64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "process_files_per_hour(\n",
    "    s3_bucket_name,\n",
    "    list_of_hourly_dirs_flat,\n",
    "    headers,\n",
    "    file_names.tolist(),\n",
    "    # tweet_search_terms,\n",
    "    # case_sensitive_tweet_search_terms,\n",
    "    # joined_tweet_search_terms_no_spaces,\n",
    "    # crypto_terms,\n",
    "    # religious_terms,\n",
    "    # inappropriate_terms,\n",
    "    # video_games_terms,\n",
    "    # misc_unwanted_terms,\n",
    "    # non_english_terms,\n",
    "    # min_num_words_tweet,\n",
    "    processed_data_dir,\n",
    "    proc_zip_fname,\n",
    "    path_to_folder,\n",
    "    session.region_name,\n",
    "    dtypes_dict,\n",
    "    cleanup_local_files,\n",
    "    upload_to_s3,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c721cbe-75eb-4823-8eb5-18a90fe4623f",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. Since file objects per hour are being combined into a single `DataFrame`, it is possible to use `pandas` (in-memory) for processing hourly data as only a few thousand file objects were created per hour (based on the search filters set during streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830fc2e-bd61-4e36-9c1d-5c7ae4a9e55c",
   "metadata": {},
   "source": [
    "## (OPTIONAL) Retrieve `.zip` File, Extract and Load into Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455581d-f5f8-4091-b67b-a3b1f778589b",
   "metadata": {},
   "source": [
    "The `.zip` file created above is now downloaded from S3 and extracted. Its contents, the hourly `.parquet` files are then read into separate `pandas` DataFrame and combined into a single `pandas` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2c44f-8529-4a0a-8264-93a561e3f502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if \"data/\" in processed_data_dir:\n",
    "    if not os.path.exists(os.path.join(processed_data_dir, proc_zip_fname)):\n",
    "        download_file_from_s3(\n",
    "            s3_bucket_name, path_to_folder, processed_data_dir, proc_zip_fname, session.region_name\n",
    "        )\n",
    "        extract_zip_file(os.path.join(processed_data_dir, proc_zip_fname), processed_data_dir)\n",
    "    proc_files = glob(f\"{processed_data_dir}/*.parquet.gzip\")\n",
    "    storage_options = None\n",
    "else:\n",
    "    storage_options = {\n",
    "        \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "        \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    }\n",
    "    proc_files = get_s3_bucket_file_list(s3_bucket_name, processed_data_dir)\n",
    "dfs_processed = [read_parquet(f, None, storage_options, False) for f in proc_files]\n",
    "df = pd.concat(dfs_processed, ignore_index=True)\n",
    "display(\n",
    "    df[[\"id\", \"text\", \"user_joined\", \"created_at\"]]\n",
    "    .isna()\n",
    "    .sum()\n",
    "    .to_frame()\n",
    "    .T.add_suffix(\"__nans\")\n",
    "    .assign(num_rows=len(df))\n",
    ")\n",
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(\n",
    "        df.isna()\n",
    "        .sum()\n",
    "        .rename(\"nans\")\n",
    "        .to_frame()\n",
    "        .merge(\n",
    "            df.dtypes.rename(\"dtype\").to_frame(),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "with pd.option_context(\"display.max_columns\", None):\n",
    "    display(\n",
    "        df[\n",
    "            [\n",
    "                \"id\",\n",
    "                \"created_at\",\n",
    "                \"source_text\",\n",
    "                \"user_name\",\n",
    "                \"user_screen_name\",\n",
    "                \"user_joined\",\n",
    "                \"text\",\n",
    "            ]\n",
    "        ].head().style.set_caption(f\"Loaded {len(df):,} rows of processed data\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1970085-d1e1-44dd-9b02-c483b4008529",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. For the tweets meeting the required subject for this project, combining all the hourly `.parquet` files into a **single** in-memory `pandas` DataFrame is sufficient. The ability to hold a memory-bound framework to hold this (hourly) data depends on the\n",
    "   - particular **subject chosen for this project**\n",
    "   - filters applied to remove tweets with irrelevant subjects (crypto, video games, etc.)\n",
    "\n",
    "   For a different choice of subject or filters, this may not always be possible and out-of-memory `DataFrame`s will be needed to store the hourly data before it is exported to disk (in a `.parquet` file). In the next notebook, `PySpark` will be used to load all the `.parquet` files into a single PySpark DataFrame, which will be used for further data processing.\n",
    "2. One of the requirements of this project was to use big-data tools to manage the individual steps in the end-to-end data lifecycle (loading and processing). For this reason, PySpark is used in the next notebook for data processing. If this requirement was not in place, it would be more efficient to perform all data processing in this notebook (using memory-bound tools). Again, for a different choice of subject or filters, this may not be possible. However, for the current use-case, in-memory frameworks can be used to process hourly data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1616bfb6-026b-4270-8c2b-bc3fd20c492f",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f527a-8a60-4744-839a-77c495aff4d9",
   "metadata": {},
   "source": [
    "1. JWST\n",
    "   - [Countries that built the JWST](https://www.webb.nasa.gov/content/about/faqs/faq.html#countries)\n",
    "   - [JWST Team (space agencies and NASA centers)](https://www.nasa.gov/mission_pages/webb/team/index.html)\n",
    "   - [JWST could prove Stephen Hawking's theory](https://www.republicworld.com/science/space/james-webb-space-telescope-may-provide-data-to-prove-stephen-hawkings-dark-matter-theory-articleshow.html)\n",
    "   - [team of project managers and project scientists](https://jwst.nasa.gov/content/meetTheTeam/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb834791-eb01-4474-92ad-8ffd6dcc4eb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c5dfd-386e-4072-ae21-6b58addd23eb",
   "metadata": {},
   "source": [
    "<span style=\"float:left;\">\n",
    "    <a href=\"./2_create_sagemaker_resources.ipynb\"><< 2 - Create Sagemaker Resources</a>\n",
    "</span>\n",
    "\n",
    "<span style=\"float:right;\">\n",
    "    <a href=\"./4_filter_data.ipynb\">4 - Filter Data >></a>\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark:Python",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
