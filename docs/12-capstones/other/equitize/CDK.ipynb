{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Discovery Capture from SQL and NoSQL to a Data Warehouse!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are ingesting large amounts of data into SQL and NoSQL. You got big data! For Data Engineering, there are the 3 Vs: volume, velocity, variety. Both SQL and NoSQL can take care of volume and velocity if they are transactional databases. However NoSQL can take care of variety with semi-structured and unstrucuted data. However you also want OLAP data warehouse for easy querying for business analytics. To replicate the source databases to the target data warehouse is a process called Change Discovery Capture (CDC)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Every 5 minutes, Eventbridge triggers a Lambda to load txns.csv to RDS. Since I defined the table with no primary key/uniqueness restriction, the table gets appended. AWS DMS (data migration service) task is synchronize the data from RDS to Redshift via CDC.\n",
    "- Every 5 minutes, Eventbridge triggers a Lambda to load trades.json to DynamoDB. Any INSERTS or UPDATES triggers DynamoDB stream to trigger another separate Lambda that will write those new records into a file stored in an S3 bucket. Every 5 minutes, another Lambda will load files from the S3 bucket to the Redshift cluster, then delete the files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For observability, you can inspect the Lambda's Cloudwatch logs: runtime duration, failures, and count of endpoint hits. If you are fancy, you can add metrics & alarms to the Lambda (and API Gateway). For the business/operations/SRE team, you can add New Relic to the Lambda such that there will be \"single pane of glass\" for 24/7 monitoring. You can also inspect the API Gateway's dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import aws_cdk as cdk\n",
    "\n",
    "from aws_cdk import (\n",
    "    BundlingOptions,\n",
    "    Duration,\n",
    "    RemovalPolicy,\n",
    "    SecretValue,\n",
    "    Stack,\n",
    "    aws_dms as dms,\n",
    "    aws_dynamodb as dynamodb,\n",
    "    aws_ec2 as ec2,\n",
    "    aws_events as events,\n",
    "    aws_events_targets as events_targets,\n",
    "    aws_iam as iam,\n",
    "    aws_lambda as _lambda,\n",
    "    aws_lambda_event_sources as event_sources,\n",
    "    aws_rds as rds,\n",
    "    aws_redshift as redshift,\n",
    "    aws_s3 as s3,\n",
    ")\n",
    "\n",
    "from constructs import Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedshiftService(Construct):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Construct,\n",
    "        construct_id: str,\n",
    "        environment: dict,\n",
    "        security_group: ec2.SecurityGroup,\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id)  # required\n",
    "        self.redshift_full_commands_full_access_role = iam.Role(\n",
    "            self,\n",
    "            \"RedshiftClusterRole\",\n",
    "            assumed_by=iam.ServicePrincipal(\"redshift.amazonaws.com\"),\n",
    "            managed_policies=[\n",
    "                iam.ManagedPolicy.from_aws_managed_policy_name(\n",
    "                    \"AmazonRedshiftAllCommandsFullAccess\"\n",
    "                ),  ### later principle of least privileges\n",
    "            ],\n",
    "        )\n",
    "        self.redshift_cluster = redshift.CfnCluster(  ### refactor as its own Construct\n",
    "            self,\n",
    "            \"RedshiftCluster\",\n",
    "            cluster_type=\"single-node\",  # for demo purposes\n",
    "            number_of_nodes=1,  # for demo purposes\n",
    "            node_type=\"dc2.large\",  # for demo purposes\n",
    "            db_name=environment[\"REDSHIFT_DATABASE_NAME\"],\n",
    "            master_username=environment[\"REDSHIFT_USER\"],\n",
    "            master_user_password=environment[\"REDSHIFT_PASSWORD\"],\n",
    "            iam_roles=[self.redshift_full_commands_full_access_role.role_arn],\n",
    "            # cluster_subnet_group_name=demo_cluster_subnet_group.ref,\n",
    "            vpc_security_group_ids=[security_group.security_group_id],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDSService(Construct):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Construct,\n",
    "        construct_id: str,\n",
    "        environment: dict,\n",
    "        vpc: ec2.Vpc,\n",
    "        security_group: ec2.SecurityGroup,\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id)  # required\n",
    "        self.rds_instance = rds.DatabaseInstance(\n",
    "            self,\n",
    "            \"RDSToCDCToRedshift\",\n",
    "            engine=rds.DatabaseInstanceEngine.mysql(\n",
    "                version=rds.MysqlEngineVersion.VER_8_0_28\n",
    "            ),\n",
    "            instance_type=ec2.InstanceType(\n",
    "                \"t3.micro\"\n",
    "            ),  # for demo purposes; otherwise defaults to m5.large\n",
    "            credentials=rds.Credentials.from_username(\n",
    "                username=environment[\"RDS_USER\"],\n",
    "                password=SecretValue.unsafe_plain_text(environment[\"RDS_PASSWORD\"]),\n",
    "            ),\n",
    "            database_name=environment[\"RDS_DATABASE_NAME\"],\n",
    "            port=environment[\"RDS_PORT\"],\n",
    "            vpc=vpc,\n",
    "            vpc_subnets=ec2.SubnetSelection(\n",
    "                subnet_type=ec2.SubnetType.PUBLIC\n",
    "            ),  ### will have to figure out VPC\n",
    "            security_groups=[security_group],\n",
    "            parameters={\"binlog_format\": \"ROW\"},\n",
    "            publicly_accessible=True,  ### will have to figure out VPC\n",
    "            removal_policy=RemovalPolicy.DESTROY,\n",
    "            delete_automated_backups=True,\n",
    "        )\n",
    "\n",
    "        self.load_data_to_rds_lambda = _lambda.Function(\n",
    "            self,\n",
    "            \"LoadDataToRDSLambda\",\n",
    "            runtime=_lambda.Runtime.PYTHON_3_9,\n",
    "            code=_lambda.Code.from_asset(\n",
    "                \"source/load_data_to_rds_lambda\",\n",
    "                # exclude=[\".venv/*\"],  # seems to no longer do anything if use BundlingOptions\n",
    "                bundling=BundlingOptions(\n",
    "                    image=_lambda.Runtime.PYTHON_3_9.bundling_image,\n",
    "                    command=[\n",
    "                        \"bash\",\n",
    "                        \"-c\",\n",
    "                        \" && \".join(\n",
    "                            [\n",
    "                                \"pip install -r requirements.txt -t /asset-output\",\n",
    "                                \"cp handler.py txns.csv /asset-output\",  # need to cp instead of mv\n",
    "                            ]\n",
    "                        ),\n",
    "                    ],\n",
    "                ),\n",
    "            ),\n",
    "            handler=\"handler.lambda_handler\",\n",
    "            timeout=Duration.seconds(3),  # should be fairly quick\n",
    "            memory_size=128,  # in MB\n",
    "            environment={\n",
    "                \"RDS_USER\": environment[\"RDS_USER\"],\n",
    "                \"RDS_PASSWORD\": environment[\"RDS_PASSWORD\"],\n",
    "                \"RDS_DATABASE_NAME\": environment[\"RDS_DATABASE_NAME\"],\n",
    "                \"RDS_TABLE_NAME\": environment[\"RDS_TABLE_NAME\"],\n",
    "                \"CSV_FILENAME\": environment[\"CSV_FILENAME\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # connect the AWS resources\n",
    "        self.load_data_to_rds_lambda.add_environment(\n",
    "            key=\"RDS_HOST\", value=self.rds_instance.db_instance_endpoint_address\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDCFromRDSToRedshiftService(Construct):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Construct,\n",
    "        construct_id: str,\n",
    "        environment: dict,\n",
    "        rds_endpoint_address: str,\n",
    "        redshift_endpoint_address: str,\n",
    "        security_group_id: str,\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id)  # required\n",
    "        self.dms_rds_source_endpoint = dms.CfnEndpoint(\n",
    "            self,\n",
    "            \"RDSSourceEndpoint\",\n",
    "            endpoint_type=\"source\",\n",
    "            engine_name=\"mysql\",\n",
    "            server_name=rds_endpoint_address,\n",
    "            port=environment[\"RDS_PORT\"],\n",
    "            username=environment[\"RDS_USER\"],\n",
    "            password=environment[\"RDS_PASSWORD\"],\n",
    "        )\n",
    "        self.dms_redshift_target_endpoint = dms.CfnEndpoint(\n",
    "            self,\n",
    "            \"RedshiftTargetEndpoint\",\n",
    "            endpoint_type=\"target\",\n",
    "            engine_name=\"redshift\",\n",
    "            database_name=environment[\"REDSHIFT_DATABASE_NAME\"],\n",
    "            server_name=redshift_endpoint_address,\n",
    "            port=5439,\n",
    "            username=environment[\"REDSHIFT_USER\"],\n",
    "            password=environment[\"REDSHIFT_PASSWORD\"],\n",
    "        )\n",
    "        self.dms_replication_instance = dms.CfnReplicationInstance(\n",
    "            self,\n",
    "            \"DMSReplicationInstance\",\n",
    "            replication_instance_class=\"dms.t3.micro\",  # for demo purposes\n",
    "            vpc_security_group_ids=[security_group_id],\n",
    "        )\n",
    "        self.dms_replication_task = dms.CfnReplicationTask(\n",
    "            self,\n",
    "            \"DMSReplicationTask\",\n",
    "            migration_type=\"cdc\",\n",
    "            replication_instance_arn=self.dms_replication_instance.ref,  # appears that\n",
    "            source_endpoint_arn=self.dms_rds_source_endpoint.ref,  # `ref` means\n",
    "            target_endpoint_arn=self.dms_redshift_target_endpoint.ref,  # arn\n",
    "            table_mappings=json.dumps(\n",
    "                {\n",
    "                    \"rules\": [\n",
    "                        {\n",
    "                            \"rule-type\": \"selection\",\n",
    "                            \"rule-id\": \"1\",\n",
    "                            \"rule-name\": \"1\",\n",
    "                            \"object-locator\": {\n",
    "                                \"schema-name\": \"%\",\n",
    "                                \"table-name\": environment[\"RDS_TABLE_NAME\"],\n",
    "                            },\n",
    "                            \"rule-action\": \"include\",\n",
    "                            \"filters\": [],\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ),\n",
    "            replication_task_settings=json.dumps({\"Logging\": {\"EnableLogging\": True}}),\n",
    "        )\n",
    "\n",
    "        self.start_dms_replication_task_lambda = _lambda.Function(\n",
    "            self,\n",
    "            \"StartDMSReplicationTaskLambda\",\n",
    "            runtime=_lambda.Runtime.PYTHON_3_9,\n",
    "            code=_lambda.Code.from_asset(\n",
    "                \"source/start_dms_replication_task_lambda\",\n",
    "                exclude=[\".venv/*\"],\n",
    "            ),\n",
    "            handler=\"handler.lambda_handler\",\n",
    "            timeout=Duration.seconds(1),  # should be instantaneous\n",
    "            memory_size=128,  # in MB\n",
    "        )\n",
    "        self.start_dms_replication_task_lambda.add_to_role_policy(\n",
    "            iam.PolicyStatement(\n",
    "                actions=[\"dms:StartReplicationTask\", \"dms:DescribeReplicationTasks\"],\n",
    "                resources=[\"*\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # connect the AWS resources\n",
    "        self.start_dms_replication_task_lambda.add_environment(\n",
    "            key=\"DMS_REPLICATION_TASK_ARN\",\n",
    "            value=self.dms_replication_task.ref,  # appears `ref` means arn\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamoDBService(Construct):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Construct,\n",
    "        construct_id: str,\n",
    "        environment: dict,\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id)  # required\n",
    "        self.dynamodb_table = dynamodb.Table(\n",
    "            self,\n",
    "            \"DynamoDBTableToCDCToRedshift\",\n",
    "            partition_key=dynamodb.Attribute(\n",
    "                name=\"id\", type=dynamodb.AttributeType.STRING\n",
    "            ),\n",
    "            stream=dynamodb.StreamViewType.NEW_IMAGE,\n",
    "            # CDK wil not automatically deleted DynamoDB during `cdk destroy`\n",
    "            # (as DynamoDB is a stateful resource) unless explicitly specified by the following line\n",
    "            removal_policy=RemovalPolicy.DESTROY,\n",
    "        )\n",
    "        self.cdc_from_dynamodb_to_redshift_s3_bucket = s3.Bucket(\n",
    "            self,\n",
    "            \"DynamoDBStreamToRedshiftS3Bucket\",\n",
    "            removal_policy=RemovalPolicy.DESTROY,\n",
    "            auto_delete_objects=True,\n",
    "        )\n",
    "\n",
    "        self.load_data_to_dynamodb_lambda = _lambda.Function(\n",
    "            self,\n",
    "            \"LoadDataToDynamoDBLambda\",\n",
    "            runtime=_lambda.Runtime.PYTHON_3_9,\n",
    "            code=_lambda.Code.from_asset(\n",
    "                \"source/load_data_to_dynamodb_lambda\",\n",
    "                exclude=[\".venv/*\"],\n",
    "            ),\n",
    "            handler=\"handler.lambda_handler\",\n",
    "            timeout=Duration.seconds(3),  # should be fairly quick\n",
    "            memory_size=128,  # in MB\n",
    "            environment={\"JSON_FILENAME\": environment[\"JSON_FILENAME\"]},\n",
    "        )\n",
    "        self.write_dynamodb_stream_to_s3_lambda = _lambda.Function(\n",
    "            self,\n",
    "            \"WriteDynamoDBStreamToS3Lambda\",\n",
    "            runtime=_lambda.Runtime.PYTHON_3_9,\n",
    "            code=_lambda.Code.from_asset(\n",
    "                \"source/write_dynamodb_stream_to_s3_lambda\",\n",
    "                exclude=[\".venv/*\"],\n",
    "            ),\n",
    "            handler=\"handler.lambda_handler\",\n",
    "            timeout=Duration.seconds(3),  # should be fairly quick\n",
    "            memory_size=128,  # in MB\n",
    "            environment={  # apparently \"AWS_REGION\" is not allowed as a Lambda env variable\n",
    "                \"AWSREGION\": environment[\"AWS_REGION\"],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # connect the AWS resources\n",
    "        self.load_data_to_dynamodb_lambda.add_environment(\n",
    "            key=\"DYNAMODB_TABLE_NAME\", value=self.dynamodb_table.table_name\n",
    "        )\n",
    "        self.dynamodb_table.grant_write_data(self.load_data_to_dynamodb_lambda)\n",
    "        self.write_dynamodb_stream_to_s3_lambda.add_environment(\n",
    "            key=\"S3_FOR_DYNAMODB_STREAM_TO_REDSHIFT\",\n",
    "            value=self.cdc_from_dynamodb_to_redshift_s3_bucket.bucket_name,\n",
    "        )\n",
    "        self.write_dynamodb_stream_to_s3_lambda.add_event_source(\n",
    "            event_sources.DynamoEventSource(\n",
    "                self.dynamodb_table,\n",
    "                starting_position=_lambda.StartingPosition.LATEST,\n",
    "                # filters=[{\"event_name\": _lambda.FilterRule.is_equal(\"INSERT\")}]\n",
    "            )\n",
    "        )\n",
    "        self.cdc_from_dynamodb_to_redshift_s3_bucket.grant_write(\n",
    "            self.write_dynamodb_stream_to_s3_lambda\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDCFromDynamoDBToRedshiftService(Construct):\n",
    "    def __init__(\n",
    "        self,\n",
    "        scope: Construct,\n",
    "        construct_id: str,\n",
    "        environment: dict,\n",
    "        cdc_from_dynamodb_to_redshift_s3_bucket: s3.Bucket,\n",
    "        redshift_endpoint_address: str,\n",
    "        redshift_role_arn: str,\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id)  # required\n",
    "        self.lambda_redshift_full_access_role = iam.Role(\n",
    "            self,\n",
    "            \"LambdaRedshiftFullAccessRole\",\n",
    "            assumed_by=iam.ServicePrincipal(\"lambda.amazonaws.com\"),\n",
    "            managed_policies=[\n",
    "                iam.ManagedPolicy.from_aws_managed_policy_name(\n",
    "                    \"service-role/AWSLambdaBasicExecutionRole\"\n",
    "                ),\n",
    "                iam.ManagedPolicy.from_aws_managed_policy_name(\n",
    "                    \"AmazonRedshiftFullAccess\"\n",
    "                ),  ### later principle of least privileges\n",
    "            ],\n",
    "        )\n",
    "        self.load_s3_files_from_dynamodb_stream_to_redshift_lambda = _lambda.Function(\n",
    "            self,\n",
    "            \"LoadS3FilesFromDynamoDBStreamToRedshiftLambda\",\n",
    "            runtime=_lambda.Runtime.PYTHON_3_9,\n",
    "            code=_lambda.Code.from_asset(\n",
    "                \"source/load_s3_files_from_dynamodb_stream_to_redshift_lambda\",\n",
    "                exclude=[\".venv/*\"],\n",
    "            ),\n",
    "            handler=\"handler.lambda_handler\",\n",
    "            timeout=Duration.seconds(10),  # may take some time if many files\n",
    "            memory_size=128,  # in MB\n",
    "            environment={\n",
    "                \"REDSHIFT_USER\": environment[\"REDSHIFT_USER\"],\n",
    "                \"REDSHIFT_DATABASE_NAME\": environment[\"REDSHIFT_DATABASE_NAME\"],\n",
    "                \"REDSHIFT_SCHEMA_NAME\": environment[\"REDSHIFT_SCHEMA_NAME\"],\n",
    "                \"REDSHIFT_TABLE_NAME\": environment[\"REDSHIFT_TABLE_NAME\"],\n",
    "                \"AWSREGION\": environment[\n",
    "                    \"AWS_REGION\"\n",
    "                ],  # apparently \"AWS_REGION\" is not allowed as a Lambda env variable\n",
    "            },\n",
    "            role=self.lambda_redshift_full_access_role,\n",
    "        )\n",
    "\n",
    "        # connect the AWS resources\n",
    "        lambda_environment_variables = {\n",
    "            \"S3_FOR_DYNAMODB_STREAM_TO_REDSHIFT\": cdc_from_dynamodb_to_redshift_s3_bucket.bucket_name,\n",
    "            \"REDSHIFT_ENDPOINT_ADDRESS\": redshift_endpoint_address,\n",
    "            \"REDSHIFT_ROLE_ARN\": redshift_role_arn,\n",
    "        }\n",
    "        for key, value in lambda_environment_variables.items():\n",
    "            self.load_s3_files_from_dynamodb_stream_to_redshift_lambda.add_environment(\n",
    "                key=key, value=value\n",
    "            )\n",
    "        cdc_from_dynamodb_to_redshift_s3_bucket.grant_read_write(\n",
    "            self.load_s3_files_from_dynamodb_stream_to_redshift_lambda\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDCStack(Stack):\n",
    "    def __init__(\n",
    "        self, scope: Construct, construct_id: str, environment: dict, **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(scope, construct_id, **kwargs)\n",
    "        self.default_vpc = ec2.Vpc.from_lookup(self, \"DefaultVPC\", is_default=True)\n",
    "        self.security_group_for_rds_redshift_dms = ec2.SecurityGroup(\n",
    "            self,\n",
    "            \"SecurityGroupForRDSRedshiftDMS\",\n",
    "            vpc=self.default_vpc,\n",
    "            allow_all_outbound=True,\n",
    "        )\n",
    "        self.security_group_for_rds_redshift_dms.add_ingress_rule(  # for RDS + DMS\n",
    "            peer=ec2.Peer.any_ipv4(),\n",
    "            connection=ec2.Port.tcp(environment[\"RDS_PORT\"]),\n",
    "        )\n",
    "        self.security_group_for_rds_redshift_dms.add_ingress_rule(  # for Redshift + DMS\n",
    "            peer=ec2.Peer.any_ipv4(),\n",
    "            connection=ec2.Port.tcp(environment[\"REDSHIFT_PORT\"]),\n",
    "        )\n",
    "\n",
    "        self.redshift_service = RedshiftService(\n",
    "            self,\n",
    "            \"RedshiftService\",\n",
    "            environment=environment,\n",
    "            security_group=self.security_group_for_rds_redshift_dms,\n",
    "        )\n",
    "        self.rds_service = RDSService(\n",
    "            self,\n",
    "            \"RDSService\",\n",
    "            environment=environment,\n",
    "            vpc=self.default_vpc,\n",
    "            security_group=self.security_group_for_rds_redshift_dms,\n",
    "        )\n",
    "        self.cdc_from_rds_to_redshift_service = CDCFromRDSToRedshiftService(\n",
    "            self,\n",
    "            \"CDCFromRDSToRedshiftService\",\n",
    "            environment=environment,\n",
    "            rds_endpoint_address=self.rds_service.rds_instance.db_instance_endpoint_address,\n",
    "            redshift_endpoint_address=self.redshift_service.redshift_cluster.attr_endpoint_address,\n",
    "            security_group_id=self.security_group_for_rds_redshift_dms.security_group_id,\n",
    "        )\n",
    "        self.dynamodb_service = DynamoDBService(\n",
    "            self, \"DynamoDBService\", environment=environment\n",
    "        )\n",
    "        self.cdc_from_dynamodb_to_redshift_service = CDCFromDynamoDBToRedshiftService(\n",
    "            self,\n",
    "            \"CDCFromDynamoDBToRedshiftService\",\n",
    "            environment=environment,\n",
    "            cdc_from_dynamodb_to_redshift_s3_bucket=self.dynamodb_service.cdc_from_dynamodb_to_redshift_s3_bucket,\n",
    "            redshift_endpoint_address=self.redshift_service.redshift_cluster.attr_endpoint_address,  # appears redshift_cluster.attr_id is broken,\n",
    "            redshift_role_arn=self.redshift_service.redshift_full_commands_full_access_role.role_arn,\n",
    "        )\n",
    "\n",
    "        # schedule Lambdas to run\n",
    "        self.scheduled_eventbridge_event = events.Rule(\n",
    "            self,\n",
    "            \"RunEvery5Minutes\",\n",
    "            event_bus=None,  # scheduled events must be on \"default\" bus\n",
    "            schedule=events.Schedule.rate(Duration.minutes(5)),\n",
    "        )\n",
    "        lambda_functions = [\n",
    "            self.rds_service.load_data_to_rds_lambda,\n",
    "            self.cdc_from_rds_to_redshift_service.start_dms_replication_task_lambda,\n",
    "            self.dynamodb_service.load_data_to_dynamodb_lambda,\n",
    "            self.cdc_from_dynamodb_to_redshift_service.load_s3_files_from_dynamodb_stream_to_redshift_lambda,\n",
    "        ]\n",
    "        for lambda_function in lambda_functions:\n",
    "            self.scheduled_eventbridge_event.add_target(\n",
    "                target=events_targets.LambdaFunction(\n",
    "                    handler=lambda_function,\n",
    "                    retry_attempts=3,\n",
    "                    ### then put in DLQ\n",
    "                ),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = cdk.App()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_REGION = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = boto3.client(\"sts\").get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = {\n",
    "            \"AWS_REGION\": \"us-east-1\",\n",
    "            \"CSV_FILENAME\": \"txns.csv\",\n",
    "            \"JSON_FILENAME\": \"trades.json\",\n",
    "\n",
    "            \"RDS_USER\": \"admin\",\n",
    "            \"RDS_PASSWORD\": \"password\",\n",
    "            \"RDS_DATABASE_NAME\": \"rds_to_redshift_database\",\n",
    "            \"RDS_TABLE_NAME\": \"rds_cdc_table\",\n",
    "            \"RDS_PORT\": 3306,\n",
    "\n",
    "            \"REDSHIFT_USER\": \"admin\",\n",
    "            \"REDSHIFT_PASSWORD\": \"Password1\",\n",
    "            \"REDSHIFT_DATABASE_NAME\": \"redshift_database\",\n",
    "            \"REDSHIFT_SCHEMA_NAME\": \"dynamodb_schema\",\n",
    "            \"REDSHIFT_TABLE_NAME\": \"dynamodb_stream_cdc\",\n",
    "            \"REDSHIFT_PORT\": 5439\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = CDCStack(\n",
    "    app,\n",
    "    \"CDCStack\",\n",
    "    env=cdk.Environment(account=account, region=environment[\"AWS_REGION\"]),\n",
    "    environment=environment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.synth()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
