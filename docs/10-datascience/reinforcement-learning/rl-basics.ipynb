{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_iFAN-MxTto"
      },
      "source": [
        "# Introduction to Reinforcement Learning (Playground)\n",
        "> Learn RL Q-learning by simulating a discrete and continuous toy environment manually as well as with Gym library\n",
        "\n",
        "- toc: true\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [RL, Gym, Playground, Concept, QLearning]\n",
        "- image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTSs3Y8Lv5_w"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh1c5II-xTts"
      },
      "source": [
        "| |  |\n",
        "| :-: | -:|\n",
        "| Vision | Build reinforcement learning based recommender agents that can learn and adjust behavior in real-time|\n",
        "| Mission | To achieve big, we have to start small and this tutorial is one step towards our vision. Our mission is to learn Q-learning policy based method. |\n",
        "| Scope | Policy-based RL methods, Playground |\n",
        "| Task | Maximize the reward in discrete environment, Learn continuous reward policy to maximize reward in continuous environment|\n",
        "| Data | Simulation |\n",
        "| Tool | Gym, Colab, Python, OpenCV |\n",
        "| Technique | Q-learning policy in discrete and continuous environments |\n",
        "| Process | 1) Design a simple 2-D board-based discrete environment, 2) Train RL agent using Randomwalk as well as Q-policy with Bellman, 3) Increase the environment complexity by adding more variables/contraints, 4) Setup Gym Cartpole environment, 5) Train RL agent on Cartpole continuous environment  |\n",
        "| Takeaway | RL has potential to be used in recommender systems |\n",
        "| Credit | [Microsoft, Dmitry](https://github.com/microsoft/ML-For-Beginners/tree/main/8-Reinforcement) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtxNqfHhvmbb"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SMJShY-vplY"
      },
      "source": [
        "We will train machine learning algorithms that will help Peter:\n",
        "\n",
        "- **Explore** the surrounding area and build an optimal navigation map\n",
        "- **Learn** how to use a skateboard and balance on it, in order to move around faster.\n",
        "\n",
        "> [Peter and the Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) is a musical fairy tale written by a Russian composer Sergei Prokofiev. It is a story about young pioneer Peter, who bravely goes out of his house to the forest clearing to chase the wolf."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![1](https://user-images.githubusercontent.com/62965911/224305123-a63b509c-d239-4c9a-bc0a-37bbe6816fca.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG0w_lLOvzy5"
      },
      "source": [
        "### Background\n",
        "\n",
        "Instead of looking for existing game data, **Reinforcement Learning** (RL) is based on the idea of *making the computer play* many times and observing the result. Thus, to apply Reinforcement Learning, we need two things:\n",
        "\n",
        "- **An environment** and **a simulator** which allow us to play a game many times. This simulator would define all the game rules as well as possible states and actions.\n",
        "- **A reward function**, which would tell us how well we did during each move or game.\n",
        "\n",
        "The main difference between other types of machine learning and RL is that in RL we typically do not know whether we win or lose until we finish the game. Thus, we cannot say whether a certain move alone is good or not - we only receive a reward at the end of the game. And our goal is to design algorithms that will allow us to train a model under uncertain conditions. We will learn about one RL algorithm called **Q-learning**.\n",
        "\n",
        "Reinforcement learning involves three important concepts: the agent, some states, and a set of actions per state. By executing an action in a specified state, the agent is given a reward. Again imagine the computer game Super Mario. You are Mario, you are in a game level, standing next to a cliff edge. Above you is a coin. You being Mario, in a game level, at a specific position ... that's your state. Moving one step to the right (an action) will take you over the edge, and that would give you a low numerical score. However, pressing the jump button would let score a point and you would stay alive. That's a positive outcome and that should award you a positive numerical score.\n",
        "\n",
        "By using reinforcement learning and a simulator (the game), you can learn how to play the game to maximize the reward which is staying alive and scoring as many points as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-7EMawPwCAP"
      },
      "source": [
        "### Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvW2bwhJlmSe"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/recohut/reco-static/raw/master/media/images/ms8rl/images.zip\n",
        "!unzip images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpJ-EiqpwFIg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3b7vlXdjkEQ"
      },
      "source": [
        "## Section 1\n",
        "\n",
        "In this section, we will learn how to apply Reinforcement learning to a problem of path finding. The setting is inspired by [Peter and the Wolf](https://en.wikipedia.org/wiki/Peter_and_the_Wolf) musical fairy tale by Russian composer [Sergei Prokofiev](https://en.wikipedia.org/wiki/Sergei_Prokofiev). It is a story about young pioneer Peter, who bravely goes out of his house to the forest clearing to chase the wolf. We will train machine learning algorithms that will help Peter to explore the surroinding area and build an optimal navigation map."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![2](https://user-images.githubusercontent.com/62965911/224305168-025c1a66-92a8-4b78-8824-80bd9b58cf70.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NdtWJ5Ivb6P"
      },
      "source": [
        "Peter and his friends need to escape the hungry wolf! Image by [Jen Looper](https://twitter.com/jenlooper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdkzWlIwQm5"
      },
      "source": [
        "### Basic helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCIxnOM6wPeT"
      },
      "outputs": [],
      "source": [
        "def clip(min,max,x):\n",
        "    if x<min:\n",
        "        return min\n",
        "    if x>max:\n",
        "        return max\n",
        "    return x\n",
        "\n",
        "def imload(fname,size):\n",
        "    img = cv2.imread(fname)\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img,(size,size),interpolation=cv2.INTER_LANCZOS4)\n",
        "    img = img / np.max(img)\n",
        "    return img\n",
        "\n",
        "def draw_line(dx,dy,size=50):\n",
        "    p=np.ones((size-2,size-2,3))\n",
        "    if dx==0:\n",
        "        dx=0.001\n",
        "    m = (size-2)//2\n",
        "    l = math.sqrt(dx*dx+dy*dy)*(size-4)/2\n",
        "    a = math.atan(dy/dx)\n",
        "    cv2.line(p,(int(m-l*math.cos(a)),int(m-l*math.sin(a))),(int(m+l*math.cos(a)),int(m+l*math.sin(a))),(0,0,0),1)\n",
        "    s = -1 if dx<0 else 1\n",
        "    cv2.circle(p,(int(m+s*l*math.cos(a)),int(m+s*l*math.sin(a))),3,0)\n",
        "    return p   \n",
        "\n",
        "def probs(v):\n",
        "    v = v-v.min()\n",
        "    if (v.sum()>0):\n",
        "        v = v/v.sum()\n",
        "    return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIfDdsvsqNCd"
      },
      "source": [
        "### The Environment\n",
        "\n",
        "For simplicity, let's consider Peter's world to be a square board of size `width` x `height`. Each cell in this board can either be:\n",
        "* **ground**, on which Peter and other creatures can walk\n",
        "* **water**, on which you obviously cannot walk\n",
        "* **a tree** or **grass** - a place where you cat take some rest\n",
        "* **an apple**, which represents something Peter would be glad to find in order to feed himself\n",
        "* **a wolf**, which is dangerous and should be avoided"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxjhQ-N4wTwp"
      },
      "source": [
        "### Creating `Board` environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZhINF4jNCYmO"
      },
      "outputs": [],
      "source": [
        "class Board:\n",
        "    class Cell:\n",
        "        empty = 0\n",
        "        water = 1\n",
        "        wolf = 2\n",
        "        tree = 3\n",
        "        apple = 4\n",
        "    def __init__(self,width,height,size=50):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.size = size+2\n",
        "        self.matrix = np.zeros((width,height))\n",
        "        self.grid_color = (0.6,0.6,0.6)\n",
        "        self.background_color = (1.0,1.0,1.0)\n",
        "        self.grid_thickness = 1\n",
        "        self.grid_line_type = cv2.LINE_AA\n",
        "        self.pics = {\n",
        "            \"wolf\" : imload('images/wolf.png',size-4),\n",
        "            \"apple\" : imload('images/apple.png',size-4),\n",
        "            \"human\" : imload('images/human.png',size-4)\n",
        "        }\n",
        "        self.human = (0,0)\n",
        "        self.frame_no = 0\n",
        "\n",
        "    def randomize(self,water_size=5, num_water=3, num_wolves=1, num_trees=5, num_apples=3,seed=None):\n",
        "        if seed:\n",
        "            random.seed(seed)\n",
        "        for _ in range(num_water):\n",
        "            x = random.randint(0,self.width-1)\n",
        "            y = random.randint(0,self.height-1)\n",
        "            for _ in range(water_size):\n",
        "                self.matrix[x,y] = Board.Cell.water\n",
        "                x = clip(0,self.width-1,x+random.randint(-1,1))\n",
        "                y = clip(0,self.height-1,y+random.randint(-1,1))\n",
        "        for _ in range(num_trees):\n",
        "            while True:\n",
        "                x = random.randint(0,self.width-1)\n",
        "                y = random.randint(0,self.height-1)\n",
        "                if self.matrix[x,y]==Board.Cell.empty:\n",
        "                    self.matrix[x,y] = Board.Cell.tree # tree\n",
        "                    break\n",
        "        for _ in range(num_wolves):\n",
        "            while True:\n",
        "                x = random.randint(0,self.width-1)\n",
        "                y = random.randint(0,self.height-1)\n",
        "                if self.matrix[x,y]==Board.Cell.empty:\n",
        "                    self.matrix[x,y] = Board.Cell.wolf # wolf\n",
        "                    break\n",
        "        for _ in range(num_apples):\n",
        "            while True:\n",
        "                x = random.randint(0,self.width-1)\n",
        "                y = random.randint(0,self.height-1)\n",
        "                if self.matrix[x,y]==Board.Cell.empty:\n",
        "                    self.matrix[x,y] = Board.Cell.apple\n",
        "                    break\n",
        "\n",
        "    def at(self,pos=None):\n",
        "        if pos:\n",
        "            return self.matrix[pos[0],pos[1]]\n",
        "        else:\n",
        "            return self.matrix[self.human[0],self.human[1]]\n",
        "\n",
        "    def is_valid(self,pos):\n",
        "        return pos[0]>=0 and pos[0]<self.width and pos[1]>=0 and pos[1] < self.height\n",
        "\n",
        "    def move_pos(self, pos, dpos):\n",
        "        return (pos[0] + dpos[0], pos[1] + dpos[1])\n",
        "\n",
        "    def move(self,dpos,check_correctness=True):\n",
        "        new_pos = self.move_pos(self.human,dpos)\n",
        "        if self.is_valid(new_pos) or not check_correctness:\n",
        "            self.human = new_pos\n",
        "\n",
        "    def random_pos(self):\n",
        "        x = random.randint(0,self.width-1)\n",
        "        y = random.randint(0,self.height-1)\n",
        "        return (x,y)\n",
        "\n",
        "    def random_start(self):\n",
        "        while True:\n",
        "            pos = self.random_pos()\n",
        "            if self.at(pos) == Board.Cell.empty:\n",
        "                self.human = pos\n",
        "                break\n",
        "\n",
        "\n",
        "    def image(self,Q=None):\n",
        "        img = np.zeros((self.height*self.size+1,self.width*self.size+1,3))\n",
        "        img[:,:,:] = self.background_color\n",
        "        # Draw water\n",
        "        for x in range(self.width):\n",
        "            for y in range(self.height):\n",
        "                if (x,y) == self.human:\n",
        "                    ov = self.pics['human']\n",
        "                    img[self.size*y+2:self.size*y+ov.shape[0]+2,self.size*x+2:self.size*x+2+ov.shape[1],:] = np.minimum(ov,1.0)\n",
        "                    continue\n",
        "                if self.matrix[x,y] == Board.Cell.water:\n",
        "                    img[self.size*y:self.size*(y+1),self.size*x:self.size*(x+1),:] = (0,0,1.0)\n",
        "                if self.matrix[x,y] == Board.Cell.wolf:\n",
        "                    ov = self.pics['wolf']\n",
        "                    img[self.size*y+2:self.size*y+ov.shape[0]+2,self.size*x+2:self.size*x+2+ov.shape[1],:] = np.minimum(ov,1.0)\n",
        "                if self.matrix[x,y] == Board.Cell.apple: # apple\n",
        "                    ov = self.pics['apple']\n",
        "                    img[self.size*y+2:self.size*y+ov.shape[0]+2,self.size*x+2:self.size*x+2+ov.shape[1],:] = np.minimum(ov,1.0)\n",
        "                if self.matrix[x,y] == Board.Cell.tree: # tree\n",
        "                    img[self.size*y:self.size*(y+1),self.size*x:self.size*(x+1),:] = (0,1.0,0)\n",
        "                if self.matrix[x,y] == Board.Cell.empty and Q is not None:\n",
        "                    p = probs(Q[x,y])\n",
        "                    dx,dy = 0,0\n",
        "                    for i,(ddx,ddy) in enumerate([(-1,0),(1,0),(0,-1),(0,1)]):\n",
        "                        dx += ddx*p[i]\n",
        "                        dy += ddy*p[i]\n",
        "                        l = draw_line(dx,dy,self.size)\n",
        "                        img[self.size*y+2:self.size*y+l.shape[0]+2,self.size*x+2:self.size*x+2+l.shape[1],:] = l\n",
        "\n",
        "        # Draw grid\n",
        "        for i in range(self.height+1):\n",
        "            img[:,i*self.size] = 0.3\n",
        "            #cv2.line(img,(0,i*self.size),(self.width*self.size,i*self.size), self.grid_color, self.grid_thickness,lineType=self.grid_line_type)\n",
        "        for j in range(self.width+1):\n",
        "            img[j*self.size,:] = 0.3\n",
        "            #cv2.line(img,(j*self.size,0),(j*self.size,self.height*self.size), self.grid_color, self.grid_thickness,lineType=self.grid_line_type)\n",
        "        return img\n",
        "\n",
        "    def plot(self,Q=None):\n",
        "        plt.figure(figsize=(11,6))\n",
        "        plt.imshow(self.image(Q),interpolation='hanning')\n",
        "\n",
        "    def saveimage(self,filename,Q=None):\n",
        "        cv2.imwrite(filename,255*self.image(Q)[...,::-1])\n",
        "\n",
        "    def walk(self,policy,save_to=None,start=None):\n",
        "        n = 0\n",
        "        if start:\n",
        "            self.human = start\n",
        "        else:\n",
        "            self.random_start()\n",
        "\n",
        "        while True:\n",
        "            if save_to:\n",
        "                self.saveimage(save_to.format(self.frame_no))\n",
        "                self.frame_no+=1\n",
        "            if self.at() == Board.Cell.apple:\n",
        "                return n # success!\n",
        "            if self.at() in [Board.Cell.wolf, Board.Cell.water]:\n",
        "                return -1 # eaten by wolf or drowned\n",
        "            while True:\n",
        "                a = policy(self)\n",
        "                new_pos = self.move_pos(self.human,a)\n",
        "                if self.is_valid(new_pos) and self.at(new_pos)!=Board.Cell.water:\n",
        "                    self.move(a) # do the actual move\n",
        "                    break\n",
        "            n+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLNTBzV9jkEX"
      },
      "source": [
        "Let's now create a random board and see how it looks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "zBWRzsdejkEY",
        "outputId": "54d9c446-770d-40d9-fc2f-514bd61434a4"
      },
      "outputs": [],
      "source": [
        "width, height = 8,8\n",
        "m = Board(width,height)\n",
        "m.randomize(seed=13)\n",
        "m.plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![3](https://user-images.githubusercontent.com/62965911/224305174-0de09ffe-66a6-4dae-aff0-3658db0bc9e0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCnF08TMjkEa"
      },
      "source": [
        "### Actions and Policy\n",
        "\n",
        "In our example, Peter's goal would be to find an apple, while avoiding the wolf and other obstacles. To do this, he can essentially walk around until he finds an apple. Therefore, at any position he can chose between one of the following actions: up, down, left and right. We will define those actions as a dictionary, and map them to pairs of corresponding coordinate changes. For example, moving right (`R`) would correspond to a pair `(1,0)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QgjSRrEcjkEb"
      },
      "outputs": [],
      "source": [
        "actions = { \"U\" : (0,-1), \"D\" : (0,1), \"L\" : (-1,0), \"R\" : (1,0) }\n",
        "action_idx = { a : i for i,a in enumerate(actions.keys()) }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkFJeg3RjkEd"
      },
      "source": [
        "The strategy of our agent (Peter) is defined by a so-called **policy**. Let's consider the simplest policy called **random walk**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDl_6XWgqIfc"
      },
      "source": [
        "### Random walk\n",
        "\n",
        "Let's first solve our problem by implementing a random walk strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpycj37QjkEd",
        "outputId": "4ca47030-d79a-47ca-ce43-1239802fe7b5",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def random_policy(m):\n",
        "    return random.choice(list(actions))\n",
        "\n",
        "def walk(m,policy,start_position=None):\n",
        "    n = 0 # number of steps\n",
        "    # set initial position\n",
        "    if start_position:\n",
        "        m.human = start_position \n",
        "    else:\n",
        "        m.random_start()\n",
        "    while True:\n",
        "        if m.at() == Board.Cell.apple:\n",
        "            return n # success!\n",
        "        if m.at() in [Board.Cell.wolf, Board.Cell.water]:\n",
        "            return -1 # eaten by wolf or drowned\n",
        "        while True:\n",
        "            a = actions[policy(m)]\n",
        "            new_pos = m.move_pos(m.human,a)\n",
        "            if m.is_valid(new_pos) and m.at(new_pos)!=Board.Cell.water:\n",
        "                m.move(a) # do the actual move\n",
        "                break\n",
        "        n+=1\n",
        "\n",
        "walk(m,random_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5j0t1ckjkEf"
      },
      "source": [
        "Let's run random walk experiment several times and see the average number of steps taken:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQRd5UD5jkEf",
        "outputId": "7dd6d449-749c-4598-a07e-14bcfbc0bfc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average path length = 32.87096774193548, eaten by wolf: 7 times\n"
          ]
        }
      ],
      "source": [
        "def print_statistics(policy):\n",
        "    s,w,n = 0,0,0\n",
        "    for _ in range(100):\n",
        "        z = walk(m,policy)\n",
        "        if z<0:\n",
        "            w+=1\n",
        "        else:\n",
        "            s += z\n",
        "            n += 1\n",
        "    print(f\"Average path length = {s/n}, eaten by wolf: {w} times\")\n",
        "\n",
        "print_statistics(random_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "S6G_1vde1-vF",
        "outputId": "dd50097e-5d46-426b-e5db-9c5b70e38b7f"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(open('images/random_walk.gif','rb').read())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![4](https://user-images.githubusercontent.com/62965911/224305175-2ab63d36-dd98-49f8-9a3e-1ad244e0a239.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUyie9kjkEg"
      },
      "source": [
        "### Reward Function\n",
        "\n",
        "To make our policy more intelligent, we need to understand which moves are \"better\" than others.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7JUe6SsUjkEg"
      },
      "outputs": [],
      "source": [
        "move_reward = -0.1\n",
        "goal_reward = 10\n",
        "end_reward = -10\n",
        "\n",
        "def reward(m,pos=None):\n",
        "    pos = pos or m.human\n",
        "    if not m.is_valid(pos):\n",
        "        return end_reward\n",
        "    x = m.at(pos)\n",
        "    if x==Board.Cell.water or x == Board.Cell.wolf:\n",
        "        return end_reward\n",
        "    if x==Board.Cell.apple:\n",
        "        return goal_reward\n",
        "    return move_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGp5RMaLjkEh"
      },
      "source": [
        "### Q-Learning\n",
        "\n",
        "Build a Q-Table, or multi-dimensional array. Since our board has dimensions `width` x `height`, we can represent Q-Table by a numpy array with shape `width` x `height` x `len(actions)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "USWH9c0AjkEh"
      },
      "outputs": [],
      "source": [
        "Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ettSq3VjkEi"
      },
      "source": [
        "Pass the Q-Table to the plot function in order to visualize the table on the board:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "16tqn0OvjkEi",
        "outputId": "37013049-c7e9-4d1d-ab6c-24475202199d"
      },
      "outputs": [],
      "source": [
        "m.plot(Q)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![5](https://user-images.githubusercontent.com/62965911/224305182-a17152e5-477b-4442-b1e2-00f58389fc4b.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evws9q19sVvF"
      },
      "source": [
        "In the center of each cell there is an \"arrow\" that indicates the preferred direction of movement. Since all directions are equal, a dot is displayed.\n",
        "\n",
        "Now we need to run the simulation, explore our environment, and learn a better distribution of Q-Table values, which will allow us to find the path to the apple much faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhQsmbx5jkEi"
      },
      "source": [
        "### Essence of Q-Learning: Bellman Equation and  Learning Algorithm\n",
        "\n",
        "Let's now write a pseudo-code for our leaning algorithm:\n",
        "\n",
        "* Initialize Q-Table Q with equal numbers for all states and actions\n",
        "* Set learning rate $\\alpha\\leftarrow 1$\n",
        "* Repeat simulation many times\n",
        "   1. Start at random position\n",
        "   1. Repeat\n",
        "        1. Select an action $a$ at state $s$\n",
        "        2. Exectute action by moving to a new state $s'$\n",
        "        3. If we encounter end-of-game condition, or total reward is too small - exit simulation  \n",
        "        4. Compute reward $r$ at the new state\n",
        "        5. Update Q-Function according to Bellman equation: $Q(s,a)\\leftarrow (1-\\alpha)Q(s,a)+\\alpha(r+\\gamma\\max_{a'}Q(s',a'))$\n",
        "        6. $s\\leftarrow s'$\n",
        "        7. Update total reward and decrease $\\alpha$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF1fEh4Hws9F"
      },
      "source": [
        "### Exploit vs. Explore\n",
        "\n",
        "The best approach is to balance between exploration and exploitation. As we learn more about our environment, we would be more likely to follow the optimal route, however, choosing the unexplored path once in a while."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3WOMk4hwvOc"
      },
      "source": [
        "### Python Implementation\n",
        "\n",
        "Now we are ready to implement the learning algorithm. Before that, we also need some function that will convert arbitrary numbers in the Q-Table into a vector of probabilities for corresponding actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7IR4kLtBjkEj"
      },
      "outputs": [],
      "source": [
        "def probs(v,eps=1e-4):\n",
        "    v = v-v.min()+eps\n",
        "    v = v/v.sum()\n",
        "    return v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6usMtFQvjkEj"
      },
      "source": [
        "We add a small amount of `eps` to the original vector in order to avoid division by 0 in the initial case, when all components of the vector are identical.\n",
        "\n",
        "The actual learning algorithm we will run for 10000 experiments, also called **epochs**: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ4XcabCjkEk",
        "outputId": "2618f33f-c04e-46f2-bb59-c2f1c6e036a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch = 9999 0 steps\r"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "lpath = []\n",
        "\n",
        "for epoch in range(10000):\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Epoch = {epoch}\",end='')\n",
        "\n",
        "    # Pick initial point\n",
        "    m.random_start()\n",
        "    \n",
        "    # Start travelling\n",
        "    n=0\n",
        "    cum_reward = 0\n",
        "    while True:\n",
        "        x,y = m.human\n",
        "        v = probs(Q[x,y])\n",
        "        a = random.choices(list(actions),weights=v)[0]\n",
        "        dpos = actions[a]\n",
        "        m.move(dpos,check_correctness=False) # we allow player to move outside the board, which terminates episode\n",
        "        r = reward(m)\n",
        "        cum_reward += r\n",
        "        if r==end_reward or cum_reward < -1000:\n",
        "            print(f\" {n} steps\",end='\\r')\n",
        "            lpath.append(n)\n",
        "            break\n",
        "        alpha = np.exp(-n / 3000)\n",
        "        gamma = 0.5\n",
        "        ai = action_idx[a]\n",
        "        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())\n",
        "        n+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCkoL3jQjkEk"
      },
      "source": [
        "After executing this algorithm, the Q-Table should be updated with values that define the attractiveness of different actions at each step. Visualize the table here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "6GiDSyoajkEl",
        "outputId": "2233e4d9-741d-4c2d-e9fc-61352bfed3b0"
      },
      "outputs": [],
      "source": [
        "m.plot(Q)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![6](https://user-images.githubusercontent.com/62965911/224305189-aa03ed20-e184-4d98-86ba-4e06fc3a6f9c.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzcMSc9ZjkEl"
      },
      "source": [
        "### Checking the Policy\n",
        "\n",
        "Since Q-Table lists the \"attractiveness\" of each action at each state, it is quite easy to use it to define the efficient navigation in our world. In the simplest case, we can just select the action corresponding to the highest Q-Table value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MerDWi1-jkEl",
        "outputId": "b914e69c-242b-4bad-8ebd-40c439b8cbfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 25,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def qpolicy_strict(m):\n",
        "        x,y = m.human\n",
        "        v = probs(Q[x,y])\n",
        "        a = list(actions)[np.argmax(v)]\n",
        "        return a\n",
        "\n",
        "walk(m,qpolicy_strict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UDtIiMPjkEm"
      },
      "source": [
        "If you try the code above several times, you may notice that sometimes it just \"hangs\", and you need to press the STOP button in the notebook to interrupt it. \n",
        "\n",
        "> **Task 1:** Modify the `walk` function to limit the maximum length of path by a certain number of steps (say, 100), and watch the code above return this value from time to time.\n",
        "\n",
        "> **Task 2:** Modify the `walk` function so that it does not go back to the places where is has already been previously. This will prevent `walk` from looping, however, the agent can still end up being \"trapped\" in a location from which it is unable to escape. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n1K3lv5jkEm",
        "outputId": "68c45f4a-fdfe-40c8-cce6-59c8f0107660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average path length = 3.45, eaten by wolf: 0 times\n"
          ]
        }
      ],
      "source": [
        "def qpolicy(m):\n",
        "        x,y = m.human\n",
        "        v = probs(Q[x,y])\n",
        "        a = random.choices(list(actions),weights=v)[0]\n",
        "        return a\n",
        "\n",
        "print_statistics(qpolicy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blDX_QH5jkEn"
      },
      "source": [
        "### Investigating the Learning Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "MnUwchoFjkEn",
        "outputId": "a7d04c66-bd3c-4efe-cd1c-947eead2ca40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f95090412d0>]"
            ]
          },
          "execution_count": 27,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(lpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NIAwaSDjkEn"
      },
      "source": [
        "What we see here is that at first the average path length increased. This is probably due to the fact that when we know nothing about the environment - we are likely to get trapped into bad states, water or wolf. As we learn more and start using this knowledge, we can explore the environment for longer, but we still do not know well where apples are.\n",
        "\n",
        "Once we learn enough, it becomes easier for the agent to achieve the goal, and the path length starts to decrease. However, we are still open to exploration, so we often diverge away from the best path, and explore new options, making the path longer than optimal.\n",
        "\n",
        "What we also observe on this graph, is that at some point the length increased abruptly. This indicates stochastic nature of the process, and that we can at some point \"spoil\" the Q-Table coefficients, by overwriting them with new values. This ideally should be minimized by decreasing learning rate (i.e. towards the end of training we only adjust Q-Table values by a small value).\n",
        "\n",
        "Overall, it is important to remember that the success and quality of the learning process significantly depends on parameters, such as leaning rate, learning rate decay and discount factor. Those are often called **hyperparameters**, to distinguish them from **parameters** which we optimize during training (eg. Q-Table coefficients). The process of finding best hyperparameter values is called **hyperparameter optimization**, and it deserves a separate topic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMd3CzpjjkEo"
      },
      "source": [
        "### Exercise\n",
        "#### A More Realistic Peter and the Wolf World\n",
        "\n",
        "In our situation, Peter was able to move around almost without getting tired or hungry. In a more realistic world, he has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic by implementing the following rules:\n",
        "\n",
        "1. By moving from one place to another, Peter loses **energy** and gains some **fatigue**.\n",
        "2. Peter can gain more energy by eating apples.\n",
        "3. Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)\n",
        "4. Peter needs to find and kill the wolf\n",
        "5. In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.\n",
        "\n",
        "Modify the reward function above according to the rules of the game, run the reinforcement learning algorithm to learn the best strategy for winning the game, and compare the results of random walk with your algorithm in terms of number of games won and lost.\n",
        "\n",
        "\n",
        "> **Note**: You may need to adjust hyperparameters to make it work, especially the number of epochs. Because the success of the game (fighting the wolf) is a rare event, you can expect much longer training time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EfnTPdholDN"
      },
      "source": [
        "## Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXIH5_BGozIG"
      },
      "source": [
        "Let's implement the above exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "ucnJOnt0ozIM",
        "outputId": "feb90774-61dd-4b5d-ff7a-f1e74039b40a"
      },
      "outputs": [],
      "source": [
        "width, height = 8,8\n",
        "m = Board(width,height)\n",
        "m.randomize(seed=13)\n",
        "m.plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![7](https://user-images.githubusercontent.com/62965911/224305190-e43db422-9e56-4df1-87e3-426fa30bbb49.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7vlMUy6uozIO"
      },
      "outputs": [],
      "source": [
        "actions = { \"U\" : (0,-1), \"D\" : (0,1), \"L\" : (-1,0), \"R\" : (1,0) }\n",
        "action_idx = { a : i for i,a in enumerate(actions.keys()) }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIfsGRZ8ozIO"
      },
      "source": [
        "### Defining state\n",
        "\n",
        "In our new game rules, we need to keep track of energy and fatigue at each board state. Thus we will create an object `state` that will carry all required information about current problem state, including state of the board, current levels of energy and fatigue, and whether we can win the wolf while at terminal state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QJJnXKllozIP"
      },
      "outputs": [],
      "source": [
        "class state:\n",
        "    def __init__(self,board,energy=10,fatigue=0,init=True):\n",
        "        self.board = board\n",
        "        self.energy = energy\n",
        "        self.fatigue = fatigue\n",
        "        self.dead = False\n",
        "        if init:\n",
        "            self.board.random_start()\n",
        "        self.update()\n",
        "\n",
        "    def at(self):\n",
        "        return self.board.at()\n",
        "\n",
        "    def update(self):\n",
        "        if self.at() == Board.Cell.water:\n",
        "            self.dead = True\n",
        "            return\n",
        "        if self.at() == Board.Cell.tree:\n",
        "            self.fatigue = 0\n",
        "        if self.at() == Board.Cell.apple:\n",
        "            self.energy = 10\n",
        "\n",
        "    def move(self,a):\n",
        "        self.board.move(a)\n",
        "        self.energy -= 1\n",
        "        self.fatigue += 1\n",
        "        self.update()\n",
        "\n",
        "    def is_winning(self):\n",
        "        return self.energy > self.fatigue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sxKhMjQozIQ"
      },
      "source": [
        "Let's try to solve the problem using random walk and see if we succeed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4cH6LjfozIR",
        "outputId": "c3269034-1283-4178-ff8d-2286021c8a02",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 31,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def random_policy(state):\n",
        "    return random.choice(list(actions))\n",
        "\n",
        "def walk(board,policy):\n",
        "    n = 0 # number of steps\n",
        "    s = state(board)\n",
        "    while True:\n",
        "        if s.at() == Board.Cell.wolf:\n",
        "            if s.is_winning():\n",
        "                return n # success!\n",
        "            else:\n",
        "                return -n # failure!\n",
        "        if s.at() == Board.Cell.water:\n",
        "            return 0 # died\n",
        "        a = actions[policy(m)]\n",
        "        s.move(a)\n",
        "        n+=1\n",
        "\n",
        "walk(m,random_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8AAEPvcozIS",
        "outputId": "77035353-86b4-441c-a704-afd955886c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed by wolf = 5, won: 1 times, drown: 94 times\n"
          ]
        }
      ],
      "source": [
        "def print_statistics(policy):\n",
        "    s,w,n = 0,0,0\n",
        "    for _ in range(100):\n",
        "        z = walk(m,policy)\n",
        "        if z<0:\n",
        "            w+=1\n",
        "        elif z==0:\n",
        "            n+=1\n",
        "        else:\n",
        "            s+=1\n",
        "    print(f\"Killed by wolf = {w}, won: {s} times, drown: {n} times\")\n",
        "\n",
        "print_statistics(random_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEgFPeLAozIS"
      },
      "source": [
        "### Reward Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Me5f2_5kozIT"
      },
      "outputs": [],
      "source": [
        "def reward(s):\n",
        "    r = s.energy-s.fatigue\n",
        "    if s.at()==Board.Cell.wolf:\n",
        "        return 100 if s.is_winning() else -100\n",
        "    if s.at()==Board.Cell.water:\n",
        "        return -100\n",
        "    return r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8fq4SxoozIU"
      },
      "source": [
        "### Q-Learning algorithm\n",
        "\n",
        "The actual learning algorithm stays pretty much unchanged, we will use `state` instead of just board position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ow7pamUeozIU"
      },
      "outputs": [],
      "source": [
        "Q = np.ones((width,height,len(actions)),dtype=np.float)*1.0/len(actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_NU7h54NozIU"
      },
      "outputs": [],
      "source": [
        "def probs(v,eps=1e-4):\n",
        "    v = v-v.min()+eps\n",
        "    v = v/v.sum()\n",
        "    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dhfa2ZRozIV",
        "outputId": "77d8e8e2-f64d-47e7-f5ab-4e9ded7ad8bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch = 9999 48 steps\r"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "lpath = []\n",
        "\n",
        "for epoch in range(10000):\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Epoch = {epoch}\",end='')\n",
        "\n",
        "    # Pick initial point\n",
        "    s = state(m)\n",
        "    \n",
        "    # Start travelling\n",
        "    n=0\n",
        "    cum_reward = 0\n",
        "    while True:\n",
        "        x,y = s.board.human\n",
        "        v = probs(Q[x,y])\n",
        "        while True:\n",
        "            a = random.choices(list(actions),weights=v)[0]\n",
        "            dpos = actions[a]\n",
        "            if s.board.is_valid(s.board.move_pos(s.board.human,dpos)):\n",
        "                break \n",
        "        s.move(dpos)\n",
        "        r = reward(s)\n",
        "        if abs(r)==100: # end of game\n",
        "            print(f\" {n} steps\",end='\\r')\n",
        "            lpath.append(n)\n",
        "            break\n",
        "        alpha = np.exp(-n / 3000)\n",
        "        gamma = 0.5\n",
        "        ai = action_idx[a]\n",
        "        Q[x,y,ai] = (1 - alpha) * Q[x,y,ai] + alpha * (r + gamma * Q[x+dpos[0], y+dpos[1]].max())\n",
        "        n+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "wGi_eDOZozIW",
        "outputId": "d2760985-fcfa-456c-b5d5-693269dff4d0"
      },
      "outputs": [],
      "source": [
        "m.plot(Q)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![8](https://user-images.githubusercontent.com/62965911/224305193-84aed922-a19f-4823-851f-409db6367b3e.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25dRA0QBozIZ"
      },
      "source": [
        "### Results\n",
        "\n",
        "Let's see if we were successful training Peter to fight the wolf!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLOPmQjjozIZ",
        "outputId": "be4c2453-fa71-4c4f-8811-b1959c91997d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Killed by wolf = 1, won: 9 times, drown: 90 times\n"
          ]
        }
      ],
      "source": [
        "def qpolicy(m):\n",
        "        x,y = m.human\n",
        "        v = probs(Q[x,y])\n",
        "        a = random.choices(list(actions),weights=v)[0]\n",
        "        return a\n",
        "\n",
        "print_statistics(qpolicy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEZgpUidozIa"
      },
      "source": [
        "We now see much less cases of drowning, but Peter is still not always able to kill the wolf. Try to experiment and see if you can improve this result by playing with hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "_Re70UjcozIa",
        "outputId": "b366c062-c685-4334-9ca3-eb9350aeed2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9508f2aa10>]"
            ]
          },
          "execution_count": 39,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(lpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkf6gIILss62"
      },
      "source": [
        "## Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df-3tGBlozIb"
      },
      "source": [
        "In this section, we will apply the same principles of Q-Learning to a problem with continuous state, i.e. a state that is given by one or more real numbers. We will deal with the following problem:\n",
        "\n",
        "Problem: If Peter wants to escape from the wolf, he needs to be able to move faster. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![9](https://user-images.githubusercontent.com/62965911/224305197-80e43838-c7d1-4c02-996b-ea3bdd8604e2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmIQwHJasy0U"
      },
      "source": [
        "> Note: We will use a simplified version of balancing known as a CartPole problem. In the cartpole world, we have a horizontal slider that can move left or right, and the goal is to balance a vertical pole on top of the slider."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4cl98EatNIl"
      },
      "source": [
        "### OpenAI Gym\n",
        "In previous sections, the rules of the game and the state were given by the Board class which we defined ourselves. Here we will use a special simulation environment, which will simulate the physics behind the balancing pole. One of the most popular simulation environments for training reinforcement learning algorithms is called a Gym, which is maintained by OpenAI. By using this gym we can create difference environments from a cartpole simulation to Atari games."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH115T2htBqT"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LSvVZV2ytdbs"
      },
      "outputs": [],
      "source": [
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyQ2VxqquNoZ"
      },
      "source": [
        "### CartPole Skating\n",
        "\n",
        "> **Problem**: If Peter wants to escape from the wolf, he needs to be able to move faster than him. We will see how Peter can learn to skate, in particular, to keep balance, using Q-Learning.\n",
        "\n",
        "First, let's install the gym and import required libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "XCWZjoGiuNoc"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5K2-a4WuNof"
      },
      "source": [
        "### Create a cartpole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO3OiR-juNof",
        "outputId": "91b488c1-ce38-4850-972c-fcfc82d67fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discrete(2)\n",
            "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "print(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "WSo-Gun_12Wq",
        "outputId": "bd9d3e24-6daa-4017-89f4-6c6f6e10565a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(open('images/cartpole-nobalance.gif','rb').read())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![10](https://user-images.githubusercontent.com/62965911/224305201-4fb37363-1cec-4599-a215-fcb6a164dfb6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXSYrv5fuNog"
      },
      "source": [
        "To see how the environment works, let's run a short simulation for 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-9-6ZSpuNoh",
        "outputId": "23931c92-6b18-463f-8354-a4e0838f40c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "\n",
        "for i in range(100):\n",
        "   env.render()\n",
        "   env.step(env.action_space.sample())\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0j7KKYtuNoi"
      },
      "source": [
        "During simulation, we need to get observations in order to decide how to act. In fact, `step` function returns us back current observations, reward function, and the `done` flag that indicates whether it makes sense to continue the simulation or not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3DzNjN8uNoj",
        "outputId": "5a7a7536-0c25-4810-9658-9658fea4e5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00220021  0.22944733 -0.01140885 -0.26823641] -> 1.0\n",
            "[ 0.00678916  0.03449004 -0.01677358  0.02082636] -> 1.0\n",
            "[ 0.00747896 -0.16038739 -0.01635705  0.30817019] -> 1.0\n",
            "[ 0.00427121 -0.3552725  -0.01019365  0.59564999] -> 1.0\n",
            "[-0.00283424 -0.55025032  0.00171935  0.88510464] -> 1.0\n",
            "[-0.01383925 -0.35515175  0.01942144  0.59296271] -> 1.0\n",
            "[-0.02094228 -0.16030698  0.0312807   0.30646021] -> 1.0\n",
            "[-0.02414842  0.03435559  0.0374099   0.02380424] -> 1.0\n",
            "[-0.02346131 -0.16128233  0.03788599  0.32805174] -> 1.0\n",
            "[-0.02668696 -0.35692259  0.04444702  0.63243736] -> 1.0\n",
            "[-0.03382541 -0.55263553  0.05709577  0.93877995] -> 1.0\n",
            "[-0.04487812 -0.74847881  0.07587137  1.24884322] -> 1.0\n",
            "[-0.05984769 -0.55440705  0.10084823  0.98085784] -> 1.0\n",
            "[-0.07093584 -0.36077073  0.12046539  0.72147828] -> 1.0\n",
            "[-0.07815125 -0.55733493  0.13489496  1.04951899] -> 1.0\n",
            "[-0.08929795 -0.36423521  0.15588534  0.80203969] -> 1.0\n",
            "[-0.09658265 -0.17155524  0.17192613  0.56216916] -> 1.0\n",
            "[-0.10001376 -0.36861978  0.18316951  0.90370564] -> 1.0\n",
            "[-0.10738615 -0.56568675  0.20124363  1.24791231] -> 1.0\n",
            "[-0.11869989 -0.76273776  0.22620187  1.59629079] -> 1.0\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "   env.render()\n",
        "   obs, rew, done, info = env.step(env.action_space.sample())\n",
        "   print(f\"{obs} -> {rew}\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGgds2pLuNol"
      },
      "source": [
        "We can get min and max value of those numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7mpurQGuNom",
        "outputId": "66b80ac4-1730-4ebb-ab97-52d895d72bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
            "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space.low)\n",
        "print(env.observation_space.high)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q58CGRuuuNon"
      },
      "source": [
        "### State Discretization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RFkszuoUuNoo"
      },
      "outputs": [],
      "source": [
        "def discretize(x):\n",
        "    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-lE7kD0uNoo"
      },
      "source": [
        "Let's also explore other discretization method using bins:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utuRsW2EuNop",
        "outputId": "eaa6fce3-b12b-49a8-c7d2-893bbfed484a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample bins for interval (-5,5) with 10 bins\n",
            " [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n"
          ]
        }
      ],
      "source": [
        "def create_bins(i,num):\n",
        "    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
        "\n",
        "print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n",
        "\n",
        "ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
        "nbins = [20,20,10,10] # number of bins for each parameter\n",
        "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
        "\n",
        "def discretize_bins(x):\n",
        "    return tuple(np.digitize(x[i],bins[i]) for i in range(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azWPrudJuNop"
      },
      "source": [
        "Let's now run a short simulation and observe those discrete environment values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WF03T_kuNoq",
        "outputId": "4f285684-4d4c-4b1d-f819-4a60029a8fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, 0, -1, -3)\n",
            "(0, 1, -1, -6)\n",
            "(0, 2, -3, -9)\n",
            "(0, 3, -4, -12)\n",
            "(0, 3, -7, -15)\n",
            "(0, 4, -10, -18)\n",
            "(0, 3, -14, -15)\n",
            "(0, 4, -17, -19)\n",
            "(0, 3, -21, -16)\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "   #env.render()\n",
        "   obs, rew, done, info = env.step(env.action_space.sample())\n",
        "   #print(discretize_bins(obs))\n",
        "   print(discretize(obs))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0VxfarVuNoq"
      },
      "source": [
        "### Q-Table Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JEP_MAaruNoq"
      },
      "outputs": [],
      "source": [
        "Q = {}\n",
        "actions = (0,1)\n",
        "\n",
        "def qvalues(state):\n",
        "    return [Q.get((state,a),0) for a in actions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGnBdkfjuNor"
      },
      "source": [
        "### Let's Start Q-Learning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "SMlCWSK2uNor"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "alpha = 0.3\n",
        "gamma = 0.9\n",
        "epsilon = 0.90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bDVZ5sDuNor",
        "outputId": "24bc34fb-d2c9-4382-f84b-74e590e2fb9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0: 36.0, alpha=0.3, epsilon=0.9\n",
            "5000: 95.7114, alpha=0.3, epsilon=0.9\n",
            "10000: 143.0132, alpha=0.3, epsilon=0.9\n",
            "15000: 162.9334, alpha=0.3, epsilon=0.9\n",
            "20000: 182.181, alpha=0.3, epsilon=0.9\n",
            "25000: 193.3598, alpha=0.3, epsilon=0.9\n",
            "30000: 213.3228, alpha=0.3, epsilon=0.9\n",
            "35000: 223.3262, alpha=0.3, epsilon=0.9\n",
            "40000: 238.8152, alpha=0.3, epsilon=0.9\n",
            "45000: 247.5302, alpha=0.3, epsilon=0.9\n",
            "50000: 275.7824, alpha=0.3, epsilon=0.9\n",
            "55000: 281.1164, alpha=0.3, epsilon=0.9\n",
            "60000: 307.8212, alpha=0.3, epsilon=0.9\n",
            "65000: 293.3144, alpha=0.3, epsilon=0.9\n",
            "70000: 302.5454, alpha=0.3, epsilon=0.9\n",
            "75000: 294.7994, alpha=0.3, epsilon=0.9\n",
            "80000: 296.515, alpha=0.3, epsilon=0.9\n",
            "85000: 304.9482, alpha=0.3, epsilon=0.9\n",
            "90000: 304.2536, alpha=0.3, epsilon=0.9\n",
            "95000: 314.6624, alpha=0.3, epsilon=0.9\n"
          ]
        }
      ],
      "source": [
        "def probs(v,eps=1e-4):\n",
        "    v = v-v.min()+eps\n",
        "    v = v/v.sum()\n",
        "    return v\n",
        "\n",
        "Qmax = 0\n",
        "cum_rewards = []\n",
        "rewards = []\n",
        "for epoch in range(100000):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    cum_reward=0\n",
        "    # == do the simulation ==\n",
        "    while not done:\n",
        "        s = discretize(obs)\n",
        "        if random.random()<epsilon:\n",
        "            # exploitation - chose the action according to Q-Table probabilities\n",
        "            v = probs(np.array(qvalues(s)))\n",
        "            a = random.choices(actions,weights=v)[0]\n",
        "        else:\n",
        "            # exploration - randomly chose the action\n",
        "            a = np.random.randint(env.action_space.n)\n",
        "\n",
        "        obs, rew, done, info = env.step(a)\n",
        "        cum_reward+=rew\n",
        "        ns = discretize(obs)\n",
        "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n",
        "    cum_rewards.append(cum_reward)\n",
        "    rewards.append(cum_reward)\n",
        "    # == Periodically print results and calculate average reward ==\n",
        "    if epoch%5000==0:\n",
        "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
        "        if np.average(cum_rewards) > Qmax:\n",
        "            Qmax = np.average(cum_rewards)\n",
        "            Qbest = Q\n",
        "        cum_rewards=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMEBfBeJuNot"
      },
      "source": [
        "### Plotting Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "0WcrMLqSuNot",
        "outputId": "8de083da-53d9-4a29-cda8-2a564396df5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f94f97e0b50>]"
            ]
          },
          "execution_count": 56,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wiNZcfBuNou"
      },
      "source": [
        "From this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate **running average** over series of experiments, let's say 100. This can be done conveniently using `np.convolve`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "MhuXZ33xuNov",
        "outputId": "ebf51aa6-5e79-41f3-d72f-c93b1fd3db0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f94f9755d10>]"
            ]
          },
          "execution_count": 57,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light",
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def running_average(x,window):\n",
        "    return np.convolve(x,np.ones(window)/window,mode='valid')\n",
        "\n",
        "plt.plot(running_average(rewards,100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsEpGxlPuNow"
      },
      "source": [
        "### Varying Hyperparameters and Seeing the Result in Action\n",
        "\n",
        "Now it would be interesting to actually see how the trained model behaves. Let's run the simulation, and we will be following the same action selection strategy as during training: sampling according to the probability distribution in Q-Table: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "cQ_1zQN6uNox"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "   s = discretize(obs)\n",
        "   env.render()\n",
        "   v = probs(np.array(qvalues(s)))\n",
        "   a = random.choices(actions,weights=v)[0]\n",
        "   obs,_,done,_ = env.step(a)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYNA9-4nuNox"
      },
      "source": [
        "### Saving result to an animated GIF\n",
        "\n",
        "If you want to impress your friends, you may want to send them the animated GIF picture of the balancing pole. To do this, we can invoke `env.render` to produce an image frame, and then save those to animated GIF using PIL library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCcqOkjwuNoy",
        "outputId": "14a8ecac-dedf-457a-a014-93a7ade61ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "363\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "obs = env.reset()\n",
        "done = False\n",
        "i=0\n",
        "ims = []\n",
        "while not done:\n",
        "   s = discretize(obs)\n",
        "   img=env.render(mode='rgb_array')\n",
        "   ims.append(Image.fromarray(img))\n",
        "   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))\n",
        "   a = random.choices(actions,weights=v)[0]\n",
        "   obs,_,done,_ = env.step(a)\n",
        "   i+=1\n",
        "env.close()\n",
        "ims[0].save('images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "1zy5x_Et1fYI",
        "outputId": "eb3adc6f-b805-4cb2-c856-34bafd90f59e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(open('images/cartpole-balance.gif','rb').read())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![11](https://user-images.githubusercontent.com/62965911/224305202-b0f48ad3-71a5-4f56-88e9-3f7fb1cbb42e.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN3kAptehBxZp98XN4CPabY",
      "mount_file_id": "1NnDFlwcw7FNGhD1eKGx3DaZ1g0FhFGoq",
      "name": "2021-07-19-rl-basics.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
