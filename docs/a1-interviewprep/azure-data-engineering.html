<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-a1-interviewprep/azure-data-engineering">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Azure Data Engineering | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/a1-interviewprep/azure-data-engineering"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Azure Data Engineering | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Case study - data lake"><meta data-rh="true" property="og:description" content="Case study - data lake"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/a1-interviewprep/azure-data-engineering"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/a1-interviewprep/azure-data-engineering" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/a1-interviewprep/azure-data-engineering" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.47c7b9d5.css">
<link rel="preload" href="/assets/js/runtime~main.251db5a0.js" as="script">
<link rel="preload" href="/assets/js/main.1462881d.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><main class="docMainContainer_gTbr docMainContainerEnhanced_Uz_u"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Azure Data Engineering</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="case-study---data-lake">Case study - data lake<a class="hash-link" href="#case-study---data-lake" title="Direct link to heading">​</a></h2><p>In a case study question, a use case will be described in detail with multiple inputs such as business requirements and technical requirements. You will have to carefully read the question and understand the requirements before answering the question.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="background">Background<a class="hash-link" href="#background" title="Direct link to heading">​</a></h3><p>Let&#x27;s assume you are a data architect in a retail company that has both online and bricks and mortar outlets all over the world. You have been asked to design their data processing solution. The leadership team wants to see a unified dashboard of daily, monthly, and yearly revenue reports in a graphical format, from across all their geographic locations and the online store.</p><p>The company has analysts who are SQL experts.</p><p>For simplicity, let&#x27;s assume that the retail outlets are in friendly countries, so there is no limitation in terms of moving data across the countries.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="technical-details">Technical details<a class="hash-link" href="#technical-details" title="Direct link to heading">​</a></h3><p>The online transaction data gets collected into Azure SQL instances that are geographically spread out. The overall size is about 10 GB per day.</p><p>The bricks and mortar point of sale transactions are getting collected in local country-specific SQL Server databases with different schemas. The size is about 20 GB per day.</p><p>The store details are stored as JSON files in Azure Data Lake Gen2.</p><p>The inventory data is available as CSV files in Azure Data Lake Gen2. The size is about 500 MB per day.</p><blockquote><p>TIP: The trick is to identify key terminologies such as file formats, streaming, or batching (based on the frequency of reports), the size of the data, security restrictions - if any, and technologies to be used, such as SQL in this case (as the analysts are SQL experts). Once we have all this data, the decision-making process becomes a bit simpler.</p></blockquote><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-1">Question 1<a class="hash-link" href="#question-1" title="Direct link to heading">​</a></h4><p>Choose the right storage solution to collect and store all the different types of data.</p><p>[Options: Azure Synapse SQL pool, Azure Data Lake Gen2, Azure Files, Event Hubs]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution">Solution<a class="hash-link" href="#solution" title="Direct link to heading">​</a></h4><p><strong>Azure Data Lake Gen2</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation">Explanation<a class="hash-link" href="#explanation" title="Direct link to heading">​</a></h4><ul><li>ADLS Gen2 can handle multiple different types of formats and can store petabytes of data. Hence, it would suit our use case.</li><li>Azure Synapse SQL pool is for storing processed data in SQL tables.</li><li>Azure Files are file sharing storage services that can be accessed via <strong>Server Message Block</strong> (<strong>SMB</strong>) or <strong>Network File System</strong> (<strong>NFS</strong>) protocols. They are used to share application settings, as extended on-premises file servers, and so on.</li><li>Event Hubs is used for streaming real-time events and not an actual analytical data store.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-2">Question 2<a class="hash-link" href="#question-2" title="Direct link to heading">​</a></h4><p>Choose the mechanism to copy data over into your common storage.</p><p>[Choices: PolyBase, Azure Data Factory, Azure Databricks, Azure Stream Analytics]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution-1">Solution<a class="hash-link" href="#solution-1" title="Direct link to heading">​</a></h4><p><strong>Azure Data Factory</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-1">Explanation<a class="hash-link" href="#explanation-1" title="Direct link to heading">​</a></h4><ul><li>ADF provides connectors to read data from a huge variety of sources, both on the cloud and on-premises. Hence, it will be a good fit for this situation.</li><li>PolyBase is mostly used for converting data from different formats to standard SQL table formats and copying them into Synapse SQL pool.</li><li>Azure Databricks can be used for batch and Spark stream processing, not for storing large volumes of data.</li><li>Azure Stream Analytics is used for stream processing, not for storing large volumes of data.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-3">Question 3<a class="hash-link" href="#question-3" title="Direct link to heading">​</a></h4><p>Choose storage to store the daily, monthly, and yearly data for the analysts to query and generate reports using SQL.</p><p>[Choices: Azure Databricks, Azure Queues, Synapse SQL pool, Azure Data Factory]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution-2">Solution<a class="hash-link" href="#solution-2" title="Direct link to heading">​</a></h4><p><strong>Synapse SQL pool</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-2">Explanation<a class="hash-link" href="#explanation-2" title="Direct link to heading">​</a></h4><ul><li><p>Synapse SQL pool is a data warehouse solution that perfectly fits the requirements for storing data to generate reports and find insights. The daily, monthly, and yearly data is usually the data that is cleaned, filtered, joined, aggregated from various sources, and stored in pre-defined schemas for easy analysis. Since Synapse SQL pools are natively SQL-based, it works well for analysts of the company who are SQL experts.</p></li><li><p>Azure Databricks is used for batch and stream processing, not for storing large volumes of data. Hence, it wouldn&#x27;t fit the bill for our use case.</p></li><li><p>Azure Queues storage is a messaging service that can hold millions of messages and that can be processed asynchronously. Hence, it wouldn&#x27;t fit the bill for our use case.</p></li><li><p>Azure Data Factory is used to copy/move data, do basic transformations, and orchestrate pipelines. It cannot be used for storing data.</p><blockquote><p>TIP: If you find terminologies that you are not aware of, use the principle of negation to find the most suitable answer. In this case, if you didn&#x27;t know what Azure Queues does, you can try to establish whether any of the other options is a good solution and then go with it. Or, if you are not sure, try to eliminate the obviously wrong answers and take an educated guess.</p></blockquote></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-4">Question 4<a class="hash-link" href="#question-4" title="Direct link to heading">​</a></h4><p>Visualize the insights generated in a graphical format.</p><p>[Choices: Azure Data Factory, Synapse Serverless SQL pool, Power BI, Azure Databricks]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution-3">Solution<a class="hash-link" href="#solution-3" title="Direct link to heading">​</a></h4><p><strong>Power BI</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-3">Explanation<a class="hash-link" href="#explanation-3" title="Direct link to heading">​</a></h4><ul><li>Power BI can generate insights from various sources of data, such as Synapse SQL pools, Azure Stream Analytics, Azure SQL, and Cosmos DB. It provides a very rich set of tools to graphically display the data.</li><li>ADF provides connectors to read data from a huge variety of sources and orchestration support. Hence, it will not be a good fit for this situation.</li><li>Synapse SQL pool is a data warehouse solution that can be used to process data and store it, to be used by business intelligence tools such as Power BI.</li><li>Azure Databricks can be used for visualizing data patterns, but not usually for generating and visualizing graphical insights.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="case-study---data-lake-1">Case study - data lake<a class="hash-link" href="#case-study---data-lake-1" title="Direct link to heading">​</a></h2><p>The case study questions will have a detailed description of the case followed by the questions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="background-1">Background<a class="hash-link" href="#background-1" title="Direct link to heading">​</a></h3><p>You have been hired to build a ticket scanning system for a country&#x27;s railways department. Millions of passengers will be traveling on the trains every day. It has been observed that some passengers misuse their tickets by sharing them with others or using them for more rides than allowed. The railway officers want a real-time system to track such fraud occurrences.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="technical-details-1">Technical details<a class="hash-link" href="#technical-details-1" title="Direct link to heading">​</a></h3><ul><li>A ticket is considered fraudulent it if is used more than 10 times a day.</li><li>Build a real-time alerting system to generate alerts whenever such fraud happens.</li><li>Generate a monthly fraud report of the number of incidents and the train stations where it happens.</li></ul><p>You need to build a data pipeline. Recommend the services that can be used to build such a fraud detection system.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-1-1">Question 1<a class="hash-link" href="#question-1-1" title="Direct link to heading">​</a></h4><p>You recommend the following components to be used:</p><ul><li>Azure Blob storage to consume the data</li><li>Azure Stream Analytics to process the fraud alerts</li><li>Power BI to display the monthly report</li></ul><p>[Options: Correct/ Incorrect]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution-4">Solution<a class="hash-link" href="#solution-4" title="Direct link to heading">​</a></h4><p><strong>Incorrect</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-4">Explanation<a class="hash-link" href="#explanation-4" title="Direct link to heading">​</a></h4><p>We cannot use Azure Blob storage to consume real-time data. It is used to store different formats of data for analytical processing or long-term storage.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-2-1">Question 2<a class="hash-link" href="#question-2-1" title="Direct link to heading">​</a></h4><p>You recommend a system to use:</p><ul><li>IOT Hub to consume the data</li><li>Azure Stream Analytics to process the fraud alerts</li><li>Azure Databricks to store the monthly data and generate the reports</li><li>Power BI to display the monthly report</li></ul><p>[Options: Correct/ Incorrect]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution-5">Solution<a class="hash-link" href="#solution-5" title="Direct link to heading">​</a></h4><p><strong>Incorrect</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-5">Explanation<a class="hash-link" href="#explanation-5" title="Direct link to heading">​</a></h4><p>We cannot use Azure Databricks to store the monthly data. It is not a storage service; it is a compute service.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="question-3-1">Question 3<a class="hash-link" href="#question-3-1" title="Direct link to heading">​</a></h4><p>You recommend a system to use:</p><ul><li>IOT Hub to consume the data</li><li>Azure Stream Analytics to process the fraud alerts</li><li>Azure Synapse SQL pool to store the monthly data and generate the reports</li><li>Power BI to display the monthly report</li></ul><p>[Options: Correct/ Incorrect]</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution-6">Solution<a class="hash-link" href="#solution-6" title="Direct link to heading">​</a></h4><p><strong>Correct</strong></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-6">Explanation<a class="hash-link" href="#explanation-6" title="Direct link to heading">​</a></h4><p>IOT Hub can be used to consume real-time data and feed it to Azure Stream Analytics. Stream Analytics can perform real-time fraud detection and store the aggregated results in Synapse SQL pool. Synapse SQL pool can store petabytes of data for longer durations to generate reports. Power BI can graphically display both the real-time alerts and monthly reports. So, this is the right set of options.</p><p>Let&#x27;s look at a data visualization question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-visualization">Data visualization<a class="hash-link" href="#data-visualization" title="Direct link to heading">​</a></h2><p>You have data from various data sources in JSON and CSV formats that has been copied over into Azure Data Lake Gen2. You need to graphically visualize the data. What tool would you use?</p><ul><li>Power BI</li><li>Azure Databricks/Synapse Spark</li><li>Azure Data Factory</li><li>Azure Storage Explorer</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-7">Solution<a class="hash-link" href="#solution-7" title="Direct link to heading">​</a></h3><p><strong>Azure Databricks/Synapse Spark</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-7">Explanation<a class="hash-link" href="#explanation-7" title="Direct link to heading">​</a></h3><ul><li><p>Azure Databricks Spark or Synapse Spark provides graphing options that can be used to sample and visualize data.</p></li><li><p>Power BI is not used to visualize raw data. It is used to visualize insights derived from processed data.</p></li><li><p>Azure Data Factory provides options to preview the data, but not many options for graphically visualizing it.</p></li><li><p>Storage Explorer helps explore the filesystem but doesn&#x27;t have the ability to visualize the data graphically.</p><blockquote><p>TIP: Look for the nuances in the question. The moment we see <em>graphically visualize</em>, we tend to select Power BI. But Azure Databricks Spark has built-in graphing tools that can help visualize the data. Power BI is used to build and display insights from processed data.</p></blockquote></li></ul><p>Let&#x27;s look at a data partition question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-partitioning">Data partitioning<a class="hash-link" href="#data-partitioning" title="Direct link to heading">​</a></h2><p>You have a table as follows in Azure SQL:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">TABLE</span><span class="token plain"> Books {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   BookID </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">NOT</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">NULL</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   CategoryID </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">NOT</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">NULL</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   BookName </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   AuthorID </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ISBN </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">40</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Assume there are 100 million entries in this table. <strong>CategoryID</strong> has about 25 entries and 60% of the books align to about 20 categories. You need to optimize the performance of this table for queries that aggregate on <strong>CategoryID</strong>. What partitioning technique would you use and what key would you choose?</p><ul><li>Vertical partitioning with <strong>CategoryID</strong></li><li>Horizontal partitioning with <strong>BookID</strong></li><li>Vertical partitioning with <strong>BookID</strong></li><li>Horizontal partitioning with <strong>CategoryID</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-8">Solution<a class="hash-link" href="#solution-8" title="Direct link to heading">​</a></h3><p>Horizontal partitioning with <strong>CategoryID</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-8">Explanation<a class="hash-link" href="#explanation-8" title="Direct link to heading">​</a></h3><ul><li>Horizontal partitioning with <strong>CategoryID</strong> is the right choice as we need to horizontally partition (shard) the data based on <strong>categoryID</strong>, which has a fairly good distribution. This can speed up the processing by distributing the data evenly across the partitions.</li><li>Vertical partitioning with <strong>CategoryID</strong> - Splitting the table vertically will not optimize as we will have to scan through the entire database to aggregate the categories. Vertical partitioning is effective when we need to speed up queries only based on a few columns.</li><li>Horizontal partitioning with <strong>BookID</strong> - Horizontal partitioning (sharding) is fine, but the key we are looking to optimize is the categories. So <strong>BookID</strong> will not create the optimal partitions.</li><li>Vertical partitioning with <strong>BookID</strong> - For the same reason as vertical partitioning with <strong>CategoryID</strong>, vertical partitions will not be efficient as we need to access all the rows.</li></ul><p>Let&#x27;s look at a Synapse SQL pool design question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="synapse-sql-pool-table-design---1">Synapse SQL pool table design - 1<a class="hash-link" href="#synapse-sql-pool-table-design---1" title="Direct link to heading">​</a></h2><p>You are the architect of a cab company. You are designing the schema to store trip information. You have a large fact table that has a billion rows. You have dimension tables in the range of 500--600 MB and you have daily car health data in the range of 50 GB. The car health data needs to be loaded into a staging table as quickly as possible. What distributions would you choose for each of these types of data?</p><ul><li>A - Fact table</li><li>B - Dimension tables</li><li>C - Staging table</li></ul><p>[Options: Round Robin, Hash, Replicated]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-9">Solution<a class="hash-link" href="#solution-9" title="Direct link to heading">​</a></h3><ul><li>A - Fact table - Hash</li><li>B - Dimension tables - Replicated</li><li>C - Staging table - Round Robin</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-9">Explanation<a class="hash-link" href="#explanation-9" title="Direct link to heading">​</a></h3><ul><li><strong>Replicated</strong> - Use replication to copy small tables to all the nodes so that the processing is much faster without much network traffic.</li><li><strong>Hash</strong> - Use hash distribution for fact tables that contain millions or billions of rows/are several GBs in size. For small tables, hash distribution will not be very performant.</li><li><strong>Round Robin</strong> - Use round robin for staging tables where you want to quickly load the data.</li></ul><p>Let&#x27;s look at another Synapse SQL pool design question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="synapse-sql-pool-table-design---2">Synapse SQL pool table design - 2<a class="hash-link" href="#synapse-sql-pool-table-design---2" title="Direct link to heading">​</a></h2><p>You are a data engineer for an online bookstore. The bookstore processes hundreds of millions of transactions every month. It has a Catalog table of about 100 MB. Choose the optimal distribution for the Catalog table and complete the following script:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">TABLE</span><span class="token plain"> Catalogue </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   BookID </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   BookName </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   ISBN: </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   FORMAT: </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">20</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">WITH</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token keyword" style="color:#00009f">CLUSTERED</span><span class="token plain"> COLUMNSTORE </span><span class="token keyword" style="color:#00009f">INDEX</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   DISTRIBUTION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ___________</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>[Options: <strong>ROUND-ROBIN</strong>, <strong>REPLICATE</strong>, <strong>HASH</strong>, <strong>PARTITION</strong>]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-10">Solution<a class="hash-link" href="#solution-10" title="Direct link to heading">​</a></h3><p><strong>Replicate</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-10">Explanation<a class="hash-link" href="#explanation-10" title="Direct link to heading">​</a></h3><ul><li>Replicate distribution copies the data to all the compute nodes. Hence, the processing will be much faster in the case of smaller tables.</li><li><strong>Hash</strong> - Use hash distribution for fact tables that contain millions of rows or are several GBs in size. For small tables, hash distribution will not be very performant.</li><li><strong>Round Robin</strong> - Use round robin for staging tables where you want to quickly load the data.</li><li><strong>Partition</strong> - This is used for data partitioning, which is not our use case.</li></ul><p>Let&#x27;s look at a slowly changing dimension question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="slowly-changing-dimensions">Slowly changing dimensions<a class="hash-link" href="#slowly-changing-dimensions" title="Direct link to heading">​</a></h2><p>Identify the type of SCD by looking at this table definition:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">TABLE</span><span class="token plain"> DimCustomer </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   SurrogateID </span><span class="token keyword" style="color:#00009f">IDENTITY</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   CustomerID </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Name </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   Email </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">100</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   StartDate </span><span class="token keyword" style="color:#00009f">DATE</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   EndDate </span><span class="token keyword" style="color:#00009f">DATE</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   IsActive </span><span class="token keyword" style="color:#00009f">INT</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>[Options: SCD Type 1, SCD Type 2, SCD Type 3]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-11">Solution<a class="hash-link" href="#solution-11" title="Direct link to heading">​</a></h3><p><strong>SCD Type 2</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-11">Explanation<a class="hash-link" href="#explanation-11" title="Direct link to heading">​</a></h3><p>SCD Type 2 keeps track of all the previous records using the <strong>StartDate</strong>, <strong>EndDate</strong>, and, optionally, an <strong>IsActive</strong> or a <strong>VersionNumber</strong> field.</p><p>Let&#x27;s look at a storage tier-based question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="storage-tiers">Storage tiers<a class="hash-link" href="#storage-tiers" title="Direct link to heading">​</a></h2><p>You are a data engineer working with an ad serving company. There are three types of data the company wants to store in Azure Blob storage. Select the storage tiers that you should recommend for each of the following scenarios.</p><ul><li>A - Auditing data for the last 5 years for yearly financial reporting</li><li>B - Data to generate monthly customer expenditure reports</li><li>C - Media files to be displayed in online ads</li></ul><p>[Options: Hot, Cold, Archive]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-12">Solution<a class="hash-link" href="#solution-12" title="Direct link to heading">​</a></h3><p>A - Archive</p><p>B - Cold</p><p>C - Hot</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-12">Explanation<a class="hash-link" href="#explanation-12" title="Direct link to heading">​</a></h3><ul><li>Auditing data is accessed rarely and the use case says yearly financial reporting. So, this is a good candidate for the archive tier. The archive tier requires the data to be stored for at least 180 days.</li><li>Monthly customer expenditure data is not used frequently, so it is a good candidate for cold storage. Cold storage requires the data to be stored for at least 30 days.</li><li>Media files to be displayed in ads will be used every time the ad is displayed. Hence, this needs to be on the hot tier.</li></ul><p>Let&#x27;s look at a disaster recovery question next.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="disaster-recovery">Disaster recovery<a class="hash-link" href="#disaster-recovery" title="Direct link to heading">​</a></h2><p>You work in a stock trading company that stores most of its data on ADLS Gen2 and the company wants to ensure that the business continues uninterrupted even when an entire data center goes down. Select the disaster recovery option(s) that you should choose for such a requirement:</p><ul><li><strong>Geo-Redundant Storage</strong> (<strong>GRS</strong>)</li><li><strong>Zone-Redundant Storage</strong> (<strong>ZRS</strong>)</li><li><strong>Geo-Zone-Redundant Storage</strong> (<strong>GZRS</strong>)</li><li><strong>Locally Redundant Storage</strong> (<strong>LRS</strong>)</li><li><strong>Geo-Replication</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-13">Solution<a class="hash-link" href="#solution-13" title="Direct link to heading">​</a></h3><ul><li><strong>Geo-Redundant Storage</strong> (<strong>GRS</strong>) or <strong>Geo-Zone-Redundant Storage</strong> (<strong>GZRS</strong>)</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-13">Explanation<a class="hash-link" href="#explanation-13" title="Direct link to heading">​</a></h3><ul><li><p>Both <strong>Geo-Redundant Storage</strong> (<strong>GRS</strong>) and <strong>Geo-Zone-Redundant Storage</strong> (<strong>GZRS</strong>) can ensure that the data will be available even if entire data centers or regions go down. The difference between GRS and GZRS is that in GRS, the data is synchronously copied three times within the primary region using the LRS technique, but in GZRS, the data is synchronously copied three times within the primary region using ZRS. With GRS and GZRS, the data in the secondary region will not be available for simultaneous read or write access. If you need simultaneous read access in the secondary regions, you could use the <strong>Read-Access - Geo-Redundant Storage</strong> (<strong>RA-GRS</strong>) or <strong>Read-Access Geo-Zone-Redundant Storage</strong> (<strong>RA-GZRS</strong>) options.</p></li><li><p>LRS - LRS provides only local redundancy, but doesn&#x27;t guarantee data availability if entire data centers or regions go down.</p></li><li><p>ZRS - ZRS provides zone-level redundancy but doesn&#x27;t hold up if the entire data center or region goes down.</p></li><li><p>Geo-replication - This is an Azure SQL replication feature that replicates the entire SQL server to another region and provides read-only access in the secondary region.</p><blockquote><p>TIP: If you notice any options that you are not aware of, don&#x27;t panic. Just look at the ones you are aware of and check whether any of those could be the answer. For example, in the preceding question, if you had not read about geo-replication, it would have still been okay because the answer was among the choices that you already knew.</p></blockquote></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="synapse-sql-external-tables">Synapse SQL external tables<a class="hash-link" href="#synapse-sql-external-tables" title="Direct link to heading">​</a></h2><p>Fill in the missing code segment to read Parquet data from an ADLS Gen2 location into Synapse Serverless SQL:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">IF</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">NOT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">EXISTS</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> sys</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">external_file_formats</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> name </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;SynapseParquetFormat&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> ____________ </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">SynapseParquetFormat</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">WITH</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FORMAT_TYPE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> PARQUET</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">IF</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">NOT</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">EXISTS</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token keyword" style="color:#00009f">SELECT</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> sys</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">external_data_sources</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">WHERE</span><span class="token plain"> name </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;sample_acct&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> _____________ </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sample_acct</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">WITH</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        LOCATION   </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;https://sample_acct.dfs.core.windows.net/users&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> ______________ TripsExtTable </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">TripID</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">varchar</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">DriverID</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">varchar</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">50</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">WITH</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    LOCATION </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&#x27;path/to/*.parquet&#x27;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    DATA_SOURCE </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">sample_acct</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    FILE_FORMAT </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">SynapseParquetFormat</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GO</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>[Options: <strong>TABLE</strong>, <strong>EXTERNAL TABLE</strong>, <strong>EXTERNAL FILE FORMAT</strong>, <strong>EXTERNAL DATA SOURCE</strong>, <strong>VIEW</strong>, <strong>FUNCTION</strong>]</p><p>You can reuse the options provided above for more than one blank if needed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-14">Solution<a class="hash-link" href="#solution-14" title="Direct link to heading">​</a></h3><p><strong>EXTERNAL FILE FORMAT</strong>, <strong>EXTERNAL DATA SOURCE</strong>, <strong>EXTERNAL TABLE</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-14">Explanation<a class="hash-link" href="#explanation-14" title="Direct link to heading">​</a></h3><ul><li>The correct keywords are <strong>EXTERNAL FILE FORMAT</strong>, <strong>EXTERNAL DATA SOURCE</strong>, and <strong>EXTERNAL TABLE</strong> in the order in which they appear in the question.</li><li>You cannot use <strong>TABLE</strong> as this is not an internal table. We are reading external Parquet data as an external table.</li><li>You cannot use <strong>VIEW</strong> as views are logical projections of existing tables.</li><li>You cannot use <strong>FUNCTION</strong> as this is not a UDF.</li></ul><p>Let&#x27;s next look at some sample questions from the data processing section.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-lake-design">Data lake design<a class="hash-link" href="#data-lake-design" title="Direct link to heading">​</a></h2><p>You are working in a marketing firm. The firm provides social media sentiment analysis to its customers. It captures data from various social media websites, Twitter feeds, product reviews, and other online forums.</p><p>Technical requirements:</p><ul><li>The input data includes files in CSV, JSON, image, video, and plain text formats.</li><li>The data is expected to have inconsistencies such as duplicate entries and missing fields.</li><li>The overall data size would be about 5 petabytes every month.</li><li>The engineering team are experts in Scala and Python and would like a Notebook experience.</li><li>Engineers must be able to visualize the data for debugging purposes.</li><li>The reports have to be generated on a daily basis.</li><li>The reports should have charts with the ability to filter and sort data directly in the reports.</li></ul><p>You need to build a data pipeline to accomplish the preceding requirements. What are the components you would select for the following zones of your data lake?</p><p><strong>Landing zone</strong>:</p><p>[Options: Azure Data Lake Gen2, Azure Blob storage, Azure Synapse SQL, Azure Data Factory]</p><p><strong>Transformation zone</strong>:</p><p>[Options: Synapse SQL pool, Azure Databricks Spark, Azure Stream Analytics]</p><p><strong>Serving zone</strong>:</p><p>[Options: Synapse SQL pool, Azure Data Lake Gen2, Azure Stream Analytics]</p><p><strong>Reporting</strong>:</p><p>[Azure Databricks Spark, Power BI, the Azure portal]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-15">Solution<a class="hash-link" href="#solution-15" title="Direct link to heading">​</a></h3><ul><li><strong>Landing zone</strong>: Azure Blob storage</li><li><strong>Transformation zone</strong>: Azure Databricks Spark</li><li><strong>Serving zone</strong>: Synapse SQL pool</li><li><strong>Reporting</strong>: Power BI</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-15">Explanation<a class="hash-link" href="#explanation-15" title="Direct link to heading">​</a></h3><p><strong>Landing zone</strong>:</p><ul><li>Since the input contains a wide variety of data formats, including images and videos, it is better to store them in Azure Blob storage.</li><li>Azure Data Lake Gen2 provides a hierarchical namespace and is usually a good storage choice for data lakes. But since this use case includes images and videos, it is not recommended here.</li><li>Synapse SQL pool is a data warehouse solution that can be used to process data and store it to be used by business intelligence tools such as Power BI.</li><li>Azure Data Factory provides connectors to read data from a huge variety of sources and orchestration support. Hence, it will not be a good fit for this situation.</li></ul><p><strong>Transformation zone</strong>:</p><ul><li>Since the requirement includes cleaning up the incoming data, visualizing the data, and transforming the different formats into a standard schema that can be consumed by reports, Azure Databricks would fit the bill. Azure Databricks also supports Notebooks with Scala and Python support.</li><li>Synapse SQL pool can be used to store the processed data generated by Azure Databricks, but would not be a good fit for Scala and Python support.</li><li>Azure Stream Analytics is used for real-time processing. Hence, it will not work for our use case.</li></ul><p><strong>Serving zone</strong>:</p><ul><li>Synapse SQL pool, being a data warehouse that can support petabytes of data, would be a perfect choice here.</li><li>Azure Data Lake Gen2 provides a hierarchical namespace and is usually a good storage choice for data lake landing zones, but not for the serving zone. Serving zones need to be able to serve the results quickly to BI systems, so usually SQL-based or key-value-based services work the best.</li><li>Azure Stream Analytics is used for real-time data processing. Hence, it will not work for our use case.</li></ul><p><strong>Reporting</strong>:</p><ul><li>Power BI is a graphical business intelligence tool that can help visualize data insights. It provides a rich set of graphs and data filtering, aggregating, and sorting options.</li><li>The Azure portal is the starting page for all Azure services. It is the control center for all services that provides options for creating, deleting, managing, and monitoring the services.</li></ul><p>Let&#x27;s next look at an ASA windowed aggregates question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="asa-windows">ASA windows<a class="hash-link" href="#asa-windows" title="Direct link to heading">​</a></h2><p>You are working for a credit card company. You have been asked to design a system to detect credit card transaction fraud. One of the scenarios is to check whether a credit card has been used more than 3 times within the last 10 mins. The system is already configured to use Azure Event Hubs and Azure Stream Analytics. You have decided to use the windowed aggregation feature of ASA. Which of the following solutions would work? (Select one or more)</p><ul><li>A - Use a tumbling window with a size of 10 mins and check whether the count for the same credit card &gt; 3.</li><li>B - Use a sliding window with a size of 10 mins and check whether the count for the same credit card &gt; 3.</li><li>C - Use a hopping window with a size of 10 mins and a hop of 3 mins and check whether the count for the same credit card &gt; 3.</li><li>D - Use a session window with a size of 10 mins and check whether the count for the same credit card &gt; 3.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-16">Solution<a class="hash-link" href="#solution-16" title="Direct link to heading">​</a></h3><p><strong>B - Sliding Window</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-16">Explanation<a class="hash-link" href="#explanation-16" title="Direct link to heading">​</a></h3><ul><li>A sliding window has a fixed size, but the window moves forward only when events are either added or removed. Otherwise, it won&#x27;t emit any results. This will work perfectly as the window is of a fixed size and is moving after considering each and every event in progressive windows of 10 mins. This is a typical use case for a sliding window: <em>For every 10 seconds, alert if an event appears more than 5 times</em>.</li><li>A tumbling window calculates the number of events in fixed-size non-overlapping windows, so it might miss out on counting the events across window boundaries. Here&#x27;s a typical use case: <em>Find the number of events grouped by card number, in 10-second-wide tumbling windows</em>.</li><li>A hopping window calculates the count of events at every X interval, for the previous Y window width duration. If the overlap window is not big enough, this will also miss counting the events across window boundaries. Here&#x27;s a typical use case: <em>Every 10 seconds, fetch the transaction count for the last 20 seconds</em>.</li><li>Session windows don&#x27;t have fixed sizes. We need to specify a maximum window size and a timeout duration for session windows. The session window tries to grab as many events as possible within the max window size. Since this is not a fixed-size window, it will not work for our use case. Here&#x27;s a typical use case: <em>Find the number of trips that occur within 5 seconds of each other</em>.</li></ul><p>Let&#x27;s next look at a Spark transformation question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="spark-transformation">Spark transformation<a class="hash-link" href="#spark-transformation" title="Direct link to heading">​</a></h2><p>You work for a cab company that is storing trip data in Parquet format and fare data in CSV format. You are required to generate a report to list all the trips aggregated using the <strong>City</strong> field. The report should contain all fields from both files.</p><p><strong>Trip file format (Parquet)</strong>:</p><p><strong>tripId, driverId, City, StartTime, EndTime</strong></p><p><strong>Fare file format (CSV)</strong>:</p><p><strong>tripId, Fare</strong></p><p>Fill in the blanks of the following code snippet to achieve the preceding objective:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token operator" style="color:#393A34">%</span><span class="token operator" style="color:#393A34">%</span><span class="token plain">scala</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val fromcsv </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">options</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Map</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;inferSchema&quot;</span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token string" style="color:#e3116c">&quot;true&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token string" style="color:#e3116c">&quot;header&quot;</span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token string" style="color:#e3116c">&quot;true&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">csv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;abfss://path/to/csv/*&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val fromparquet </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">options</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">Map</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;inferSchema&quot;</span><span class="token operator" style="color:#393A34">-</span><span class="token operator" style="color:#393A34">&gt;</span><span class="token string" style="color:#e3116c">&quot;true&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">parquet</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;abfss:// abfss://path/to/parquet/*&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val joinDF </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> fromcsv</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">________</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">fromparquet</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">fromcsv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;tripId&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">==</span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> fromparquet</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;tripId&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token string" style="color:#e3116c">&quot;inner&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">_________</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;City&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>[Options: <strong>join</strong>, <strong>orderBy</strong>, <strong>select</strong>, <strong>groupBy</strong>]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-17">Solution<a class="hash-link" href="#solution-17" title="Direct link to heading">​</a></h3><p><strong>Join</strong>, <strong>groupBy</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-17">Explanation<a class="hash-link" href="#explanation-17" title="Direct link to heading">​</a></h3><ul><li><strong>join()</strong> - To join two tables based on the provided conditions</li><li><strong>groupBy()</strong> - Used to aggregate values based on some column values, such as <strong>City</strong> in this case</li><li><strong>select()</strong> - To select the data from a subset of columns</li><li><strong>orderBy()</strong> - To sort the rows by a particular column</li></ul><p>Let&#x27;s next look at an ADF integration runtime-based question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adf---integration-runtimes">ADF - integration runtimes<a class="hash-link" href="#adf---integration-runtimes" title="Direct link to heading">​</a></h2><p>You are working as a data engineer for a tax consulting firm. The firm processes thousands of tax forms for its customers every day. Your firm is growing and has decided to move to the cloud, but they want to be in a hybrid mode as they already have invested in a good set of on-premises servers for data processing. You plan to use ADF to copy data over nightly. Which of the following integration runtimes would you suggest?</p><ul><li>A - Azure integration runtime</li><li>B - Self-Hosted integration runtime</li><li>C - Azure - SSIS integration runtime</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-18">Solution<a class="hash-link" href="#solution-18" title="Direct link to heading">​</a></h3><p><strong>B - Self-Hosted Integration Runtime</strong>: Since this is an on-premises to the cloud use case, the self-hosted integration would be ideal. Also, since they have their local compute available, it would become much easier to set up the IR on the local servers.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-18">Explanation<a class="hash-link" href="#explanation-18" title="Direct link to heading">​</a></h3><p><strong>Azure Integration Runtime</strong> - This is the default option and supports connecting data stores and compute services across public endpoints. Use this option to copy data between Azure-hosted services.</p><p><strong>Self-Hosted Integration Runtime</strong> - Use the self-hosted IR when you need to copy data between on-premises clusters and Azure services. You will need machines or VMs on the on-premises private network to install a self-hosted integration runtime.</p><p><strong>Azure - SSIS Integration Runtime</strong> - The SSIS IRs are used for <strong>SQL Server Integration Services</strong> (<strong>SSIS</strong>) lift and shift use cases.</p><p>Let&#x27;s next look at a question on ADF triggers.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adf-triggers">ADF triggers<a class="hash-link" href="#adf-triggers" title="Direct link to heading">​</a></h2><p>Choose the right kind of trigger for your ADF pipelines:</p><ul><li>A - Trigger when a file gets deleted in Azure Blob storage</li><li>B - To handle custom events in Event Grid</li><li>C - Trigger a pipeline every Monday and Wednesday at 9:00 A.M. EST</li><li>D - Trigger a pipeline daily at 9:00 A.M. EST but wait for the previous run to complete</li></ul><p>[Options: Storage event trigger, Custom event trigger, Tumbling window trigger, Schedule trigger]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-19">Solution<a class="hash-link" href="#solution-19" title="Direct link to heading">​</a></h3><ul><li>A - Storage event trigger</li><li>B - Custom event trigger</li><li>C - Schedule trigger</li><li>D - Tumbling window trigger</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-19">Explanation<a class="hash-link" href="#explanation-19" title="Direct link to heading">​</a></h3><ul><li><strong>Schedule trigger</strong> - These are triggers that get fired on fixed schedules. You specify the start date, recurrence, and end date, and ADF takes care of firing the pipeline at the mentioned date and time.</li><li><strong>Tumbling window trigger</strong> - These are stateful scheduled triggers that are aware of the previous pipeline runs and offer retry capabilities.</li><li><strong>Storage event trigger</strong> - These are triggers that get fired on Blob storage events such as creating or deleting a file.</li><li><strong>Custom trigger</strong> - These are triggers that work on custom events mainly for Event Grid.</li></ul><p>Let&#x27;s next look at a question from the data security section.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tdealways-encrypted">TDE/Always Encrypted<a class="hash-link" href="#tdealways-encrypted" title="Direct link to heading">​</a></h2><p>You have configured active geo-replication on an Azure Synapse SQL instance. You are worried that the data might be accessible from the replicated instances or backup files and need to safeguard it. Which security solution do you configure?</p><ul><li>Enable Always Encrypted</li><li>Enable <strong>Transport Layer Security</strong> (<strong>TLS</strong>)</li><li>Enable <strong>Transparent Data Encryption</strong> (<strong>TDE</strong>)</li><li>Enable row-level security</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-20">Solution<a class="hash-link" href="#solution-20" title="Direct link to heading">​</a></h3><p>Enable<strong> Transparent Data Encryption</strong> (<strong>TDE</strong>)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-20">Explanation<a class="hash-link" href="#explanation-20" title="Direct link to heading">​</a></h3><ul><li>TDE encrypts the complete database, including offline access files such as backup files and log files.</li><li>Always Encrypted is used to encrypt specific columns of database tables, not the complete database or the offline files.</li><li>TLS is for encrypting data in motion. It doesn&#x27;t deal with encrypting databases.</li><li>Row-level security is for hiding selected rows from non-privileged database users. It doesn&#x27;t encrypt the database itself.</li></ul><p>Let&#x27;s next look at an Azure SQL/Synapse SQL auditing question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="auditing-azure-sqlsynapse-sql">Auditing Azure SQL/Synapse SQL<a class="hash-link" href="#auditing-azure-sqlsynapse-sql" title="Direct link to heading">​</a></h2><p>You work for a financial institution that stores all transactions in an Azure SQL database. You are required to keep track of all the delete activities on the SQL server. Which of the following activities should you perform? (Select one or more correct options)</p><ul><li>Create alerts using Azure SQL Metrics.</li><li>Enable auditing.</li><li>Configure Log Analytics as the destination for the audit logs.</li><li>Build custom metrics for delete events.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-21">Solution<a class="hash-link" href="#solution-21" title="Direct link to heading">​</a></h3><ul><li>Enable auditing.</li><li>Configure Log Analytics as the destination for the audit logs.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-21">Explanation<a class="hash-link" href="#explanation-21" title="Direct link to heading">​</a></h3><ul><li>Enabling auditing will track all the events in the database, including delete activities.</li><li>Configuring the destination as Log Analytics or Storage (Blob) will suffice the requirement to keep track of the activities. Log Analytics provides the advantage of Kusto queries, which can be run to analyze the audit logs. In the case of Blob storage, we will have to write custom code to analyze the audit logs.</li><li>Building custom metrics is not required as the audit function will automatically keep track of all deletions.</li><li>Creating alerts is not required as the requirement is to only keep track of the delete activities, not to alert.</li></ul><p>Let&#x27;s next look at a <strong>Dynamic Data Masking</strong> (<strong>DDM</strong>) question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-data-masking">Dynamic data masking<a class="hash-link" href="#dynamic-data-masking" title="Direct link to heading">​</a></h2><p>You need to partially mask the numbers of an SSN column. Only the last four digits should be visible. Which of the following solutions would work?</p><ul><li>A -- <strong>ALTER TABLE <!-- -->[dbo]<!-- -->.<!-- -->[Customer]<!-- --> ALTER COLUMN SSN ADD MASKED WITH FUNCTION =&#x27;PARTIAL(4, &quot;xxx-xx-&quot;, 4)&#x27;);</strong></li><li>B -- <strong>ALTER TABLE <!-- -->[dbo]<!-- -->.<!-- -->[Customer]<!-- --> ALTER COLUMN SSN ADD MASKED WITH FUNCTION = &#x27;PARTIAL(4, &quot;xxx-xx-&quot;, 0)&#x27;);</strong></li><li>C -- <strong>ALTER TABLE <!-- -->[dbo]<!-- -->.<!-- -->[Customer]<!-- --> ALTER COLUMN SSN ADD MASKED WITH (FUNCTION = &#x27;PARTIAL(0,&quot;xxx-xx-&quot;, 4)&#x27;);</strong></li><li>D -- <strong>ALTER TABLE <!-- -->[dbo]<!-- -->.<!-- -->[Customer]<!-- --> ALTER COLUMN SSN ADD MASKED WITH FUNCTION = &#x27;PARTIAL(&quot;xxx-xx-&quot;)&#x27;);</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-22">Solution<a class="hash-link" href="#solution-22" title="Direct link to heading">​</a></h3><p><strong>C</strong> -- <strong>ALTER TABLE <!-- -->[dbo]<!-- -->.<!-- -->[Customer]<!-- --> ALTER COLUMN SSN ADD MASKED WITH FUNCTION =&#x27;PARTIAL(0, &#x27;xxx-xx-&#x27;, 4)&#x27;);</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-22">Explanation<a class="hash-link" href="#explanation-22" title="Direct link to heading">​</a></h3><p>The syntax for partial masking is <strong>partial(prefix,<!-- -->[padding]<!-- -->,suffix)</strong>.</p><p>Let&#x27;s next look at an RBAC-based question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="rbac---posix">RBAC - POSIX<a class="hash-link" href="#rbac---posix" title="Direct link to heading">​</a></h2><p>Let&#x27;s assume that you are part of the engineering AAD security group in your company. The sales team has a directory with the following details:</p><table><thead><tr><th>Container</th><th>Owner</th><th>Permission (POSIX)</th></tr></thead><tbody><tr><td>/Sales</td><td>Sales AAD security group</td><td>740</td></tr></tbody></table><p>Will you be able to read the files under the <strong>/Sales</strong> directory?</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-23">Solution<a class="hash-link" href="#solution-23" title="Direct link to heading">​</a></h3><p><strong>No</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-23">Explanation<a class="hash-link" href="#explanation-23" title="Direct link to heading">​</a></h3><p>In POSIX representation, there are numbers to indicate the permissions for <strong>Owner</strong>, <strong>Owner Group</strong>, and <strong>Others</strong>.</p><p>In our question, the 740 would expand into:</p><p><strong>Owner</strong>: 7 (Read - 4, Write - 2, Execute - 1. Total: 4+2+1 = 7) // Can Read, Write, and Execute</p><p><strong>Owner Group</strong>: 4 (Read - 4, Write - 0, Execute - 0. Total: 4+0+0 = 4) // Can Read, but not Write or Execute</p><p><strong>Others</strong>: 0 (Read - 0, Write - 0, Execute - 0. Total: 0+0+0 = 0) // Cannot Read, Write, or Execute</p><p>So, the answer to the question would be <em>No</em>. Since you are part of the engineering security group, you would fall under the <strong>Other</strong> category, which doesn&#x27;t have any permissions.</p><p>Let&#x27;s next look at a row-level security question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="row-level-security">Row-level security<a class="hash-link" href="#row-level-security" title="Direct link to heading">​</a></h2><p>You are building a learning management system and you want to ensure that a teacher can see only the students in their class with any <strong>SELECT</strong> queries. Here is the <strong>STUDENT</strong> table:</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">CREATE</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">TABLE</span><span class="token plain"> StudentTable {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  StudentId </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">20</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  StudentName </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">40</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  TeacherName sysname</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  Grade </span><span class="token keyword" style="color:#00009f">VARCHAR</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token number" style="color:#36acaa">3</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Fill in the missing sections of the following row-level security script:</p><ol><li><p>Step 1:</p><p>CREATE <strong>__</strong> Security.TeacherPredicate (@TeacherName AS sysname)</p><p>RETURNS TABLE</p><p>AS RETURN SELECT 1</p><p>WHERE @TeacherName = USER_NAME()</p></li></ol><p>[Options: <strong>FUNCTION</strong>, <strong>TABLE</strong>, <strong>VIEW</strong>]</p><ol><li><p>Step 2:</p><p>CREATE <strong><strong>___</strong></strong> PrivFilter</p><p>ADD FILTER PREDICATE Security<strong>.TeacherPredicate </strong>(<strong>TeacherName</strong>)</p><p>ON StudentTable WITH (STATE = ON);</p></li></ol><p>[Options: <strong>SECURITY POLICY</strong>, <strong>TABLE</strong>, <strong>VIEW</strong>]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-24">Solution<a class="hash-link" href="#solution-24" title="Direct link to heading">​</a></h3><ul><li>Step 1: <strong>FUNCTION</strong></li><li>Step 2: <strong>SECURITY POLICY</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-24">Explanation<a class="hash-link" href="#explanation-24" title="Direct link to heading">​</a></h3><ul><li>Step 1: You must create a <strong>FUNCTION</strong> that can be applied as a <strong>FILTER PREDICATE</strong>, not a <strong>TABLE</strong> or a <strong>VIEW</strong>.</li><li>Step 2: You must create a <strong>SECURITY POLICY</strong> that can be applied on the table, not a <strong>TABLE</strong> or a <strong>VIEW</strong>.</li></ul><p>Let&#x27;s next look at a few sample questions from the monitoring and optimization section.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="blob-storage-monitoring">Blob storage monitoring<a class="hash-link" href="#blob-storage-monitoring" title="Direct link to heading">​</a></h2><p>You have been hired as an external consultant to evaluate Azure Blob storage. Your team has been using Blob storage for a month now. You want to find the usage and availability of the Blob storage.</p><p><strong>Question 1</strong>:</p><p>You can find the Blob storage usage from the <strong>Storage Metrics</strong> tab.</p><p>[Options: Yes/No]</p><p><strong>Question 2</strong>:</p><p>You can find the Blob availability metrics from the <strong>Storage Metrics</strong> tab.</p><p>[Options: Yes/No]</p><p><strong>Question 3</strong>:</p><p>You can find the Blob availability metrics from the <strong>Azure Monitor</strong> -&gt; <strong>Storage Accounts</strong> --&gt; <strong>Insights </strong>tab.</p><p>[Options: Yes/No]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-25">Solution<a class="hash-link" href="#solution-25" title="Direct link to heading">​</a></h3><ol><li><strong>Question 1</strong>: <strong>Yes</strong></li><li><strong>Question 2</strong>: <strong>No</strong></li><li><strong>Question 3</strong>: <strong>Yes</strong></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-25">Explanation<a class="hash-link" href="#explanation-25" title="Direct link to heading">​</a></h3><p>You can find the Blob storage usage from the <strong>Metrics</strong> tab on the Storage portal page. But it doesn&#x27;t have the store availability metrics. To look at the availability, you will have to go to Azure Monitor and click on the <strong>Insights</strong> tab under <strong>Storage accounts</strong>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="t-sql-optimization">T-SQL optimization<a class="hash-link" href="#t-sql-optimization" title="Direct link to heading">​</a></h2><p>You are running a few T-SQL queries and realize that the queries are taking much longer than before. You want to analyze why the queries are taking longer. Which of the following solutions will work? (Select one or more).</p><ul><li>A - Create a diagnostic setting for Synapse SQL pool to send the <strong>ExecRequests</strong> and <strong>Waits</strong> logs to Log Analytics and analyze the diagnostics table using Kusto to get the details of the running query and the query waits.</li><li>B - Run a T-SQL query against the <strong>sys.dm_pdw_exec_requests</strong> and <strong>sys.dm_pdw_waits</strong> table to get the details of the running query and the query waits.</li><li>C - Go to the Synapse SQL Metrics dashboard and look at the query execution and query wait metrics.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-26">Solution<a class="hash-link" href="#solution-26" title="Direct link to heading">​</a></h3><p><strong>B</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-26">Explanation<a class="hash-link" href="#explanation-26" title="Direct link to heading">​</a></h3><p>The diagnostic setting for Synapse SQL pool didn&#x27;t provide the options for <strong>ExecRequests</strong> and <strong>Waits</strong> as of writing this book.</p><p><strong>sys.dm_pdw_exec_requests</strong> - Contains all the current and recently active requests in Azure Synapse Analytics.</p><p><strong>sys.dm_pdw_waits</strong> - Contains details of the wait states in a query, including locks and waits on transmission queues.</p><p>The SQL Metrics dashboard doesn&#x27;t provide the query performance details.</p><p>Let&#x27;s next look at an ADF monitoring question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="adf-monitoring">ADF monitoring<a class="hash-link" href="#adf-monitoring" title="Direct link to heading">​</a></h2><p>There are two sub-questions for this question. Select all the statements that apply.</p><p><strong>Question 1</strong>:</p><p>How would you monitor ADF pipeline performance for the last month?</p><ul><li>A - Use the ADF pipeline <strong>activity</strong> dashboard.</li><li>B - Create a diagnostic setting, route the pipeline data to Log Analytics, and use Kusto to analyze the performance data.</li></ul><p>[Options: A, B]</p><p><strong>Question 2</strong>:</p><p>How would you monitor ADF pipeline performance for the last 3 months?</p><ul><li>A - Use the ADF pipeline <strong>activity</strong> dashboard.</li><li>B - Create a diagnostic setting, route the pipeline data to Log Analytics, and use Kusto to analyze the performance data.</li></ul><p>[Options: A, B]</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-27">Solution<a class="hash-link" href="#solution-27" title="Direct link to heading">​</a></h3><p><strong>Question 1</strong>:</p><p><strong>A</strong> and <strong>B</strong></p><p><strong>Question 2</strong>:</p><p><strong>Only A</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-27">Explanation<a class="hash-link" href="#explanation-27" title="Direct link to heading">​</a></h3><p>The ADF activity dashboard only keeps 45 days of data. Beyond that, we need to use Azure Monitoring and Log Analytics.</p><p>Let&#x27;s next look at an ASA alert-related question.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="setting-up-alerts-in-asa">Setting up alerts in ASA<a class="hash-link" href="#setting-up-alerts-in-asa" title="Direct link to heading">​</a></h2><p>Select the four steps required to set up an alert to fire if SU % goes above 80%. Arrange the steps in the right order:</p><ul><li>A - Configure diagnostic settings.</li><li>B - Define the actions to be done when the alert is triggered.</li><li>C - Select the signal as SU % utilization.</li><li>D - Redirect logs to Log Analytics and use Kusto to check for Threshold &gt; 80%</li><li>E - Select the scope as your Azure Stream Analytics job.</li><li>F - Set the alert logic as <strong>Greater Than</strong> Threshold value <strong>80</strong>%.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-28">Solution<a class="hash-link" href="#solution-28" title="Direct link to heading">​</a></h3><p><strong>E, C, F, B</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="explanation-28">Explanation<a class="hash-link" href="#explanation-28" title="Direct link to heading">​</a></h3><p>The steps involved in setting up an ASA alert for SU % utilization are as follows:</p><ul><li>Select the scope as your Azure Stream Analytics job.</li><li>Select the signal as SU % utilization.</li><li>Set the alert logic as <strong>Greater Than</strong> Threshold value <strong>80</strong>%.</li><li>Define the actions to be done when the alert is triggered.</li></ul><p>Diagnostic settings and Log Analytics are not required. The required SU % utilization metric is already available as part of ASA metrics.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#case-study---data-lake" class="table-of-contents__link toc-highlight">Case study - data lake</a><ul><li><a href="#background" class="table-of-contents__link toc-highlight">Background</a></li><li><a href="#technical-details" class="table-of-contents__link toc-highlight">Technical details</a></li></ul></li><li><a href="#case-study---data-lake-1" class="table-of-contents__link toc-highlight">Case study - data lake</a><ul><li><a href="#background-1" class="table-of-contents__link toc-highlight">Background</a></li><li><a href="#technical-details-1" class="table-of-contents__link toc-highlight">Technical details</a></li></ul></li><li><a href="#data-visualization" class="table-of-contents__link toc-highlight">Data visualization</a><ul><li><a href="#solution-7" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-7" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#data-partitioning" class="table-of-contents__link toc-highlight">Data partitioning</a><ul><li><a href="#solution-8" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-8" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#synapse-sql-pool-table-design---1" class="table-of-contents__link toc-highlight">Synapse SQL pool table design - 1</a><ul><li><a href="#solution-9" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-9" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#synapse-sql-pool-table-design---2" class="table-of-contents__link toc-highlight">Synapse SQL pool table design - 2</a><ul><li><a href="#solution-10" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-10" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#slowly-changing-dimensions" class="table-of-contents__link toc-highlight">Slowly changing dimensions</a><ul><li><a href="#solution-11" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-11" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#storage-tiers" class="table-of-contents__link toc-highlight">Storage tiers</a><ul><li><a href="#solution-12" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-12" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#disaster-recovery" class="table-of-contents__link toc-highlight">Disaster recovery</a><ul><li><a href="#solution-13" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-13" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#synapse-sql-external-tables" class="table-of-contents__link toc-highlight">Synapse SQL external tables</a><ul><li><a href="#solution-14" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-14" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#data-lake-design" class="table-of-contents__link toc-highlight">Data lake design</a><ul><li><a href="#solution-15" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-15" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#asa-windows" class="table-of-contents__link toc-highlight">ASA windows</a><ul><li><a href="#solution-16" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-16" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#spark-transformation" class="table-of-contents__link toc-highlight">Spark transformation</a><ul><li><a href="#solution-17" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-17" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#adf---integration-runtimes" class="table-of-contents__link toc-highlight">ADF - integration runtimes</a><ul><li><a href="#solution-18" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-18" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#adf-triggers" class="table-of-contents__link toc-highlight">ADF triggers</a><ul><li><a href="#solution-19" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-19" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#tdealways-encrypted" class="table-of-contents__link toc-highlight">TDE/Always Encrypted</a><ul><li><a href="#solution-20" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-20" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#auditing-azure-sqlsynapse-sql" class="table-of-contents__link toc-highlight">Auditing Azure SQL/Synapse SQL</a><ul><li><a href="#solution-21" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-21" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#dynamic-data-masking" class="table-of-contents__link toc-highlight">Dynamic data masking</a><ul><li><a href="#solution-22" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-22" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#rbac---posix" class="table-of-contents__link toc-highlight">RBAC - POSIX</a><ul><li><a href="#solution-23" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-23" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#row-level-security" class="table-of-contents__link toc-highlight">Row-level security</a><ul><li><a href="#solution-24" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-24" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#blob-storage-monitoring" class="table-of-contents__link toc-highlight">Blob storage monitoring</a><ul><li><a href="#solution-25" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-25" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#t-sql-optimization" class="table-of-contents__link toc-highlight">T-SQL optimization</a><ul><li><a href="#solution-26" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-26" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#adf-monitoring" class="table-of-contents__link toc-highlight">ADF monitoring</a><ul><li><a href="#solution-27" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-27" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li><li><a href="#setting-up-alerts-in-asa" class="table-of-contents__link toc-highlight">Setting up alerts in ASA</a><ul><li><a href="#solution-28" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#explanation-28" class="table-of-contents__link toc-highlight">Explanation</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.251db5a0.js"></script>
<script src="/assets/js/main.1462881d.js"></script>
</body>
</html>