<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-datascience/data-encoding">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Data Encoding | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/datascience/data-encoding"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Data Encoding | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Discrete Data"><meta data-rh="true" property="og:description" content="Discrete Data"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/datascience/data-encoding"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/datascience/data-encoding" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/datascience/data-encoding" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.47c7b9d5.css">
<link rel="preload" href="/assets/js/runtime~main.251db5a0.js" as="script">
<link rel="preload" href="/assets/js/main.1462881d.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/basics/de-basics">Getting Started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/cloud/cloud-basics">Cloud Computing</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/language/sql/sql-basics">Programming</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/storage/serialization">Data Storage</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/processing/databricks">Data Processing</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/data-modeling/sql-data-modeling">Data Modeling</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/extraction/api">Data Extraction</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/orchestration/airflow">Data Pipelines</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/visualization/flask">Data Visualization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/devops">DevOps</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/mathematics">Mathematics</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/foundations/basics/origin">Data Science</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/foundations/basics/origin">Basics</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/basics/origin">The data science origin story</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/basics/use-cases">Use Cases</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/basics/deployment">Model Deployment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/data-splits">Data Splits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/bias-variance-tradeoff">Bias-Variance Trade-Off</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/metrics-and-evaluation">Metrics and Evaluation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/data-preparation">Data Preparation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/datascience/data-encoding">Data Encoding</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/feature-selection">Feature Selection</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/datascience/algorithms/linear-regression">Algorithms</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/datascience/timeseries/prophet">Time-Series Forecasting</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/datascience/deep-learning/deep-learning-basics">Deep Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/mlops">MLOps</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/casestudies/99group">Extras</a></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Data Science</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Basics</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Data Encoding</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Data Encoding</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="discrete-data">Discrete Data<a class="hash-link" href="#discrete-data" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="label-encoding">Label Encoding<a class="hash-link" href="#label-encoding" title="Direct link to heading">​</a></h3><p>Label encoding is perhaps the simplest and most direct method of encoding discrete data – each unique category is associated with a single integer label. This is almost always not the final encoding that you should use for categorical variables, since attaching encodings in this way forces us to make arbitrary decisions that lead to meaningful outcomes. If we associate a category value “Dog” with 1 but “Snake” with 2, the model has access to the explicitly coded quantitative relationship that “Snake” is two times “Dog” in magnitude or that “Snake” is larger than “Dog.” Moreover, there is no good reason “Dog” should be labeled 1 and “Snake” should be labeled 2 instead of vice versa. However, label encoding is the basis/primary step upon which many other encodings can be applied. Thus, it is useful to understand how to implement it.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230728515-0bcd376f-f8ad-465b-abfb-6c0e86995d88.png" alt="label_encoding" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="one-hot-encoding">One-Hot Encoding<a class="hash-link" href="#one-hot-encoding" title="Direct link to heading">​</a></h3><p>In cases of categorical variables in which no definitive quantitative label can be attached, the simplest satisfactory choice is generally one-hot encoding. If there are n unique classes, we create n binary columns, each representing whether the item belongs to that class or not. Thus, there will be one “1” across each of the n columns for every row (and all others as “0”).</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230728517-dc3be36c-a669-42a5-8052-daeaea45ebaa.png" alt="onehot-encoding" class="img_ev3q"></p><p>One problem that may arise from using one-hot encoding, however, is multicollinearity. Multicollinearity occurs when several features are highly correlated such that one can reliably be predicted as a linear relationship of the others. In a one-hot encoding, the sum of feature values across each row is always 1; if we know the values of all other features for some row, we also know the value of the remaining feature.</p><p>This can become problematic because each feature is no longer independent, whereas many machine learning algorithms like K-Nearest Neighbors (KNN) and regression assume that each dimension of the dataset is not correlated with any other. While multicollinearity may only have a marginal negative effect on model performance, the larger problem is the effect on parameter interpretation. If two independent variables in a Linear Regression model are highly correlated, their resulting parameters after training become almost meaningless because the model could have performed just as well with another set of parameters (e.g., switching the two parameters, ambiguous multiplicity of solutions). Highly correlated features act as approximate duplicates, which means that the corresponding coefficients are halved as well.</p><p>One simple method to address multicollinearity in one-hot encoding is to randomly drop one (or several) of the features in the encoded feature set. This has the effect of disrupting a uniform sum of 1 across each row while still retaining a unique combination of values for each item (one of the categories will be defined by all zeros, since the feature that would have been marked as “1” was dropped). The disadvantage is that the equality of representation across classes of the encoding is now unbalanced, which may disrupt certain machine learning models – especially ones that utilize regularization. The take-away for best performance: choose either regularization + feature selection or column dropping, but not both.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="binary-encoding">Binary Encoding<a class="hash-link" href="#binary-encoding" title="Direct link to heading">​</a></h3><p>Two weaknesses of one-hot encoding – sparsity and multicollinearity – can be addressed, or at least improved, with binary encoding. The categorical feature is label encoded (i.e., each unique category is associated with an integer); the labels are converted to binary form and transferred to a set of features in which each column is one place value. That is, a column is created for each digit place of the binary representation.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230728572-d774acbd-89cf-4cce-89aa-8d885ccff95e.png" alt="binary-encoding" class="img_ev3q"></p><p>Because we use binary representations rather than allocating one column per unique class, we more compactly represent the same information (i.e., the same class has the same combination of “1”s and “0”s across the features) at the cost of decreased interpretability (i.e., it’s not clear what each column represents). Moreover, there is no reliable multicollinearity between each of the features used to represent the categorical information.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="frequency-encoding">Frequency Encoding<a class="hash-link" href="#frequency-encoding" title="Direct link to heading">​</a></h3><p>Label encoding, one-hot encoding, and binary encoding each offer methods of encoding that reflect the “pure identity” of each unique class; that is, we devise quantitative methods to assign a unique identity to each class.</p><p>However, we can both assign unique values to each class and communicate additional information about each class at once. To frequency-encode a feature, we replace each categorical value with the proportion of how often that class appears in the dataset. With frequency encoding, we communicate how often that class appears in the dataset, which may be of value to whatever algorithm is processing it.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230728632-f5ed3606-ede0-4846-b51e-67c6bfd09076.png" alt="frequency-encoding" class="img_ev3q"></p><p>Like one-hot encoding and binary encoding, we attach a unique quantitative representation to each class (although frequency encoding does not guarantee a unique quantitative representation, especially in small datasets). With this encoding scheme, however, the actual value of the representation lies on a continuous scale, and quantitative relationships between the encodings are not arbitrary but instead communicate some piece of information. In the preceding example, “Denver” is “three times” Miami because it appears three times as often in the dataset.</p><p>Frequency encoding is the most powerful when the dataset is representative and free of bias. If this is not the case, the actual quantitative encodings may be meaningless in the sense of not providing relevant and truthful/representative information for the purposes of modeling.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="target-encoding">Target Encoding<a class="hash-link" href="#target-encoding" title="Direct link to heading">​</a></h3><p>Frequency encoding is often unsatisfactory because it often doesn’t directly reflect information in a class that is directly relevant to a model that uses it. Target encoding is an attempt to model the relationship more directly between the categorical class x and the dependent variable y to be predicted by replacing each class with the mean or median (respectively) value of y for that class. It is assumed that the target class is already in quantitative form, although the target does not necessarily need to be continuous (i.e. a regression problem) to be used in target encoding. For instance, taking the mean of binary classification labels, which are either 0 or 1, gives insight into the proportion of items in the dataset with that class that were associated with class 0. This can be interpreted as the probability the item belongs to a target class given only one independent feature.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230728947-2e21e162-0a82-4747-8bd9-240d143e136c.png" alt="525591_1_En_2_Fig12_HTML" class="img_ev3q"></p><p>Figure: Target encoding using the mean</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230728948-7dd2208a-5b83-4c32-bf91-33c77ca2f189.png" alt="525591_1_En_2_Fig13_HTML" class="img_ev3q"></p><p>Figure: Target encoding using the median</p><p>Note that target encoding can lead to data leakage if encoding is performed before the training and validation sets are split, since the averaging function incorporates information from both the training and validation sets. In order to prevent this, encode the training and validation sets separately after splitting. If the validation dataset is too small, target-encoding the set independently may yield skewed, unrepresentative encodings. In this case, you can use averages per class from the training dataset. This form of “data leakage” is not inherently problematic, since we are using training data to inform operation on the validation set rather than using validation data to inform operation on the training set.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="leave-one-out-encoding">Leave-One-Out Encoding<a class="hash-link" href="#leave-one-out-encoding" title="Direct link to heading">​</a></h3><p>Mean-based target encoding can be quite powerful, but it suffers from the presence of outliers. If outliers are present that skew the mean, their effects are imprinted across the entire dataset. Leave-one-out encoding is a variation on the target encoding scheme by leaving the “current” item/row out of consideration when calculating the mean for all items of that class. Like target encoding, encoding should be performed separately on training and validation sets to prevent data leakage.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230729170-913e381b-941d-451a-99c8-f3e9e2aece07.png" alt="525591_1_En_2_Fig15_HTML" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="james-stein-encoding">James-Stein Encoding<a class="hash-link" href="#james-stein-encoding" title="Direct link to heading">​</a></h3><p>Target encoding and leave-one-out encoding assume that each categorical feature is directly and linearly related to the dependent variable. We can take a more sophisticated approach to encoding with James-Stein encoding by incorporating both the overall mean for a feature and the individual mean per class for a feature into an encoding (Figure 2-17). This is achieved by defining the encoding for a category as a weighted sum of the overall mean and individual mean per class via a parameter β, which is bounded by 0 ≤ β ≤ 1.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230729233-a7cfcf61-0db4-413b-8235-55243b45b0b0.png" alt="525591_1_En_2_Fig17_HTML" class="img_ev3q"></p><p>When β = 0, James-Stein encoding is the same as mean-based target encoding. On the other hand, when β = 1, James-Stein encoding replaces all values in a column with the average dependent variable value, regardless of individual class values.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="weight-of-evidence">Weight of Evidence<a class="hash-link" href="#weight-of-evidence" title="Direct link to heading">​</a></h3><p>The weight of evidence (WoE) technique originated from credit scoring; it was used to measure how “separable” good customers (paid back a loan) were from bad customers (defaulted on a loan) across a group i (this could be something like customer location, history, etc.)</p><p>Weight of evidence is often presented as representing how much the evidence undermines or supports the hypothesis. In the context of categorical encoding, the “hypothesis” is that the selected categorical feature can cleanly divide classes such that we can reliably predict which class an item falls in given only information about its inclusion or exclusion from the group i. The “evidence” is the actual distribution of target values within a certain group i.</p><p>We can also generalize this to multiclass problems by finding the WoE for each class, in which “class 0” is “in class” and “class 1” is “not in class”; the weight of evidence of the complete dataset can then be found by somehow aggregating the individual class-specific WoE calculation, for example, by taking the mean.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="continuous-data">Continuous Data<a class="hash-link" href="#continuous-data" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="min-max-scaling">Min-Max Scaling<a class="hash-link" href="#min-max-scaling" title="Direct link to heading">​</a></h3><p>Min-max scaling generally refers to the scaling of the range of a dataset such that it is between 0 and 1 – the minimum value of the dataset is 0, and the maximum is 1, but the relative distances between points remain the same.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="robust-scaling">Robust Scaling<a class="hash-link" href="#robust-scaling" title="Direct link to heading">​</a></h3><p>From the formula for min-max scaling, we see that each scaled value of the dataset is directly impacted by the maximum and minimum values. Hence, outliers significantly impact the scaling operation. Robust scaling subtracts the median value from all values in the dataset and divides by the interquartile range.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="standardization">Standardization<a class="hash-link" href="#standardization" title="Direct link to heading">​</a></h3><p>More commonly, machine learning algorithms assume that data is standardized – that is, in the form of a normal distribution with unit variance (standard deviation of 1) and zero mean (centered at 0). Assuming the input data is already somewhat normally distributed, standardization subtracts the dataset’s mean and divides by the dataset’s standard deviation. This has the effect of shifting the dataset mean to 0 and scaling the standard deviation to 1.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="text-data">Text Data<a class="hash-link" href="#text-data" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="raw-vectorization">Raw Vectorization<a class="hash-link" href="#raw-vectorization" title="Direct link to heading">​</a></h3><p>Raw vectorization can be thought of as “one-hot encoding” for text: it is an explicit quantitative representation of the information contained within text. Rather than assigning each text a unique class, texts are generally vectorized as a sequence of language units, like characters or words. These are also referred to as tokens. Each of these words or characters is considered to be a unique class, which can be one-hot encoded. Then, a passage of text is a sequence of one-hot encodings.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230732506-ccfe1770-c289-4702-a652-73ffb8abdc97.png" alt="525591_1_En_2_Fig28_HTML" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bag-of-words">Bag of Words<a class="hash-link" href="#bag-of-words" title="Direct link to heading">​</a></h3><p>In order to reduce the sheer dimensionality/size of a raw vectorization text representation, we can use the Bag of Words (BoW) model to “collapse” raw vectorizations. In Bag of Words, we count how many times each language unit appears in a text sample while ignoring the specific order and context in which the language units were used.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230732553-d1e7cec3-760f-4f8d-9e8c-0c5a153572db.png" alt="525591_1_En_2_Fig29_HTML" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="n-grams">N-Grams<a class="hash-link" href="#n-grams" title="Direct link to heading">​</a></h3><p>We can be more sophisticated than the Bag of Words model by counting the number of unique two-word combinations, or bigrams. This can help reveal context and multiplicity of word meaning; for instance, the Paris in the stripped (no punctuation, no capitalization) text except “paris france” is very different from the Paris in “paris hilton.” We can consider each bigram to be its own term and encode it as such.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/230732621-0c4d3fd4-f080-4647-b7ad-62cfbcb51011.png" alt="525591_1_En_2_Fig30_HTML" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tf-idf">TF-IDF<a class="hash-link" href="#tf-idf" title="Direct link to heading">​</a></h3><p>Another weakness of the Bag of Words model is that the number of times a word appears in the text may not be a good indicator of how important or relevant it is. For instance, the word “the” appears seven times in this paragraph, more than any other. Does this mean that the word “the” is the most significant or holds the most meaning?</p><p>No, the word “the” is primarily an artifact of grammar/syntactic structure and reflects little semantic meaning, at least in contexts we are generally concerned with. We usually address the problem of text-saturating syntactic tokens by removing so-called “stop words” from a corpus before encoding it.</p><p>However, there are many words left over from stop word screening that hold semantic value but suffer from another problem that the word “the” creates: Because of the structure of the corpus, certain words inherently appear very often throughout the text. This does not mean that they are more important. Consider a corpus of customer reviews for a jacket: naturally, the word “jacket” will appear very often (e.g., “I bought this jacket…,” “This jacket arrived at my house…”), but in actuality it is not very relevant to our analysis. We know that the corpus is about the jacket and care instead about words that may occur less but mean more, like “bad” (e.g., “This jacket is bad”), “durable” (e.g., “Such a durable jacket!”), or “good” (e.g., “This was a good buy”).</p><p>We can formalize this intuition by using TF-IDF, or Term Frequency–Inverse Document Frequency, encoding. The logic behind TF-IDF encoding is that we care more about terms that appear often in one document (Term Frequency) but not very often across the entire corpus (Inverse Document Frequency). TF-IDF is calculated by weighting these two effects against each other.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="word2vec">Word2Vec<a class="hash-link" href="#word2vec" title="Direct link to heading">​</a></h3><p>Previous discussion on encoding methods focused on relatively simplistic attempts to capture a text sample’s meaning by attempting to extract one “dimension” or perspective. The Bag of Words model, for instance, captures meaning simply by counting how often a word appears in the text. The Term Frequency–Inverse Document Frequency encoding method attempts to improve upon this scheme by defining a slightly more sophisticated level of meaning by balancing the occurrence of a word in a document with its occurrence in the complete corpus. In these encoding schemes, there is always one perspective or dimension of the text that we leave out and simply cannot capture.</p><p>With deep neural networks, however, we can capture more complex relationships between text samples – the nuances of word usage (e.g., “Paris,” “Hilton,” and “Paris Hilton” all mean very different things!), grammatical exceptions, conventions, cultural significance, etc. The Word2Vec family of algorithms associates each word with a fixed-length vector representing latent (“hidden”, “implicit”) features.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="time-data">Time Data<a class="hash-link" href="#time-data" title="Direct link to heading">​</a></h2><p>Time/temporal data often appears in practical tabular datasets. For instance, a tabular dataset of online customer reviews may have a timestamp down to the second indicating exactly when it was posted. Alternatively, a tabular dataset of medical data might be associated with the day it was collected, but not the exact time. A tabular dataset of quarterly company earnings reports will contain temporal data by quarter. Time is a dynamic and complex data type that takes on many different forms and sizes. Luckily, because time is both so rich with information and well-understood, it is relatively easy to encode time or temporal features.</p><p>There are several methods to convert time data into a quantitative representation to make it readable to machine learning and deep learning models. The simplest method is simply to assign a time unit as a base unit and represent each time value as a multiple of base units from a starting time. The base unit should generally be the most relevant unit of time to the prediction problem; for instance, if time is stored as a month, date, and year and the prediction task is to predict sales, the base unit is a day, and we would represent each date as the number of days since a starting date (a convenient starting position like January 1, 1900, or simply the earliest date in the dataset). On the other hand, in a physics lab, we may need a base unit of a nanosecond due to high required precision, and time may be represented as the number of nanoseconds since some determined starting time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="geographical-data">Geographical Data<a class="hash-link" href="#geographical-data" title="Direct link to heading">​</a></h2><p>Many tabular datasets will contain geographical data, in which a location is somehow specified in the dataset. Similarly to temporal/time data, geographical data can exist in several different levels of scope – by continent, country, state/province, city, zip code, address, or longitude and latitude, to name a few. Because of the information-rich and highly context-dependent nature of geographical data, there aren’t well-established, sweeping guidelines on encoding geographical data. However, you can use many of the previously discussed encoding tools and strategies to your advantage here.</p><p>If your dataset contains geographical data in categorical form, like by country or state/province, you can use previously discussed categorical encoding methods, like one-hot encoding or target encoding.</p><p>Latitude and longitude are precise geospatial location indicators already in quantitative form, so there is no requirement for further encoding. However, you may find it valuable to add relevant abstract information derived from the latitude and longitude to the dataset, like which country the location falls in.</p><p>When working with specific addresses, you can extract multiple relevant features, like the country, state/province, zip code, and so on. You can also derive the exact longitude and latitude from the address and append both to the dataset as continuous quantitative representations of the address location.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/datascience/data-preparation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Data Preparation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/datascience/feature-selection"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Feature Selection</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#discrete-data" class="table-of-contents__link toc-highlight">Discrete Data</a><ul><li><a href="#label-encoding" class="table-of-contents__link toc-highlight">Label Encoding</a></li><li><a href="#one-hot-encoding" class="table-of-contents__link toc-highlight">One-Hot Encoding</a></li><li><a href="#binary-encoding" class="table-of-contents__link toc-highlight">Binary Encoding</a></li><li><a href="#frequency-encoding" class="table-of-contents__link toc-highlight">Frequency Encoding</a></li><li><a href="#target-encoding" class="table-of-contents__link toc-highlight">Target Encoding</a></li><li><a href="#leave-one-out-encoding" class="table-of-contents__link toc-highlight">Leave-One-Out Encoding</a></li><li><a href="#james-stein-encoding" class="table-of-contents__link toc-highlight">James-Stein Encoding</a></li><li><a href="#weight-of-evidence" class="table-of-contents__link toc-highlight">Weight of Evidence</a></li></ul></li><li><a href="#continuous-data" class="table-of-contents__link toc-highlight">Continuous Data</a><ul><li><a href="#min-max-scaling" class="table-of-contents__link toc-highlight">Min-Max Scaling</a></li><li><a href="#robust-scaling" class="table-of-contents__link toc-highlight">Robust Scaling</a></li><li><a href="#standardization" class="table-of-contents__link toc-highlight">Standardization</a></li></ul></li><li><a href="#text-data" class="table-of-contents__link toc-highlight">Text Data</a><ul><li><a href="#raw-vectorization" class="table-of-contents__link toc-highlight">Raw Vectorization</a></li><li><a href="#bag-of-words" class="table-of-contents__link toc-highlight">Bag of Words</a></li><li><a href="#n-grams" class="table-of-contents__link toc-highlight">N-Grams</a></li><li><a href="#tf-idf" class="table-of-contents__link toc-highlight">TF-IDF</a></li><li><a href="#word2vec" class="table-of-contents__link toc-highlight">Word2Vec</a></li></ul></li><li><a href="#time-data" class="table-of-contents__link toc-highlight">Time Data</a></li><li><a href="#geographical-data" class="table-of-contents__link toc-highlight">Geographical Data</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.251db5a0.js"></script>
<script src="/assets/js/main.1462881d.js"></script>
</body>
</html>