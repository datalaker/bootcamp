<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-datascience/computer-vision/README">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Computer Vision | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/datascience/computer-vision"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Computer Vision | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Categories"><meta data-rh="true" property="og:description" content="Categories"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/datascience/computer-vision"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/datascience/computer-vision" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/datascience/computer-vision" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.e4e34528.css">
<link rel="preload" href="/assets/js/runtime~main.166317d0.js" as="script">
<link rel="preload" href="/assets/js/main.3580ce8c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/bootcamp">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bootcamp">Recohut Data Bootcamps</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/developer-foundations/install-anaconda">foundations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/storage/apache-couchdb">storage</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/processing/apache-beam">processing</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/data-modeling/3nf-data-modeling">data-modeling</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/extraction">Data Extraction</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data Extraction&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/orchestration">Orchestration</a><button aria-label="Toggle the collapsible sidebar category &#x27;Orchestration&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/devops">DevOps</a><button aria-label="Toggle the collapsible sidebar category &#x27;DevOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/visualization">Visualization</a><button aria-label="Toggle the collapsible sidebar category &#x27;Visualization&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/datascience/algorithms/decision-trees">datascience</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/datascience/algorithms/decision-trees">algorithms</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/datascience/basics/cross-domain">basics</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/bayesian-optimization">Bayesian Optimization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/bias-variance-tradeoff">Bias-Variance Trade-Off</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/datascience/challenges">Challenges</a><button aria-label="Toggle the collapsible sidebar category &#x27;Challenges&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible menu__list-item-collapsible--active"><a class="menu__link menu__link--sublist menu__link--active" aria-current="page" aria-expanded="true" tabindex="0" href="/docs/datascience/computer-vision">Computer Vision</a><button aria-label="Toggle the collapsible sidebar category &#x27;Computer Vision&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/computer-vision/lab-agri-setallite-image-segmentation">Agricultural Satellite Image Segmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/computer-vision/lab-image-analytics-tensorflow">Image Analytics with Tensorflow</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/data-encoding">Data Encoding</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/data-preparation">Data Preparation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/data-splits">Data Splits</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/datascience/deep-learning/deep-learning-basics">deep-learning</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/feature-selection">Feature Selection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/metrics-and-evaluation">Metrics and Evaluation</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/datascience/nlp">Natural Language Processing (NLP)</a><button aria-label="Toggle the collapsible sidebar category &#x27;Natural Language Processing (NLP)&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/datascience/recsys">Recommender Systems</a><button aria-label="Toggle the collapsible sidebar category &#x27;Recommender Systems&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/regression">Regression</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/datascience/reinforcement-learning">Experimentation</a><button aria-label="Toggle the collapsible sidebar category &#x27;Experimentation&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/datascience/sagemaker">Amazon Sagemaker</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/assignments">Assignments</a><button aria-label="Toggle the collapsible sidebar category &#x27;Assignments&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/capstones">Capstones</a><button aria-label="Toggle the collapsible sidebar category &#x27;Capstones&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/mlops">MLOps</a><button aria-label="Toggle the collapsible sidebar category &#x27;MLOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/a1-interviewprep">Interview Preparation</a><button aria-label="Toggle the collapsible sidebar category &#x27;Interview Preparation&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/a3-casestudies">Case Studies</a><button aria-label="Toggle the collapsible sidebar category &#x27;Case Studies&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/b3-misc/explore-further">b3-misc</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/mathematics">Mathematics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Mathematics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">datascience</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Computer Vision</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Computer Vision</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="categories">Categories<a class="hash-link" href="#categories" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-similarity">Image Similarity<a class="hash-link" href="#image-similarity" title="Direct link to heading">​</a></h3><p>Image similarity is the measure of how similar two images are. In other words, it quantifies the degree of similarity between intensity patterns in two images.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822791-efe44b82-976d-46ca-81ee-eeb700ee641d.png" alt="content-concepts-raw-computer-vision-image-similarity-slide19" class="img_ev3q"></p><ul><li><strong>Applications:</strong> Duplicate product detection, image clustering, visual search, product recommendations.</li><li><strong>Scope:</strong> Fine-tuning on classes for greater accuracy</li><li><strong>Tools:</strong> TFHub</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models">Models<a class="hash-link" href="#models" title="Direct link to heading">​</a></h4><ul><li><em><a href="https://arxiv.org/abs/1710.05649" target="_blank" rel="noopener noreferrer">DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. arXiv, 2017.</a></em></li><li>ConvNets: Pre-trained models like MobileNet, EfficientNet, BiT-L/BiT-M can be used to convert images into vectors. These models can be found on TFHub. For more accuracy, fine-tuning can be done.</li><li>FAISS: <em><a href="https://arxiv.org/abs/1702.08734" target="_blank" rel="noopener noreferrer">Billion-scale similarity search with GPUs. arXiv, 2017.</a></em> - Faiss is a library for efficient similarity search and clustering of dense vectors.</li><li>Siamese Network: Siamese network is a neural network that contains two or more identical subnetwork. The purpose of this network is to find the similarity or comparing the relationship between two comparable things. Unlike the classification task that uses cross-entropy as the loss function, the siamese network usually uses contrastive loss or triplet loss.</li><li>Similarity Measures: L1 (Manhattan distance), L2 (Euclidean distance), Hinge Loss for Triplets.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow">Process flow<a class="hash-link" href="#process-flow" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect Images - Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended.</li><li>Step 2: Encoder Fine-tuning - Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.</li><li>Step 3: Image Vectorization - Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save the feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.</li><li>Step 4: Metadata and Indexing - We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index objects for later use.</li><li>Step 5: UAT Testing - Wrap the model inference engine in API for client testing. We will receive an image from user, encode it with our image encoder, find Top-K similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.</li><li>Step 6: Deployment - Deploy the model on cloud or edge as per the requirement.</li><li>Step 7: Documentation - Prepare the documentation and transfer all assets to the client.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases">Use Cases<a class="hash-link" href="#use-cases" title="Direct link to heading">​</a></h4><p><strong>Multi-endpoint API Similarity System</strong></p><p>The task was to build an API that will support multiple endpoints. Each endpoint supports a separate similarity system. We built 2 endpoints: endpoint 1 would find tok-K most similar fashion images and endpoint 2 would find top-K most similar food images. Checkout the notion <a href="https://www.notion.so/Multi-endpoint-Image-Similarity-System-159b47b635ea42299a0214551630e740" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Beanstalk Image Similarity System</strong></p><p>There are 2 endpoints in the API - one for training and the other for inference. During training, the system will receive a zipped file of images. At the time of inference, this trained system would receive an image over inference endpoint and send back top-K most similar images with a confidence score. The API was deployed on AWS beanstalk. Checkout the notion <a href="https://www.notion.so/Image-Similarity-AWS-b8f33261750047a69744e91a554eabff" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Image + Text Similarity</strong></p><p>Use the textual details and images of products, find the exact similar product among different groups. Around 35 GB of retail product images was scraped and used to build the system. Checkout the notion <a href="https://www.notion.so/Image-Text-Similarity-fe5130324ae14ab48a30c93444348f4a" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Siamese Network Image Similarity on MNIST</strong></p><p>Siamese networks are incredibly powerful networks, responsible for significant increases in face recognition, signature verification, and prescription pill identification applications. The objective was to build image pairs for the siamese network, train the siamese network with TF Keras, and then compare image similarity with this siamese network.</p><p><strong>Visual Recommendation</strong></p><p>Use image similarity to recommend users visually similar products based on what they searched. Checkout the notion <a href="https://www.notion.so/Image-Similarity-Detection-in-Action-with-Tensorflow-2-0-c2b4421d75dd42a3a1becf9c98251ccb" target="_blank" rel="noopener noreferrer">here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="object-detection">Object Detection<a class="hash-link" href="#object-detection" title="Direct link to heading">​</a></h3><p>Object detection is a computer vision technique that allows us to identify and locate objects in an image or video.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822793-68a55d64-a10e-4ae2-aa2b-53c3c8100117.png" alt="content-concepts-raw-computer-vision-object-detection-slide29" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h4><ul><li><strong>Applications:</strong> Crowd counting, Self-driving cars, Video surveillance, Face detection, Anomaly detection</li><li><strong>Scope:</strong> Detect objects in images and videos, 2-dimensional bounding boxes, Real-time</li><li><strong>Tools:</strong> Detectron2, TF Object Detection API, OpenCV, TFHub, TorchVision</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-1">Models<a class="hash-link" href="#models-1" title="Direct link to heading">​</a></h4><ul><li><em><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener noreferrer">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. arXiv, 2016.</a></em></li><li><em><a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener noreferrer">SSD: Single Shot MultiBox Detector. CVPR, 2016.</a></em></li><li><a href="https://arxiv.org/abs/1804.02767" target="_blank" rel="noopener noreferrer"><em>YOLOv3: An Incremental Improvement. arXiv, 2018.</em></a></li><li><em><a href="https://arxiv.org/abs/1911.09070" target="_blank" rel="noopener noreferrer">EfficientDet: Scalable and Efficient Object Detection. CVPR, 2020.</a></em> - It achieved 55.1 AP on COCO test-dev with 77M parameters.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-1">Process flow<a class="hash-link" href="#process-flow-1" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets</li><li>Step 2: Create Labels - This step is required only if the object category is not available in any pre-trained model or labels are not freely available on the web. To create the labels (bounding boxes) using either open-source tools like Labelme or any other professional tool</li><li>Step 3: Data Acquisition - Setup the database connection and fetch the data into python environment</li><li>Step 4: Data Exploration - Explore the data, validate it and create preprocessing strategy</li><li>Step 5: Data Preparation - Clean the data and make it ready for modeling</li><li>Step 6: Model Building - Create the model architecture in python and perform a sanity check</li><li>Step 7: Model Training - Start the training process and track the progress and experiments</li><li>Step 8: Model Validation - Validate the final set of models and select/assemble the final model</li><li>Step 9: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 10: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 11: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-1">Use Cases<a class="hash-link" href="#use-cases-1" title="Direct link to heading">​</a></h4><p><strong>Automatic License Plate Recognition</strong></p><p>Recognition of vehicle license plate number using various methods including YOLO4 object detector and Tesseract OCR. Checkout the notion <a href="https://www.notion.so/Automatic-License-Plate-Recognition-10ec22181b454b1facc99abdeadbf78f" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Object Detection App</strong></p><p>This is available as a streamlit app. It detects common objects. 3 models are available for this task - Caffe MobileNet-SSD, Darknet YOLO3-tiny, and Darknet YOLO3. Along with common objects, this app also detects human faces and fire. Checkout the notion <a href="https://www.notion.so/Object-Detector-App-c60fddae2fcd426ab763261436fb15d8" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Logo Detector</strong></p><p>Build a REST API to detect logos in images. API will receive 2 zip files - 1) a set of images in which we have to find the logo and 2) an image of the logo. Deployed the model in AWS Elastic Beanstalk. Checkout the notion <a href="https://www.notion.so/Logo-Detection-91bfe4953dcf4558807b342efe05a9ff" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>TF Object Detection API Experiments</strong></p><p>The TensorFlow Object Detection API is an open-source framework built on top of TensorFlow that makes it easy to construct, train, and deploy object detection models. We did inference on pre-trained models, few-shot training on single class, few-shot training on multiple classes and conversion to TFLite model. Checkout the notion <a href="https://www.notion.so/Tensorflow-Object-Detection-API-499b017e502d4950a9d448fb35a41d58" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Pre-trained Inference Experiments</strong></p><p>Inference on 6 pre-trained models - Inception-ResNet (TFHub), SSD-MobileNet (TFHub), PyTorch YOLO3, PyTorch SSD, PyTorch Mask R-CNN, and EfficientDet. Checkout the notion <a href="https://www.notion.so/Object-Detection-Inference-Experiments-568fa092b1d34471b676fd43a42974b2" target="_blank" rel="noopener noreferrer">here</a> and <a href="https://www.notion.so/Object-Detection-Inference-with-Pre-trained-models-da9e2e5bfab944bc90f568f6bc4b3e1f" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Object Detection App</strong></p><p>TorchVision Mask R-CNN model Gradio App. Checkout the notion <a href="https://www.notion.so/MaskRCNN-TorchVision-Object-Detection-Gradio-App-c22f2a13ab63493b9b38720b20c50051" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Real-time Object Detector in OpenCV</strong></p><p>Build a model to detect common objects like scissors, cups, bottles, etc. using the MobileNet SSD model in the OpenCV toolkit. It will task input from the camera and detect objects in real-time. Checkout the notion <a href="https://www.notion.so/Object-Detection-with-OpenCV-MobileNet-SSD-38ff496d2f0d427185a9c51cebc1ddf2" target="_blank" rel="noopener noreferrer">here</a>. Available as a Streamlit app also (this app is not real-time).</p><p><strong>EfficientDet Fine-tuning</strong></p><p>Fine-tune YOLO4 model on new classes. Checkout the notion <a href="https://www.notion.so/EfficientDet-fine-tuning-01a6ffd1e11f4dc1941073aff4b9b486" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>YOLO4 Fine-tuning</strong></p><p>Fine-tune YOLO4 model on new classes. Checkout the notion <a href="https://www.notion.so/YOLO-4-b32c2d2a4b8644b59f1c05e6887ffcca" target="_blank" rel="noopener noreferrer">here</a>.</p><p><strong>Detectron2 Fine-tuning</strong></p><p>Fine-tune Detectron2 Mask R-CNN (with PointRend) model on new classes. Checkout the notion <a href="https://www.notion.so/YOLO-4-b32c2d2a4b8644b59f1c05e6887ffcca" target="_blank" rel="noopener noreferrer">here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-segmentation">Image Segmentation<a class="hash-link" href="#image-segmentation" title="Direct link to heading">​</a></h3><p>Image segmentation is the task of assigning labels to each pixel of an image.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822790-17e672b8-9485-4189-be5c-38032dc37d11.png" alt="content-concepts-raw-computer-vision-image-segmentation-slide46" class="img_ev3q"></p><ul><li><strong>Applications:</strong> Medical imaging, self-driving cars, satellite imaging</li><li><strong>Scope:</strong> Semantic and Instance masks, 2D pixel-mask, Real-time</li><li><strong>Tools:</strong> Detectron2, TFHub, TorchVision, DeepLab</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-2">Models<a class="hash-link" href="#models-2" title="Direct link to heading">​</a></h4><ul><li><em><a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener noreferrer">U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv, 2015.</a></em> - It was originally designed to perform medical image segmentation but it works well on a wide variety of tasks, from segmenting cells on microscope images to detecting ships or houses on photos taken from satellites.</li><li><em><a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener noreferrer">Mask R-CNN. arXiv, 2017.</a></em> - The Mask R-CNN framework is built on top of Faster R-CNN. ****So, for a given image, Mask R-CNN, in addition to the class label and bounding box coordinates for each object, will also return the object mask.</li><li>DeepLabV3+: <em><a href="https://arxiv.org/abs/1802.02611" target="_blank" rel="noopener noreferrer">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. arXiv, 2018.</a></em> - It achieves a mean IOU score of 89% on the PASCAL VOC 2012 dataset.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-2">Process flow<a class="hash-link" href="#process-flow-2" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets</li><li>Step 2: Create Labels - This step is required only if the object category is not available in any pre-trained model or labels are not freely available on the web. To create the labels (pixel masks) using either open-source tools like Labelme or any other professional tool</li><li>Step 3: Data Acquisition - Setup the database connection and fetch the data into python environment</li><li>Step 4: Data Exploration - Explore the data, validate it and create preprocessing strategy</li><li>Step 5: Data Preparation - Clean the data and make it ready for modeling</li><li>Step 6: Model Building - Create the model architecture in python and perform a sanity check</li><li>Step 7: Model Training - Start the training process and track the progress and experiments</li><li>Step 8: Model Validation - Validate the final set of models and select/assemble the final model</li><li>Step 9: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 10: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 11: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-2">Use Cases<a class="hash-link" href="#use-cases-2" title="Direct link to heading">​</a></h4><p><strong>Satellite Image Segmentation for Agricultural Fields</strong></p><p>An image with 1800 x 1135 resolution and 60 channels. Every Month 5 bands images were shot from agricultural land for 12 months. There is 8 type of croplands. Task is to classify all unknown label pixels into one of these 8 categories. U-Net model was trained from scratch on patches. Checkout <a href="https://www.notion.so/F991454-Satellite-Image-Segmentation-for-Agricultural-Fields-9914b549617746578c509e0382deb211" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Detectron2 Fine-tuning</strong></p><p>Fine-tune Detectron2 Mask R-CNN (with PointRend) model on new classes. It supports semantic, instance, and panoptic segmentation. We fine-tuned on balloons, chipsets, and faces. Checkout <a href="https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Industrial Use Cases for Image Segmentation</strong></p><p>Experimented with 3 industrial use cases - Carvana Vehicle Image Masking, Airbus Ship Detection, and Severstal Steel Defect Detection. Checkout <a href="https://www.notion.so/Kaggle-Image-Segmentation-Experiments-770728c2ef9a493da20863789b112d78" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Real-time segmentation on Videos</strong></p><p>Real-time tracking and segmentation with SiamMask, semantic segmentation with LightNet++ and instance segmentation with YOLACT. Checkout <a href="https://www.notion.so/Image-Segmentation-Inference-Experiments-26fac32c220f419a902121129b2924db" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Image Segmentation Exercises</strong></p><p>Thresholding with Otsu and Riddler–Calvard, Image segmentation with self-organizing maps, Random Walk segmentation with scikit-image, Skin color segmentation with the GMM–EM algorithm, Medical image segmentation, Deep semantic segmentation, Deep instance segmentation. Checkout <a href="https://www.notion.so/Image-Segmentation-Exercises-cc3262c55d374fb684362f5d333fb91a" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>TorchVision Inference Experiments</strong></p><p>FCN-ResNet and DeepLabV3 (both are available in TorchVision library) inference. Available as a streamlit app. Checkout <a href="https://www.notion.so/FCN-ResNet-vs-DeepLab-App-FF841-5168fac2ed0b42b1ad95a0b9e8b26d53" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="face-detection-and-recognition">Face Detection and Recognition<a class="hash-link" href="#face-detection-and-recognition" title="Direct link to heading">​</a></h3><p>Analyze the facial features like age, gender, emotion, and identity.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822789-86a34319-c100-48be-9581-ea5671cca059.png" alt="content-concepts-raw-computer-vision-facial-analytics-img" class="img_ev3q"></p><p><strong>Applications:</strong> Identity verification, emotion detection</p><p><strong>Scope:</strong> Human faces only, Real-time</p><p><strong>Tools:</strong> OpenCV, dlib</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-3">Models<a class="hash-link" href="#models-3" title="Direct link to heading">​</a></h4><ul><li><em><a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf" target="_blank" rel="noopener noreferrer">FaceNet: A Unified Embedding for Face Recognition and Clustering. CVPR, 2015.</a></em></li><li><a href="https://arxiv.org/abs/1905.00641v2" target="_blank" rel="noopener noreferrer">RetinaFace: Single-stage Dense Face Localisation in the Wild. arXiv, 2019.</a></li><li><em><a href="https://arxiv.org/abs/1608.01041v2" target="_blank" rel="noopener noreferrer">FER+: Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution. arXiv, 2016.</a></em></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-3">Process flow<a class="hash-link" href="#process-flow-3" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets</li><li>Step 2: Create Labels - Compile a metadata table containing a unique id (preferably the same as the image name) for each face id.</li><li>Step 3: Data Preparation - Setup the database connection and fetch the data into the environment. Explore the data, validate it, and create a preprocessing strategy. Clean the data and make it ready for modeling</li><li>Step 4: Model Building - Create the model architecture in python and perform a sanity check. Start the training process and track the progress and experiments. Validate the final set of models and select/assemble the final model</li><li>Step 5: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 6: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 7: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-3">Use Cases<a class="hash-link" href="#use-cases-3" title="Direct link to heading">​</a></h4><p><strong>Automatic Attendance System via Webcam</strong></p><p>We use Face Recognition library and OpenCV to create a real-time webcam-based attendance system that will automatically recognizes the face and log an attendance into the excel sheet. Check out <a href="https://www.notion.so/Face-Recognition-based-Automated-Attendance-System-dfb6f70527994ea4be11caf69b054350" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Detectron2 Fine-tuning for face detection</strong></p><p>Fine-tuned detectron2 on human face dataset to detect the faces in images and videos. Check out <a href="https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="object-tracking">Object Tracking<a class="hash-link" href="#object-tracking" title="Direct link to heading">​</a></h3><p>Object tracking is the process of 1)** Taking an initial set of object detections (such as an input set of bounding box coordinates, 2) Creating a unique ID for each of the initial detections, and then 3) tracking each of the objects as they move around frames in a video, maintaining the assignment of unique IDs.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822794-bbebf98d-269b-46f7-bd51-aa9f417105c8.png" alt="content-concepts-raw-computer-vision-object-tracking-img" class="img_ev3q"></p><ul><li><strong>Applications:</strong> In-store consumer behavior tracking, Apply security policies like crowd management, traffic management, vision-based control, human-computer interface, medical imaging, augmented reality, robotics.</li><li><strong>Scope:</strong> Track objects in images and videos, 2-dimensional tracking, Bounding boxes and pixel masks, Single and Multiple Object Tracking</li><li><strong>Tools:</strong> Detectron2, OpenCV</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-4">Models<a class="hash-link" href="#models-4" title="Direct link to heading">​</a></h4><ul><li><strong>FairMOT</strong>: <em><a href="https://arxiv.org/abs/2004.01888" target="_blank" rel="noopener noreferrer">On the Fairness of Detection and Re-Identification in Multiple Object Tracking. arXiv, 2020.</a></em></li><li><strong>DeepSORT</strong>: <em><a href="https://arxiv.org/abs/1703.07402" target="_blank" rel="noopener noreferrer">Simple Online and Realtime Tracking with a Deep Association Metric. arXiv, 2017.</a></em> - Detect object with models like YOLO or Mask R-CNN and then track using DeepSORT.</li><li><strong>GOTURN</strong>: <em><a href="https://arxiv.org/abs/1604.01802" target="_blank" rel="noopener noreferrer">Learning to Track at 100 FPS with Deep Regression Networks. arXiv, 2016.</a></em> - CNN offline learning tracker.</li><li><strong>MDNet</strong>: <em><a href="https://arxiv.org/abs/1808.08834" target="_blank" rel="noopener noreferrer">Real-Time MDNet. arXiv, 2018.</a></em> - CNN online learning tracker.</li><li><strong>ROLO</strong>: <em><a href="https://arxiv.org/abs/1607.05781" target="_blank" rel="noopener noreferrer">Spatially Supervised Recurrent Convolutional Neural Networks for Visual Object Tracking. arXiv, 2016.</a></em> - CNN + LSTM tracker.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-4">Process flow<a class="hash-link" href="#process-flow-4" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect the data - Capture via camera, scrap from the internet or use public datasets</li><li>Step 2: Train Object Detection Model - Train an object detector model (or use existing one if available in open-source domain)</li><li>Step 3: Annotate the data - Apply object detector on the images to create a training set for object tracking</li><li>Step 4: Data Preparation - Clean the data and make it ready for modeling</li><li>Step 5: Train the Tracker - Build and train an object tracking model (e.g. DeepSORT, FairMOT) to accurately track the target object in images/videos. Track the progress and experiments</li><li>Step 6: Model Validation - Validate the final set of models and select/assemble the final model</li><li>Step 7: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 8: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 9: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-4">Use Cases<a class="hash-link" href="#use-cases-4" title="Direct link to heading">​</a></h4><p><strong>Pedestrian Tracking</strong></p><p>Pedestrian Tracking with YOLOv3 and DeepSORT. Check out <a href="https://www.notion.so/Pedestrian-Tracking-with-YOLOv3-and-DeepSORT-a38ea37a2abf4755aacc691bd6b859a1" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Object Tracking</strong></p><p>Object tracking with FRCNN and SORT. Check out <a href="https://www.notion.so/Object-tracking-with-FRCNN-and-SORT-e555d6174d2e4c1e993526c89555f96b" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Object Tracking</strong></p><p>Tested out 5 algorithms on videos - OpticalFlow, DenseFlow, Camshift, MeanShift and Single Object Tracking with OpenCV. Check out <a href="https://www.notion.so/Object-Tracking-with-OpenCV-and-Python-2bf91e9f6f49405ca40409c392a2d429" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Social Distancing Violation Detection</strong></p><p><strong>People and Vehicle Counter Detection</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pose-estimation">Pose Estimation<a class="hash-link" href="#pose-estimation" title="Direct link to heading">​</a></h3><p>Pose estimation is a computer vision task that infers the pose of a person or object in an image or video. This is typically done by identifying, locating, and tracking the number of key points on a given object or person. For objects, this could be corners or other significant features. And for humans, these key points represent major joints like an elbow or knee.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822795-bdcc650d-08c1-4e8a-9b67-f3a8a0aba2a5.png" alt="content-concepts-raw-computer-vision-pose-estimation-img" class="img_ev3q"></p><ul><li><strong>Applications:</strong> Activity recognition, motion capture, fall detection, plank pose corrector, yoga pose identifier, body ration estimation</li><li><strong>Scope:</strong> 2D skeleton map, Human Poses, Single and Multi-pose, Real-time</li><li><strong>Tools:</strong> Tensorflow PoseNet API</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-5">Models<a class="hash-link" href="#models-5" title="Direct link to heading">​</a></h4><ul><li><em><a href="https://arxiv.org/abs/1812.08008" target="_blank" rel="noopener noreferrer">OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv, 2016.</a></em> - A standard bottom-up model that supports real-time multi-person 2D pose estimation. The authors of the paper have shared two models – one is trained on the Multi-Person Dataset ( MPII ) and the other is trained on the COCO dataset. The COCO model produces 18 points, while the MPII model outputs 15 points.</li><li>PoseNet is a machine learning model that allows for Real-time Human Pose Estimation. PoseNet can be used to estimate either a single pose or multiple poses PoseNet v1 is trained on MobileNet backbone and v2 on ResNet backbone.</li></ul><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822796-8d922553-a504-43cf-9718-2ef84a23bc0b.png" alt="content-concepts-raw-computer-vision-pose-estimation-slide52" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-5">Process flow<a class="hash-link" href="#process-flow-5" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect Images - Capture via camera, scrap from the internet or use public datasets</li><li>Step 2: Create Labels - Use a pre-trained model like PoseNet, OpenPose to identify the key points. These key points are our labels for pose estimation based classification task. If the model is not compatible/available for the required key points (e.g. identify the cap and bottom of a bottle product to measure if manufacturing is correct), we have to first train a pose estimation model using transfer learning in that case (this is out of scope though, as we are only focusing on human poses and pre-trained models are already available for this use case)</li><li>Step 3: Data Preparation - Setup the database connection and fetch the data into the environment. Explore the data, validate it, and create a preprocessing strategy. Clean the data and make it ready for modeling</li><li>Step 4: Model Building - Create the model architecture in python and perform a sanity check. Start the training process and track the progress and experiments. Validate the final set of models and select/assemble the final model</li><li>Step 5: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 6: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 7: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-5">Use Cases<a class="hash-link" href="#use-cases-5" title="Direct link to heading">​</a></h4><p><strong>OpenPose Experiments</strong></p><p>Four types of experiments with pre-trained OpenPose model - Single and Multi-Person Pose Estimation with OpenCV, Multi-Person Pose Estimation with PyTorch and Pose Estimation on Videos. Check out <a href="https://www.notion.so/Pose-Estimation-with-OpenPose-2D8F5-7f01bee1534243f3836728d03a419969" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Pose Estimation Inference Experiments</strong></p><p>Experimented with pre-trained pose estimation models. Check out <a href="https://www.notion.so/Pose-Estimation-with-OpenPifPaf-7517E-8cb982455e01478e876c52e9324d8e6b" target="_blank" rel="noopener noreferrer">this</a> notion for experiments with the OpenPifPaf model, <a href="https://www.notion.so/Pose-Estimation-with-Keypoint-RCNN-in-TorchVision-96e6aad0f36f44d3bff28e60525c6d31" target="_blank" rel="noopener noreferrer">this</a> one for the TorchVision Keypoint R-CNN model, and <a href="https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb" target="_blank" rel="noopener noreferrer">this</a> notion for the Detectron2 model.</p><p><strong>Pose Detection on the Edge</strong></p><p>Train the pose detector using Teachable machine, employing the PoseNet model (multi-person real-time pose estimation) as the backbone and serve it to the web browser using ml5.js. This system will infer the end-users pose in real-time via a web browser. Check out <a href="https://teachablemachine.withgoogle.com/train/pose" target="_blank" rel="noopener noreferrer">this</a> link and <a href="https://www.notion.so/ml5-js-Pose-Estimation-with-PoseNet-5661cefe46b449998cc31838441dc26a" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Pose Detection on the Edge using OpenVINO</strong></p><p>Optimize the pre-trained pose estimation model using the OpenVINO toolkit to make it ready to serve at the edge (e.g. small embedded devices) and create an OpenVINO inference engine for real-time inference. Check out <a href="https://www.notion.so/OpenVINO-4c4fc4f167cc4601ade5795a241a60da" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scene-text-recognition">Scene Text Recognition<a class="hash-link" href="#scene-text-recognition" title="Direct link to heading">​</a></h3><p>Text—as a fundamental tool of communicating information—scatters throughout natural scenes, e.g., street signs, product labels, license plates, etc. Automatically reading text in natural scene images is an important task in machine learning and gains increasing attention due to a variety of applications. For example, accessing text in images can help the visually impaired understand the surrounding environment. To enable autonomous driving, one must accurately detect and recognize every road sign. Indexing text in images would enable image search and retrieval from billions of consumer photos on the internet.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822802-f8a98007-4d46-42bf-9198-0132a6a8a270.png" alt="content-concepts-raw-computer-vision-scene-text-recognition-img" class="img_ev3q"></p><ul><li><strong>Applications:</strong> Indexing of multimedia archives, recognizing signs in driver assisted systems, providing scene information to visually impaired people, identifying vehicles by reading their license plates.</li><li><strong>Scope:</strong> No scope decided yet.</li><li><strong>Tools:</strong> OpenCV, Tesseract, PaddleOCR</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-6">Models<a class="hash-link" href="#models-6" title="Direct link to heading">​</a></h4><ul><li><strong>Semantic Reasoning Networks</strong>: <em><a href="https://arxiv.org/abs/2003.12294v1" target="_blank" rel="noopener noreferrer">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks. arXiv, 2020.</a></em></li><li><strong>Differentiable Binarization</strong>: <em><a href="https://arxiv.org/abs/1911.08947v2" target="_blank" rel="noopener noreferrer">Real-time Scene Text Detection with Differentiable Binarization. arXiv, 2019.</a></em></li><li><strong>CRAFT</strong>: <em><a href="https://arxiv.org/abs/1904.01941v1" target="_blank" rel="noopener noreferrer">Character Region Awareness for Text Detection. arXiv, 2019.</a></em></li><li><em><a href="https://arxiv.org/abs/1704.03155v2" target="_blank" rel="noopener noreferrer">EAST: An Efficient and Accurate Scene Text Detector. arXiv, 2017.</a></em></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-6">Process flow<a class="hash-link" href="#process-flow-6" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect Images - Fetch from database, scrap from the internet or use public datasets. Setup the database connection and fetch the data into python environment.</li><li>Step 2: Data Preparation - Explore the data, validate it and create preprocessing strategy. Clean the data and make it ready for processing.</li><li>Step 3: Model Building - Apply different kinds of detection, recognition and single-shot models on the images. Track the progress and experiments. Validate the final set of models and select/assemble the final model.</li><li>Step 4: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 5: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 6: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-6">Use Cases<a class="hash-link" href="#use-cases-6" title="Direct link to heading">​</a></h4><p><strong>Scene Text Detection with EAST Tesseract</strong></p><p>Detect the text in images and videos using EAST model. Read the characters using Tesseract. Check out <a href="https://www.notion.so/Scene-Text-Detection-with-EAST-Tesseract-583f882db70b43b5b3005d89ced8d8fd" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Scene Text Recognition with DeepText</strong></p><p>Detect and Recognize text in images with an end-to-end model named DeepText. Check out <a href="https://www.notion.so/Scene-Text-Recognition-with-DeepText-3dbc00e6bdf548a3b8539be1adb8f2d5" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Automatic License Plate Recognition</strong></p><p>Read the characters on the license plate image using Tesseract OCR. Check out <a href="https://www.notion.so/Automatic-License-Plate-Recognition-10ec22181b454b1facc99abdeadbf78f" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Keras OCR Toolkit Experiment</strong></p><p>Keras OCR is a deep learning based toolkit for text recognition in images. Check out <a href="https://www.notion.so/Keras-OCR-d15bff7629fa4fbf8d8a7fb21d2a69c5" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>OCR Experiments</strong></p><p>Experiments with three OCR tools - Tesseract OCR, Easy OCR, and Arabic OCR. Check out <a href="https://www.notion.so/OCR-Simple-Experiments-a606ff9003b14de589073864c150aa81" target="_blank" rel="noopener noreferrer">this</a> and <a href="https://www.notion.so/Optical-Character-Recognition-6eec9092cc70455a91dd92278e4677a8" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>PaddleOCR Experiments</strong></p><p>Experiments with state of the art lightweight and multi-lingual OCR. Check out <a href="https://www.notion.so/Paddle-OCR-5ab56a38a594478da92314f246159193" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="video-action-recognition">Video Action Recognition<a class="hash-link" href="#video-action-recognition" title="Direct link to heading">​</a></h3><p>This is the task of identifying human activities/actions (e.g. eating, playing) in videos. In other words, this task classifies segments of videos into a set of pre-defined categories.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/216822803-052c478d-b961-487b-a68f-702948660afc.png" alt="content-concepts-raw-computer-vision-video-action-recognition-img" class="img_ev3q"></p><ul><li><strong>Applications:</strong> Automated surveillance, elderly behavior monitoring, human-computer interaction, content-based video retrieval, and video summarization.</li><li><strong>Scope:</strong> Human Action only</li><li><strong>Tools:</strong> OpenCV</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="models-7"><strong>Models</strong><a class="hash-link" href="#models-7" title="Direct link to heading">​</a></h4><ul><li><strong>3D-ResNet</strong>: <strong><a href="https://arxiv.org/abs/1711.09577" target="_blank" rel="noopener noreferrer">Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?</a></strong> - the authors explore how existing state-of-the-art 2D architectures (such as ResNet, ResNeXt, DenseNet, etc.) can be extended to video classification via 3D kernels.</li><li><strong>R(2+1)D</strong>: This model was pre-trained on 65 million social media videos and fine-tuned on Kinetics400.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="process-flow-7">Process flow<a class="hash-link" href="#process-flow-7" title="Direct link to heading">​</a></h4><ul><li>Step 1: Collect videos - Capture via camera, scrap from the internet or use public datasets</li><li>Step 2: Create Labels - Use open-source tools like VGA Video Annotator for video annotation</li><li>Step 3: Data Acquisition - Setup the database connection and fetch the data into python environment</li><li>Step 4: Data Exploration - Explore the data, validate it and create preprocessing strategy</li><li>Step 5: Data Preparation - Clean the data and make it ready for modeling</li><li>Step 6: Model Building - Create the model architecture in python and perform a sanity check</li><li>Step 7: Model Training - Start the training process and track the progress and experiments</li><li>Step 8: Model Validation - Validate the final set of models and select/assemble the final model</li><li>Step 9: UAT Testing - Wrap the model inference engine in API for client testing</li><li>Step 10: Deployment - Deploy the model on cloud or edge as per the requirement</li><li>Step 11: Documentation - Prepare the documentation and transfer all assets to the client</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="use-cases-7">Use Cases<a class="hash-link" href="#use-cases-7" title="Direct link to heading">​</a></h4><p><strong>Kinetics 3D CNN Human Activity Recognition</strong></p><p>This dataset consists of 400 human activity recognition classes, at least 400 video clips per class (downloaded via YouTube) and a total of 300,000 videos. Check out <a href="https://www.notion.so/Kinetics-3D-CNN-Human-Activity-Recognition-fd10fd7b5858459cba65dc4a6cb73630" target="_blank" rel="noopener noreferrer">this</a> notion.</p><p><strong>Action Recognition using R(2+1)D Model</strong></p><p>VGA Annotator was used for creating the video annotation for training. Check out <a href="https://www.notion.so/Action-Recognition-using-R-2-1-D-Model-4c796f308aed40f29fc230a757af98e8" target="_blank" rel="noopener noreferrer">this</a> notion.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="labs">Labs<a class="hash-link" href="#labs" title="Direct link to heading">​</a></h2><ol><li>Video Classification Modeling with X3D Model [<a href="/docs/datascience/10-datascience/computer-vision/lab-video-classification">source code</a>]</li></ol></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/datascience/challenges/video-sharing-analysis"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Video Sharing Analysis</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/datascience/computer-vision/lab-agri-setallite-image-segmentation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Agricultural Satellite Image Segmentation</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#categories" class="table-of-contents__link toc-highlight">Categories</a><ul><li><a href="#image-similarity" class="table-of-contents__link toc-highlight">Image Similarity</a></li><li><a href="#object-detection" class="table-of-contents__link toc-highlight">Object Detection</a></li><li><a href="#image-segmentation" class="table-of-contents__link toc-highlight">Image Segmentation</a></li><li><a href="#face-detection-and-recognition" class="table-of-contents__link toc-highlight">Face Detection and Recognition</a></li><li><a href="#object-tracking" class="table-of-contents__link toc-highlight">Object Tracking</a></li><li><a href="#pose-estimation" class="table-of-contents__link toc-highlight">Pose Estimation</a></li><li><a href="#scene-text-recognition" class="table-of-contents__link toc-highlight">Scene Text Recognition</a></li><li><a href="#video-action-recognition" class="table-of-contents__link toc-highlight">Video Action Recognition</a></li></ul></li><li><a href="#labs" class="table-of-contents__link toc-highlight">Labs</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.166317d0.js"></script>
<script src="/assets/js/main.3580ce8c.js"></script>
</body>
</html>