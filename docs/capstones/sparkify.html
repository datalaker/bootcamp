<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-capstones/sparkify/README">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Sparkify | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/capstones/sparkify"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Sparkify | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Sparkify SQL Data Modeling with Postgres"><meta data-rh="true" property="og:description" content="Sparkify SQL Data Modeling with Postgres"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/capstones/sparkify"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/capstones/sparkify" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/capstones/sparkify" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.099cbecb.css">
<link rel="preload" href="/assets/js/runtime~main.40c54d56.js" as="script">
<link rel="preload" href="/assets/js/main.dbd2f5fe.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/getting-started">Getting Started</a><button aria-label="Toggle the collapsible sidebar category &#x27;Getting Started&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/foundations/cloud/cloud-computing">Cloud Computing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Cloud Computing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/language/sql/sql-basics">Programming</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/storage/serialization">Data Storage</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/processing/databricks">Data Processing</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/data-modeling">Data Modeling</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data Modeling&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/extraction/api">Data Extraction</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/orchestration/airflow">Data Pipelines</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/visualization/flask">Data Visualization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/devops">DevOps</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/mathematics">Mathematics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Mathematics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/basics/origin">Data Science</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/category/case-studies">Extras</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/case-studies">Case Studies</a><button aria-label="Toggle the collapsible sidebar category &#x27;Case Studies&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/category/capstones">Capstones</a><button aria-label="Toggle the collapsible sidebar category &#x27;Capstones&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/ServerlessStreamingApp">Serverless Streaming Data on AWS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/acled">ACLED</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/citibike-trip-histories">Citi Bike Trip Histories</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/city-pollution">Air Pollution Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/city-traffic-drone">Airflow - City Traffic Drone</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/climate">Global Historical Climatology Network Daily Data Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/cloudmaze">Building End to end data pipeline in AWS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/dbt-redshift">README</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/digitalskola">DigitalSkola</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/disaster-response">Disaster Response Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/funflix">Funflix</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/hmc">Datalake Schema Correction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/kinesis-flink-beam">Log Analytics and Processing in Real-Time</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/kinesis-flink-etl">Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/kortex">Kortex</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/lufthansa">Lufthansa API</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/movie-sentiment">Movie Review Sentiment Analysis Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/multi-touch-attribution">Multi-touch Attribution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/recofront">Building Recommender System from Scratch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/reddit">Reddit Submissions, Authors and Subreddits analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/redshield">AWS Kafka and DynamoDB for real time fraud detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/robust-data-pipeline">Data Pipeline with dbt, Airflow and Great Expectations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/smartcity">Smartcity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/capstones/sparkify">Sparkify</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/spotify">Spotify</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/twitter-sentiment-glue">Twitter data Topic Analysis and Realtime Sentiment Analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/us-immigration">US Immigration analysis and data pipeline</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/interviewprep">Interview Preparation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/interviewprep/resume-buildup">Resume Buildup</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/misc/resources">Resources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/misc/extras">Extras</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Extras</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/capstones"><span itemprop="name">Capstones</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Sparkify</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Sparkify</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sparkify-sql-data-modeling-with-postgres">Sparkify SQL Data Modeling with Postgres<a class="hash-link" href="#sparkify-sql-data-modeling-with-postgres" title="Direct link to heading">​</a></h2><p>In this, we will model the data with Postgres and build an ETL pipeline using Python. The fact and dimension tables for a star database schema for a particular analytic focus is defined, and an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL was developed.</p><p>A startup called Sparkify wants to analyze the data they&#x27;ve been collecting on songs and user activity on their new music streaming application. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don&#x27;t have an easy way to query their data, which resides in a directory of JSON logs on user activity on the application, as well as a directory with JSON meta-data on the songs in their application.</p><p>They&#x27;d like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis. The role of this project is to create a database schema and ETL pipeline for this analysis.</p><p>We will model the data with Postgres and build an ETL pipeline using Python. The fact and dimension tables for a star database schema for a particular analytic focus is defined, and an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL was developed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="songs-dataset">Songs dataset<a class="hash-link" href="#songs-dataset" title="Direct link to heading">​</a></h3><p>Songs dataset is a subset of <a href="http://millionsongdataset.com/" target="_blank" rel="noopener noreferrer">Million Song Dataset</a>. Each file in the dataset is in JSON format and contains meta-data about a song and the artist of that song.</p><p>Sample record:</p><div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;num_songs&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;artist_id&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;ARJIE2Y1187B994AB7&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;artist_latitude&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token null keyword" style="color:#00009f">null</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;artist_longitude&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token null keyword" style="color:#00009f">null</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;artist_location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;artist_name&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Line Renaud&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;song_id&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;SOUPIRU12A6D4FA1E1&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;title&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Der Kleine Dompfaff&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;duration&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">152.92036</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;year&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="logs-dataset">Logs dataset<a class="hash-link" href="#logs-dataset" title="Direct link to heading">​</a></h3><p>Logs dataset is generated by <a href="https://github.com/Interana/eventsim" target="_blank" rel="noopener noreferrer">Event Simulator</a>. These log files in JSON format simulate activity logs from a music streaming application based on specified configurations.</p><p>Sample record:</p><div class="language-json codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-json codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token punctuation" style="color:#393A34">{</span><span class="token property" style="color:#36acaa">&quot;artist&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token null keyword" style="color:#00009f">null</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;auth&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Logged In&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;firstName&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Walter&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;gender&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;M&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;itemInSession&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;lastName&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Frye&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;length&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token null keyword" style="color:#00009f">null</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;level&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;free&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;location&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;San Francisco-Oakland-Hayward, CA&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;method&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;GET&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token property" style="color:#36acaa">&quot;page&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;Home&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;registration&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1540919166796.0</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;sessionId&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">38</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;song&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token null keyword" style="color:#00009f">null</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;status&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">200</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;ts&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1541105830796</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;userAgent&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;\&quot;Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\&quot;&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token property" style="color:#36acaa">&quot;userId&quot;</span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;39&quot;</span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="quality">Quality<a class="hash-link" href="#quality" title="Direct link to heading">​</a></h3><ol><li>LogDataset : userID - Found users with empty string &#x27;&#x27;, firstName as None</li><li>LogDataset : Major of the artist_id &amp; song_id is null</li><li>Artists table doesn&#x27;t have list of all the artists found in log</li><li>Songs table doesn&#x27;t have all the songs found in log</li><li>LogDataset : Logs are ordered by timestamp, so they need to be sorted. This enables latest user level to be updated in the users table</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tidiness">Tidiness<a class="hash-link" href="#tidiness" title="Direct link to heading">​</a></h3><ol><li>LogDataset : ts : timestamp column as int64 needs to converted to timestamp</li><li>SongPlays table : Add new column songplay_id as serial ( auto-increment )</li><li>user : table : Adding column ts</li><li>songplays : table : Adding columns itemInSession, song, artist</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="database-schema-design---entity-relation-diagram-erd">Database Schema Design - Entity Relation Diagram (ERD)<a class="hash-link" href="#database-schema-design---entity-relation-diagram-erd" title="Direct link to heading">​</a></h3><p>The Star Database Schema used for data modeling in this ETL pipeline. There is one fact table containing all the metrics (facts) associated to each event (user actions), and four dimensions tables, containing associated information such as user name, artist name, song meta-data etc. This model enables to search the database schema with the minimum number of <em>SQL JOIN</em>s possible and enable fast read queries. The amount of data we need to analyze is not big enough to require big data solutions or NoSQL databases.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310998-b894e4b2-b6ba-4a2d-a84b-531e67a32cbc.png" alt="data_schema" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sparkify-nosql-data-modeling-with-cassandra">Sparkify NoSQL Data Modeling with Cassandra<a class="hash-link" href="#sparkify-nosql-data-modeling-with-cassandra" title="Direct link to heading">​</a></h2><p>In this, we will model the data with Apache Cassandra and build an ETL pipeline using Python. The ETL pipeline transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables. We will create separate denormalized tables for answering specific queries, properly using partition keys and clustering columns.</p><p>A startup called Sparkify wants to analyze the data they&#x27;ve been collecting on songs and user activity on their new music streaming application. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don&#x27;t have an easy way to query their data, which resides in a directory of JSON logs on user activity on the application, as well as a directory with JSON meta-data on the songs in their application.</p><p>They&#x27;d like a data engineer to create a Apache Cassandra database which can create queries on song play data to answer the questions and make meaningful insights. The role of this project is to create a database schema and ETL pipeline for this analysis.</p><p>We will model the data with Apache Cassandra and build an ETL pipeline using Python. The ETL pipeline transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables. We will create separate denormalized tables for answering specific queries, properly using partition keys and clustering columns.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="event-dataset">*<strong>*Event Dataset**</strong><a class="hash-link" href="#event-dataset" title="Direct link to heading">​</a></h3><p>Event dataset is a collection of CSV files containing the information of user activity across a period of time. Each file in the dataset contains the information regarding the song played, user information and other attributes .</p><p>List of available data columns :</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userId</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="keyspace-schema-design"><strong>Keyspace Schema Design</strong><a class="hash-link" href="#keyspace-schema-design" title="Direct link to heading">​</a></h3><p>The keyspace design is shown in the image below. Each table is modeled to answer a specific known query. This model enables to query the database schema containing huge amounts of data. Relational databases are not suitable in this scenario due to the magnitude of data.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310889-b8b76e08-346b-4607-b3ac-a732dd83c442.png" alt="keyspace" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sparkify-data-lake-with-aws-and-pyspark">Sparkify Data Lake with AWS and PySpark<a class="hash-link" href="#sparkify-data-lake-with-aws-and-pyspark" title="Direct link to heading">​</a></h2><p>In this lab, we will build a data lake on AWS S3 and build an ETL pipeline for a data lake hosted on S3. The data is loaded from S3 and processed into analytics tables using Spark and the processed data is loaded back into S3 in the form of parquet files.</p><p>The data stored on S3 buckets is extracted and processed using Spark, and is then inserted into the fact and dimensional tables.</p><p>A startup called Sparkify wants to analyze the data they&#x27;ve been collecting on songs and user activity on their new music streaming application. Sparkify has grown their user base and song database large and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the application, as well as a directory with JSON metadata on the songs in their application.</p><p>They&#x27;d like a data engineer to build an ETL pipeline that extracts their data from S3,  processes them using Spark, and loads the data back into S3 as a set of fact and dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to. The role of this project is to create a data lake on cloud (AWS S3) and build ETL pipeline for this process.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-description">Project Description<a class="hash-link" href="#project-description" title="Direct link to heading">​</a></h3><p>In this project, we will build a data lake on AWS S3 and build an ETL pipeline for a data lake hosted on S3. The data is loaded from S3 and processed into analytics tables using Spark and the processed data is loaded back into S3 in the form of parquet files.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="built-with">Built With<a class="hash-link" href="#built-with" title="Direct link to heading">​</a></h3><ul><li>python</li><li>AWS</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset<a class="hash-link" href="#dataset" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="song-dataset">Song Dataset<a class="hash-link" href="#song-dataset" title="Direct link to heading">​</a></h4><p>Songs dataset is a subset of <a href="http://millionsongdataset.com/" target="_blank" rel="noopener noreferrer">Million Song Dataset</a>. Each file in the dataset is in JSON format and contains meta-data about a song and the artist of that song. The dataset is hosted at S3 bucket <code>s3://sparsh-dend/song_data</code>.</p><p>Sample Record :</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;num_songs&quot;: 1, &quot;artist_id&quot;: &quot;ARJIE2Y1187B994AB7&quot;, &quot;artist_latitude&quot;: null, &quot;artist_longitude&quot;: null, &quot;artist_location&quot;: &quot;&quot;, &quot;artist_name&quot;: &quot;Line Renaud&quot;, &quot;song_id&quot;: &quot;SOUPIRU12A6D4FA1E1&quot;, &quot;title&quot;: &quot;Der Kleine Dompfaff&quot;, &quot;duration&quot;: 152.92036, &quot;year&quot;: 0}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="log-dataset">Log Dataset<a class="hash-link" href="#log-dataset" title="Direct link to heading">​</a></h4><p>Logs dataset is generated by <a href="https://github.com/Interana/eventsim" target="_blank" rel="noopener noreferrer">Event Simulator</a>. These log files in JSON format simulate activity logs from a music streaming application based on specified configurations. The dataset is hosted at S3 bucket <code>s3://sparsh-dend/log_data</code>.</p><p>Sample Record :</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;artist&quot;: null, &quot;auth&quot;: &quot;Logged In&quot;, &quot;firstName&quot;: &quot;Walter&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;itemInSession&quot;: 0, &quot;lastName&quot;: &quot;Frye&quot;, &quot;length&quot;: null, &quot;level&quot;: &quot;free&quot;, &quot;location&quot;: &quot;San Francisco-Oakland-Hayward, CA&quot;, &quot;method&quot;: &quot;GET&quot;,&quot;page&quot;: &quot;Home&quot;, &quot;registration&quot;: 1540919166796.0, &quot;sessionId&quot;: 38, &quot;song&quot;: null, &quot;status&quot;: 200, &quot;ts&quot;: 1541105830796, &quot;userAgent&quot;: &quot;\&quot;Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\&quot;&quot;, &quot;userId&quot;: &quot;39&quot;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-model-erd">Data Model ERD<a class="hash-link" href="#data-model-erd" title="Direct link to heading">​</a></h3><p>The Star Database Schema (Fact and Dimension Schema) is used for data modeling in this ETL pipeline. There is one fact table containing all the metrics (facts) associated to each event (user actions), and four dimensions tables, containing associated information such as user name, artist name, song meta-data etc. This model enables to search the database schema with the minimum number of <em>SQL JOIN</em>s possible and enable fast read queries.</p><p>The data stored on S3 buckets is extracted and processed using Spark, and is then inserted into the fact and dimensional tables. This tables are stored back to S3 in parquet files, organized for optimal performance. An entity relationship diagram (ERD) of the data model is given below.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310795-2ba21d81-1c32-4e7b-b6d6-9de37c5d002e.png" alt="database" class="img_ev3q"></p><p>In this project (Py)Spark is used to process large amount of data for a
fictional music streaming service, which is called Sparkify.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem">Problem<a class="hash-link" href="#problem" title="Direct link to heading">​</a></h3><p>The scenario that should be solved in this project is:
Sparkify has gained a lot new users and the song database as well as the
recorded song plays have increased over time.</p><p>Sparkify has created a dump of the data in Amazon S3 storage.
This dump currently consists of JSON logs of the user activity and metadata
on the songs.</p><p>To improve analysis of the data, the data should be transformed into a star
schema.
The stored data should then be stored again on S3 for further usage.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution">Solution<a class="hash-link" href="#solution" title="Direct link to heading">​</a></h3><p>We read the data from with (Py)Spark and transform it into a star schema.
Finally we store the results as parquet files from which they can be easily
processed further on.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started">Getting started<a class="hash-link" href="#getting-started" title="Direct link to heading">​</a></h3><p>A working Python (&gt;= Python 3.6) environment is required.</p><p>In this enviroment run</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> pyspark</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>or use the provided Anaconda environment</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">conda create -p ./.conda-env --file conda.txt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda activate ./.conda-env</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Then execute the <code>etl.py</code> script, which can either be executed in a local
mode or the remote mode:</p><p>Local mode:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># make sure to first unzip the sample data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token function" style="color:#d73a49">bash</span><span class="token plain"> unzip_data.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python etl.py </span><span class="token builtin class-name">local</span><span class="token plain"> --help   </span><span class="token comment" style="color:#999988;font-style:italic"># list all parameters</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python etl.py </span><span class="token builtin class-name">local</span><span class="token plain">          </span><span class="token comment" style="color:#999988;font-style:italic"># runs the script in local mode (with default params)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Remote mode:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">python etl.py remote --help  </span><span class="token comment" style="color:#999988;font-style:italic"># list all parameters</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">python etl.py remote --s3-bucket-target s3a://your-bucket-id</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>When running in remote mode, make sure to enter AWS credentials in the
<code>dl.cfg</code> file first.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sparkify-data-warehouse-with-redshift">Sparkify Data Warehouse with Redshift<a class="hash-link" href="#sparkify-data-warehouse-with-redshift" title="Direct link to heading">​</a></h2><p>Sharpen your data warehousing skills and deepen your understanding of data infrastructure. Create cloud-based data warehouses on Amazon Web Services (AWS). Build a data warehouse on AWS and build an ETL pipeline for a database hosted on Redshift. The data is loaded from S3 buckets to staging tables on Redshift and modeled into fact and dimensions tables to perform analytics and obtain meaningful insights.</p><p>In this lab AWS Redshift is used as a Data Warehouse for a fictional music streaming service, which is called Sparkify.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-1">Problem<a class="hash-link" href="#problem-1" title="Direct link to heading">​</a></h3><p>The scenario that should be solved in this lab is:
Sparkify has gained a lot new users and the song database as well as the recorded song plays have increased over time.</p><p>Sparkify has created a dump of the data in Amazon S3 storage.
This dump currently consists of JSON logs of the user activity and metadata on the songs.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-1">Solution<a class="hash-link" href="#solution-1" title="Direct link to heading">​</a></h3><p>We use AWS Redshift as a data warehouse in order to make the data available to a wider audience.
The data will first be loaded from S3 into a staging area in Redshift, where we create two tables that stores the data in a simlar schema as the dumped data on S3.</p><p>In a second step the data is then transformed into a star schema which is optimized for data analytics queries.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310794-262587b0-2c11-448b-8afc-b153f2c98069.png" alt="database-data-warehousing" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started-1">Getting started<a class="hash-link" href="#getting-started-1" title="Direct link to heading">​</a></h3><p>Make sure to have a AWS Redshift cluster up and running and enter the login details in the <code>dwh.cfg</code> file.
The cluster must be configured, such that it can be reached from public if the scripts are not executed within the VPC.</p><p>Then start the pipeline:</p><ol><li>Create the tables by running <code>python create_tables.py</code> from the terminal.</li><li>Run the ETL pipeline to extract the data from S3 to Redshift and transform/load it into the target tables.</li></ol><p>Both scripts log also to stdout, so that we can see what is happening while we create tables or load data into Redshift.
The ETL step takes quite some time, so please be patient.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-layout">Data Layout<a class="hash-link" href="#data-layout" title="Direct link to heading">​</a></h3><p>The data layout in the Data Warehouse follows the star schema.
It consists of one fact table <code>fact_songplay</code> and four dimension tables: <code>dim_user</code>, <code>dim_song</code>, <code>dim_artist</code> and <code>dim_time</code>.</p><p>In a larger cluster I expect analytic queries to be user and song centric, which means that these tables will often be joined.
As long as these tables do not grow too much in size it makes sense to replicate them to all nodes to mitigate data transfer across nodes.
Because of this tables <code>dim_user</code> and <code>dim_song</code> have attached <code>diststyle ALL</code> in their <code>CREATE TABLE</code> statements.</p><p>Also I expect queries to be mostly interested in the latest events, which is why <code>fact_songplay.start_time</code> is the leading column for the sort key.
This means that Redshift can <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html" target="_blank" rel="noopener noreferrer">skip entire blocks that fall outside a time range</a> in queries that select data based on the time.</p><p>Since the artist table <code>dim_artist</code> is not shared with all nodes in the cluster, opposite to <code>dim_user</code> and <code>dim_song</code>, the column <code>artist_id</code> (<code>fact_songplay.artist_id</code>, <code>dim_artist.artist_id</code>) as been defined as distribution key.
Because <code>dim_artist.artist_id</code> is defined as sort key it should <a href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html" target="_blank" rel="noopener noreferrer">enable the query optimizer to choose a sort merge join instead of a slower hash join</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="transformations">Transformations<a class="hash-link" href="#transformations" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dimension-artist">Dimension: Artist<a class="hash-link" href="#dimension-artist" title="Direct link to heading">​</a></h4><p>For demonstration purposes:
When inserting data into the <code>dim_artist</code> table, I check for <code>NULL</code> values or empty strings in the column <code>location</code>.
I don&#x27;t want to have empty values in this column, so I replace the missing value with <code>N/A</code>.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dimension-time">Dimension: Time<a class="hash-link" href="#dimension-time" title="Direct link to heading">​</a></h4><p>As usual various values are extracted from the timestamp, such as <code>hour</code>, <code>day</code>, <code>week</code>, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sparkify-data-pipeline-with-airflow">Sparkify Data Pipeline with Airflow<a class="hash-link" href="#sparkify-data-pipeline-with-airflow" title="Direct link to heading">​</a></h2><p>Sparkify wants to analyze the data they&#x27;ve been collecting on songs and user activity on their new music streaming application. Sparkify has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and have come to the conclusion that the best tool to achieve this is Apache Airflow.</p><p>Sparkify has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and have come to the conclusion that the best tool to achieve this is Apache Airflow.</p><p>They&#x27;d like a data engineer to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run data quality tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.</p><p>The source data resides in S3 and needs to be processed in Sparkify&#x27;s data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.</p><p>Schedule, automate, and monitor data pipelines using Apache Airflow. Run data quality checks, track data lineage, and work with data pipelines in production.</p><p>In this milestone, we will build data pipelines using Apache Airflow using custom defined operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.</p><p>The data stored on S3 buckets is staged and then inserted to fact and dimensional tables on Redshift using Airflow pipelines.</p><p><a href="https://knowledgetree.notion.site/Building-a-Data-Pipeline-for-Sparkify-Music-Company-Data-Pipelines-with-Airflow-Shared-bc7d6c79c5be426ba75e2cb2501fb663" target="_blank" rel="noopener noreferrer">Read this for more information</a></p><p>They&#x27;d like a data engineer to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run data quality tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.</p><p>The source data resides in S3 and needs to be processed in Sparkify&#x27;s data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.</p><p>Schedule, automate, and monitor data pipelines using Apache Airflow. Run data quality checks, track data lineage, and work with data pipelines in production.</p><p>Task:</p><p>Build data pipeline using Apache Airflow using custom defined operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.</p><p>Process steps:</p><ol><li>Upload the Sparkify data to S3 Bucket</li><li>Update the Bucket and Path information in DAG</li><li>Add the helpers and operators in airflow plugins directory</li><li>Add connection: {&#x27;connection_id&#x27;:&#x27;aws_default&#x27;, &#x27;connection_type&#x27;:&#x27;amazon_web_services&#x27;, &#x27;aws_access_key_id&#x27;:{AWS_ACCESS_KEY}, &#x27;aws_secret_access_key&#x27;:{AWS_SECRET_KEY}}</li><li>Add connection: {&#x27;connection_id&#x27;:&#x27;redshift&#x27;, &#x27;connection_type&#x27;:&#x27;amazon_redshift&#x27;, &#x27;host&#x27;:{HOST}, &#x27;port&#x27;:{PORT}, &#x27;schema&#x27;:{DATABASE}, &#x27;login&#x27;:{USER}, &#x27;password&#x27;:{PASSWORD}}</li><li>Run the DAG</li></ol><p>In this project Airflow is used to coordinate tasks that process large
amounts of data for a fictional music streaming service, which is called
Sparkify.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-2">Problem<a class="hash-link" href="#problem-2" title="Direct link to heading">​</a></h3><p>The scenario that should be solved in this project is:
Sparkify has gained a lot new users and the song database as well as the
recorded song plays have increased over time.</p><p>Sparkify has created a dump of the data in Amazon S3 storage.
This dump currently consists of JSON logs of the user activity and metadata
on the songs.</p><p>To improve analysis of the data, the data should be transformed into a star
schema.
The stored data should then be stored in a AWS Redshift Data Warehouse.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="solution-2">Solution<a class="hash-link" href="#solution-2" title="Direct link to heading">​</a></h3><p>We use Airflow to automatically read data in periodic intervals from S3 and
write them into a staging table in Amazon Redshift.</p><p>We load both datasets in parallel into the staging area.</p><p>After that we create fact and dimension tables.
First we create the fact dimension table and in a second step we create all
dimension tables in parallel.</p><p>In a last step we run data quality checks on the transformed data.</p><p>Our DAG looks like this:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310961-59d74f2c-6d75-4af0-9c5f-1814ca037733.png" alt="project5-dag" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started-2">Getting started<a class="hash-link" href="#getting-started-2" title="Direct link to heading">​</a></h3><p>A working Python (&gt;= Python 3.6) environment is required.</p><p>In this enviroment run</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pip </span><span class="token function" style="color:#d73a49">install</span><span class="token plain"> -r requirements.txt</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>or use the provided Anaconda environment</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">conda create -p ./.conda-env </span><span class="token assign-left variable" style="color:#36acaa">python</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">3.7</span><span class="token plain"> --file requirements.txt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">conda activate ./.conda-env</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Then start Airflow:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token function" style="color:#d73a49">bash</span><span class="token plain"> run-airflow.sh</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This will start the Airflow scheduler and webserver in parallel.</p><p>Make sure to have a AWS Redshift cluster running.
The setup requires you to have two credentials stored in your Airflow
instance as connections:</p><table><thead><tr><th align="left">Credentials Name</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">aws_credentials</td><td align="left">Credentials for your AWS user. Use fields <code>login</code> and <code>password</code> and set your <code>access_key</code> and <code>secret_access_key</code> here.</td></tr><tr><td align="left">redshift</td><td align="left">Choose &quot;Postgres&quot; as the connection type. Enter your AWS Redshift credentials here. Your database name should be stored in the field named &quot;Schema&quot;.</td></tr></tbody></table><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310782-b43e1286-aacb-497e-aa41-945388fec290.png" alt="airflow-credentials-1" class="img_ev3q"></p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310783-12138d3a-364b-4f71-b18d-8f298013ea81.png" alt="airflow-credentials-2" class="img_ev3q"></p><p>Also make sure that the tables exist on the cluster.
The SQL query to create them can be found in the file <code>create_tables.sql</code>.</p><p>Then go to https://localhost:8080 and enable the DAG <code>udac_project_dag</code> to start the process.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-project">About The Project<a class="hash-link" href="#about-the-project" title="Direct link to heading">​</a></h3><p>A startup called Sparkify wants to analyze the data they&#x27;ve been collecting on songs and user activity on their new music streaming application. Sparkify has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and have come to the conclusion that the best tool to achieve this is Apache Airflow.</p><p>They&#x27;d like a data engineer to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run data quality tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.</p><p>The source data resides in S3 and needs to be processed in Sparkify&#x27;s data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-description-1">Project Description<a class="hash-link" href="#project-description-1" title="Direct link to heading">​</a></h3><p>In this project, we will build data pipelines using Apache Airflow using custom defined operators to perform tasks such as staging the data, filling the data warehouse, and running checks on the data as the final step.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="built-with-1">Built With<a class="hash-link" href="#built-with-1" title="Direct link to heading">​</a></h3><ul><li>python</li><li>AWS</li><li>Apache Airflow</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataset-1">Dataset<a class="hash-link" href="#dataset-1" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="song-dataset-1">Song Dataset<a class="hash-link" href="#song-dataset-1" title="Direct link to heading">​</a></h4><p>Songs dataset is a subset of <a href="http://millionsongdataset.com/" target="_blank" rel="noopener noreferrer">Million Song Dataset</a>. Each file in the dataset is in JSON format and contains meta-data about a song and the artist of that song. The dataset is hosted at S3 bucket <code>s3://sparsh-dend/song_data</code>.</p><p>Sample Record :</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;num_songs&quot;: 1, &quot;artist_id&quot;: &quot;ARJIE2Y1187B994AB7&quot;, &quot;artist_latitude&quot;: null, &quot;artist_longitude&quot;: null, &quot;artist_location&quot;: &quot;&quot;, &quot;artist_name&quot;: &quot;Line Renaud&quot;, &quot;song_id&quot;: &quot;SOUPIRU12A6D4FA1E1&quot;, &quot;title&quot;: &quot;Der Kleine Dompfaff&quot;, &quot;duration&quot;: 152.92036, &quot;year&quot;: 0}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="log-dataset-1">Log Dataset<a class="hash-link" href="#log-dataset-1" title="Direct link to heading">​</a></h4><p>Logs dataset is generated by <a href="https://github.com/Interana/eventsim" target="_blank" rel="noopener noreferrer">Event Simulator</a>. These log files in JSON format simulate activity logs from a music streaming application based on specified configurations. The dataset is hosted at S3 bucket <code>s3://sparsh-dend/log_data</code>.</p><p>Sample Record :</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">{&quot;artist&quot;: null, &quot;auth&quot;: &quot;Logged In&quot;, &quot;firstName&quot;: &quot;Walter&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;itemInSession&quot;: 0, &quot;lastName&quot;: &quot;Frye&quot;, &quot;length&quot;: null, &quot;level&quot;: &quot;free&quot;, &quot;location&quot;: &quot;San Francisco-Oakland-Hayward, CA&quot;, &quot;method&quot;: &quot;GET&quot;,&quot;page&quot;: &quot;Home&quot;, &quot;registration&quot;: 1540919166796.0, &quot;sessionId&quot;: 38, &quot;song&quot;: null, &quot;status&quot;: 200, &quot;ts&quot;: 1541105830796, &quot;userAgent&quot;: &quot;\&quot;Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\&quot;&quot;, &quot;userId&quot;: &quot;39&quot;}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="database-schema-design">Database Schema Design<a class="hash-link" href="#database-schema-design" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-model-erd-1">Data Model ERD<a class="hash-link" href="#data-model-erd-1" title="Direct link to heading">​</a></h4><p>The Star Database Schema (Fact and Dimension Schema) is used for data modeling in this ETL pipeline. There is one fact table containing all the metrics (facts) associated to each event (user actions), and four dimensions tables, containing associated information such as user name, artist name, song meta-data etc. This model enables to search the database schema with the minimum number of <em>SQL JOIN</em>s possible and enable fast read queries.</p><p>The data stored on S3 buckets is staged and then inserted to fact and dimensional tables on Redshift. The whole process in orchestrated using Airflow which is set to execute periodically once every hour.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310793-1d7b3a80-2001-4885-9ef4-6c1bebc85bcb.png" alt="database-data-pipeline" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="apache-airflow-orchestration">Apache Airflow Orchestration<a class="hash-link" href="#apache-airflow-orchestration" title="Direct link to heading">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dag-structure">DAG Structure<a class="hash-link" href="#dag-structure" title="Direct link to heading">​</a></h4><p>The DAG parameters are set according to the following :</p><ul><li>The DAG does not have dependencies on past runs</li><li>DAG has schedule interval set to hourly</li><li>On failure, the task are retried 3 times</li><li>Retries happen every 5 minutes</li><li>Catchup is turned off</li><li>Email are not sent on retry</li></ul><p>The DAG dependency graph is given below.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/215310788-f962236c-a910-499a-8e10-badbf6518d60.png" alt="dag" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="operators">Operators<a class="hash-link" href="#operators" title="Direct link to heading">​</a></h4><p>Operators create necessary tables, stage the data, transform the data, and run checks on data quality. Connections and Hooks are configured using Airflow&#x27;s built-in functionalities. All of the operators and task run SQL statements against the Redshift database.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="stage-operator">Stage Operator<a class="hash-link" href="#stage-operator" title="Direct link to heading">​</a></h4><p>The stage operator loads any JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. The operator&#x27;s parameters specify where in S3, the file is loaded and what is the target table.</p><ul><li><strong>Task to stage JSON data is included in the DAG and uses the RedshiftStage operator</strong>: There is a task that to stages data from S3 to Redshift (Runs a Redshift copy statement).</li><li><strong>Task uses params</strong>: Instead of running a static SQL statement to stage the data, the task uses parameters to generate the copy statement dynamically. It also contains a templated field that allows it to load timestamped files from S3 based on the execution time and run backfills.</li><li><strong>Logging used</strong>: The operator contains logging in different steps of the execution.</li><li><strong>The database connection is created by using a hook and a connection</strong>: The SQL statements are executed by using a Airflow hook.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="fact-and-dimension-operators">Fact and Dimension Operators<a class="hash-link" href="#fact-and-dimension-operators" title="Direct link to heading">​</a></h4><p>The dimension and fact operators make use of the SQL helper class to run data transformations. Operators take as input the SQL statement from the helper class and target the database on which to run the query against. A target table is also defined that contains the results of the transformation.</p><p>Dimension loads are done with the truncate-insert pattern where the target table is emptied before the load. There is a parameter that allows switching between insert modes when loading dimensions. Fact tables are massive so they only allow append type functionality.</p><ul><li><strong>Set of tasks using the dimension load operator is in the DAG</strong>: Dimensions are loaded with on the LoadDimension operator.</li><li><strong>A task using the fact load operator is in the DAG</strong>: Facts are loaded with on the LoadFact operator.</li><li><strong>Both operators use params</strong>: Instead of running a static SQL statement to stage the data, the task uses parameters to generate the copy statement dynamically.</li><li><strong>The dimension task contains a param to allow switch between append and insert-delete functionality</strong>: The DAG allows to switch between append-only and delete-load functionality.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="data-quality-operator">Data Quality Operator<a class="hash-link" href="#data-quality-operator" title="Direct link to heading">​</a></h4><p>The data quality operator is used to run checks on the data itself. The operator&#x27;s main functionality is to receive one or more SQL based test cases along with the expected results and execute the tests. For each the test, the test result and expected result are checked and if there is no match, the operator raises an exception and the task is retried and fails eventually.</p><p>For example one test could be a SQL statement that checks if certain column contains NULL values by counting all the rows that have NULL in the column. We do not want to have any NULLs so expected result would be 0 and the test would compare the SQL statement&#x27;s outcome to the expected result.</p><ul><li><strong>A task using the data quality operator is in the DAG and at least one data quality check is done</strong>: Data quality check is done with correct operator.</li><li><strong>The operator raises an error if the check fails</strong>: The DAG either fails or retries n times.</li><li><strong>The operator is parametrized</strong>: Operator uses params to get the tests and the results, tests are not hard coded to the operator.</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="airflow-ui-views-of-dag-and-plugins">Airflow UI views of DAG and plugins<a class="hash-link" href="#airflow-ui-views-of-dag-and-plugins" title="Direct link to heading">​</a></h4><p>The DAG follows the data flow provided in the instructions, all the tasks have a dependency and DAG begins with a start_execution task and ends with a end_execution task.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="project-structure">Project structure<a class="hash-link" href="#project-structure" title="Direct link to heading">​</a></h3><p>Files in this repository:</p><table><thead><tr><th align="center">File / Folder</th><th align="center">Description</th></tr></thead><tbody><tr><td align="center">dags</td><td align="center">Folder at the root of the project, where DAGs and SubDAGS are stored</td></tr><tr><td align="center">images</td><td align="center">Folder at the root of the project, where images are stored</td></tr><tr><td align="center">plugins/helpers</td><td align="center">Contains a SQL helper class for easy querying</td></tr><tr><td align="center">plugins/operators</td><td align="center">Contains the custom operator to perform the DAG tasks</td></tr><tr><td align="center">create_tables.sql</td><td align="center">Contains SQL commands to create the necessary tables on Redshift</td></tr><tr><td align="center">README</td><td align="center">Readme file</td></tr></tbody></table><h3 class="anchor anchorWithStickyNavbar_LWe7" id="getting-started-3">Getting Started<a class="hash-link" href="#getting-started-3" title="Direct link to heading">​</a></h3><p>Clone the repository into a local machine using</p><div class="language-sh codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sh codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">git clone https://github.com/vineeths96/Data-Engineering-Nanodegree</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a class="hash-link" href="#prerequisites" title="Direct link to heading">​</a></h4><p>These are the prerequisites to run the program.</p><ul><li>python 3.7</li><li>AWS account</li><li>Apache Airflow</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-run">How to run<a class="hash-link" href="#how-to-run" title="Direct link to heading">​</a></h4><p>Follow the steps to extract and load the data into the data model.</p><ol><li>Set up Apache Airflow to run in local</li><li>Navigate to <code>Project 5 Data Pipelines</code> folder</li><li>Set up <code>AWS Connection</code> and <code>Redshift Connection</code> to Airflow using necessary values</li><li>In Airflow, turn the DAG execution ON</li><li>View the Web UI for detailed insights about the operation</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="project-structure-1">Project Structure<a class="hash-link" href="#project-structure-1" title="Direct link to heading">​</a></h2><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">├── [ 60K]  01-postgres-datamodel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 12K]  REPORT.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 384]  dwh.cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 47K]  etl.ipynb</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 320]  nbs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── [ 256]  src</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── [106K]  02-cassandra-datamodel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 13K]  REPORT.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 28K]  cassandra-datamodel-ecs.ipynb</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 65K]  cassandra-datamodel.ipynb</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 185]  cassandra-docker-compose.yml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [  96]  level1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── [ 256]  level2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── [ 64K]  03-datalake</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [1.8K]  _EMRJupyterNotebook.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [7.6K]  _athena.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [6.4K]  _emr.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 22K]  _notes.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 460]  ecs-params-spark.yml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 460]  ecs-params.yml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 192]  nbs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 224]  outputs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [1.2K]  spark-docker-compose.yml</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [  99]  spark.cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 17K]  spark_datalake.ipynb</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [4.7K]  spark_etl.py</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── [ 640]  src</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── [ 51K]  04-redshift-warehousing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 11K]  _notes.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 39K]  _report.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 192]  cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 224]  nbs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── [ 448]  src</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── [ 59K]  05-data-pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 55K]  _notes.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 256]  airflow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [3.2K]  airflow_pipeline.ipynb</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   ├── [ 192]  myLabs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│   └── [  96]  src</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── [ 11K]  IMAGES.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├── [ 31K]  README.md</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└── [1.6M]  data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── [  42]  download.sh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── [834K]  event_datafile.csv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ├── [834K]  event_datafile_new.csv</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    └── [ 456]  log_json_path.json</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> 2.0M used in 19 directories, 27 files</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/capstones/smartcity"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Smartcity</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/capstones/spotify"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Spotify</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#sparkify-sql-data-modeling-with-postgres" class="table-of-contents__link toc-highlight">Sparkify SQL Data Modeling with Postgres</a><ul><li><a href="#songs-dataset" class="table-of-contents__link toc-highlight">Songs dataset</a></li><li><a href="#logs-dataset" class="table-of-contents__link toc-highlight">Logs dataset</a></li><li><a href="#quality" class="table-of-contents__link toc-highlight">Quality</a></li><li><a href="#tidiness" class="table-of-contents__link toc-highlight">Tidiness</a></li><li><a href="#database-schema-design---entity-relation-diagram-erd" class="table-of-contents__link toc-highlight">Database Schema Design - Entity Relation Diagram (ERD)</a></li></ul></li><li><a href="#sparkify-nosql-data-modeling-with-cassandra" class="table-of-contents__link toc-highlight">Sparkify NoSQL Data Modeling with Cassandra</a><ul><li><a href="#event-dataset" class="table-of-contents__link toc-highlight">*<strong>*Event Dataset**</strong></a></li><li><a href="#keyspace-schema-design" class="table-of-contents__link toc-highlight"><strong>Keyspace Schema Design</strong></a></li></ul></li><li><a href="#sparkify-data-lake-with-aws-and-pyspark" class="table-of-contents__link toc-highlight">Sparkify Data Lake with AWS and PySpark</a><ul><li><a href="#project-description" class="table-of-contents__link toc-highlight">Project Description</a></li><li><a href="#built-with" class="table-of-contents__link toc-highlight">Built With</a></li><li><a href="#dataset" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#data-model-erd" class="table-of-contents__link toc-highlight">Data Model ERD</a></li><li><a href="#problem" class="table-of-contents__link toc-highlight">Problem</a></li><li><a href="#solution" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#getting-started" class="table-of-contents__link toc-highlight">Getting started</a></li></ul></li><li><a href="#sparkify-data-warehouse-with-redshift" class="table-of-contents__link toc-highlight">Sparkify Data Warehouse with Redshift</a><ul><li><a href="#problem-1" class="table-of-contents__link toc-highlight">Problem</a></li><li><a href="#solution-1" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#getting-started-1" class="table-of-contents__link toc-highlight">Getting started</a></li><li><a href="#data-layout" class="table-of-contents__link toc-highlight">Data Layout</a></li><li><a href="#transformations" class="table-of-contents__link toc-highlight">Transformations</a></li></ul></li><li><a href="#sparkify-data-pipeline-with-airflow" class="table-of-contents__link toc-highlight">Sparkify Data Pipeline with Airflow</a><ul><li><a href="#problem-2" class="table-of-contents__link toc-highlight">Problem</a></li><li><a href="#solution-2" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#getting-started-2" class="table-of-contents__link toc-highlight">Getting started</a></li><li><a href="#about-the-project" class="table-of-contents__link toc-highlight">About The Project</a></li><li><a href="#project-description-1" class="table-of-contents__link toc-highlight">Project Description</a></li><li><a href="#built-with-1" class="table-of-contents__link toc-highlight">Built With</a></li><li><a href="#dataset-1" class="table-of-contents__link toc-highlight">Dataset</a></li><li><a href="#database-schema-design" class="table-of-contents__link toc-highlight">Database Schema Design</a></li><li><a href="#apache-airflow-orchestration" class="table-of-contents__link toc-highlight">Apache Airflow Orchestration</a></li><li><a href="#project-structure" class="table-of-contents__link toc-highlight">Project structure</a></li><li><a href="#getting-started-3" class="table-of-contents__link toc-highlight">Getting Started</a></li></ul></li><li><a href="#project-structure-1" class="table-of-contents__link toc-highlight">Project Structure</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.40c54d56.js"></script>
<script src="/assets/js/main.dbd2f5fe.js"></script>
</body>
</html>