<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-capstones/kortex/README">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Kortex | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/capstones/kortex"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Kortex | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Objectives"><meta data-rh="true" property="og:description" content="Objectives"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/capstones/kortex"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/capstones/kortex" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/capstones/kortex" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.099cbecb.css">
<link rel="preload" href="/assets/js/runtime~main.1db50233.js" as="script">
<link rel="preload" href="/assets/js/main.c578965a.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/getting-started">Getting Started</a><button aria-label="Toggle the collapsible sidebar category &#x27;Getting Started&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/foundations/cloud/cloud-computing">Cloud Computing</a><button aria-label="Toggle the collapsible sidebar category &#x27;Cloud Computing&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/language/sql/sql-basics">Programming</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/storage/serialization">Data Storage</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/processing/databricks">Data Processing</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/category/data-modeling">Data Modeling</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data Modeling&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/extraction/api">Data Extraction</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/orchestration/airflow">Data Pipelines</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/visualization/flask">Data Visualization</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/devops">DevOps</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/mathematics">Mathematics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Mathematics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/foundations/basics/origin">Data Science</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/category/case-studies">Extras</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/category/case-studies">Case Studies</a><button aria-label="Toggle the collapsible sidebar category &#x27;Case Studies&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" tabindex="0" href="/docs/category/capstones">Capstones</a><button aria-label="Toggle the collapsible sidebar category &#x27;Capstones&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/ServerlessStreamingApp">Serverless Streaming Data on AWS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/acled">ACLED</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/citibike-trip-histories">Citi Bike Trip Histories</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/city-pollution">Air Pollution Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/city-traffic-drone">Airflow - City Traffic Drone</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/climate">Global Historical Climatology Network Daily Data Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/cloudmaze">Building End to end data pipeline in AWS</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/dbt-redshift">README</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/digitalskola">DigitalSkola</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/disaster-response">Disaster Response Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/funflix">Funflix</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/hmc">Datalake Schema Correction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/kinesis-flink-beam">Log Analytics and Processing in Real-Time</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/kinesis-flink-etl">Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/capstones/kortex">Kortex</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/lufthansa">Lufthansa API</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/movie-sentiment">Movie Review Sentiment Analysis Pipeline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/multi-touch-attribution">Multi-touch Attribution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/recofront">Building Recommender System from Scratch</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/reddit">Reddit Submissions, Authors and Subreddits analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/redshield">AWS Kafka and DynamoDB for real time fraud detection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/robust-data-pipeline">Data Pipeline with dbt, Airflow and Great Expectations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/smartcity">Smartcity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/sparkify">Sparkify</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/spotify">Spotify</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/twitter-sentiment-glue">Twitter data Topic Analysis and Realtime Sentiment Analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/capstones/us-immigration">US Immigration analysis and data pipeline</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/interviewprep">Interview Preparation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/interviewprep/resume-buildup">Resume Buildup</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/misc/resources">Resources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/misc/extras">Extras</a></li></ul></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Extras</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/category/capstones"><span itemprop="name">Capstones</span></a><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Kortex</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Kortex</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="objectives">Objectives<a class="hash-link" href="#objectives" title="Direct link to heading">​</a></h2><ol><li>Demonstrate the skills required for an entry-level data engineering role.</li><li>Implement various concepts in the data engineering lifecycle.</li><li>Showcase working knowledge with Python, Relational Databases, NoSQL Data Stores, Big Data Engines, Data Warehouses, and Data Pipelines.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><p>As part of the capstone project, you will assume the role of an Associate Data Engineer who has recently joined an e-commerce organization. You will be presented with a business challenge that requires building a data platform for retailer data analytics.</p><p>In this Capstone project, you will:</p><ol><li>Design a data platform that uses MySQL as an OLTP database and MongoDB as a NoSQL database.</li><li>Design and implement a data warehouse and generate reports from the data.</li><li>Design a reporting dashboard that reflects the key metrics of the business.</li><li>Extract data from OLTP, and NoSQL databases, transform it and load it into the data warehouse, and then create an ETL pipeline.</li><li>And finally, create a Spark connection to the data warehouse, and then deploy a machine learning model.</li></ol><p>In Module 1, you will design the OLTP database for an E-Commerce website, populate the OLTP Database with the data provided and automate  the export of the daily incremental data into the data warehouse.</p><p>In Module 2, you will set up a NoSQL database to store the catalog data for an E-Commerce website, load the E-Commerce catalog data into the NoSQL database, and query the E-Commerce catalog data in the NoSQL database.</p><p>In Module 3, you will design the schema for a data warehouse based on the schema of the OLTP and NoSQL databases. You’ll then create the schema and load the data into fact and dimension tables, automate the daily incremental data insertion into the data warehouse, and create Cubes and Rollups to make the reporting easier.</p><p>In Module 4, you will create a Cognos data source that points to a data warehouse table, create a bar chart of Quarterly sales of cell phones, create a pie chart of sales of electronic goods by category, and create a line chart of total sales per month for the year 2020.</p><p>In Module 5, you will extract data from OLTP, NoSQL, and MongoDB databases into CSV format. You will then transform the OLTP data to suit the data warehouse schema, and then load the transformed data into the data warehouse.</p><p>Finally, you will verify that the data is loaded properly.</p><p>In the sixth and final module, you will use your skills in Big Data Analytics to create a Spark connection to the data warehouse, and then deploy a machine learning model on SparkML for making sales projections.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-1---data-platform-architecture-and-oltp-database">Module 1 - Data Platform Architecture and OLTP Database<a class="hash-link" href="#module-1---data-platform-architecture-and-oltp-database" title="Direct link to heading">​</a></h2><p>In this assignment, you will perform three exercises with multiple tasks:</p><ol><li>The first exercise requires you to design the schema for the OLTP database by storing data like row ID, product ID, customer ID, price, quantity, and time stamp of sale.</li><li>In the second exercise, you will load this data into the OLTP database by importing the data and then listing the tables in the database. You will also write a query to find out the count of records in the tables.</li><li>In the final exercise, you will automate admin tasks by creating an index on the timestamp field and listing all the indexes of salesdata table. You will also write a bash script that exports records created in the table into another file.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario">Scenario<a class="hash-link" href="#scenario" title="Direct link to heading">​</a></h3><p>You are a data engineer at an e-commerce company. Your company needs you to design a data platform that uses MySQL as an OLTP database. You will be using MySQL to store the OLTP data.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-1">Objectives<a class="hash-link" href="#objectives-1" title="Direct link to heading">​</a></h3><p>In this assignment you will:</p><ul><li>design the schema for OLTP database.</li><li>load data into OLTP database.</li><li>automate admin tasks.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment">Assignment<a class="hash-link" href="#assignment" title="Direct link to heading">​</a></h3><ol><li>Create a database named sales.</li><li>Design a table named <code>sales_data</code> based on the sample data given.</li><li>Create the sales_data table in <code>sales</code> database.</li><li>Import the data from sales_data.csv into <code>sales_data</code> table.</li><li>List the tables in the database <code>sales</code>.</li><li>Write a query to find out the count of records in the tables sales_data.</li><li>Create an index named <code>ts</code> on the timestamp field.</li><li>List indexes on the table sales_data.</li><li>Exports all the rows in the sales_data table to a file named <code>sales_data.sql</code>.</li><li>End of assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-2---querying-data-in-nosql-databases">Module 2 - Querying Data in NoSQL Databases<a class="hash-link" href="#module-2---querying-data-in-nosql-databases" title="Direct link to heading">​</a></h2><p>In this assignment, you will</p><ol><li>first import the JSON file to the MongoDB server into a database and a collection (electronics) then list out all the databases and the collections in the database catalog.</li><li>Next, you will list the first five documents in the collection, and then you will write a query to find the count of laptops, the number of mobile phones with a screen size of 6 inches, and the average screen size of smartphones.</li><li>Finally, you will export the ID, type, and model fields from the collection into a Comma Separated Values (CSV) file.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-1">Scenario<a class="hash-link" href="#scenario-1" title="Direct link to heading">​</a></h3><p>You are a data engineer at an e-commerce company. Your company needs you to design a data platform that uses MongoDB as a NoSQL database. You will be using MongoDB to store the e-commerce catalog data.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-2">Objectives<a class="hash-link" href="#objectives-2" title="Direct link to heading">​</a></h3><p>In this assignment you will:</p><ul><li>Import data into a MongoDB database</li><li>Query data in a MongoDB database</li><li>Export data from MongoDB</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-1">Assignment<a class="hash-link" href="#assignment-1" title="Direct link to heading">​</a></h3><ol><li>Import <code>catalog.json</code> into mongodb server into a database named &#x27;catalog&#x27; and a collection named &#x27;electronics&#x27;.</li><li>List out all the databases.</li><li>List out all the collections in the database <code>catalog</code>.</li><li>Create an index on the field &quot;type&quot;.</li><li>Write a query to find the count of laptops.</li><li>Write a query to find the number of <code>smart phones</code> with screen size of 6 inches.</li><li>Write a query to find out the average screen size of <code>smart phones</code>.</li><li>Export the fields <code>_id</code>, &quot;type&quot;, &quot;model&quot;, from the &#x27;electronics&#x27; collection into a file named <code>electronics.csv</code>.</li><li>End of assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-3a---data-warehousing">Module 3a - Data Warehousing<a class="hash-link" href="#module-3a---data-warehousing" title="Direct link to heading">​</a></h2><p>The e-commerce company has provided you with sample data. You will start your project by designing a Star Schema for the warehouse by identifying the columns for the various dimension and fact tables in the schema. You will name your database as softcart and then use the ERD design tool to design the table softcartDimDate using fields such as DateID, Month, Monthname, and so on. The company would like to have the ability to generate the report on a yearly, monthly, daily, and weekday basis.</p><p>The following tasks require you to design the dimension tables softcartDimCategory, softcartDimCountry, and softcartFactSales using the ERD design tool. Finally, you will use the ERD design tool to design the required relationships (one-to-one, one-to-many, and so on) amongst the tables.</p><p>In the second exercise, you will load the data into the data warehouse. Your senior Data Engineer has reviewed your design and made a few improvements to your schema design.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-2">Scenario<a class="hash-link" href="#scenario-2" title="Direct link to heading">​</a></h3><p>You are a data engineer hired by an ecommerce company named SoftCart.com. The company retails download only items like E-Books, Movies, Songs etc. The company has international presence and customers from all over the world. The company would like to create a data warehouse so that it can create reports like</p><ul><li>total sales per year per country</li><li>total sales per month per category</li><li>total sales per quarter per country</li><li>total sales per category per country</li></ul><p>You will use your data warehousing skills to design and implement a data warehouse for the company.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-3">Objectives<a class="hash-link" href="#objectives-3" title="Direct link to heading">​</a></h3><p>In this assignment you will:</p><ul><li>Design a Data Warehouse</li><li>Create the schema in the Data Warehouse</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-2">Assignment<a class="hash-link" href="#assignment-2" title="Direct link to heading">​</a></h3><ol><li>Design the table <code>softcartDimDate</code>. The company is looking at a granularity of a day. Which means they would like to have the ability to generate the report on yearly, monthly, daily, and weekday basis.</li><li>Design tool design the table <code>softcartDimCategory</code>.</li><li>Design the table <code>softcartDimItem</code>.</li><li>Design the table <code>softcartDimCountry</code>.</li><li>Design the table <code>softcartFactSales</code>.</li><li>Design the required relationships(one-to-one, one-to-many etc) amongst the tables.</li><li>Download the schema sql from ERD tool and create the schema in a database named <code>staging</code>.</li><li>End of the assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-3b---data-warehousing-reporting">Module 3b - Data Warehousing Reporting<a class="hash-link" href="#module-3b---data-warehousing-reporting" title="Direct link to heading">​</a></h2><p>The first exercise requires you to load the data provided by the company into the tables in CSV format by performing a series of tasks. You will load that data into the DimDate table, DimCategory table, DimCountry table, and fact table FactSales.</p><p>In the second exercise, you will query the loaded data by creating a grouping sets query, rollup query, and cube query using the columns Orderid*, Category, and Price collected. Finally, you will create an MQT named Total_sales_per_country using the country and total sales columns.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-3">Scenario<a class="hash-link" href="#scenario-3" title="Direct link to heading">​</a></h3><p>You are a data engineer hired by an ecommerce company named SoftCart.com . The company retails download only items like E-Books, Movies, Songs etc. The company has international presence and customers from all over the world. You have designed the schema for the data warehouse in the previous assignment. Data engineering is a team game. Your senior data engineer reviewed your design. Your schema design was improvised to suit the production needs of the company. In this assignment you will generate reports out of the data in the data warehouse.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-4">Objectives<a class="hash-link" href="#objectives-4" title="Direct link to heading">​</a></h3><p>In this assignment you will:</p><ul><li>Load data into Data Warehouse</li><li>Write aggregation queries</li><li>Create MQTs</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-3">Assignment<a class="hash-link" href="#assignment-3" title="Direct link to heading">​</a></h3><ol><li>Load <code>DimDate.csv</code> into the dimension table <code>DimDate</code>.</li><li>Load <code>DimCategory.csv</code> into the dimension table <code>DimCategory</code>.</li><li>Load <code>DimCountry.csv</code> into the dimension table <code>DimCountry</code>.</li><li>Load <code>FactSales.csv</code> into the fact table <code>FactSales</code>.</li><li>Create a grouping sets query using the columns country, category, totalsales.</li><li>Create a rollup query using the columns year, country, and totalsales.</li><li>Create a cube query using the columns year, country, and average sales.</li><li>Create an MQT named total_sales_per_country that has the columns country and total_sales.</li><li>End of the assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-4---data-analytics">Module 4 - Data Analytics<a class="hash-link" href="#module-4---data-analytics" title="Direct link to heading">​</a></h2><p>In this assignment, you will perform a couple of exercises with multiple tasks.</p><p>The first exercise requires you to load data into the data warehouse. You will first import data from the downloaded CSV file into a table and then list the first ten rows in the table.</p><p>In the second exercise, you will create a data source that points to the table in your database. In the final exercise, you will create a dashboard by performing tasks such as creating a bar chart of Quarterly sales of mobile phones, a pie chart of category-wise sales of electronic goods, and a line chart of month-wise total sales for the year 2020.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-4">Scenario<a class="hash-link" href="#scenario-4" title="Direct link to heading">​</a></h3><p>You are a data engineer at an e-commerce company. Your company has finished setting up a datawarehouse. Now you are assigned the responsibility to design a reporting dashboard that reflects the key metrics of the business.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-5">Objectives<a class="hash-link" href="#objectives-5" title="Direct link to heading">​</a></h3><p>In this assignment you will:</p><ul><li>Create a dashboard using Cognos Analytics</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-4">Assignment<a class="hash-link" href="#assignment-4" title="Direct link to heading">​</a></h3><ol><li>Load the <code>ecommerce.csv</code> data into a table named <code>sales_history</code>.</li><li>List the first 10 rows in the table <code>sales_history</code>.</li><li>Create a data source in Cognos that points to the table <code>sales_history</code>.</li><li>Create a line chart of month wise total sales for the year 2020.</li><li>Create a pie chart of category wise total sales.</li><li>Create a bar chart of Quarterly sales of mobile phones.</li><li>End of assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-5a---etl-using-python">Module 5a - ETL using Python<a class="hash-link" href="#module-5a---etl-using-python" title="Direct link to heading">​</a></h2><p>The Data Warehouse gets information from several sources including the transactional (OLTP) database. Transactional data from the OLTP database (in this case MySQL) needs to be propagated to the Warehouse on frequent basis. This data movement can be updated using ETL processes.</p><p>In this first part of the assignment, you will setup an ETL process using Python to extract new transactional data for each day from the MySQL database, transform it and then load it into the data warehouse in Redshift.</p><p>You will begin by preparing the lab environment by starting the MySQL server. You will then create a sales database in MySQL and import the sales.sql into the sales database. You will then be uploading the sales.csv to a table in Redshift.</p><p>The final task requires you to automate the extraction of daily incremental data and load yesterday&#x27;s data into the data warehouse. You will write a python script that automatically loads yesterday&#x27;s data from the production database into the data warehouse.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-5">Scenario<a class="hash-link" href="#scenario-5" title="Direct link to heading">​</a></h3><p>You are a data engineer at an e-commerce company. You need to keep data synchronized between different databases/data warehouses as a part of your daily routine. One task that is routinely performed is the sync up of staging data warehouse and production data warehouse. Automating this sync up will save you a lot of time and standardize your process. You will be given a set of python scripts to start with. You will use/modify them to perform the incremental data load from MySQL server which acts as a staging warehouse to the Redshift which is a production data warehouse. This script will be scheduled by the data engineers to sync up the data between the staging and production data warehouse.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-6">Objectives<a class="hash-link" href="#objectives-6" title="Direct link to heading">​</a></h3><p>In this assignment you will write a python program that will:</p><ul><li>Connect to Redshift data warehouse and identify the last row on it.</li><li>Connect to MySQL staging data warehouse and find all rows later than the last row on the datawarehouse.</li><li>Insert the new data in the MySQL staging data warehouse into the Redshift production data warehouse.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-5">Assignment<a class="hash-link" href="#assignment-5" title="Direct link to heading">​</a></h3><ol><li>Create a database named <code>sales</code>.</li><li>Import the data in the file <code>sales.sql</code> into the <code>sales</code> database.</li><li>Modify <code>mysqlconnect.py</code> suitably and make sure you are able to connect to the MySQL server instance.</li><li>Modify <code>redshiftconnect.py</code> suitably and make sure you are able to connect to your Redshift warehouse.</li><li>Load <code>sales.csv</code> into a table named <code>sales_data</code> on your Redshift warehouse.</li><li>You will be using <code>automation.py</code> as a scafolding program to execute the tasks in this assignment.</li></ol><p><strong>Automate loading of incremental data into the data warehouse</strong></p><p>One of the routine tasks that is carried out around a data warehouse is the extraction of daily new data from the operational database and loading it into the data warehouse. In this exercise you will automate the extraction of incremental data, and loading it into the data warehouse.</p><ol><li>In the program <code>automation.py</code> implement the function get_last_rowid(). This function must connect to the Redshift data warehouse and return the last rowid.</li><li>In the program <code>automation.py</code> implement the function get_latest_records(). This function must connect to the MySQL database and return all records later than the given last_rowid.</li><li>In the program <code>automation.py</code> implement the function insert_records(). This function must connect to the Redshift data warehouse and insert all the given records.</li><li>Run the program <code>automation.py</code> and test if the synchronization is happening as expected.</li><li>End of the assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-5b---data-pipelines-using-apache-airflow">Module 5b - Data Pipelines using Apache AirFlow<a class="hash-link" href="#module-5b---data-pipelines-using-apache-airflow" title="Direct link to heading">​</a></h2><p>Our data platform includes a Big Data repository that is used for analytics using Machine Learning with Apache Spark. This Big Data repository gets data from several sources including the Data Warehouse and the Web Server log. As data from the web server log is logged, it needs to be added to the Big Data system on a frequent basis - making it an ideal process to automate using a data pipeline.</p><p>In this second part of the assignment, you will create and run a DAG using Apache Airflow to extract daily data from the web server log, process it, and store it in a format to prepare it for loading into the Big Data platform.</p><p>To complete this part of the assignment, you will perform a couple of exercises. In the first exercise, you will perform a series of tasks to create a DAG that runs daily. You will create a task that extracts the IP address field from the webserver log file and then saves it into a text file. The next task should filter out all the occurrences of ipaddress &quot;198.46.149.143&quot; from text file and save the output to a new text file. In the final task creation, you will load the data by archiving the transformed text file into a TAR file. Before moving on to the next exercise, you will define the task pipeline as per the given details.</p><p>In the second exercise, you will get the DAG operational by saving the defined DAG into a PY file. Further, you will submit, unpause and then monitor the DAG runs for the Airflow console.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="scenario-6">Scenario<a class="hash-link" href="#scenario-6" title="Direct link to heading">​</a></h3><p>Write a pipeline that analyzes the web server log file, extracts the required lines(ending with html) and fields(time stamp, size ) and transforms (bytes to mb) and load (append to an existing file.)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="objectives-7">Objectives<a class="hash-link" href="#objectives-7" title="Direct link to heading">​</a></h3><p>In this assignment you will author an Apache Airflow DAG that will:</p><ul><li>Extract data from a web server log file</li><li>Transform the data</li><li>Load the transformed data into a tar file</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="assignment-6">Assignment<a class="hash-link" href="#assignment-6" title="Direct link to heading">​</a></h3><ol><li>Start Apache Airflow.</li><li>Create a DAG named <code>process_web_log</code> that runs daily.</li><li>Create a task named <code>extract_data</code>. This task should extract the ipaddress field from the web server log file and save it into a file named extracted_data.txt.</li><li>Create a task named <code>transform_data</code>. This task should filter out all the occurrences of ipaddress &quot;198.46.149.143&quot; from extracted_data.txt and save the output to a file named <code>transformed_data.txt</code>.</li><li>Create a task named <code>load_data</code>. This task should archive the file <code>transformed_data.txt</code> into a tar file named <code>weblog.tar</code>.</li><li>Define the task pipeline as per the details given below:
| Task | Functionality |
| --- | --- |
| First task | <code>extract_data</code> |
| Second task | <code>transform_data</code> |
| Third task | <code>load_data</code> |</li><li>Save the DAG you defined into a file named <code>process_web_log.py</code>.</li><li>Run the DAG.</li><li>End of the assignment.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="module-6---big-data-analytics-with-spark">Module 6 - Big Data Analytics with Spark<a class="hash-link" href="#module-6---big-data-analytics-with-spark" title="Direct link to heading">​</a></h2><p>In this assignment, you will perform a number of tasks to analyse search terms on the e-commerce webserver. You will run your analysis against a CSV file containing the webserver data. You will load this file into a Spark dataframe and print the results of your queries against this dataset. You will then load a pretrained sales forecasting model and use this to predict the sales for 2023.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/capstones/kinesis-flink-etl"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/capstones/lufthansa"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Lufthansa API</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#objectives" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#module-1---data-platform-architecture-and-oltp-database" class="table-of-contents__link toc-highlight">Module 1 - Data Platform Architecture and OLTP Database</a><ul><li><a href="#scenario" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-1" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-2---querying-data-in-nosql-databases" class="table-of-contents__link toc-highlight">Module 2 - Querying Data in NoSQL Databases</a><ul><li><a href="#scenario-1" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-2" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment-1" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-3a---data-warehousing" class="table-of-contents__link toc-highlight">Module 3a - Data Warehousing</a><ul><li><a href="#scenario-2" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-3" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment-2" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-3b---data-warehousing-reporting" class="table-of-contents__link toc-highlight">Module 3b - Data Warehousing Reporting</a><ul><li><a href="#scenario-3" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-4" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment-3" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-4---data-analytics" class="table-of-contents__link toc-highlight">Module 4 - Data Analytics</a><ul><li><a href="#scenario-4" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-5" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment-4" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-5a---etl-using-python" class="table-of-contents__link toc-highlight">Module 5a - ETL using Python</a><ul><li><a href="#scenario-5" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-6" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment-5" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-5b---data-pipelines-using-apache-airflow" class="table-of-contents__link toc-highlight">Module 5b - Data Pipelines using Apache AirFlow</a><ul><li><a href="#scenario-6" class="table-of-contents__link toc-highlight">Scenario</a></li><li><a href="#objectives-7" class="table-of-contents__link toc-highlight">Objectives</a></li><li><a href="#assignment-6" class="table-of-contents__link toc-highlight">Assignment</a></li></ul></li><li><a href="#module-6---big-data-analytics-with-spark" class="table-of-contents__link toc-highlight">Module 6 - Big Data Analytics with Spark</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.1db50233.js"></script>
<script src="/assets/js/main.c578965a.js"></script>
</body>
</html>