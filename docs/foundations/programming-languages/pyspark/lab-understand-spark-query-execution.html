<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-foundations/programming-languages/pyspark/lab-understand-spark-query-execution/README">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Understanding Spark Query Execution | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/docs/foundations/programming-languages/pyspark/lab-understand-spark-query-execution"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Understanding Spark Query Execution | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="To write efficient Spark applications, we need to have some understanding of how Spark executes queries. Having a good understanding of how Spark executes a given query helps big data developers/engineers work efficiently with large volumes of data."><meta data-rh="true" property="og:description" content="To write efficient Spark applications, we need to have some understanding of how Spark executes queries. Having a good understanding of how Spark executes a given query helps big data developers/engineers work efficiently with large volumes of data."><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/docs/foundations/programming-languages/pyspark/lab-understand-spark-query-execution"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/foundations/programming-languages/pyspark/lab-understand-spark-query-execution" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/docs/foundations/programming-languages/pyspark/lab-understand-spark-query-execution" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.e4e34528.css">
<link rel="preload" href="/assets/js/runtime~main.166317d0.js" as="script">
<link rel="preload" href="/assets/js/main.3580ce8c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/bootcamp">Docs</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/bootcamp">Recohut Data Bootcamps</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/foundations/developer-foundations/install-anaconda">foundations</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/foundations/developer-foundations/install-anaconda">developer-foundations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/foundations/data-engineering-foundations/batch-data-processing">data-engineering-foundations</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/foundations/cloud-essentials/aws-commands">cloud-essentials</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/foundations/programming-languages/pyspark/broadcasting">programming-languages</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/foundations/programming-languages/pyspark/broadcasting">pyspark</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/broadcasting">Broadcasting</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/caching">Caching</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/cheat-sheet">cheat-sheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/dataframe">PySpark DataFrame</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/execution-plan">Spark Execution Plan</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/install">Installing Spark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-bcg">BCG Case Study</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-pyspark-basics">Pyspark Basics</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-pyspark-nyctaxi">Pyspark NYC Taxi</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-spark-optimizations">Spark Optimizations for Analytics Workloads</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-spark-optimizations-2">Spark Optimizations</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-sql-to-pyspark">SQL to PySpark Code Conversion</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-uber-analysis">Uber Data Analysis in Pyspark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-understand-spark-query-execution">Understanding Spark Query Execution</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lab-window-functions">Window Functions in Spark</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/lazy-processing">Lazy Processing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/methods-operations">Methods, Operations and Functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/partitioning">Partitioning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/pyspark-vs-pandas">PySpark vs Pandas</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/foundations/programming-languages/pyspark/udf">What are UDFs in PySpark</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/foundations/programming-languages/python">Python</a><button aria-label="Toggle the collapsible sidebar category &#x27;Python&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/foundations/programming-languages/scala">Scala</a><button aria-label="Toggle the collapsible sidebar category &#x27;Scala&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/foundations/programming-languages/sql/aggregate-functions">sql</a></div></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/storage/apache-couchdb">storage</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/processing/apache-beam">processing</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/data-modeling/3nf-data-modeling">data-modeling</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/extraction">Data Extraction</a><button aria-label="Toggle the collapsible sidebar category &#x27;Data Extraction&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/orchestration">Orchestration</a><button aria-label="Toggle the collapsible sidebar category &#x27;Orchestration&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/devops">DevOps</a><button aria-label="Toggle the collapsible sidebar category &#x27;DevOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/visualization">Visualization</a><button aria-label="Toggle the collapsible sidebar category &#x27;Visualization&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/datascience/algorithms/decision-trees">datascience</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/assignments">Assignments</a><button aria-label="Toggle the collapsible sidebar category &#x27;Assignments&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/capstones">Capstones</a><button aria-label="Toggle the collapsible sidebar category &#x27;Capstones&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/mlops">MLOps</a><button aria-label="Toggle the collapsible sidebar category &#x27;MLOps&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/a1-interviewprep">Interview Preparation</a><button aria-label="Toggle the collapsible sidebar category &#x27;Interview Preparation&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/a3-casestudies">Case Studies</a><button aria-label="Toggle the collapsible sidebar category &#x27;Case Studies&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/b3-misc/explore-further">b3-misc</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/docs/mathematics">Mathematics</a><button aria-label="Toggle the collapsible sidebar category &#x27;Mathematics&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav><button type="button" title="Collapse sidebar" aria-label="Collapse sidebar" class="button button--secondary button--outline collapseSidebarButton_PEFL"><svg width="20" height="20" aria-hidden="true" class="collapseSidebarButtonIcon_kv0_"><g fill="#7a7a7a"><path d="M9.992 10.023c0 .2-.062.399-.172.547l-4.996 7.492a.982.982 0 01-.828.454H1c-.55 0-1-.453-1-1 0-.2.059-.403.168-.551l4.629-6.942L.168 3.078A.939.939 0 010 2.528c0-.548.45-.997 1-.997h2.996c.352 0 .649.18.828.45L9.82 9.472c.11.148.172.347.172.55zm0 0"></path><path d="M19.98 10.023c0 .2-.058.399-.168.547l-4.996 7.492a.987.987 0 01-.828.454h-3c-.547 0-.996-.453-.996-1 0-.2.059-.403.168-.551l4.625-6.942-4.625-6.945a.939.939 0 01-.168-.55 1 1 0 01.996-.997h3c.348 0 .649.18.828.45l4.996 7.492c.11.148.168.347.168.55zm0 0"></path></g></svg></button></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">foundations</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">programming-languages</span><meta itemprop="position" content="2"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">pyspark</span><meta itemprop="position" content="3"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Understanding Spark Query Execution</span><meta itemprop="position" content="4"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Understanding Spark Query Execution</h1><p>To write efficient Spark applications, we need to have some understanding of how Spark executes queries. Having a good understanding of how Spark executes a given query helps big data developers/engineers work efficiently with large volumes of data.</p><p>Query execution is a very broad subject, and, in this lab, we will understand jobs, stages, and tasks. We will also learn how Spark lazy evaluation works, how to check and understand the execution plan when working with DataFrames or SparkSQL, how joins work in Spark and the different types of join algorithms Spark uses while joining two tables. We will also learn about the input, output, and shuffle partitions and the storage benefits of using different file formats.</p><p>Knowing about the internals will help you troubleshoot and debug your Spark applications more efficiently. By the end of this lab, you will know how to execute Spark queries, as well as how to write and debug your Spark applications more efficiently.</p><p>We&#x27;re going to cover the following recipes:</p><ul><li>Introduction to jobs, stages, and tasks</li><li>Checking the execution details of all the executed Spark queries via the Spark UI</li><li>Deep diving into schema inference</li><li>Looking into the query execution plan</li><li>How joins work in Spark</li><li>Learning about input partitions</li><li>Learning about output partitions</li><li>Learning about shuffle partitions</li><li>The storage benefits of different file types</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="setup">Setup<a class="hash-link" href="#setup" title="Direct link to heading">​</a></h2><p>You will find a dbc file in assets folder. Import that file in databricks. It will create a folder named <code>job-process</code>. Inside this folder, you will find the notebooks we are using in this lab.</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218468595-84f79396-c6a4-487c-a60f-2b80caa9bcb2.png" alt="Screen Shot 2023-02-13 at 6 48 31 PM" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-to-jobs-stages-and-tasks">Introduction to jobs, stages, and tasks<a class="hash-link" href="#introduction-to-jobs-stages-and-tasks" title="Direct link to heading">​</a></h2><p>In this recipe, you will learn how Spark breaks down an application into job, stages, and tasks. You will also learn how to create <strong>directed acyclic graphs</strong> (<strong>DAGs</strong>) and how pipelining works in Spark query execution.</p><p>By the end of this recipe, you will have learned how to check the DAG you&#x27;ve created for the query you have executed and look at the jobs, stages, and tasks associated with a specific query.</p><p>We are using <code>01-introduction-to-jobs-stages-and-tasks</code> notebook.</p><p>Follow these steps to check the jobs, number of stages, and tasks Spark created while executing the query via the Spark UI:</p><p>Step 1 - Run the notebook cells upto first aggregate display and open the <code>SQL / DataFrame</code> DAG:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218369607-e6659ec1-1a79-4bf1-9e64-43103e5f8f1e.png" alt="Databricks Shell - Details for Query 14" class="img_ev3q"></p><p>Under Spark Jobs, we can see that two stages have been created. Multiple stages are created when there is a shuffle involved. A shuffle operation is involved when the data is moved across the nodes, and this happens when we are using transformations such as average and sum. The sum transformation needs to collect data from various executors and send it to the driver node when actions such as Show display or Limit are called. As you can see, there is an exchange operator involved as we are performing aggregations on the data that we read from the CSV file.</p><p>Here, you can also see that there are multiple sortAggregates in the DAG. One was created by the mapper, while another was created after the exchange operator. This happens before the results are given to the driver.</p><p>Step 2 - Now run the next aggregate and display. Go to the DAG:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218370127-9962eaed-c152-4b1e-bb07-fdd5b6307831.png" alt="Databricks Shell - Details for Query 15" class="img_ev3q"></p><p>Your task is to compare these 2 dags and analyze the differences. For your help, I am putting the 2 queries here:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">#Query 1 - Group By on C_MKTSEGMENT.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_cust_agg </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> df_cust</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">groupBy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_MKTSEGMENT&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">agg</span><span class="token punctuation" style="color:#393A34">(</span><span class="token builtin">sum</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cast</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;decimal(20,3)&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;sum_acctbal&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     avg</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;avg_acctbal&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     </span><span class="token builtin">max</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;max_bonus&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">orderBy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;avg_acctbal&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">ascending</span><span class="token operator" style="color:#393A34">=</span><span class="token boolean" style="color:#36acaa">False</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic">#Query 2 - Without groupBy and check the DAG for this query execution by executing the display command in the next cell</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_cust_agg </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> df_cust</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">agg</span><span class="token punctuation" style="color:#393A34">(</span><span class="token builtin">sum</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cast</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&#x27;decimal(20,3)&#x27;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;sum_acctbal&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     avg</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;avg_acctbal&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> \</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">     </span><span class="token builtin">max</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;max_bonus&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token comment" style="color:#999988;font-style:italic">#.orderBy(&quot;avg_acctbal&quot;,ascending=False)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="deep-diving-into-schema-inference">Deep diving into schema inference<a class="hash-link" href="#deep-diving-into-schema-inference" title="Direct link to heading">​</a></h2><p>In this recipe, you will learn about the benefits of explicitly specifying a schema while reading any file format data from an <strong>ADLS Gen-2,</strong> <strong>Azure Blob storage</strong> or <strong>S3</strong> storage account.</p><p>By the end of this recipe, you will have learned how Spark executes a query when a schema is inferred versus explicitly specified.</p><p>We are using <code>02-schema-inference</code> notebook.</p><p>You can learn about the benefits of specifying a schema while reading the files from your storage account. First, we will read the set of files by inferring the schema from the file and explicitly specify the schema. Then, we will look at how Spark&#x27;s execution differs in both scenarios:</p><p>We will read the CSV files directly from the mount point without specifying any schema options.</p><p>Spark uses the concept of lazy evaluation, which means until an action is called, Spark will not execute the query. In the preceding query, we haven&#x27;t invoked an action, and all transformation are lazy in nature. This means that you should not see a DAG when there are no actions in the query. However, after executing the preceding query, you will see that the Spark optimizer has created a DAG:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Reading customer csv files in a dataframe with specifying the schema explicitly</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_cust </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;csv&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">option</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;header&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">option</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;inferSchema&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">load</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;s3a://wysde-datasets/spark/Customer/csvFiles&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, we will create a schema and explicitly provide it while reading the <strong>.csv</strong> files in a DataFrame.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df_cust_sch </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;csv&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">option</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;header&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">schema</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cust_schema</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">load</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;s3a://wysde-datasets/spark/Customer/csvFiles&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>After running the preceding query, you will see that Spark hasn&#x27;t generated a DAG for the query and that the execution time has reduced from 16.9 seconds to 1.83 seconds.</p><p>Thus, it is always recommended to specify the schema wherever possible, as this will improve the performance of your queries. This is because you&#x27;re not creating the DAG when no action is being triggered. But in scenarios where we don&#x27;t have a schema at hand and we want to create one, we can read one file from the entire folder and get the schema for the underlying dataset:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token comment" style="color:#999988;font-style:italic"># Using first file</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_cust_sch </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">read</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">format</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;csv&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">option</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;header&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token boolean" style="color:#36acaa">True</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">load</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;s3a://wysde-datasets/spark/Customer/csvFiles/part-00000-tid-3200334632332214470-9b4dec79-7e2e-495d-8657-3b5457ed3753-108-1-c000.csv&quot;</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>When the preceding query is executed, you will see that a DAG has been created but not multiple stages and that fewer tasks have been completed. You will also see that the execution duration is only 2.34 seconds. After executing the preceding query, you can get the schema by running the <strong>df_cust_sch.printSchema()</strong> command, which you can use to define a schema and use it while reading the <strong>.csv</strong> files.</p><p>By inferring the schema, you are asking Spark to identify all the columns and the data types of the column(s). Providing the schema while creating a DataFrame removes the overhead of reading the data types and columns we expect from the <strong>.csv</strong> files. Only at runtime does it validate whether the schema is matching the columns in the files. If the column(s) that are mentioned in the schema can&#x27;t be found in the <strong>.csv</strong> files, then you will see <strong>NULL</strong> as the value in those columns. Ensure the column&#x27;s name maps to the columns you have in the underlying <strong>.csv</strong> files.</p><p>As we mentioned in the preceding section, if you are not sure what the schema of the dataset it, you can get the schema by creating the DataFrame. Do this by reading only one file and printing the schema that Spark has inferred. You can use the schema that was identified by Spark, and then define the required schema that will be used to read all the <strong>.csv</strong> files and improve the performance of your query.</p><p><strong>There&#x27;s more...</strong></p><p>To identify why it takes more time to read the <strong>.csv</strong> files when an action isn&#x27;t performed, check the number of partitions that Spark created while reading the <strong>.csv</strong> files to infer the schema. Based on the number of cores in your cluster, you will see a significantly high number. If you specify just one file to infer the schema, you will see a different number for the partition count.</p><p>In case of only one csv file, only one partition was created and that the time it takes to read one file is much faster than reading all the files in the folder. We will discuss the different types of partitions later in this lab.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="looking-into-the-query-execution-plan">Looking into the query execution plan<a class="hash-link" href="#looking-into-the-query-execution-plan" title="Direct link to heading">​</a></h2><p>It&#x27;s important to understand the execution plan and how to view its different stages when the Spark optimizer executes a query using a dataframe or the SparkSQL API.</p><p>In this recipe, we will learn how to create a logical and physical plan and the different stages involved. By the end of this recipe, you will have generated an execution plan using the dataframe API or the SparkSQL API and have a fair understanding of the different stages involved.</p><p>We are using <code>03-query-execution-plan</code> notebook.</p><p>Let&#x27;s learn how to generate an execution plan using the dataframe API or SparkSQL API, understand the various stages involved, and identify how the execution plan can be viewed from the Spark UI.</p><p>First read the csv files into dataframe and then create an aggregate query using market segment by running the following code. We will filter the DataFrame on the machinery market segment and get all the records where <strong>AvgAcctBal&gt;4500</strong>. We can also check the execution plan.</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df_agg </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> df_cust_sch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">groupBy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_MKTSEGMENT&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">agg</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    avg</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_ACCTBAL&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">alias</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;AvgAcctBal&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_agg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">where</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">df_agg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">C_MKTSEGMENT </span><span class="token operator" style="color:#393A34">==</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;MACHINERY&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">where</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df_agg</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">AvgAcctBal </span><span class="token operator" style="color:#393A34">&gt;</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">4500</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">show</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>After running the preceding query, we must get the DAG for the query we executed. Click on <strong>ID for your query</strong>. When you scroll to the end, you will find details of the execution plan for the dataframe or SparkSQL API. It will contain the <strong>Parsed Logical Plan</strong>, <strong>Analyzed Logical Plan</strong>, <strong>Optimized Logical Plan</strong>, and <strong>Physical Plan stages</strong>:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218402422-2d93a404-64c3-49fa-b324-7e7ff9b840aa.jpg" alt="B12761_03_19" class="img_ev3q"></p><p>Now, let&#x27;s learn how to get the plan using the <strong>Explain()</strong> function. The plans that are generated using the dataframe API and SparkSQL API is the same. Create a <strong>tempview</strong>, and execute the following command to generate an execution plan using both APIs:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">df_cust_sch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">createOrReplaceTempView</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;df_Customer&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sql </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">sql</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;SELECT C_MKTSEGMENT, count(1) FROM df_Customer GROUP BY C_MKTSEGMENT &quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataframe </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> df_cust_sch\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">groupBy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_MKTSEGMENT&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">\</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sql</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">explain</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataframe</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">explain</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The following is the output of executing the preceding code:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218404255-1a52e385-2eb3-4639-ae32-b07b4bcfc7c2.png" alt="Screen Shot 2023-02-13 at 1 40 52 PM" class="img_ev3q"></p><p>Here, you can see that the plan is similar to the query&#x27;s, which was executed using two different APIs.</p><p><strong>How it works...</strong></p><p>You can use either the Spark UI or the <strong>Explain()</strong> command to get the execution plan for the query. Whether you use the dataframe API or the SparkSQL API, the execution plan that&#x27;s generated by Spark is similar. This gives the developer the flexibility to use their API of choice.</p><p>If the data has been partitioned, you can also view the partition filters by using the appropriate partitions. This will help you avoid reading all the data. If there is a <strong>where</strong> clause, you can find the <strong>PushedFilters</strong> value that was populated for the predicate clause you are using in your query. The following example shows the where clause in the query. Here, you will find <strong>PushedFilters</strong>, which helps in restricting the data that is fetched while reading the data:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218403897-aeee8a19-6b8f-4748-951a-4d4986763961.png" alt="Screen Shot 2023-02-13 at 1 36 28 PM" class="img_ev3q"></p><p>At the core of SparkSQL is the Catalyst optimizer, which breaks down a query that&#x27;s been submitted by the user into multiple phases. It does this by using optimization techniques, which efficiently execute the query:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218404405-5daaa820-409e-4b98-b209-805fc37a1d85.jpeg" alt="B12761_03_22" class="img_ev3q"></p><p>Here, we can see that there are multiple phases that the Spark Catalyst optimizer goes through. It shows how a query that&#x27;s been submitted for execution is parsed into multiple logical and physical plans and then converted into Define acronym, which then gets executed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-joins-work-in-spark">How joins work in Spark <a class="hash-link" href="#how-joins-work-in-spark" title="Direct link to heading">​</a></h2><p>In this recipe, you will learn how query joins are executed by the Spark optimizer using different types of sorting algorithms such as <strong>SortMerge</strong> and <strong>BroadcastHash</strong> joins. You will learn how to identify which algorithm has been used by looking at the DAG that Spark generates. You will also learn how to use the hints that are provided in the queries to influence the optimizer to use a specific join algorithm.</p><p>We are using <code>04-joins</code> notebook.</p><p>Let&#x27;s learn how to identify the <strong>join</strong> algorithm the optimizer uses to join two DataFrames.</p><p>First, run initial cells of the notebook to load the orders and customer dataframes with the given schema and data paths.</p><p>Now, get the default value for <strong>autoBroadcastJoinThreshold</strong>. The output of the preceding query should be <strong>10485760</strong>, which is 10 MB. We will change the default value for <strong>autoBroadcastJoinThreshold</strong> to 2 MB from 10 MB to simulate that we are joining two large tables. The size of the customer DataFrame is around 5 MB.</p><p>We will execute a query that will perform an equijoin between the two DataFrames. Then, we will look at the DAG that Spark creates:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218411843-39181aff-f888-493d-8776-fe7bdd692f2a.png" alt="Databricks Shell - Details for Query 38" class="img_ev3q"></p><p>Here, we can see that the <strong>BroadcastHash</strong> join algorithm is being used.</p><p><strong>How it works...</strong></p><p>The Spark query optimizer identifies which join algorithm should be used based on the size of the data in both tables/DataFrames that were used in the query. The <strong>SortMerge</strong> join is used when the size of both tables/DataFrames is large. If the table/DataFrame that&#x27;s used in the join size is less than 10 MB, then the <strong>BroadcastHash</strong> join algorithm will be used. So, when we are joining a big table to another big table, the <strong>SortMerge</strong> join will be used, while if we are joining a big table to a small table, then the <strong>BroadcastHash</strong> join will be used.</p><p>In the <strong>BroadcastHash</strong> join, there is <strong>BroadcastExchange</strong>. Here, a smaller dataset is transferred to every executor on the worker nodes, and it avoids data shuffle between the nodes. This approach improves the query&#x27;s performance as no shuffle operation happens. This is because all the executors have the smaller table cached locally, in memory. The optimizer checks if the size of the smaller table is less than 10 MB and then broadcasts the smaller table to all the executors. If the size of the table(s) is greater than 10 MB, then the <strong>SortMerge</strong> algorithm will be used to join the two DataFrames/tables in a Spark 2.4.5 cluster.</p><p>Spark 3.0.x or later, the behavior is a little different, as they auto-optimize it further and selects <strong>BroadcastHash</strong> strategy even though you set the parameter to 2 MB. When we use a Spark 3.0.x cluster, the optimizer generates a different DAG when we set the value for <strong>autoBroadcastJoinThreshold</strong> to less than 10 MB. When we change the <strong>autoBroadcastJoinThreshold</strong> value to 2 MB by attaching the notebook to a Spark 3.x cluster, the Spark engine will use a <strong>BroadcastHash</strong> join algorithm to execute the query.</p><p>To force a <strong>sortmerge</strong> join, Spark 3.0.x and later has introduced a new hint called <strong>merge</strong>. We can use this to force the optimizer to use a <strong>SortMerge</strong> join. Execute the query having a merge hint and check the DAG for <strong>SortMerge</strong>:</p><p><img loading="lazy" src="https://user-images.githubusercontent.com/62965911/218415065-47c215c7-45ec-413b-8727-51764c13ab1f.png" alt="Databricks Shell - Details for Query 40" class="img_ev3q"></p><p>By using the DAG, you will see that the Spark query optimizer is using a <strong>SortMerge</strong> join to join the two dataframes.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="learning-about-partitions">Learning about partitions <a class="hash-link" href="#learning-about-partitions" title="Direct link to heading">​</a></h2><p>Partitions are subsets of files in memory or storage. In Spark, partitions are more utilized compared to the Hive system or SQL databases. Spark uses partitions for parallel processing and to gain maximum performance.</p><p>Spark and Hive partitions are different; Spark processes data in memory, whereas Hive partitions are in storage. In this recipe, we will cover three different partitions; that is, the input, shuffle, and output partitions.</p><p>We are using <code>05-partitions</code> notebook.</p><p>Let&#x27;s start by looking at input partitions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="input-partitions">Input partitions<a class="hash-link" href="#input-partitions" title="Direct link to heading">​</a></h3><p>Apache Spark has a layered architecture, and the driver nodes communicate with the worker nodes to get the job done. All the data processing happens in the worker nodes. When the job is submitted for processing, each data partition is sent to the specific executors. Each executor processes one partition at a time. Hence, the time it takes each executor to process data is directly proportional to the size and number of partitions. The more partitions there are, the more work will be distributed across the executors. There will also be fewer partitions. This means that processing will be done faster and in larger chunks.</p><p>You can manipulate partitions to speed up data processing.</p><p>While sending the partition to each executor, you can control the size of the partitions to get optimized performance.</p><p>By specifying the <strong>spark.sql.files.maxPartitionBytes</strong> parameter, you can control the number of bytes that can be packed into single partition. You can do this while reading data from JSON, ORC, and Parquet files.</p><p>First, load the dataframe and see the number of partitions that were created for the dataframe. Considering the default block size, 10 partitions will be created.</p><p>Now, you can change the default partition size to 1 MB. When you load the dataframe and check the number of partitions, you will see that 30 partitions have been created.</p><p>You can tweak the partitions of the files to achieve better parallelism.You can test this by creating partitions based on the number of cores in your cluster.</p><p><strong>How it works...</strong></p><p>Spark reads a HDFS file as a single partition for each file split. Spark&#x27;s default block size is 128 MB.</p><p>Let&#x27;s understand this by taking an example of 60 GB uncompressed text data files that have been stored in a HDFS filesystem. As you already know, the default block size is 128 MB, so 480 blocks will be created. There will also be 480 partitions. If the data can&#x27;t be divided, then Spark will use the default number of partitions.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="output-partitions">Output partitions <a class="hash-link" href="#output-partitions" title="Direct link to heading">​</a></h3><p>Saving partitioned data using the proper condition can significantly boost performance while you&#x27;re reading and retrieving data for further processing.</p><p>Reading the required partition limits the number of files and partitions that Spark reads while querying data. It also helps with dynamic partition pruning.</p><p>But sometimes, too many optimizations can make things worse. For example, if you have several partitions, data is scattered within multiple files, so searching the data for particular conditions in the initial query can take time. Also, memory utilization will be more while processing the metadata table as it contains several partitions.</p><p>While saving the in-memory data to disk, you must consider the partition sizes as Spark produces files for each task. Let&#x27;s consider a scenario: if the cluster configuration has more memory for processing the dataframe and saving it as larger partition sizes, then processing the same data even further with a smaller cluster configuration may cause issues while you&#x27;re reading the saved data.</p><p>There are two ways to manage partitions: by using the <strong>repartitioning</strong> and <strong>coalesce</strong> operations.</p><p>First, load the necessary data into the dataframe and check the partitions created. you will see that 10 tasks have been created for 10 partitions. Spark created 10 tasks to achieve parallelism.</p><p>If you have a very large file, you can increase the number of output partitions that Spark will write the output to using many cores.</p><p>You can repartition the existing dataframe using the <strong>repartition</strong> function. You can also pass the name of the column where you want the data to be partitioned.</p><p>The <strong>coalesce</strong> method is used to reduce parallelism while processing data. It avoids fully shuffling data by reducing the number of partitions. You should be careful when applying <strong>coalesce</strong> as it may also reduce the number of partitions you are expecting, which will impact parallel processing and performance.</p><p>You can remediate the problem we mentioned in the preceding step by passing <strong>shuffle = true</strong>. This adds a shuffle step, but reshuffled partitions will use full cluster resources wherever they&#x27;re required.</p><p>You can use the <strong>maxRecordsPerFile</strong> parameter to control the size of the files. With this parameter we are asking Spark to create files with certain number of records.</p><p><strong>How it works...</strong></p><p>The <strong>repartitioning</strong> operation reduces or increases the number of partitions that the data in the cluster will be split by. This is an expensive operation and involves fully shuffling the data over the network and redistributing the data evenly. When this operation is performed, data is serialized, moved, and then deserialized.</p><p>You should only use repartitioning when you understand when it can speed up data processing by Spark jobs. The <strong>coalesce</strong> operation uses existing partitions and reduces the number of partitions to minimize the amount of data that&#x27;s shuffled. Coalesce results in partitions that contain different amounts of data. In this case, the coalesce operation executor leaves data in a minimum number of partitions and only moves data from redundant nodes. Therefore, the coalesce operation usually runs faster than the repartition operation. But sometimes, when there is a significant difference in the partition sizes, it is slower.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shuffle-partitions">Shuffle partitions<a class="hash-link" href="#shuffle-partitions" title="Direct link to heading">​</a></h3><p>In this recipe part, you will learn how to set the <strong>spark.sql.shuffle.partitions</strong> parameter and see the impact it has on performance when there are fewer partitions.</p><p>Most of the time, in the case of wide transformations, where data is required from other partitions, Spark performs a data shuffle. Unfortunately, you can&#x27;t avoid such transformations, but we can configure parameters to reduce the impact this has on performance.</p><p>Wide transformations uses shuffle partitions to shuffle data. However, irrespective of the data&#x27;s size or the number of executors, the number of partitions is set to <strong>200</strong>.</p><p>The data shuffle procedure is triggered by data transformations such as <strong>join()</strong>, <strong>union()</strong>, <strong>groupByKey(</strong>), <strong>reduceBykey()</strong>, and so on. The <strong>spark.sql.shuffle.partitions</strong> configuration sets the number of partitions to use during data shuffling. The partition numbers are set to 200 by default when Spark performs data shuffling.</p><p>Let&#x27;s learn how the execution time gets reduced when the number of shuffle partitions are reduced for small datasets:</p><p>First, load your dataframe from the csv files. Then execute the following code snippet and see how long it takes to execute. Note that the time varies according to the volume of data in the dataframe:</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">conf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token builtin">set</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;spark.sql.shuffle.partitions&quot;</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">200</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">mktSegmentDF </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> df</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">groupBy</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;C_MKTSEGMENT&quot;</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">collect</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The preceding code took 2.96 seconds to execute on the cluster we used with <strong>spark.sql.shuffle.partitions</strong> set to <strong>200</strong> (default value).</p><p>Now, change the value of the <strong>spark.sql.shuffle.partitions</strong> parameter to <strong>30</strong> and look at the execution time for the same query. Note that the execution time when <strong>spark.sql.shuffle.partitions</strong> is set to <strong>30</strong> is 1.57 seconds.</p><p>You will see that the time taken after changing the number of partitions to <strong>30</strong> is comparatively less than when the <strong>spark.sql.shuffle.partitions</strong> parameter value was <strong>200</strong>.</p><p><strong>How it works...</strong></p><p>Spark shuffling can increase or decrease the performance of your job, so based on your memory, data size, and processor, the <strong>spark.sql.shuffle.partitions</strong> configuration value must be set. When the data is small, then the number of partitions should be reduced; otherwise, too many partitions containing less data will be created, resulting in too many tasks with less data to process. However, when the data size is huge, having a higher number for the shuffle partition from default 200 might improve the query execution time. There is no direct formula to get the right number for shuffle partitions. It depends on the cluster size and the size of data you are processing.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="storage-benefits-of-different-file-types">Storage benefits of different file types<a class="hash-link" href="#storage-benefits-of-different-file-types" title="Direct link to heading">​</a></h2><p>Storage formats are a way to define how data is stored in a file. Hadoop doesn&#x27;t have a default file format, but it supports multiple file formats for storing data. Some of the common storage formats for Hadoop are as follows:</p><ul><li>Text files</li><li>Sequence files</li><li>Parquet files</li><li><strong>Record-columnar (RC)</strong> files</li><li><strong>Optimized row columnar (ORC)</strong> files</li><li>Avro files</li></ul><p>Choosing a write file format will provide significant advantages, such as the following:</p><ul><li>Optimized performance while reading and writing data</li><li>Schema evolution support (allows us to change the attributes in a dataset)</li><li>Higher compression, resulting in less storage space being required</li><li>Splittable files (files can be read in parts)</li></ul><p>Let&#x27;s focus on columnar storage formats as they are widely used in big data applications because of how they store data and can be queried by the SQL engine. The columnar format is very useful when a subset of data needs to be referred to. However, when most of the columns in a dataset need to be fetched, then row-oriented storage formats are beneficial.</p><p>The following are some columnar file formats:</p><ul><li><strong>RC files</strong>: This stands for <strong>record columnar files</strong>. They provide many advantages over non-columnar files, such as fast data loading, quick query processing, and highly efficient storage space utilization. RC files are a good option for querying data, but writing them requires more memory and computation. Also, they don&#x27;t support schema evolution.</li><li><strong>ORC files</strong>: This stands for <strong>optimized row columnar files</strong>. They have almost the same advantages and disadvantages as RC files. However, ORC files have better compression. They were designed for Hive and cannot be used with non-Hive MapReduce interfaces such as Pig, Java, or Impala.</li><li><strong>Parquet files</strong>: Parquet is a columnar data format that is suitable for large-scale queries. Parquet is just as good as RC and ORC in terms of performance while reading data, but it is slower when writing compared to other columnar file formats. Parquet supports schema evolution, which is not supported in RC and ORC file formats. Parquet also supports column pruning and predicate pushdown, which are not supported in CSV or JSON.</li></ul><p>Now, let&#x27;s look at partition pruning and predicate pushdown:</p><ul><li><strong>Partition pruning</strong>: When you are dealing with terabytes of data, it is very difficult to retrieve the required data in a performant way. In this case, if files support partition pruning, then data can be retrieved faster. Partition pruning is a performance optimization technique that restricts the number of files and partitions that Spark can read while querying data. When partitioning is done, data is stored according to the partitioning scheme that&#x27;s been segregated in the hierarchical folder structure and when data is queried, only a particular partition where data is available will be searched.</li><li><strong>Predicate pushdown</strong>: This is a condition in Spark queries that&#x27;s used to filter the data that&#x27;s restricting the number of records being returned from databases, thus improving the query&#x27;s performance. While writing Spark queries, you need to ensure that the partition key columns are included in the filter condition of the query. Using predicate pushdown lets you skip over huge portions of the data while you&#x27;re scanning and processing.</li></ul><p>In this recipe, you will learn about and compare the different storage spaces that are required while saving the data in different file formats.</p><p>We are using <code>06-file-formats</code> notebook.</p><p>Let&#x27;s learn how to load data from a data file into a dataframe and then write that dataframe in different file formats:</p><ul><li><p>Load the customer data into your dataframe and sort the customer data on the <strong>C_CUSTKEY</strong> column.</p></li><li><p>Write the sorted and unsorted dataframes in different file formats, such as Parquet and JSON.</p></li><li><p>Compare the storage space that was taken up by each file. You will observe that the sorted data is the smallest, followed by unsorted and then JSON.</p></li></ul><p><strong>How it works...</strong></p><p>Parquet supports all the features that are provided by the RC and ORC file formats. It stores data in binary files with metadata. Using this metadata information, Spark can easily determine data types, column names, and compression by parsing the data files. Because of these features, it is widely used in big data applications.</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">Last updated<!-- --> on <b><time datetime="2023-04-09T13:34:30.000Z">Apr 9, 2023</time></b> by <b>sparsh</b></span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/foundations/programming-languages/pyspark/lab-uber-analysis"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Uber Data Analysis in Pyspark</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/foundations/programming-languages/pyspark/lab-window-functions"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Window Functions in Spark</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#setup" class="table-of-contents__link toc-highlight">Setup</a></li><li><a href="#introduction-to-jobs-stages-and-tasks" class="table-of-contents__link toc-highlight">Introduction to jobs, stages, and tasks</a></li><li><a href="#deep-diving-into-schema-inference" class="table-of-contents__link toc-highlight">Deep diving into schema inference</a></li><li><a href="#looking-into-the-query-execution-plan" class="table-of-contents__link toc-highlight">Looking into the query execution plan</a></li><li><a href="#how-joins-work-in-spark" class="table-of-contents__link toc-highlight">How joins work in Spark </a></li><li><a href="#learning-about-partitions" class="table-of-contents__link toc-highlight">Learning about partitions </a><ul><li><a href="#input-partitions" class="table-of-contents__link toc-highlight">Input partitions</a></li><li><a href="#output-partitions" class="table-of-contents__link toc-highlight">Output partitions </a></li><li><a href="#shuffle-partitions" class="table-of-contents__link toc-highlight">Shuffle partitions</a></li></ul></li><li><a href="#storage-benefits-of-different-file-types" class="table-of-contents__link toc-highlight">Storage benefits of different file types</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.166317d0.js"></script>
<script src="/assets/js/main.3580ce8c.js"></script>
</body>
</html>