{
  "pluginId": "default",
  "version": "current",
  "label": "Next",
  "banner": null,
  "badge": false,
  "noIndex": false,
  "className": "docs-version-current",
  "isLast": true,
  "docsSidebars": {
    "docs": [
      {
        "type": "link",
        "label": "Introduction",
        "href": "/docs/introduction",
        "docId": "introduction"
      },
      {
        "type": "category",
        "label": "Getting Started",
        "items": [
          {
            "type": "category",
            "label": "Data Engineering",
            "items": [
              {
                "type": "link",
                "label": "Data Engineering Basics",
                "href": "/docs/foundations/basics/de-basics",
                "docId": "foundations/basics/de-basics"
              },
              {
                "type": "link",
                "label": "Data Pipelines",
                "href": "/docs/foundations/basics/data-pipelines",
                "docId": "foundations/basics/data-pipelines"
              },
              {
                "type": "link",
                "label": "OLTP vs OLAP",
                "href": "/docs/foundations/basics/oltp-vs-olap",
                "docId": "foundations/basics/oltp-vs-olap"
              },
              {
                "type": "link",
                "label": "Data Storages",
                "href": "/docs/foundations/basics/data-storages",
                "docId": "foundations/basics/data-storages"
              },
              {
                "type": "link",
                "label": "SQL vs NoSQL",
                "href": "/docs/foundations/basics/sql-vs-nosql",
                "docId": "foundations/basics/sql-vs-nosql"
              },
              {
                "type": "link",
                "label": "Big Data",
                "href": "/docs/foundations/basics/big-data",
                "docId": "foundations/basics/big-data"
              },
              {
                "type": "link",
                "label": "Batch vs Incremental Data Processing",
                "href": "/docs/foundations/basics/batch-vs-incremental",
                "docId": "foundations/basics/batch-vs-incremental"
              },
              {
                "type": "link",
                "label": "Data Contract",
                "href": "/docs/foundations/basics/data-contract",
                "docId": "foundations/basics/data-contract"
              },
              {
                "type": "link",
                "label": "Data Governance",
                "href": "/docs/foundations/basics/data-governance",
                "docId": "foundations/basics/data-governance"
              },
              {
                "type": "link",
                "label": "Data Management",
                "href": "/docs/foundations/basics/data-management",
                "docId": "foundations/basics/data-management"
              },
              {
                "type": "link",
                "label": "Data Quality",
                "href": "/docs/foundations/basics/data-quality",
                "docId": "foundations/basics/data-quality"
              },
              {
                "type": "link",
                "label": "Batch Data Processing",
                "href": "/docs/foundations/basics/batch-data-processing",
                "docId": "foundations/basics/batch-data-processing"
              },
              {
                "type": "link",
                "label": "Stream and Unified Data Processing",
                "href": "/docs/foundations/basics/stream-data-processing",
                "docId": "foundations/basics/stream-data-processing"
              },
              {
                "type": "link",
                "label": "25 Most Common Interview Questions",
                "href": "/docs/foundations/basics/25-most-common-interview-questions",
                "docId": "foundations/basics/most-common-interview-questions-set1"
              },
              {
                "type": "link",
                "label": "50 Most Common Interview Questions",
                "href": "/docs/foundations/basics/50-most-common-interview-questions",
                "docId": "foundations/basics/most-common-interview-questions-set2"
              },
              {
                "type": "link",
                "label": "50 Most Asked Data Engineer Interview Questions and Answers in 2023",
                "href": "/docs/foundations/basics/50-most-common-interview-questions-2023",
                "docId": "foundations/basics/most-common-interview-questions-set3"
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Spark and Hadoop",
            "items": [
              {
                "type": "link",
                "label": "Apache Spark Basics",
                "href": "/docs/foundations/basics/spark-basics",
                "docId": "foundations/basics/spark-basics"
              },
              {
                "type": "link",
                "label": "Origin of Spark",
                "href": "/docs/foundations/basics/spark-origin",
                "docId": "foundations/basics/spark-origin"
              },
              {
                "type": "link",
                "label": "Hadoop Basics",
                "href": "/docs/foundations/basics/hadoop-basics",
                "docId": "foundations/basics/hadoop-basics"
              },
              {
                "type": "link",
                "label": "Map Reduce",
                "href": "/docs/foundations/basics/map-reduce",
                "docId": "foundations/basics/map-reduce"
              },
              {
                "type": "link",
                "label": "Hadoop vs Spark",
                "href": "/docs/foundations/basics/hadoop-vs-spark",
                "docId": "foundations/basics/hadoop-vs-spark"
              },
              {
                "type": "link",
                "label": "Interpreting a Spark DAG",
                "href": "/docs/foundations/basics/spark-dag",
                "docId": "foundations/basics/spark-dag"
              },
              {
                "type": "link",
                "label": "Spark RDDs",
                "href": "/docs/foundations/basics/rdd",
                "docId": "foundations/basics/rdd"
              },
              {
                "type": "link",
                "label": "Quiz: Spark basics",
                "href": "/docs/foundations/basics/spark-quiz",
                "docId": "foundations/basics/spark-quiz"
              }
            ],
            "collapsed": true,
            "collapsible": true
          },
          {
            "type": "category",
            "label": "Developer Essentials",
            "items": [
              {
                "type": "link",
                "label": "Install Anaconda",
                "href": "/docs/foundations/developer/install-anaconda",
                "docId": "foundations/developer/install-anaconda"
              },
              {
                "type": "link",
                "label": "Install Jupyter Notebook",
                "href": "/docs/foundations/developer/install-jupyter",
                "docId": "foundations/developer/install-jupyter"
              },
              {
                "type": "link",
                "label": "Install VS Code",
                "href": "/docs/foundations/developer/install-vscode",
                "docId": "foundations/developer/install-vscode"
              },
              {
                "type": "link",
                "label": "Lab: Explore VS Code features",
                "href": "/docs/foundations/developer/lab-explore-vscode-features",
                "docId": "foundations/developer/lab-explore-vscode-features"
              },
              {
                "type": "link",
                "label": "Setup Git",
                "href": "/docs/foundations/developer/setup-git",
                "docId": "foundations/developer/setup-git"
              },
              {
                "type": "link",
                "label": "Lab: Learn git commands",
                "href": "/docs/foundations/developer/lab-learn-git-commands",
                "docId": "foundations/developer/lab-learn-git-commands"
              },
              {
                "type": "link",
                "label": "Lab: Learn Bash Commands",
                "href": "/docs/foundations/developer/lab-bash-commands/",
                "docId": "foundations/developer/lab-bash-commands/index"
              },
              {
                "type": "link",
                "label": "Install DBeaver",
                "href": "/docs/foundations/developer/install-dbeaver",
                "docId": "foundations/developer/install-dbeaver"
              }
            ],
            "collapsed": true,
            "collapsible": true
          }
        ],
        "collapsed": true,
        "collapsible": true
      }
    ]
  },
  "docs": {
    "a1-interviewprep/algorithm-problems": {
      "id": "a1-interviewprep/algorithm-problems",
      "title": "Algorithm Problems",
      "description": "1. https://leetcode.com/problems/two-sum/"
    },
    "a1-interviewprep/azure-data-engineering": {
      "id": "a1-interviewprep/azure-data-engineering",
      "title": "Azure Data Engineering",
      "description": "Case study - data lake"
    },
    "a1-interviewprep/basic-technical-questions": {
      "id": "a1-interviewprep/basic-technical-questions",
      "title": "Basic Technical Questions",
      "description": "Interviewers use easy technical questions designed to weed out candidates without the right experience. This question assesses your experience level, comfort with specific tools, and the depth of your domain expertise. Basic technical questions include:"
    },
    "a1-interviewprep/behavioral-interview-questions": {
      "id": "a1-interviewprep/behavioral-interview-questions",
      "title": "Behavioral Interview Questions",
      "description": "Behavioral questions assess soft skills (e.g., communication, leadership, adaptability), your skill level, and how you fit into the company’s data engineering team."
    },
    "a1-interviewprep/behavioral-questions": {
      "id": "a1-interviewprep/behavioral-questions",
      "title": "Behavioral Questions",
      "description": "Tell me about yourself"
    },
    "a1-interviewprep/README": {
      "id": "a1-interviewprep/README",
      "title": "Interview Preparation",
      "description": "Whether you’re just getting into the data engineer job market or your interview is tomorrow, practice is an essential part of the interview preparation process for a data engineer. Data engineering interview questions assess your data engineering skills and domain expertise. They are based on a company’s tech stack and technology goals, and they test your ability to perform job functions. In an interview for any Engineering role, the interviewer wants to understand if you have good analytical skills, problem-solving ability, communication, work culture and ability to build technical solutions. Specific to Data Engineering, they also want to understand if you have the skills to handle large data and build scalable and robust systems."
    },
    "a1-interviewprep/resume-buildup": {
      "id": "a1-interviewprep/resume-buildup",
      "title": "Resume Buildup",
      "description": "Expert Tips to Perfect Your Data Engineer Resume"
    },
    "a3-casestudies/99group": {
      "id": "a3-casestudies/99group",
      "title": "99 Group",
      "description": "References"
    },
    "a3-casestudies/airbnb": {
      "id": "a3-casestudies/airbnb",
      "title": "Airbnb",
      "description": "Watch this video//www.youtube.com/watch?v=iokqkMfyIfo"
    },
    "a3-casestudies/amazon": {
      "id": "a3-casestudies/amazon",
      "title": "Amazon",
      "description": "Amazon Migrates 50 PB of Analytics Data from Oracle to AWS"
    },
    "a3-casestudies/bookingdotcom": {
      "id": "a3-casestudies/bookingdotcom",
      "title": "Booking.com",
      "description": "An online travel agency for reservations"
    },
    "a3-casestudies/expedia": {
      "id": "a3-casestudies/expedia",
      "title": "Expedia",
      "description": "Technology company that powers travel"
    },
    "a3-casestudies/fair": {
      "id": "a3-casestudies/fair",
      "title": "FAIR - Data Ingestion with a Cloud Data Platform",
      "description": "Fair is a financial technology company that offers a new way to shop for a car, get approved for a loan, and pay for the car. Its unique smartphone app gives customers the freedom to drive the car they want for as long as they want and gives them the flexibility to turn in the vehicle at any time. Data is essential to this business model: The company ingests and analyzes billions of data points from more than 500 data sources."
    },
    "a3-casestudies/harmony": {
      "id": "a3-casestudies/harmony",
      "title": "HARMONY - Responsive Data Pipeline",
      "description": "As one of Australasia's leading marketplace lending platforms, Harmoney has matched more than 32,000 borrowers with more than $1 billion in personal loans. Its lending experience is 100 percent online, from application to funding, necessitating complete and accurate data collection, processing, and analysis."
    },
    "a3-casestudies/helpshift": {
      "id": "a3-casestudies/helpshift",
      "title": "Helpshift",
      "description": "Technology company that powers travel"
    },
    "a3-casestudies/hometogo": {
      "id": "a3-casestudies/hometogo",
      "title": "HomeToGo",
      "description": "Vacation Rentals, Cabins, Condos, Villas, & More"
    },
    "a3-casestudies/intuit": {
      "id": "a3-casestudies/intuit",
      "title": "Intuit",
      "description": "Complete Financial Confidence"
    },
    "a3-casestudies/linkedin": {
      "id": "a3-casestudies/linkedin",
      "title": "LinkedIn",
      "description": "World's largest professional network on the internet"
    },
    "a3-casestudies/myntra": {
      "id": "a3-casestudies/myntra",
      "title": "Myntra",
      "description": "1. Janus : Data processing framework at Myntra"
    },
    "a3-casestudies/outfit7": {
      "id": "a3-casestudies/outfit7",
      "title": "Outfit7",
      "description": "Multinational video game developer"
    },
    "a3-casestudies/panoramic": {
      "id": "a3-casestudies/panoramic",
      "title": "PANORAMIC - Simplifying Data Ingestion, Transformation, And Delivery",
      "description": "Panoramic is a world leader in providing analytics to communications firms and media service providers. It helps marketers use consumer data to hone their creative strategies. Panoramic's data scientists and marketing analysts build customized data platforms that enable marketers to visualize consumer data for analysis, benchmarking, internal collaboration, and more. The company has built its business around ingesting and modeling marketing data as a foundation for generating meaningful insights."
    },
    "a3-casestudies/plexure": {
      "id": "a3-casestudies/plexure",
      "title": "Plexure",
      "description": "Data-Driven Personalized Customer Experiences"
    },
    "a3-casestudies/README": {
      "id": "a3-casestudies/README",
      "title": "Case Studies",
      "description": "Steps"
    },
    "a3-casestudies/spotify": {
      "id": "a3-casestudies/spotify",
      "title": "Spotify Discover Weekly Playlist",
      "description": "This case study uses information from various presentations that engineers from Spotify delivered in 2015, mainly the “From Idea to Execution: Spotify’s Discover Weekly” presentation. They may no longer use some of the techniques or technologies discussed, but the core takeaways from this case study are still relevant."
    },
    "a3-casestudies/starbucks": {
      "id": "a3-casestudies/starbucks",
      "title": "Starbucks",
      "description": "Multinational chain of coffeehouses and roastery reserves"
    },
    "a3-casestudies/trivago": {
      "id": "a3-casestudies/trivago",
      "title": "Trivago",
      "description": "Compare hotel prices worldwide"
    },
    "a3-casestudies/twilio": {
      "id": "a3-casestudies/twilio",
      "title": "Twilio",
      "description": "Presto on AWS at Twilio - Lesson Learned and Optimization"
    },
    "a3-casestudies/twitter": {
      "id": "a3-casestudies/twitter",
      "title": "Twitter",
      "description": "Watch this video//www.youtube.com/watch?v=OQPpEEMm6Gg"
    },
    "a3-casestudies/uber": {
      "id": "a3-casestudies/uber",
      "title": "Uber",
      "description": "Uber's Big Data Platform: 100+ Petabytes with Minute Latency"
    },
    "a3-casestudies/video-stream": {
      "id": "a3-casestudies/video-stream",
      "title": "Technology-Driven Video Advertising",
      "description": "A global leader in cable and video advertising collects data from more than six billion devices. This data is accurate, rich, and valuable as part of its strategy for providing customized, targeted advertising. The problem was that this data was huge, and the resources required to provide near-real-time targeted advertising were immense. To meet the need, the company established several hundred Hadoop servers and used them to batch process and pre-aggregate the data. Still, with this huge investment in hardware and tools, the process took one to three days to send a targeted ad to a customer."
    },
    "a3-casestudies/webshoes": {
      "id": "a3-casestudies/webshoes",
      "title": "Webshoes",
      "description": "Webshoes is a fictitious sales company of shoes and accessories that is being created. The company's business areas have defined that Webshoes will have an online store and that the store will need to have personalized experiences. The requirements that the business areas have passed to the project development team are as follows:"
    },
    "a3-casestudies/woot": {
      "id": "a3-casestudies/woot",
      "title": "Woot.com",
      "description": "Online Shopping"
    },
    "assignments/bellabeat-analysis/README": {
      "id": "assignments/bellabeat-analysis/README",
      "title": "Bellabeat Case Study",
      "description": "How can a wellness company play it smart?"
    },
    "assignments/cyclistic-analysis/README": {
      "id": "assignments/cyclistic-analysis/README",
      "title": "Cyclistic Case Study",
      "description": "How does a bike-share navigate speedy success?"
    },
    "assignments/florida-panthers/README": {
      "id": "assignments/florida-panthers/README",
      "title": "NHLAPI - SQL and Python Assignment",
      "description": "Objective"
    },
    "assignments/healthverity/README": {
      "id": "assignments/healthverity/README",
      "title": "HealthVerity",
      "description": "Thank you for taking the time to consider HealthVerity and our Data QA Engineer position. Your skills and experience line up well with what we are seeking in this role and we'd like to have you move forward in the process. Below are a few questions we'd invite you to consider and respond to in a separate text document."
    },
    "assignments/klodars-datalake-design/README": {
      "id": "assignments/klodars-datalake-design/README",
      "title": "Klodars Datalake Design",
      "description": "Problem Statement"
    },
    "assignments/logsense/README": {
      "id": "assignments/logsense/README",
      "title": "Logsense",
      "description": "Lab 1: Apache Server Log Analysis"
    },
    "assignments/mistplay-takehome/README": {
      "id": "assignments/mistplay-takehome/README",
      "title": "Mistplay Data Engineer Take Home Challenge",
      "description": "This is an exercise for you to demonstrate your design and implementation skills to solve a typical data engineering problem."
    },
    "assignments/mysql-s3-incremental/README": {
      "id": "assignments/mysql-s3-incremental/README",
      "title": "Reading data from MySQL in CSV and saving into S3",
      "description": "In this lab, we are loading the data from RDS MySQL Database into CSV and then saving that csv into S3 using Boto3."
    },
    "assignments/olx-python/README": {
      "id": "assignments/olx-python/README",
      "title": "OLX Data Engineering Challenge",
      "description": "Tasks"
    },
    "assignments/README": {
      "id": "assignments/README",
      "title": "Assignments",
      "description": ""
    },
    "assignments/sakila/README": {
      "id": "assignments/sakila/README",
      "title": "Batch and Stream Unified analytics for Sakila Music Company",
      "description": "Problem Statement"
    },
    "assignments/spiff-takehome/README": {
      "id": "assignments/spiff-takehome/README",
      "title": "Spiff Data Engineering Candidate Coding Exercises",
      "description": "This is the repo for the data engineering exercise that is part of the interview process for a data engineering role at Spiff."
    },
    "assignments/spotify-extract-load/README": {
      "id": "assignments/spotify-extract-load/README",
      "title": "Spotify Extract Load Airflow",
      "description": "arch drawio"
    },
    "assignments/ternary/README": {
      "id": "assignments/ternary/README",
      "title": "Ternary",
      "description": "Prompt"
    },
    "assignments/twitter-s3/README": {
      "id": "assignments/twitter-s3/README",
      "title": "Assignment: Extract Data from Twitter API and Load into AWS S3",
      "description": ""
    },
    "b3-misc/explore-further": {
      "id": "b3-misc/explore-further",
      "title": "Explore Further",
      "description": "Basic Concepts"
    },
    "b3-misc/extras": {
      "id": "b3-misc/extras",
      "title": "Extras",
      "description": "Data, analytics and AI maturity curve"
    },
    "b3-misc/readme_archive": {
      "id": "b3-misc/readme_archive",
      "title": "Data Engineering Intensive Training Program",
      "description": "Hit the ⭐️ button if you like the repo."
    },
    "b3-misc/resources": {
      "id": "b3-misc/resources",
      "title": "Resources",
      "description": "Courses"
    },
    "capstones/acled/README": {
      "id": "capstones/acled/README",
      "title": "ACLED",
      "description": "Objective"
    },
    "capstones/citibike-trip-histories/README": {
      "id": "capstones/citibike-trip-histories/README",
      "title": "Citi Bike Trip Histories",
      "description": "The goal of this capstone is to build an end-to-end data pipeline:"
    },
    "capstones/city-pollution/README": {
      "id": "capstones/city-pollution/README",
      "title": "Air Pollution Pipeline",
      "description": ""
    },
    "capstones/city-traffic-drone/README": {
      "id": "capstones/city-traffic-drone/README",
      "title": "Airflow - City Traffic Drone",
      "description": "City Vehicle Trajactories Data extraction and warehousing for Traffic analysis"
    },
    "capstones/climate/README": {
      "id": "capstones/climate/README",
      "title": "Global Historical Climatology Network Daily Data Pipeline",
      "description": "Objective"
    },
    "capstones/cloudmaze/README": {
      "id": "capstones/cloudmaze/README",
      "title": "Building End to end data pipeline in AWS",
      "description": "Architecture Diagram"
    },
    "capstones/dbt-redshift/README": {
      "id": "capstones/dbt-redshift/README",
      "title": "README",
      "description": "Objective"
    },
    "capstones/digitalskola/README": {
      "id": "capstones/digitalskola/README",
      "title": "DigitalSkola",
      "description": "Objective"
    },
    "capstones/disaster-response/README": {
      "id": "capstones/disaster-response/README",
      "title": "Disaster Response Pipeline",
      "description": "wall"
    },
    "capstones/funflix/README": {
      "id": "capstones/funflix/README",
      "title": "Funflix",
      "description": "You are working as a data engineer in an Australian media company Funflix. You got the following requirements and tasks to solve."
    },
    "capstones/hmc/README": {
      "id": "capstones/hmc/README",
      "title": "Datalake Schema Correction",
      "description": "Objective"
    },
    "capstones/kinesis-flink-beam/README": {
      "id": "capstones/kinesis-flink-beam/README",
      "title": "Log Analytics and Processing in Real-Time",
      "description": "Lab 1: Apache Flink on Amazon Kinesis Data Analytics"
    },
    "capstones/kinesis-flink-etl/README": {
      "id": "capstones/kinesis-flink-etl/README",
      "title": "Streaming ETL pipeline with Apache Flink and Amazon Kinesis Data Analytics",
      "description": "We will create an Amazon Kinesis Data Analytics for Apache Flink application with Amazon Kinesis Data Streams as a source and a Amazon S3 bucket as a sink. Random data is ingested using Amazon Kinesis Data Generator. The Apache Flink application code performs a word count on the streaming random data using a tumbling window of 5 minutes. The generated word count is then stored in the specified Amazon S3 bucket. Amazon Athena is used to query data generated in the Amazon S3 bucket to validate the end results."
    },
    "capstones/kortex/README": {
      "id": "capstones/kortex/README",
      "title": "Kortex",
      "description": "Objectives"
    },
    "capstones/lufthansa/README": {
      "id": "capstones/lufthansa/README",
      "title": "Lufthansa API",
      "description": "Objective"
    },
    "capstones/movie-sentiment/README": {
      "id": "capstones/movie-sentiment/README",
      "title": "Movie Review Sentiment Analysis Pipeline",
      "description": "Build a pipeline that expresses the fact artist review sentiment and film review sentiment, based on the data provided by IMDb and TMDb."
    },
    "capstones/multi-touch-attribution/README": {
      "id": "capstones/multi-touch-attribution/README",
      "title": "Multi-touch Attribution",
      "description": "Abstract"
    },
    "capstones/other/covid19-datalake/README": {
      "id": "capstones/other/covid19-datalake/README",
      "title": "Covid-19 Data Pipeline",
      "description": "Objective"
    },
    "capstones/other/data-ingestion-pulumi-lambda-snowflake/README": {
      "id": "capstones/other/data-ingestion-pulumi-lambda-snowflake/README",
      "title": "paas-data-ingestion",
      "description": "Ingest and prepare data with AWS lambdas, Snowflake and dbt in a scalable, fully replayable manner."
    },
    "capstones/other/equitize/README": {
      "id": "capstones/other/equitize/README",
      "title": "Equitize Data Migration",
      "description": "Financial Data Extraction and Storage"
    },
    "capstones/other/fireflow/airflow/notes": {
      "id": "capstones/other/fireflow/airflow/notes",
      "title": "notes",
      "description": "1. How to set project"
    },
    "capstones/other/fireflow/README": {
      "id": "capstones/other/fireflow/README",
      "title": "Build an ELT pipeline with NYC Taxi dataset",
      "description": "Objective"
    },
    "capstones/other/loop-analytics/README": {
      "id": "capstones/other/loop-analytics/README",
      "title": "Loop Analytics",
      "description": "Extracing data and loading it to Redshift, and a dbt project for transforming it."
    },
    "capstones/other/patent-analytics/README": {
      "id": "capstones/other/patent-analytics/README",
      "title": "Patent Analytics Data Pipeline",
      "description": "front"
    },
    "capstones/other/patent-analytics/REPORT": {
      "id": "capstones/other/patent-analytics/REPORT",
      "title": "Patent Analytics Data Pipeline",
      "description": "Introduction"
    },
    "capstones/other/patent-analytics/SETUP": {
      "id": "capstones/other/patent-analytics/SETUP",
      "title": "Getting Started",
      "description": "Prerequisites"
    },
    "capstones/other/patent-analytics/spark/README": {
      "id": "capstones/other/patent-analytics/spark/README",
      "title": "Patents Analytics Spark",
      "description": "This folder contains the pyspark scripts used for cleaning, performing keyword extraction, and doing the ETL job for patents analytics data."
    },
    "capstones/other/reddit-kafka-sparkstream-bigquery/README": {
      "id": "capstones/other/reddit-kafka-sparkstream-bigquery/README",
      "title": "reddit-data-engineering of r/FedEx",
      "description": "- Create Kafka cluster using docker-compose hosted on GCP VM"
    },
    "capstones/other/resale-price-realtime/README": {
      "id": "capstones/other/resale-price-realtime/README",
      "title": "Machine Learning Streamming with Kafka, Debezium and BentoML",
      "description": "This is a personal project created to simulate a Machine Learning real-time application."
    },
    "capstones/other/sqs-postgres-etl/README": {
      "id": "capstones/other/sqs-postgres-etl/README",
      "title": "Data Engineering Take Home - ETL off a SQS Qeueue",
      "description": "This challenge will focus on your ability to write a small application that can read from an AWS SQS Qeueue, transform that data, then write to a Postgres database. Your objective is to read JSON data containing user login behavior from an AWS SQS Queue. Fetch wants to hide personal identifiable information (PII). The fields device_id and ip should be masked, but in a way where it is easy for data analysts to identify duplicate values in those fields. Once you have flattened the JSON data object and masked those two fields, write each record to a Postgres database."
    },
    "capstones/other/star-jeans-etl/README": {
      "id": "capstones/other/star-jeans-etl/README",
      "title": "ETL Building for an E-commerce Jeans Company",
      "description": "Obs: The company and business problem are both fictitious, although the data is real."
    },
    "capstones/other/taxi-fare-prediction/README": {
      "id": "capstones/other/taxi-fare-prediction/README",
      "title": "NYC Taxi Fare Prediction",
      "description": "Objective"
    },
    "capstones/other/text-speech-data-pipeline/README": {
      "id": "capstones/other/text-speech-data-pipeline/README",
      "title": "Data Pipeline for Text to Speech translation model",
      "description": "Problem Statement"
    },
    "capstones/other/topic-modeling-pipeline/README": {
      "id": "capstones/other/topic-modeling-pipeline/README",
      "title": "Machine Learning with Big Data",
      "description": "Use big-data tools (PySpark) to run topic modeling (unsupervised machine learning) on Twitter data streamed using AWS Kinesis Firehose."
    },
    "capstones/other/topic-modeling-pipeline/v2/scoping": {
      "id": "capstones/other/topic-modeling-pipeline/v2/scoping",
      "title": "Scoping",
      "description": "Objective"
    },
    "capstones/other/twitter-sentiment-realtime/README": {
      "id": "capstones/other/twitter-sentiment-realtime/README",
      "title": "Twitter Sentiment Realtime",
      "description": "Objective"
    },
    "capstones/other/twitter-snowpipe/README": {
      "id": "capstones/other/twitter-snowpipe/README",
      "title": "Auto-Ingest Twitter Data into Snowflake",
      "description": "Follow this:"
    },
    "capstones/other/us-job-vacancies/README": {
      "id": "capstones/other/us-job-vacancies/README",
      "title": "Work around the world Job vacancies analysis",
      "description": "Problem Statement"
    },
    "capstones/other/youtube-insights/README": {
      "id": "capstones/other/youtube-insights/README",
      "title": "YouTube Trend Analysis Workshop",
      "description": "youtube_trending"
    },
    "capstones/README": {
      "id": "capstones/README",
      "title": "Capstones",
      "description": "ETL - ACLED"
    },
    "capstones/recofront/README": {
      "id": "capstones/recofront/README",
      "title": "Building Recommender System from Scratch",
      "description": "Overview"
    },
    "capstones/reddit/README": {
      "id": "capstones/reddit/README",
      "title": "Reddit Submissions, Authors and Subreddits analysis",
      "description": "Problem Statement"
    },
    "capstones/redshield/README": {
      "id": "capstones/redshield/README",
      "title": "AWS Kafka and DynamoDB for real time fraud detection",
      "description": "Problem Statement"
    },
    "capstones/robust-data-pipeline/README": {
      "id": "capstones/robust-data-pipeline/README",
      "title": "Data Pipeline with dbt, Airflow and Great Expectations",
      "description": "In this project, we will learn how to combine the functions of three open source tools - Airflow, dbt and Great expectations - to build, test, validate, document, and orchestrate an entire pipeline, end to end, from scratch. We are going to load the NYC Taxi data into Redshift warehouse and then transform + validate the data using dbt and great expectations."
    },
    "capstones/ServerlessStreamingApp/README": {
      "id": "capstones/ServerlessStreamingApp/README",
      "title": "Serverless Streaming Data on AWS",
      "description": "https://youtu.be/abaVSFFdU-c"
    },
    "capstones/smartcity/analytics/flink-cdk/README": {
      "id": "capstones/smartcity/analytics/flink-cdk/README",
      "title": "Welcome to your CDK Python project!",
      "description": "This is a blank project for Python development with CDK."
    },
    "capstones/smartcity/analytics/producer-app/readme": {
      "id": "capstones/smartcity/analytics/producer-app/readme",
      "title": "readme",
      "description": ""
    },
    "capstones/smartcity/analytics/producer-cdk/README": {
      "id": "capstones/smartcity/analytics/producer-cdk/README",
      "title": "README",
      "description": "<<<<<<< HEAD"
    },
    "capstones/smartcity/dataStream/README": {
      "id": "capstones/smartcity/dataStream/README",
      "title": "Amazon Kinesis Data Streams",
      "description": "Register Enhanced Fan-Out Consumer"
    },
    "capstones/smartcity/firehose/README": {
      "id": "capstones/smartcity/firehose/README",
      "title": "Amazon Kinesis Data Firehose",
      "description": "Description of files"
    },
    "capstones/sparkify/README": {
      "id": "capstones/sparkify/README",
      "title": "Sparkify",
      "description": "Sparkify SQL Data Modeling with Postgres"
    },
    "capstones/spotify/README": {
      "id": "capstones/spotify/README",
      "title": "Spotify",
      "description": "Implement Complete Data Pipeline Data Engineering Project using Spotify"
    },
    "capstones/twitter-sentiment-glue/README": {
      "id": "capstones/twitter-sentiment-glue/README",
      "title": "Twitter data Topic Analysis and Realtime Sentiment Analysis",
      "description": "Problem Statement"
    },
    "capstones/us-immigration/README": {
      "id": "capstones/us-immigration/README",
      "title": "US Immigration analysis and data pipeline",
      "description": "Problem Statement"
    },
    "data-modeling/3nf-data-modeling": {
      "id": "data-modeling/3nf-data-modeling",
      "title": "3NF/ Relational Modeling",
      "description": "Owing its origins to the entity-relationship modeling methodology, 3NF is also widely used in data warehousing to serve as a normalized layer to the further layers. It provides tremendous flexibility but can end up having really verbose queries. It is usually found in OLTP systems."
    },
    "data-modeling/cap-theorem": {
      "id": "data-modeling/cap-theorem",
      "title": "CAP Theorem",
      "description": "https://en.wikipedia.org/wiki/CAP_theorem"
    },
    "data-modeling/data-modeling-stages": {
      "id": "data-modeling/data-modeling-stages",
      "title": "Stages of Data Modeling",
      "description": "Conceptual Data Model"
    },
    "data-modeling/data-modeling-steps": {
      "id": "data-modeling/data-modeling-steps",
      "title": "Steps of Building Data Models",
      "description": "Gathering requirements"
    },
    "data-modeling/data-vault-modeling": {
      "id": "data-modeling/data-vault-modeling",
      "title": "Data Vault Modeling",
      "description": "A hybrid between 3NF and dimensional modeling, the Data Vault model is much closer to 3NF than to the dimensional model. It tries to keep the best features of 3NF, such as ease of querying highly granular, historical data, and still restructures the data into new types of tables, such as satellites, links, hubs, bridges, and PITs. A Data Vault is a more recent data modeling design pattern used to build data warehouses for enterprise-scale analytics compared to Kimball and Inmon methods. Data Vaults organize data into three different types:  hubs ,  links , and  satellites . Hubs represent core business entities, links represent relationships between hubs, and satellites store attributes about hubs or links. Data Vault focuses on agile data warehouse development where scalability, data integration/ETL and development speed are important."
    },
    "data-modeling/data-warehousing": {
      "id": "data-modeling/data-warehousing",
      "title": "Data Warehousing",
      "description": "In today's data-driven world, businesses need to have the ability to store and analyze large amounts of data in order to make informed decisions. One way to achieve this is by designing and developing a data warehouse. A data warehouse is a centralized repository that allows organizations to store and manage data from various sources in a structured manner. In this note, we will discuss seven key steps involved in designing and developing a data warehouse."
    },
    "data-modeling/designing-incremental-loading": {
      "id": "data-modeling/designing-incremental-loading",
      "title": "Designing for incremental loading",
      "description": "Incremental loading or delta loading refers to the process of loading smaller increments of data into a storage solution—for example, we could have daily data that is being loaded into a data lake or hourly data flowing into an extract, transform, load (ETL) pipeline, and so on. During data-ingestion scenarios, it is very common to do a bulk upload followed by scheduled incremental loads."
    },
    "data-modeling/designing-scd": {
      "id": "data-modeling/designing-scd",
      "title": "Designing SCDs",
      "description": "Whereas operational source systems contain only the latest version of master data, the star schema enables time travel queries to reproduce dimension attribute values on past dates when the fact transaction or event actually happened. The star schema data model allows analytical users to query historical data tying metrics to corresponding dimensional attribute values over time. Time travel is possible because dimension tables contain the exact version of the associated attributes at different time ranges. Relative to the metrics data that keeps changing on a daily or even hourly basis, the dimension attributes change less frequently. Therefore, dimensions in a star schema that keeps track of changes over time are referred to as slowly changing dimensions (SCDs)."
    },
    "data-modeling/dimensional-modeling": {
      "id": "data-modeling/dimensional-modeling",
      "title": "Dimensional Modeling",
      "description": "Dimensional modeling is a bottom-up approach to designing data warehouses in order to optimize them for analytics. Dimensional models are used to denormalize business data into dimensions (like time and product) and facts (like transactions in amounts and quantities), and different subject areas are connected via conformed dimensions to navigate to different fact tables. It is usually found in OLAP systems."
    },
    "data-modeling/inmon-vs-kimball": {
      "id": "data-modeling/inmon-vs-kimball",
      "title": "Inmon versus the Kimball data model",
      "description": "If you look at the internet, there will be many references to data modeling, but two of the most famous approaches are the Inmon method (data-driven) and the Kimball method (user-driven)."
    },
    "data-modeling/lab-airbnb-postgres-datamodel/README": {
      "id": "data-modeling/lab-airbnb-postgres-datamodel/README",
      "title": "AirBnB Postgres Datamodel",
      "description": "Objective"
    },
    "data-modeling/lab-cars-mysql-datamodel/datamodel-cars": {
      "id": "data-modeling/lab-cars-mysql-datamodel/datamodel-cars",
      "title": "Cars Data Modeling MySQL",
      "description": "ETL, data modeling and Data quality runs on cars and dealership dataset"
    },
    "data-modeling/lab-cassandra-digital-music-library/README": {
      "id": "data-modeling/lab-cassandra-digital-music-library/README",
      "title": "Create a Data Model for a Digital Music Library",
      "description": "Conceptual Data Model"
    },
    "data-modeling/lab-cassandra-email-data-model/README": {
      "id": "data-modeling/lab-cassandra-email-data-model/README",
      "title": "Create a Data Model for an Email System",
      "description": "Conceptual Data Model"
    },
    "data-modeling/lab-cassandra-hotel-reservations/README": {
      "id": "data-modeling/lab-cassandra-hotel-reservations/README",
      "title": "Hotel Reservations Data Modeling with Cassandra",
      "description": "Learning CQL Query Clauses"
    },
    "data-modeling/lab-cassandra-investment-data-model/README": {
      "id": "data-modeling/lab-cassandra-investment-data-model/README",
      "title": "Create a Data Model for Investment Accounts or Portfolios",
      "description": "Conceptual Data Model"
    },
    "data-modeling/lab-cassandra-sensor-data-model/README": {
      "id": "data-modeling/lab-cassandra-sensor-data-model/README",
      "title": "Create a Data Model for Temperature Monitoring Sensor Networks",
      "description": "Conceptual Data Model"
    },
    "data-modeling/lab-cassandra-shopping-cart-data-model/README": {
      "id": "data-modeling/lab-cassandra-shopping-cart-data-model/README",
      "title": "Create a Data Model for Online Shopping Carts",
      "description": "Conceptual Data Model"
    },
    "data-modeling/lab-disease-diagnosis/README": {
      "id": "data-modeling/lab-disease-diagnosis/README",
      "title": "Disease Diagnosis and Medic Recommendation System",
      "description": "In our ever-evolving world, there are those who frequently relocate to new locations, such as students like us, expats, who without giving a thought to their safety and well being engage in extensive social interaction, consume unusual delicacies, follow unhealthy lifestyles which includes erratic sleep activity, unsafe sexual practices, intense smoking and drinking, and little to no physical exercise. This may result in anumber of illnesses and other health issues that are difficult to treat promptly. Being away from one's own country is difficult but at times not knowing where to reach for the right medical care can even cause death. The main goal of this lab is to aid expats and students who are away from home in finding and obtaining medical care as soon as possible."
    },
    "data-modeling/lab-dvd-rental-datamodel/README": {
      "id": "data-modeling/lab-dvd-rental-datamodel/README",
      "title": "Postgres Pagila",
      "description": "Objective"
    },
    "data-modeling/lab-google-playstore-datamodel/README": {
      "id": "data-modeling/lab-google-playstore-datamodel/README",
      "title": "Google Platstore Data Modeling",
      "description": "Objective"
    },
    "data-modeling/lab-inegi-snowflake-datamodel/README": {
      "id": "data-modeling/lab-inegi-snowflake-datamodel/README",
      "title": "Inegi Snowflake Datamodel",
      "description": "National Institute of Statistics and Geography"
    },
    "data-modeling/lab-mysql-northwind-datamodel/README": {
      "id": "data-modeling/lab-mysql-northwind-datamodel/README",
      "title": "Data Modeling",
      "description": "Process Flow"
    },
    "data-modeling/lab-mysql-retail-store-datamodel/README": {
      "id": "data-modeling/lab-mysql-retail-store-datamodel/README",
      "title": "Data Modeling",
      "description": "Process Flow"
    },
    "data-modeling/lab-postgres-busrapid-transit/README": {
      "id": "data-modeling/lab-postgres-busrapid-transit/README",
      "title": "Postgres Bus Rapid Transit",
      "description": "Creating a Bus Rapid Transit (BRT) Database"
    },
    "data-modeling/lab-postgres-demographics-datamodel": {
      "id": "data-modeling/lab-postgres-demographics-datamodel",
      "title": "lab-postgres-demographics-datamodel",
      "description": "Create Fact and Dimension Tables from Denormalized Raw Data"
    },
    "data-modeling/lab-postgres-elt-datamodel/README": {
      "id": "data-modeling/lab-postgres-elt-datamodel/README",
      "title": "Postgres ELT Datamodel with PSQL",
      "description": "Create Fact and Dimension Tables from Denormalized Raw Data"
    },
    "data-modeling/lab-postgres-ewallet-datamodel/README": {
      "id": "data-modeling/lab-postgres-ewallet-datamodel/README",
      "title": "Postgres ewallet Datamodel",
      "description": "In this lab, we will design a data model for an eWallet company."
    },
    "data-modeling/lab-postgres-housing-cdc-scd/README": {
      "id": "data-modeling/lab-postgres-housing-cdc-scd/README",
      "title": "Housing Data Model with CDC and SCD Type 2",
      "description": "A critical aspect of data engineering is dealing with data change from one date to the next. In this case, we want both the current and historical value. There are many techniques and patterns to create a historical-friendly model during the ingestion and modeling phases. In this lab, we will build a data model that uses Change Data Capture and Slowly Changing Dimension Type 2 modelization to track housing prices. The code is written in Python. pandas and sqlalchemy library is used to load data from CSV files and execute SQL queries against the data warehouse."
    },
    "data-modeling/lab-sales-datamodel/README": {
      "id": "data-modeling/lab-sales-datamodel/README",
      "title": "Sales Datamodel",
      "description": "Create dimension tables"
    },
    "data-modeling/lab-sparkify-data-model-postgres/README": {
      "id": "data-modeling/lab-sparkify-data-model-postgres/README",
      "title": "Sparkify Music Company Data Model in Postgres",
      "description": "In this, we will model the data with Postgres and build an ETL pipeline using Python. The fact and dimension tables for a star database schema for a particular analytic focus is defined, and an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL was developed."
    },
    "data-modeling/normalization-vs-denormalization": {
      "id": "data-modeling/normalization-vs-denormalization",
      "title": "Normalization vs Denormalization",
      "description": "Normalization is Trying to increase data integrity by reducing the number of copies of the data. Data that needs to be added or updated will be done in as few places as possible. Denormalization is Trying to increase performance by reducing the number of joins between tables (as joins can be slow). Data integrity will take a bit of a potential hit, as there will be more copies of the data (to reduce JOINS)."
    },
    "data-modeling/nosql-data-modeling": {
      "id": "data-modeling/nosql-data-modeling",
      "title": "NoSQL Data Modeling",
      "description": "What is a NoSQL data model?"
    },
    "data-modeling/quiz": {
      "id": "data-modeling/quiz",
      "title": "Quiz",
      "description": "1. What is Data Normalization?"
    },
    "data-modeling/sql-data-modeling": {
      "id": "data-modeling/sql-data-modeling",
      "title": "SQL Data Modeling",
      "description": "What is data modeling? Data modeling is a process for representing the database objects in our real-world or business perspective. Objects in warehouses can be datasets, tables, or views. Representing the objects as close as possible to the real world is important because the end users of the data are human. Some of the most common end users are business analysts, data analysts, data scientists, BI users, or any other roles that require access to the data for business purposes."
    },
    "datascience/algorithms/decision-trees": {
      "id": "datascience/algorithms/decision-trees",
      "title": "Decision Trees",
      "description": "The concept of Decision Trees appeared in the 1960s in the field of psychology for modeling the concept of human learning, visually presenting possible outcomes to potential situations that stem from one prompt. It was around that time when people discovered the usefulness of Decision Trees in programming and mathematics. The first paper that was able to develop a concept of “Decision Tree” mathematically was published by William Belson in 1959. In 1977, various professors from Berkley and Stanford developed an algorithm known as Classification and Regression Trees (CART), which, true to its name, consisted of Classification and Regression Trees. To this day, CART still stands as an important algorithm for data analysis. In the field of ML, Decision Tree serves as one of the most popular algorithms for real-world data science problems."
    },
    "datascience/algorithms/gradient-boosting": {
      "id": "datascience/algorithms/gradient-boosting",
      "title": "Gradient Boosting",
      "description": "Gradient Boosting describes a certain technique in modeling with a variety of different algorithms that are based upon it. The first successful algorithm that utilizes Gradient Boosting is AdaBoost (Adaptive Gradient Boosting) in 1998 formulated by Leo Breiman. In 1999, Jerome Friedman composed the generalization of boosting algorithms emerging at this time, such as AdaBoost, into a single method: Gradient Boosting Machines. Quickly, the idea of Gradient Boosting Machines became extremely popular and proved to be high performing in many real-life tabular datasets. To this day, various Gradient Boosting algorithms such as AdaBoost, XGBoost, and LightGBM are the first choice for many data scientists operating on large, difficult datasets."
    },
    "datascience/algorithms/knn": {
      "id": "datascience/algorithms/knn",
      "title": "K-Nearest Neighbors",
      "description": "K-Nearest Neighbors, or KNN, is one of the simplest and the most intuitive classical machine learning algorithms. KNN was first proposed by Evelyn Fix and Joseph Lawson Hodges Jr. during a technical analysis produced for USAF in 1951. It was unique at its time as the method proposed is nonparametric: the algorithm does not make any assumptions about the statistical properties of the data. Although the paper wasn’t ever published due to the confidential nature of the work, it laid out the groundwork for the first-ever nonparametric classification method, K-Nearest Neighbors. The beauty of KNN lies in its simplicity, and unlike most other algorithms, KNN does not contain a training phase. Additional data can be incorporated seamlessly as the algorithm is memory-based, easily adapting to any new data. Over seven decades later, KNN still stands as a popular classification algorithm, and innovations are still constantly being proposed surrounding it."
    },
    "datascience/algorithms/linear-regression": {
      "id": "datascience/algorithms/linear-regression",
      "title": "Linear Regression",
      "description": "The earliest form of regression analysis, Linear Regression using Least Squares, was first published by Adrien-Marie Legendre in 1805 and then again by Carl Friedrich Gauss in 1809. Both used them to predict astronomical orbitals, specifically bodies that orbit the sun. Later in 1821, Gauss published his continued work on the Least Square Theory; however, the term regression wasn’t coined until the late nineteenth century by Francis Galton. Galton discovered a linear relationship between the weights of mother and daughter seeds across generations. To Galton, regression was merely a term to describe a biological phenomenon. It was not until Udny Yule and Karl Pearson expanded this method to a more general statistics view."
    },
    "datascience/algorithms/logistic-regression": {
      "id": "datascience/algorithms/logistic-regression",
      "title": "Logistic Regression",
      "description": "The logistic function first appeared in Pierre Francois Verhulst’s publication “Correspondance mathmematique et physique” in 1838. Then later in 1845, a more detailed version of the logistic function was published by him. However, the first practical application of such a function wasn’t apparent until 1943 when Wilson and Worcester used the logistic function in bioassay. In the following years, various advances were made toward the function, but the original logistic function is used for Logistic Regression. The Logistic Regression model found its use not only in areas related to biology but also widely in social science."
    },
    "datascience/algorithms/ml-eda/README": {
      "id": "datascience/algorithms/ml-eda/README",
      "title": "ml-eda",
      "description": ""
    },
    "datascience/algorithms/ml-eda/variable-distribution-analysis/README": {
      "id": "datascience/algorithms/ml-eda/variable-distribution-analysis/README",
      "title": "README",
      "description": "We looked at some common techniques for exploring data. We learned how to retrieve subsets of data when that is required for our analysis. We also used pandas methods to generate key statistics on features such as mean, interquartile range, and skew. This gave us a better sense of the central tendency, spread, and shape of the distribution of each feature. It also put us in a better position to identify outliers. Finally, we used the Matplotlib and Seaborn libraries to create histograms, boxplots, and violin plots. This yielded additional insights about the distribution of features, such as the length of the tail and divergence from the normal distribution."
    },
    "datascience/algorithms/random-forest": {
      "id": "datascience/algorithms/random-forest",
      "title": "Random Forest",
      "description": "Random Forest is an algorithm that provides some crucial improvements to the design of Decision Trees. Simply put, Random Forest is an ensemble of many smaller Decision Trees working together. Random Forest uses the simple concept that the wisdom of crowds is always better than one strong individual. By using lowly correlated small Decision Trees, their ensemble of predictions can outperform any single Decision Tree. The technique that Random Forest uses to ensemble smaller Decision Trees is called bagging. Bagging, also known as bootstrap aggregation, is randomly drawing different subsets, with replacements, from the training data, and the final prediction is decided by majority voting."
    },
    "datascience/basics/cross-domain": {
      "id": "datascience/basics/cross-domain",
      "title": "Cross-domain",
      "description": "A common challenge for most current recommender systems is the cold-start problem. Due to the lack of user-item interactions, the fine-tuned recommender systems are unable to handle situations with new users or new items. Recently, some works introduce the meta-optimization idea into the recommendation scenarios, i.e. predicting the user preference by only a few of past interacted items. The core idea is learning a global sharing initialization parameter for all users and then learning the local parameters for each user separately. However, most meta-learning based recommendation approaches adopt model-agnostic meta-learning for parameter initialization, where the global sharing parameter may lead the model into local optima for some users."
    },
    "datascience/basics/graph-embeddings": {
      "id": "datascience/basics/graph-embeddings",
      "title": "Graph Embeddings",
      "description": "Due to their nature, graphs can be analyzed at different levels of granularity: at the node, edge, and graph level (the whole graph), as depicted in the following figure. For each of those levels, different problems could be faced and, as a consequence, specific algorithms should be used."
    },
    "datascience/basics/graph-networks": {
      "id": "datascience/basics/graph-networks",
      "title": "Graph Networks",
      "description": "Item-Item Co-Occurrence Graph"
    },
    "datascience/basics/incremental-learning": {
      "id": "datascience/basics/incremental-learning",
      "title": "Incremental Learning",
      "description": "Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference."
    },
    "datascience/basics/meta-learning": {
      "id": "datascience/basics/meta-learning",
      "title": "Meta Learning",
      "description": "Meta learning covers a wide range of topics and has contributed to a booming study trend. Few-shot learning is one of successful branches of meta learning. We retrospect some representative meta-learning models with strong connections to our work."
    },
    "datascience/basics/model-retraining": {
      "id": "datascience/basics/model-retraining",
      "title": "Model Retraining",
      "description": "Concept Drift"
    },
    "datascience/basics/multi-objective-optimization": {
      "id": "datascience/basics/multi-objective-optimization",
      "title": "Multi-Objective Optimization",
      "description": "Recommender systems have been widely applied to several domains and applications. Traditional recommender systems usually deal with a single objective, such as minimizing the prediction errors or maximizing the ranking of the recommendation list. There is an emerging demand for multi-objective optimization so that the development of recommendation models can take multiple objectives into consideration, especially in the area of multi-stakeholder and multi-task recommender systems."
    },
    "datascience/basics/multitask-learning": {
      "id": "datascience/basics/multitask-learning",
      "title": "Multi-Task Learning",
      "description": "Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning). In those scenarios, it helps to think about what you are trying to do explicitly in terms of MTL and to draw insights from it."
    },
    "datascience/basics/offline-learning": {
      "id": "datascience/basics/offline-learning",
      "title": "Off-Policy Learning",
      "description": "Offline Reinforcement Learning"
    },
    "datascience/basics/river": {
      "id": "datascience/basics/river",
      "title": "River",
      "description": "River is a Python library for online machine learning. It is the result of a merger between creme and scikit-multiflow. River's ambition is to be the go-to library for doing machine learning on streaming data."
    },
    "datascience/basics/scalarization": {
      "id": "datascience/basics/scalarization",
      "title": "Scalarization",
      "description": "| Method | Idea | Scalarization | Characteristic |"
    },
    "datascience/bayesian-optimization": {
      "id": "datascience/bayesian-optimization",
      "title": "Bayesian Optimization",
      "description": "Optimization aims to locate the optimal set of parameters of interest across the whole domain through carefully allocating limited resources. For example, when searching for the car key at home before leaving for work in two minutes, we would naturally start with the most promising place where we would usually put the key. If it is not there, think for a little while about the possible locations and go to the next most promising place. This process iterates until the key is found. In this example, the policy is digesting the available information on previous searches and proposing the following promising location. The environment is the house itself, revealing if the key is placed at the proposed location upon each sampling."
    },
    "datascience/bias-variance-tradeoff": {
      "id": "datascience/bias-variance-tradeoff",
      "title": "Bias-Variance Trade-Off",
      "description": "Bias is a model’s ability to identify general trends and ideas in the dataset, whereas variance is the ability to model data with high precision. Since we think of bias and variance in terms of error, a lower bias or variance is better. The bias-variance relationship is often visualized as a set of dart throws on a bull’s-eye. The ideal set of throws is a cluster of hits all around the center ring, which is low bias (the general “center” of the throws is not shifted off/biased) and low variance (the collection of hits are clustered together rather than far out, indicating consistently good performance)."
    },
    "datascience/challenges/conversion-rate/README": {
      "id": "datascience/challenges/conversion-rate/README",
      "title": "README",
      "description": "https://github.com/Saadorj/DataScienceProjects/tree/master"
    },
    "datascience/challenges/creditcard-fraud/README": {
      "id": "datascience/challenges/creditcard-fraud/README",
      "title": "README",
      "description": "https://github.com/sunjoshi1991/DATA-SCIENCE-CHALLENGES/tree/master"
    },
    "datascience/challenges/employee-retention/README": {
      "id": "datascience/challenges/employee-retention/README",
      "title": "README",
      "description": "https://github.com/rebeccak1/employee-retention"
    },
    "datascience/challenges/employee-shuttle-stops": {
      "id": "datascience/challenges/employee-shuttle-stops",
      "title": "Optimization of Employee Shuttle Stops",
      "description": "Goal"
    },
    "datascience/challenges/fraud-activity/README": {
      "id": "datascience/challenges/fraud-activity/README",
      "title": "Fraud Detection",
      "description": "https://github.com/jkvalentine/Fraud_Detection/tree/master"
    },
    "datascience/challenges/funnel-analysis/README": {
      "id": "datascience/challenges/funnel-analysis/README",
      "title": "README",
      "description": "https://github.com/araj2/Funnel-Analysis/tree/master"
    },
    "datascience/challenges/loan-grant/README": {
      "id": "datascience/challenges/loan-grant/README",
      "title": "README",
      "description": "https://github.com/pawachaitanya/Loan-Granting/tree/master"
    },
    "datascience/challenges/marketing-email-campaign/README": {
      "id": "datascience/challenges/marketing-email-campaign/README",
      "title": "README",
      "description": "https://github.com/carlssonnp/Optimizing-Email-Marketing/tree/master"
    },
    "datascience/challenges/price-test/README": {
      "id": "datascience/challenges/price-test/README",
      "title": "README",
      "description": "https://www.kaggle.com/code/ffxiaozhi/data-science-take-home-challenges-pricing-test"
    },
    "datascience/challenges/README": {
      "id": "datascience/challenges/README",
      "title": "Challenges",
      "description": "1. User Referral Program"
    },
    "datascience/challenges/translate-ab-test/README": {
      "id": "datascience/challenges/translate-ab-test/README",
      "title": "README",
      "description": "https://github.com/commit-live-students/feature-engineering-for-translation-test-dataset/tree/master"
    },
    "datascience/challenges/Ultimate-data-science-take-home-challenge-sample/README": {
      "id": "datascience/challenges/Ultimate-data-science-take-home-challenge-sample/README",
      "title": "Ultimate Data Science Take Home Challenge Sample",
      "description": "A practice challenge for the DS interview process"
    },
    "datascience/challenges/user-referral-program": {
      "id": "datascience/challenges/user-referral-program",
      "title": "User Referral Program",
      "description": "Goal"
    },
    "datascience/challenges/video-sharing-analysis": {
      "id": "datascience/challenges/video-sharing-analysis",
      "title": "Video Sharing Analysis",
      "description": "Goal"
    },
    "datascience/computer-vision/lab-agri-setallite-image-segmentation/README": {
      "id": "datascience/computer-vision/lab-agri-setallite-image-segmentation/README",
      "title": "Agricultural Satellite Image Segmentation",
      "description": ""
    },
    "datascience/computer-vision/lab-image-analytics-tensorflow/README": {
      "id": "datascience/computer-vision/lab-image-analytics-tensorflow/README",
      "title": "Image Analytics with Tensorflow",
      "description": ""
    },
    "datascience/computer-vision/README": {
      "id": "datascience/computer-vision/README",
      "title": "Computer Vision",
      "description": "Categories"
    },
    "datascience/data-encoding": {
      "id": "datascience/data-encoding",
      "title": "Data Encoding",
      "description": "Discrete Data"
    },
    "datascience/data-preparation": {
      "id": "datascience/data-preparation",
      "title": "Data Preparation",
      "description": "Roughly, we can identify three primary components of data preprocessing the goal of data encoding is to make raw data both readable and “true to its natural characteristics”; the goal of feature extraction is to identify abstracted or more relevant features from within the data space; the goal of feature selection is to identify if and which features are not relevant to the predictive process and can be removed. Generally, data encoding takes precedence over the latter two components because data must be readable and representative of itself before we can attempt to extract features from it or select which features are relevant."
    },
    "datascience/data-splits": {
      "id": "datascience/data-splits",
      "title": "Data Splits",
      "description": "Training and Validation sets"
    },
    "datascience/deep-learning/deep-learning-basics": {
      "id": "datascience/deep-learning/deep-learning-basics",
      "title": "Deep Learning Basics",
      "description": "What is Deep Learning?"
    },
    "datascience/deep-learning/perceptron": {
      "id": "datascience/deep-learning/perceptron",
      "title": "The Rosenblatt Perceptron",
      "description": "The perceptron is an artificial neuron, that is, a model of a biological neuron."
    },
    "datascience/feature-selection": {
      "id": "datascience/feature-selection",
      "title": "Feature Selection",
      "description": "Feature selection refers to the process of filtering out or removing features from a dataset. There are two main reasons to perform feature selection: removing redundant (very similar information content) features and filtering out irrelevant (information content not valuable w.r.t. the target) features that may worsen model performance. The difference between feature extraction and feature selection lies in that selection reduces the number of features while extraction creates new features or modifies existing ones. A universal approach to feature selection usually consists of obtaining a measure of “usefulness” for each feature and then eliminating those that do not meet a threshold. Note that no matter which method for feature selection is used, the best result will likely come from trial and error since the optimal techniques and tools vary for datasets."
    },
    "datascience/metrics-and-evaluation": {
      "id": "datascience/metrics-and-evaluation",
      "title": "Metrics and Evaluation",
      "description": "In general terms, metrics are tools to set standards and evaluate the performance of models. In the field of machine learning and data science, obtaining a functional model is far from enough. We need to develop methods and assessment metrics to determine how well models perform on tasks that are given. Metrics used in machine learning are formulas and equations that provide specific quantitative measurements as to how well the developed methods perform on the data provided. Evaluation refers to the process of distinguishing between the better and the worse models by comparing metrics."
    },
    "datascience/nlp/lab-pdf-wordcloud-mail/README": {
      "id": "datascience/nlp/lab-pdf-wordcloud-mail/README",
      "title": "PDF to Wordcloud via Email",
      "description": "Receive a pdf via outlook mail and send back the wordcloud of that pdf in the reply"
    },
    "datascience/nlp/README": {
      "id": "datascience/nlp/README",
      "title": "Natural Language Processing (NLP)",
      "description": "Text Classification"
    },
    "datascience/nlp/text-analysis": {
      "id": "datascience/nlp/text-analysis",
      "title": "Text Analysis",
      "description": "Rapid text analysis can save lives. Let’s consider a real-world incident when US soldiers stormed a terrorist compound. In the compound, they discovered a computer containing terabytes of archived data. The data included documents, text messages, and emails pertaining to terrorist activities. The documents were too numerous to be read by any single human being. Fortunately, the soldiers were equipped with special software that could perform very fast text analysis. The software allowed the soldiers to process all of the text data without even having to leave the compound. The onsite analysis immediately revealed an active terrorist plot in a nearby neighborhood. The soldiers instantly responded to the plot and prevented a terrorist attack."
    },
    "datascience/nlp/text-cleaning": {
      "id": "datascience/nlp/text-cleaning",
      "title": "Text Cleaning",
      "description": ""
    },
    "datascience/nlp/text-processing": {
      "id": "datascience/nlp/text-processing",
      "title": "Text Processing",
      "description": "Tokenization"
    },
    "datascience/nlp/text-style-transfer": {
      "id": "datascience/nlp/text-style-transfer",
      "title": "Text Style Transfer",
      "description": "How to adapt the text to different situations, audiences and purposes by making some changes? The style of the text usually includes many aspects such as morphology, grammar, emotion, complexity, fluency, tense, tone and so on."
    },
    "datascience/nlp/word2vec": {
      "id": "datascience/nlp/word2vec",
      "title": "Lab 162 - Word2vec",
      "description": "Word2vec"
    },
    "datascience/recsys/amazon-personalize": {
      "id": "datascience/recsys/amazon-personalize",
      "title": "Amazon Personalize",
      "description": "Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications. With Amazon Personalize, you provide an activity stream from your application – clicks, page views, signups, purchases, and so forth – as well as an inventory of the items you want to recommend, such as articles, products, videos, or music. You can also choose to provide Amazon Personalize with additional demographic information from your users such as age, or geographic location. Amazon Personalize will process and examine the data, identify what is meaningful, select the right algorithms, and train and optimize a personalization model that is customized for your data. All data analyzed by Amazon Personalize is kept private and secure, and only used for your customized recommendations. You can start serving personalized recommendations via a simple API call. You pay only for what you use, and there are no minimum fees and no upfront commitments."
    },
    "datascience/recsys/lab-graph-models/README": {
      "id": "datascience/recsys/lab-graph-models/README",
      "title": "Graph RecSys Models",
      "description": "General"
    },
    "datascience/recsys/lab-movielens-nmf-pytorch/README": {
      "id": "datascience/recsys/lab-movielens-nmf-pytorch/README",
      "title": "Neural Matrix Factorization from scratch in PyTorch",
      "description": "Building a Neural Matrix Factorization model from scratch in PyTorch on MovieLens-1M dataset."
    },
    "datascience/recsys/lab-recsys-amazon-personalize/README": {
      "id": "datascience/recsys/lab-recsys-amazon-personalize/README",
      "title": "Recsys Personalize Getting Started",
      "description": "| Description | Notebook |"
    },
    "datascience/recsys/lab-recsys-evaluation/README": {
      "id": "datascience/recsys/lab-recsys-evaluation/README",
      "title": "Recommender System Evaluation",
      "description": "| Description | Notebook |"
    },
    "datascience/recsys/lab-recsys-matrix-factorizations/README": {
      "id": "datascience/recsys/lab-recsys-matrix-factorizations/README",
      "title": "RecSys Matrix Factorizations",
      "description": "Neural Matrix Factorization (NMF)"
    },
    "datascience/recsys/lab-recsys-tourism/README": {
      "id": "datascience/recsys/lab-recsys-tourism/README",
      "title": "Travel Recommendation System",
      "description": "TravelBuddy is a startup working on building am AI-based recommendation system that will recommend travel products (e.g. hotels, destination places, restaurants) to its users. You are hired as an AI developer that will handle the end-to-end development and maintenance of this AI system."
    },
    "datascience/recsys/lab-retail-product-recommend-word2vec/README": {
      "id": "datascience/recsys/lab-retail-product-recommend-word2vec/README",
      "title": "Retail Product Recommendations using word2vec",
      "description": "Creating a system that automatically recommends a certain number of products to the consumers on an E-commerce website based on the past purchase behavior of the consumers."
    },
    "datascience/recsys/lab-similar-shirt-image-recommender/README": {
      "id": "datascience/recsys/lab-similar-shirt-image-recommender/README",
      "title": "Similar Shirt Image Recommender",
      "description": "Recommender system using Deep Learning for an online e-commerce store"
    },
    "datascience/recsys/most-prominent-architectures": {
      "id": "datascience/recsys/most-prominent-architectures",
      "title": "Most Prominent Architectures",
      "description": "The Netflix three tier"
    },
    "datascience/recsys/movielens-mf-node2vec-graph-embeddings/README": {
      "id": "datascience/recsys/movielens-mf-node2vec-graph-embeddings/README",
      "title": "Recommender System with Node2vec Graph Embeddings",
      "description": "Building a movie recommender system that will learn user-item representation using graph embedding and comparing performance with other methods like matrix factorization"
    },
    "datascience/recsys/README": {
      "id": "datascience/recsys/README",
      "title": "Recommender Systems",
      "description": "Models"
    },
    "datascience/regression/README": {
      "id": "datascience/regression/README",
      "title": "Regression",
      "description": "Sklearn Regression Model"
    },
    "datascience/reinforcement-learning/bias-&-fairness": {
      "id": "datascience/reinforcement-learning/bias-&-fairness",
      "title": "Bias & Fairness",
      "description": "It can’t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will “learn” these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and it’s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutions—because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions."
    },
    "datascience/reinforcement-learning/causal-inference": {
      "id": "datascience/reinforcement-learning/causal-inference",
      "title": "Causal Inference",
      "description": "Typical recommender systems frame the recommendation task as either a distance learning problem between pairs of products, or between pairs of users and products, or as a next item prediction problem. However, a recommender system should not only attempt to model organic user behavior but influence it. This is where causal techniques help, potentially via simple modifications of standard matrix factorization methods."
    },
    "datascience/reinforcement-learning/README": {
      "id": "datascience/reinforcement-learning/README",
      "title": "Experimentation",
      "description": "Common Engineering Workflow"
    },
    "datascience/sagemaker/README": {
      "id": "datascience/sagemaker/README",
      "title": "Amazon Sagemaker",
      "description": "Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy machine learning models quickly by bringing together a broad set of purpose-built capabilities. In this demo, learn about how SageMaker can accelerate machine learning development by way of an example where we build the perfect musical playlist tailored to a user's tastes."
    },
    "datascience/timeseries/prophet": {
      "id": "datascience/timeseries/prophet",
      "title": "Prophet",
      "description": "In 2017, Facebook (now Meta) released its Prophet software as open source. This powerful tool was developed by Facebook engineers because its analysts were overwhelmed with the number of business forecasts demanded by managers. The developers of Prophet wanted to simultaneously solve two problems: 1 - completely automatic forecasting techniques are too brittle and inflexible to handle additional knowledge, and 2 - analysts who are consistently able to produce high-quality forecasts are rare and require extensive expertise. Prophet successfully solved both of these problems."
    },
    "datascience/timeseries/README": {
      "id": "datascience/timeseries/README",
      "title": "Time-Series",
      "description": ""
    },
    "devops/containers/lab-assignment-etl-docker/README": {
      "id": "devops/containers/lab-assignment-etl-docker/README",
      "title": "ETL Docker",
      "description": "Objective"
    },
    "devops/containers/lab-deploy-simple-docker-ecs/README": {
      "id": "devops/containers/lab-deploy-simple-docker-ecs/README",
      "title": "Deploy Docker in ECS",
      "description": "Create a Docker image"
    },
    "devops/containers/lab-kubernetes-kubia-app/README": {
      "id": "devops/containers/lab-kubernetes-kubia-app/README",
      "title": "Deploying a NodeJS app in Kubernetes",
      "description": "NodeJS App"
    },
    "devops/containers/README": {
      "id": "devops/containers/README",
      "title": "Containers",
      "description": "Docker"
    },
    "devops/fastapi/lab-fastapi-applications/README": {
      "id": "devops/fastapi/lab-fastapi-applications/README",
      "title": "README",
      "description": "Download by running command sh download.sh"
    },
    "devops/fastapi/project-railway-api/README": {
      "id": "devops/fastapi/project-railway-api/README",
      "title": "Railway API",
      "description": "Data Model"
    },
    "devops/fastapi/README": {
      "id": "devops/fastapi/README",
      "title": "FastAPI",
      "description": "Labs"
    },
    "devops/iac/README": {
      "id": "devops/iac/README",
      "title": "Infra as Code (IaC)",
      "description": "Amazon Cloudformation"
    },
    "devops/README": {
      "id": "devops/README",
      "title": "DevOps",
      "description": "Infra-as-Code (IaC)"
    },
    "extraction/api/lab-hackernews-git-api/README": {
      "id": "extraction/api/lab-hackernews-git-api/README",
      "title": "API HackerNews Github",
      "description": "GitHub API"
    },
    "extraction/api/lab-scoota/README": {
      "id": "extraction/api/lab-scoota/README",
      "title": "Extract data from multiple sources and load into database",
      "description": "You are working in a startup developing an e-scooter-sharing system called Scoota. It aspires to operate in the most populous cities all around the world. In each city, your company will have hundreds of e-scooters parked in the streets and allow users to rent them by the minute."
    },
    "extraction/api/README": {
      "id": "extraction/api/README",
      "title": "Data Extraction from APIs",
      "description": "What is an API?"
    },
    "extraction/faker/README": {
      "id": "extraction/faker/README",
      "title": "Faker",
      "description": "Labs"
    },
    "extraction/README": {
      "id": "extraction/README",
      "title": "Data Extraction",
      "description": "Airbyte and Fivetran"
    },
    "extraction/webscraping/lab-finance-extract-load/README": {
      "id": "extraction/webscraping/lab-finance-extract-load/README",
      "title": "Financial Data Extraction and Storage",
      "description": "For this lab, you will assume the role of data engineer working for an international financial analysis company. Your company tracks stock prices, commodities, forex rates, inflation rates.  Your job is to extract financial data from various sources like websites, APIs and files provided by various financial analysis firms. After you collect the data, you extract the data of interest to your company and transform it based on the requirements given to you. Once the transformation is complete you load that data into a database."
    },
    "extraction/webscraping/README": {
      "id": "extraction/webscraping/README",
      "title": "Web Scraping",
      "description": "Labs"
    },
    "foundations/basics/batch-data-processing": {
      "id": "foundations/basics/batch-data-processing",
      "title": "Batch Data Processing",
      "description": "Data processing involves taking source data which has been ingested into your data platform and cleansing it, combining it, and modeling it for downstream use. Historically the most popular way to transform data has been with the SQL language and data engineers have built data transformation pipelines using SQL often with the help of ETL/ELT tools. But recently many folks have also begun adopting the DataFrame API in languages like Python/Spark for this task. For the most part a data engineer can accomplish the same data transformations with either approach, and deciding between the two is mostly a matter of preference and particular use cases. That being said, there are use cases where a particular data transform can't be expressed in SQL and a different approach is needed. The most popular approach for these use cases is Python/Spark along with a DataFrame API.",
      "sidebar": "docs"
    },
    "foundations/basics/batch-vs-incremental": {
      "id": "foundations/basics/batch-vs-incremental",
      "title": "Batch vs Incremental Data Processing",
      "description": "The idea behind incremental processing is quite simple. Incremental processing extends the semantics of processing streaming data to batch processing pipelines by processing only new data each run and then incrementally updating the new results. This unlocks great cost savings due to much shorter batch pipelines as well as data freshness speedups due to being able to run them much more frequently as well.",
      "sidebar": "docs"
    },
    "foundations/basics/big-data": {
      "id": "foundations/basics/big-data",
      "title": "Big Data",
      "description": "Six Vs of big data",
      "sidebar": "docs"
    },
    "foundations/basics/data-contract": {
      "id": "foundations/basics/data-contract",
      "title": "Data Contract",
      "description": "Example",
      "sidebar": "docs"
    },
    "foundations/basics/data-governance": {
      "id": "foundations/basics/data-governance",
      "title": "Data Governance",
      "description": "What is Data Governance?",
      "sidebar": "docs"
    },
    "foundations/basics/data-management": {
      "id": "foundations/basics/data-management",
      "title": "Data Management",
      "description": "Importance of data management",
      "sidebar": "docs"
    },
    "foundations/basics/data-pipelines": {
      "id": "foundations/basics/data-pipelines",
      "title": "Data Pipelines",
      "description": "img",
      "sidebar": "docs"
    },
    "foundations/basics/data-quality": {
      "id": "foundations/basics/data-quality",
      "title": "Data Quality",
      "description": "Do your product dashboards look funky? Are your quarterly reports stale? Is the data set you're using broken or just plain wrong? Have you ever been about to sign off after a long day running queries or building data pipelines only to get pinged by your head of marketing that “the data is missing” from a critical report? What about a frantic email from your CTO about “duplicate data” in a business intelligence dashboard? Or a memo from your CEO, the same one who is so bullish on data, about a confusing or inaccurate number in his latest board deck? If any of these situations hit home for you, you’re not alone. These problems affect almost every team, yet they're usually addressed on an ad hoc basis and in a reactive manner.",
      "sidebar": "docs"
    },
    "foundations/basics/data-storages": {
      "id": "foundations/basics/data-storages",
      "title": "Data Storages",
      "description": "| Architecture          | Total cost of solution                                                                                                                                                  | Flexibility of scenarios                                                                                                                                                                      | Complexity of development                                                                                                                                                     | Maturity of ecosystem                                                                                                                                                                         | Organizational maturity required                                                                                                                                                            |",
      "sidebar": "docs"
    },
    "foundations/basics/de-basics": {
      "id": "foundations/basics/de-basics",
      "title": "Data Engineering Basics",
      "description": "What is Data Engineering?",
      "sidebar": "docs"
    },
    "foundations/basics/deployment": {
      "id": "foundations/basics/deployment",
      "title": "Model Deployment",
      "description": "Prototype vs. Production"
    },
    "foundations/basics/extras": {
      "id": "foundations/basics/extras",
      "title": "Extras",
      "description": "Unified Approach"
    },
    "foundations/basics/hadoop-basics": {
      "id": "foundations/basics/hadoop-basics",
      "title": "Hadoop Basics",
      "description": "Apache Hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. Instead of using one large computer to store and process the data, Hadoop allows clustering multiple computers to analyze massive datasets in parallel more quickly.",
      "sidebar": "docs"
    },
    "foundations/basics/hadoop-vs-spark": {
      "id": "foundations/basics/hadoop-vs-spark",
      "title": "Hadoop vs Spark",
      "description": "",
      "sidebar": "docs"
    },
    "foundations/basics/map-reduce": {
      "id": "foundations/basics/map-reduce",
      "title": "Map Reduce",
      "description": "Knowing that answering the how question is what is important to understanding big data, the first question we need to answer is how does it actually store the data? What makes it different from non-big data storage?",
      "sidebar": "docs"
    },
    "foundations/basics/model-optimization": {
      "id": "foundations/basics/model-optimization",
      "title": "Model Optimization",
      "description": "Keras Model Pruning"
    },
    "foundations/basics/most-common-interview-questions-set1": {
      "id": "foundations/basics/most-common-interview-questions-set1",
      "title": "25 Most Common Interview Questions",
      "description": "25 Most Common Interview Questions",
      "sidebar": "docs"
    },
    "foundations/basics/most-common-interview-questions-set2": {
      "id": "foundations/basics/most-common-interview-questions-set2",
      "title": "50 Most Common Interview Questions",
      "description": "50 Most Common Interview Questions",
      "sidebar": "docs"
    },
    "foundations/basics/most-common-interview-questions-set3": {
      "id": "foundations/basics/most-common-interview-questions-set3",
      "title": "50 Most Asked Data Engineer Interview Questions and Answers in 2023",
      "description": "50 Most Asked Data Engineer Interview Questions and Answers in 2023",
      "sidebar": "docs"
    },
    "foundations/basics/oltp-vs-olap": {
      "id": "foundations/basics/oltp-vs-olap",
      "title": "OLTP vs OLAP",
      "description": "Transactional databases (OLTP)",
      "sidebar": "docs"
    },
    "foundations/basics/origin": {
      "id": "foundations/basics/origin",
      "title": "The data science origin story",
      "description": "There's a saying in the data science community that's been around for a while, and it goes: \"A data scientist is better than any computer scientist at statistics, and better than any statistician at computer programming.\" This encapsulates the general skills of most data scientists, as well as the history of the field."
    },
    "foundations/basics/outliers-handling": {
      "id": "foundations/basics/outliers-handling",
      "title": "Outliers Handling",
      "description": "Remove categorical outliers"
    },
    "foundations/basics/rdd": {
      "id": "foundations/basics/rdd",
      "title": "Spark RDDs",
      "description": "An RDD (Resilient Distributed Dataset) is the primary data structure in Spark. It is a distributed collection of data that can be processed in parallel. RDDs are immutable, meaning that once an RDD is created, it cannot be modified. Instead, any transformations applied to an RDD will return a new RDD.",
      "sidebar": "docs"
    },
    "foundations/basics/spark-basics": {
      "id": "foundations/basics/spark-basics",
      "title": "Apache Spark Basics",
      "description": "What is Apache Spark?",
      "sidebar": "docs"
    },
    "foundations/basics/spark-dag": {
      "id": "foundations/basics/spark-dag",
      "title": "Interpreting a Spark DAG",
      "description": "A DAG is just a regular graph with nodes and edges but with no cycles or loops. In order to understand a Spark DAG, we first have to understand where a DAG comes into the picture during the execution of a Spark job.",
      "sidebar": "docs"
    },
    "foundations/basics/spark-origin": {
      "id": "foundations/basics/spark-origin",
      "title": "Origin of Spark",
      "description": "Big Data and Distributed Computing at Google",
      "sidebar": "docs"
    },
    "foundations/basics/spark-quiz": {
      "id": "foundations/basics/spark-quiz",
      "title": "Quiz: Spark basics",
      "description": "Below are a few questions that should come handy in the first go :",
      "sidebar": "docs"
    },
    "foundations/basics/sql-vs-nosql": {
      "id": "foundations/basics/sql-vs-nosql",
      "title": "SQL vs NoSQL",
      "description": "As you design large systems ( or even smaller ones), you need to decide the inflow-processing and outflow of data coming- and getting processed in the system.",
      "sidebar": "docs"
    },
    "foundations/basics/stream-data-processing": {
      "id": "foundations/basics/stream-data-processing",
      "title": "Stream and Unified Data Processing",
      "description": "What is an event stream?",
      "sidebar": "docs"
    },
    "foundations/basics/use-cases": {
      "id": "foundations/basics/use-cases",
      "title": "Use Cases",
      "description": "Some of the many uses of machine learning across industries:"
    },
    "foundations/basics/vector-search": {
      "id": "foundations/basics/vector-search",
      "title": "Vector Search",
      "description": "Faiss"
    },
    "foundations/cloud-essentials/aws-commands": {
      "id": "foundations/cloud-essentials/aws-commands",
      "title": "AWS Commands",
      "description": ""
    },
    "foundations/cloud-essentials/aws-containers": {
      "id": "foundations/cloud-essentials/aws-containers",
      "title": "AWS Container Services",
      "description": "1. Containers on AWS Overview: ECS | EKS | Fargate | ECR"
    },
    "foundations/cloud-essentials/azure-basics": {
      "id": "foundations/cloud-essentials/azure-basics",
      "title": "Azure Basics",
      "description": "Learning Path"
    },
    "foundations/cloud-essentials/azure-batch-processing": {
      "id": "foundations/cloud-essentials/azure-batch-processing",
      "title": "Azure Batch Processing",
      "description": "Here is a useful table reproduced from Azure that can help you decide on the technologies to use for your batch scenarios:"
    },
    "foundations/cloud-essentials/azure-data-ingestion": {
      "id": "foundations/cloud-essentials/azure-data-ingestion",
      "title": "Azure Data Ingestion",
      "description": "This is the process of getting all the raw data into the data lake. Data from various sources lands in the raw zone of the data lake. Based on where the data is coming from, such as on-premise systems, other cloud systems, and so on, we could use different ingestion tools. Let's look at some of the options available in Azure:"
    },
    "foundations/cloud-essentials/azure-fullstack-solutions": {
      "id": "foundations/cloud-essentials/azure-fullstack-solutions",
      "title": "Azure Full-stack Solutions",
      "description": "Modern Azure Data Architecture Platform"
    },
    "foundations/cloud-essentials/cloud-basics": {
      "id": "foundations/cloud-essentials/cloud-basics",
      "title": "Cloud Computing Basics",
      "description": "What is Cloud Computing?"
    },
    "foundations/cloud-essentials/cloud-comparison": {
      "id": "foundations/cloud-essentials/cloud-comparison",
      "title": "Comparison of Cloud Services",
      "description": "Service"
    },
    "foundations/cloud-essentials/dms": {
      "id": "foundations/cloud-essentials/dms",
      "title": "DMS",
      "description": "AWS Database Migration Service (AWS DMS) helps you migrate databases to AWS quickly and securely. The source database remains fully operational during the migration, minimizing downtime to applications that rely on the database. The AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases."
    },
    "foundations/cloud-essentials/ec2": {
      "id": "foundations/cloud-essentials/ec2",
      "title": "EC2",
      "description": "The foundational service that provides compute resources for customers to build their applications on AWS is called Amazon EC2. Amazon EC2 provides customers with a choice of 500+ instance types. Customers can then tailor the right combination of instance types for their business applications."
    },
    "foundations/cloud-essentials/gcp-basics": {
      "id": "foundations/cloud-essentials/gcp-basics",
      "title": "GCP Basics",
      "description": "There are a lot of services in GCP. The services are not only limited to data and analytics. They also cover other areas such as application development, machine learning, networks, source repositories, and many more. As a data engineer working on GCP, you will face situations when you need to decide which services you need to use for your organization."
    },
    "foundations/cloud-essentials/gcp-setup": {
      "id": "foundations/cloud-essentials/gcp-setup",
      "title": "GCP Setup",
      "description": "To set up GCP, please follow the steps below:"
    },
    "foundations/cloud-essentials/glue": {
      "id": "foundations/cloud-essentials/glue",
      "title": "Glue",
      "description": "AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application development. AWS Glue provides all the capabilities needed for data integration so that you can start analyzing your data and put it to use in minutes instead of months."
    },
    "foundations/cloud-essentials/iam": {
      "id": "foundations/cloud-essentials/iam",
      "title": "IAM",
      "description": "Securely manage identities and access to AWS services and resources"
    },
    "foundations/cloud-essentials/lab-aws-secrets-manager/README": {
      "id": "foundations/cloud-essentials/lab-aws-secrets-manager/README",
      "title": "AWS Secrets Manager",
      "description": "Objective"
    },
    "foundations/cloud-essentials/lab-aws-setup/README": {
      "id": "foundations/cloud-essentials/lab-aws-setup/README",
      "title": "README",
      "description": "Create AWS Account"
    },
    "foundations/cloud-essentials/lab-create-iam-policy-role/README": {
      "id": "foundations/cloud-essentials/lab-create-iam-policy-role/README",
      "title": "Create IAM Role and Policy",
      "description": "In this lab, we will learn about AWS IAM service. We will perform the following activities:"
    },
    "foundations/cloud-essentials/lab-create-your-first-ec2-instance-linux/README": {
      "id": "foundations/cloud-essentials/lab-create-your-first-ec2-instance-linux/README",
      "title": "Create Your First Amazon EC2 Instance (Linux)",
      "description": "Description"
    },
    "foundations/cloud-essentials/lab-create-your-first-vpc/README": {
      "id": "foundations/cloud-essentials/lab-create-your-first-vpc/README",
      "title": "Introduction to Virtual Private Cloud (VPC)",
      "description": "Description"
    },
    "foundations/cloud-essentials/rds": {
      "id": "foundations/cloud-essentials/rds",
      "title": "RDS",
      "description": "RDS stands for Relational Database Services"
    },
    "foundations/cloud-essentials/s3": {
      "id": "foundations/cloud-essentials/s3",
      "title": "S3",
      "description": "Amazon S3 is one of the most commonly used cloud data storage services for web applications, and high-performance compute use cases. It is Amazon's object storage service providing virtually unlimited data storage. Some of the advantages of using Amazon S3 include very high scalability, durability, data availability, security, and performance. Amazon S3 can be used for a variety of cloud-native applications, ranging from simple data storage to very large data lakes to web hosting and high-performance applications, such as training very advanced and compute-intensive ML models. Amazon S3 offers several classes of storage options with differences in terms of data access, resiliency, archival needs, and cost. We can choose the storage class that best suits our use case and business needs. There is also an option for cost saving when the access pattern is unknown or changes over time (S3 Intelligent-Tiering)."
    },
    "foundations/cloud-essentials/secrets-manager": {
      "id": "foundations/cloud-essentials/secrets-manager",
      "title": "Secrets Manager",
      "description": "AWS Secrets Management Service"
    },
    "foundations/developer/install-anaconda": {
      "id": "foundations/developer/install-anaconda",
      "title": "Install Anaconda",
      "description": "1. Download the file from https://www.anaconda.com/products/distribution and install it",
      "sidebar": "docs"
    },
    "foundations/developer/install-dbeaver": {
      "id": "foundations/developer/install-dbeaver",
      "title": "Install DBeaver",
      "description": "Go to https://dbeaver.io/download to download DBeaver.",
      "sidebar": "docs"
    },
    "foundations/developer/install-jupyter": {
      "id": "foundations/developer/install-jupyter",
      "title": "Install Jupyter Notebook",
      "description": "There are many free options to run jupyter notebooks:",
      "sidebar": "docs"
    },
    "foundations/developer/install-vscode": {
      "id": "foundations/developer/install-vscode",
      "title": "Install VS Code",
      "description": "1. Follow this guide to install VS code in your system. Alternatively, go to https://code.visualstudio.com/download and download the VS Code",
      "sidebar": "docs"
    },
    "foundations/developer/lab-bash-commands/index": {
      "id": "foundations/developer/lab-bash-commands/index",
      "title": "Lab: Learn Bash Commands",
      "description": "- pwd: Display the current directory",
      "sidebar": "docs"
    },
    "foundations/developer/lab-explore-vscode-features": {
      "id": "foundations/developer/lab-explore-vscode-features",
      "title": "Lab: Explore VS Code features",
      "description": "In this lab, we will explore the following features:",
      "sidebar": "docs"
    },
    "foundations/developer/lab-learn-git-commands": {
      "id": "foundations/developer/lab-learn-git-commands",
      "title": "Lab: Learn git commands",
      "description": "Objective",
      "sidebar": "docs"
    },
    "foundations/developer/setup-git": {
      "id": "foundations/developer/setup-git",
      "title": "Setup Git",
      "description": "One of the major problems with coding is to keep track of changes. It is also almost impossible to maintain a program you have multiple versions of. Another is the topic of collaboration and documentation. Which is super Important. Let’s say you work on a Spark application and your colleges need to make changes while you are on holiday. Without some code management they are in huge trouble: Where is the code? What have you changed last? Where is the documentation? How do we mark what we have changed? But if you put your code on GitHub your colleges can find your code. They can understand it through your documentation (please also have in-line comments) Developers can pull your code, make a new branch and do the changes. After your holiday you can inspect what they have done and merge it with your original code. and you end up having only one application.",
      "sidebar": "docs"
    },
    "foundations/programming-languages/pyspark/broadcasting": {
      "id": "foundations/programming-languages/pyspark/broadcasting",
      "title": "Broadcasting",
      "description": "Broadcasting is the process of sending a read-only variable to the worker nodes, rather than sending a copy of the variable to each worker node. This can greatly improve the performance of Spark jobs by reducing the amount of data that needs to be sent over the network."
    },
    "foundations/programming-languages/pyspark/caching": {
      "id": "foundations/programming-languages/pyspark/caching",
      "title": "Caching",
      "description": "What is Caching?"
    },
    "foundations/programming-languages/pyspark/cheat-sheet": {
      "id": "foundations/programming-languages/pyspark/cheat-sheet",
      "title": "cheat-sheet",
      "description": "PySpark Cheat Sheet"
    },
    "foundations/programming-languages/pyspark/dataframe": {
      "id": "foundations/programming-languages/pyspark/dataframe",
      "title": "PySpark DataFrame",
      "description": "Creating a DataFrame in PySpark"
    },
    "foundations/programming-languages/pyspark/execution-plan": {
      "id": "foundations/programming-languages/pyspark/execution-plan",
      "title": "Spark Execution Plan",
      "description": "Spark uses a query optimizer known as Catalyst to optimize the execution plan of Spark jobs. The execution plan is a representation of the physical execution of a query and it can be used to understand how Spark is processing data."
    },
    "foundations/programming-languages/pyspark/install": {
      "id": "foundations/programming-languages/pyspark/install",
      "title": "Installing Spark",
      "description": "To get started with PySpark, you will need to have the Spark software installed on your machine. You can download the latest version of Spark from the Apache Spark website. Once you have Spark installed, you can start using it to process data."
    },
    "foundations/programming-languages/pyspark/lab-bcg/README": {
      "id": "foundations/programming-languages/pyspark/lab-bcg/README",
      "title": "BCG Case Study",
      "description": "Dataset"
    },
    "foundations/programming-languages/pyspark/lab-pyspark-basics/README": {
      "id": "foundations/programming-languages/pyspark/lab-pyspark-basics/README",
      "title": "Pyspark Basics",
      "description": "In this lab, we will use the power of PySpark to perform various activities in databricks environment."
    },
    "foundations/programming-languages/pyspark/lab-pyspark-nyctaxi/README": {
      "id": "foundations/programming-languages/pyspark/lab-pyspark-nyctaxi/README",
      "title": "Pyspark NYC Taxi",
      "description": ""
    },
    "foundations/programming-languages/pyspark/lab-spark-optimizations-2/README": {
      "id": "foundations/programming-languages/pyspark/lab-spark-optimizations-2/README",
      "title": "Spark Optimizations",
      "description": "Performance tuning in Apache Spark plays an instrumental role in running efficient big data workloads. More often than not, the optimization techniques employed to prevent the shuffling and skewing of data drastically improve performance. In this lab, we will learn about the Spark optimization techniques directly related to Spark Core that help prevent the shuffling and skewing of data."
    },
    "foundations/programming-languages/pyspark/lab-spark-optimizations/README": {
      "id": "foundations/programming-languages/pyspark/lab-spark-optimizations/README",
      "title": "Spark Optimizations for Analytics Workloads",
      "description": "Optimizations in Apache Spark play a crucial role while building big data solutions. Knowledge and experience in tuning Spark-based workloads help organizations save costs and time while running these workloads on the cloud. In this lab, we will learn about various optimization techniques concerning Spark DataFrames and big data analytics in general. We will learn about the limitations of the collect() method and inferSchema when reading data. This will be followed by an overview of the best practices for working with CSV files, Parquet files, Pandas projects, and Koalas projects. Also, we will learn about some powerful optimization techniques, such as column predicate pushdown, column pruning, and partitioning strategies."
    },
    "foundations/programming-languages/pyspark/lab-sql-to-pyspark/README": {
      "id": "foundations/programming-languages/pyspark/lab-sql-to-pyspark/README",
      "title": "SQL to PySpark Code Conversion",
      "description": "Output:"
    },
    "foundations/programming-languages/pyspark/lab-uber-analysis/README": {
      "id": "foundations/programming-languages/pyspark/lab-uber-analysis/README",
      "title": "Uber Data Analysis in Pyspark",
      "description": "This lab can be used as a take-home assignment to learn Pyspark and Data Engineering."
    },
    "foundations/programming-languages/pyspark/lab-understand-spark-query-execution/README": {
      "id": "foundations/programming-languages/pyspark/lab-understand-spark-query-execution/README",
      "title": "Understanding Spark Query Execution",
      "description": "To write efficient Spark applications, we need to have some understanding of how Spark executes queries. Having a good understanding of how Spark executes a given query helps big data developers/engineers work efficiently with large volumes of data."
    },
    "foundations/programming-languages/pyspark/lab-window-functions/README": {
      "id": "foundations/programming-languages/pyspark/lab-window-functions/README",
      "title": "Window Functions in Spark",
      "description": ""
    },
    "foundations/programming-languages/pyspark/lazy-processing": {
      "id": "foundations/programming-languages/pyspark/lazy-processing",
      "title": "Lazy Processing",
      "description": "PySpark uses a concept called lazy processing, which means that operations on DataFrames and RDDs are not executed immediately, but rather are recorded in a lineage. The actual execution of the operations is delayed until an action is called. This allows Spark to optimize the execution plan by analyzing the entire lineage of operations, rather than executing each operation individually."
    },
    "foundations/programming-languages/pyspark/methods-operations": {
      "id": "foundations/programming-languages/pyspark/methods-operations",
      "title": "Methods, Operations and Functions",
      "description": "PySpark provides a variety of methods to work with data, some of the most commonly used are:"
    },
    "foundations/programming-languages/pyspark/partitioning": {
      "id": "foundations/programming-languages/pyspark/partitioning",
      "title": "Partitioning",
      "description": "A partition in spark is a logical chunk of data mapped to a single node in a cluster. Partitions are basic units of parallelism. Each partition is processed by a single task slot. In a multicore system, total slots for tasks will be num of executors x number of cores. Hence the number of partitions decides the task parallelism."
    },
    "foundations/programming-languages/pyspark/pyspark-vs-pandas": {
      "id": "foundations/programming-languages/pyspark/pyspark-vs-pandas",
      "title": "PySpark vs Pandas",
      "description": "Spark DataFrames were inspired by pandas, which also provides an abstraction on top of the data called a DataFrame. pandas is a widely adopted library for data manipulation and analytics. Many developers use it to extrapolate data using Python."
    },
    "foundations/programming-languages/pyspark/udf": {
      "id": "foundations/programming-languages/pyspark/udf",
      "title": "What are UDFs in PySpark",
      "description": "In PySpark, UDF stands for User-Defined Function, which is a feature that allows users to define their own functions and apply them to Spark data frames or RDDs."
    },
    "foundations/programming-languages/python/introduction-to-python": {
      "id": "foundations/programming-languages/python/introduction-to-python",
      "title": "Introduction to Python",
      "description": "A strong understanding of Python syntax, data types, operators, and control structures is essential. Data engineering using Python only gets better, and here is a list of points if you are beginning to think otherwise."
    },
    "foundations/programming-languages/python/links": {
      "id": "foundations/programming-languages/python/links",
      "title": "Explore Further",
      "description": "1. https://scrimba.com/learn/python"
    },
    "foundations/programming-languages/python/packaging/docs/README": {
      "id": "foundations/programming-languages/python/packaging/docs/README",
      "title": "Home",
      "description": "Standards - AI/ML boilerplates and snippets"
    },
    "foundations/programming-languages/python/python-snippets": {
      "id": "foundations/programming-languages/python/python-snippets",
      "title": "Python code",
      "description": "Generating a hash for given string"
    },
    "foundations/programming-languages/python/python-syntax-sugars": {
      "id": "foundations/programming-languages/python/python-syntax-sugars",
      "title": "Sweet Python Syntax Sugar for Improving Your Coding Experience by Yang Zhou",
      "description": "Github Google Colab"
    },
    "foundations/programming-languages/python/README": {
      "id": "foundations/programming-languages/python/README",
      "title": "Python",
      "description": "Concepts"
    },
    "foundations/programming-languages/scala/lab-scala-getting-started/README": {
      "id": "foundations/programming-languages/scala/lab-scala-getting-started/README",
      "title": "Getting Started with Scala",
      "description": "Why Scala?"
    },
    "foundations/programming-languages/scala/README": {
      "id": "foundations/programming-languages/scala/README",
      "title": "Scala",
      "description": "- Expressive"
    },
    "foundations/programming-languages/sql/aggregate-functions": {
      "id": "foundations/programming-languages/sql/aggregate-functions",
      "title": "SQL Aggregate Functions",
      "description": "| Aggregate function                                                  | Description                                                            |"
    },
    "foundations/programming-languages/sql/and": {
      "id": "foundations/programming-languages/sql/and",
      "title": "SQL AND",
      "description": "AND is a logical operator in SQL that allows you to select only rows that satisfy two conditions."
    },
    "foundations/programming-languages/sql/between": {
      "id": "foundations/programming-languages/sql/between",
      "title": "SQL BETWEEN",
      "description": "BETWEEN is a logical operator in SQL that allows you to select only rows that are within a specific range."
    },
    "foundations/programming-languages/sql/case": {
      "id": "foundations/programming-languages/sql/case",
      "title": "SQL CASE",
      "description": "The CASE statement is SQL's way of handling if/then logic. The CASE statement is followed by at least one pair of WHEN and THEN statements—SQL's equivalent of IF/THEN in Excel. Because of this pairing, you might be tempted to call this SQL CASE WHEN, but CASE is the accepted term."
    },
    "foundations/programming-languages/sql/challenges/assignment1/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment1/README",
      "title": "Lab - SQL Assignment",
      "description": "Activity"
    },
    "foundations/programming-languages/sql/challenges/assignment1/solution": {
      "id": "foundations/programming-languages/sql/challenges/assignment1/solution",
      "title": "solution",
      "description": "Solution"
    },
    "foundations/programming-languages/sql/challenges/assignment2/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment2/README",
      "title": "SQL Assignment",
      "description": "Learning goals"
    },
    "foundations/programming-languages/sql/challenges/assignment3/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment3/README",
      "title": "SQL Assignment",
      "description": "Part 1: Deforestation Exploration"
    },
    "foundations/programming-languages/sql/challenges/assignment4/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment4/README",
      "title": "SQL Assignment",
      "description": ""
    },
    "foundations/programming-languages/sql/challenges/assignment5/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment5/README",
      "title": "SQL Danny's Diner Assignment",
      "description": "Danny seriously loves Japanese food so in the beginning of 2021, he decides to embark upon a risky venture and opens up a cute little restaurant that sells his 3 favourite foods: sushi, curry and ramen."
    },
    "foundations/programming-languages/sql/challenges/assignment6/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment6/README",
      "title": "SQL Pizza Runner Assignment",
      "description": "Did you know that over 115 million kilograms of pizza is consumed daily worldwide??? (Well according to Wikipedia anyway…)"
    },
    "foundations/programming-languages/sql/challenges/assignment7/README": {
      "id": "foundations/programming-languages/sql/challenges/assignment7/README",
      "title": "Patients SQL Assignment",
      "description": "Queries"
    },
    "foundations/programming-languages/sql/challenges/braintree/README": {
      "id": "foundations/programming-languages/sql/challenges/braintree/README",
      "title": "README",
      "description": "Welcome to the Braintree Analytics Code Challenge!"
    },
    "foundations/programming-languages/sql/challenges/employee": {
      "id": "foundations/programming-languages/sql/challenges/employee",
      "title": "Challenge - Employee Analytics",
      "description": "Small dataset | 6 Questions"
    },
    "foundations/programming-languages/sql/challenges/yammer/README": {
      "id": "foundations/programming-languages/sql/challenges/yammer/README",
      "title": "Yammer Analytics",
      "description": "Yammer is a social network for communicating with coworkers. Individuals share documents, updates, and ideas by posting them in groups. Yammer is free to use indefinitely, but companies must pay license fees if they want access to administrative controls, including integration with user management systems like ActiveDirectory."
    },
    "foundations/programming-languages/sql/comments": {
      "id": "foundations/programming-languages/sql/comments",
      "title": "Comments",
      "description": "You can \"comment out\" pieces of code by adding combinations of characters. In other words, you can specify parts of your query that will not actually be treated like SQL code. It can be helpful to include comments that explain your thinking so that you can easily remember what you intended to do if you ever want to revisit your work. Commenting can also be useful if you want to test variations on your query while keeping all of your code intact."
    },
    "foundations/programming-languages/sql/comparison-operators": {
      "id": "foundations/programming-languages/sql/comparison-operators",
      "title": "SQL Comparison Operators",
      "description": "Comparison operators on numerical data"
    },
    "foundations/programming-languages/sql/cte": {
      "id": "foundations/programming-languages/sql/cte",
      "title": "cte",
      "description": "CTE (Common Table Expressions)"
    },
    "foundations/programming-languages/sql/cursor": {
      "id": "foundations/programming-languages/sql/cursor",
      "title": "Cursor",
      "description": "To handle a result set inside a stored procedure, you use a cursor. A cursor allows you to iterate a set of rows returned by a query and process each row individually."
    },
    "foundations/programming-languages/sql/date-functions": {
      "id": "foundations/programming-languages/sql/date-functions",
      "title": "SQL Date Functions",
      "description": "Assuming you've got some dates properly stored as a date or time data type, you can do some pretty powerful things. Maybe you'd like to calculate a field of dates a week after an existing field. Or maybe you'd like to create a field that indicates how many days apart the values in two other date fields are. These are trivially simple, but it's important to keep in mind that the data type of your results will depend on exactly what you are doing to the dates."
    },
    "foundations/programming-languages/sql/distinct": {
      "id": "foundations/programming-languages/sql/distinct",
      "title": "SQL DISTINCT",
      "description": "You'll occasionally want to look at only the unique values in a particular column. You can do this using SELECT DISTINCT syntax."
    },
    "foundations/programming-languages/sql/groupby": {
      "id": "foundations/programming-languages/sql/groupby",
      "title": "SQL GROUP BY",
      "description": "SQL aggregate function like COUNT, AVG, and SUM have something in common: they all aggregate across the entire table. But what if you want to aggregate only part of a table? For example, you might want to count the number of entries for each year."
    },
    "foundations/programming-languages/sql/having": {
      "id": "foundations/programming-languages/sql/having",
      "title": "SQL HAVING",
      "description": "You'll often encounter datasets where GROUP BY isn't enough to get what you're looking for. Let's say that it's not enough just to know aggregated stats by month. After all, there are a lot of months in this dataset. Instead, you might want to find every month during which AAPL stock worked its way over $400/share. The WHERE clause won't work for this because it doesn't allow you to filter on aggregate columns—that's where the HAVING clause comes in."
    },
    "foundations/programming-languages/sql/in": {
      "id": "foundations/programming-languages/sql/in",
      "title": "SQL IN",
      "description": "IN is a logical operator in SQL that allows you to specify a list of values that you'd like to include in the results."
    },
    "foundations/programming-languages/sql/indexes": {
      "id": "foundations/programming-languages/sql/indexes",
      "title": "Indexes",
      "description": "An index is created by a table or view to define a field that can be used to optimize queries."
    },
    "foundations/programming-languages/sql/isnull": {
      "id": "foundations/programming-languages/sql/isnull",
      "title": "SQL IS NULL",
      "description": "IS NULL is a logical operator in SQL that allows you to exclude rows with missing data from your results."
    },
    "foundations/programming-languages/sql/joins": {
      "id": "foundations/programming-languages/sql/joins",
      "title": "SQL Joins",
      "description": "It might be helpful to refer to this JOIN visualization by Patrik Spathon."
    },
    "foundations/programming-languages/sql/lab-mysql-data-ingestion/README": {
      "id": "foundations/programming-languages/sql/lab-mysql-data-ingestion/README",
      "title": "Lab: Data Ingestion to MySQL",
      "description": "Process Flow"
    },
    "foundations/programming-languages/sql/lab-postgres-sales/README": {
      "id": "foundations/programming-languages/sql/lab-postgres-sales/README",
      "title": "Postgres Sales",
      "description": "Running Dates, String and Advanced queries in Postgres on Sales data"
    },
    "foundations/programming-languages/sql/lab-sqlite-basics/README": {
      "id": "foundations/programming-languages/sql/lab-sqlite-basics/README",
      "title": "SQLite Basics",
      "description": "The following entity-relationship- (ER) diagram for the books database shows the database’s tables and the relationships among them:"
    },
    "foundations/programming-languages/sql/like": {
      "id": "foundations/programming-languages/sql/like",
      "title": "SQL LIKE",
      "description": "LIKE is a logical operator in SQL that allows you to match on similar values rather than exact ones."
    },
    "foundations/programming-languages/sql/limit": {
      "id": "foundations/programming-languages/sql/limit",
      "title": "SQL LIMIT",
      "description": "As you might expect, the limit restricts how many rows the SQL query returns."
    },
    "foundations/programming-languages/sql/links": {
      "id": "foundations/programming-languages/sql/links",
      "title": "Explore Further",
      "description": "1. SQL Cheat Sheet"
    },
    "foundations/programming-languages/sql/logical-operators": {
      "id": "foundations/programming-languages/sql/logical-operators",
      "title": "SQL Logical Operators",
      "description": "You'll likely also want to filter data using several conditions---possibly more often than you'll want to filter by only one condition. Logical operators allow you to use multiple comparison operators in one query."
    },
    "foundations/programming-languages/sql/not": {
      "id": "foundations/programming-languages/sql/not",
      "title": "SQL NOT",
      "description": "NOT is a logical operator in SQL that you can put before any conditional statement to select rows for which that statement is false."
    },
    "foundations/programming-languages/sql/or": {
      "id": "foundations/programming-languages/sql/or",
      "title": "SQL OR",
      "description": "OR is a logical operator in SQL that allows you to select rows that satisfy either of two conditions. It works the same way as AND, which selects the rows that satisfy both of two conditions."
    },
    "foundations/programming-languages/sql/orderby": {
      "id": "foundations/programming-languages/sql/orderby",
      "title": "SQL ORDER BY",
      "description": "The ORDER BY clause allows you to reorder your results based on the data in one or more columns."
    },
    "foundations/programming-languages/sql/performance-tuning": {
      "id": "foundations/programming-languages/sql/performance-tuning",
      "title": "Performance Tuning",
      "description": "SQL tuning is the process of improving SQL queries to accelerate your servers performance. It's general purpose is to reduce the amount of time it takes a user to receive a result after issuing a query, and to reduce the amount of resources used to process a query."
    },
    "foundations/programming-languages/sql/select": {
      "id": "foundations/programming-languages/sql/select",
      "title": "SQL SELECT",
      "description": "There are two required ingredients in any SQL query: SELECT and FROM---and they have to be in that order. SELECT indicates which columns you'd like to view, and FROM identifies the table that they live in."
    },
    "foundations/programming-languages/sql/sql-basics": {
      "id": "foundations/programming-languages/sql/sql-basics",
      "title": "SQL Basics",
      "description": "What is SQL?"
    },
    "foundations/programming-languages/sql/sql-interviews-questions": {
      "id": "foundations/programming-languages/sql/sql-interviews-questions",
      "title": "SQL Interview Questions",
      "description": "Topics Covered in SQL Interviews for Data Engineers"
    },
    "foundations/programming-languages/sql/sql-query": {
      "id": "foundations/programming-languages/sql/sql-query",
      "title": "SQL Query",
      "description": "Execution path of a query"
    },
    "foundations/programming-languages/sql/stored-procedures": {
      "id": "foundations/programming-languages/sql/stored-procedures",
      "title": "Stored Procedures",
      "description": "A stored procedure is a set of SQL statements stored in a database. These statements can request data entry parameters, which are used as variables during execution, and can constitute a data output."
    },
    "foundations/programming-languages/sql/string-functions": {
      "id": "foundations/programming-languages/sql/string-functions",
      "title": "SQL String Functions",
      "description": "| Name                                                                                                    | Description                                                                              |"
    },
    "foundations/programming-languages/sql/subquery": {
      "id": "foundations/programming-languages/sql/subquery",
      "title": "Subquery",
      "description": "Subqueries (also known as inner queries or nested queries) are a tool for performing operations in multiple steps. For example, if you wanted to take the sums of several columns, then average all of those values, you'd need to do each aggregation in a distinct step."
    },
    "foundations/programming-languages/sql/triggers": {
      "id": "foundations/programming-languages/sql/triggers",
      "title": "Triggers",
      "description": "Triggers are a type of stored procedure, configured to call whenever an event occurs. This trigger can be used, for example, to signalize the execution of some statements whenever new data is included in a table, or a record is edited in the table."
    },
    "foundations/programming-languages/sql/union": {
      "id": "foundations/programming-languages/sql/union",
      "title": "SQL UNION",
      "description": "SQL joins allow you to combine two datasets side-by-side, but UNION allows you to stack one dataset on top of the other. Put differently, UNION allows you to write two separate SELECT statements, and to have the results of one statement display in the same table as the results from the other statement."
    },
    "foundations/programming-languages/sql/views": {
      "id": "foundations/programming-languages/sql/views",
      "title": "Views",
      "description": "A view can be considered a virtual table because it is composed of rows and columns of data, the results of a SELECT SQL instruction in one or more database tables. Views are great resources for organizing information from different tables to create reports."
    },
    "foundations/programming-languages/sql/where": {
      "id": "foundations/programming-languages/sql/where",
      "title": "SQL WHERE",
      "description": "Once you know how to view some data using SELECT and FROM, the next step is filtering the data using the WHERE clause."
    },
    "foundations/programming-languages/sql/window-functions": {
      "id": "foundations/programming-languages/sql/window-functions",
      "title": "Window Functions",
      "description": "There is no official categorisation of Window Functions but based on the usage, we can briefly categorise them in 3 ways:"
    },
    "introduction": {
      "id": "introduction",
      "title": "Introduction",
      "description": "Hit the ⭐️ button if you like the repo",
      "sidebar": "docs"
    },
    "mathematics/probability/probability-distributions": {
      "id": "mathematics/probability/probability-distributions",
      "title": "Probability Distributions",
      "description": "Random variables are important in analysis. Probability distributions depict the distribution of the values of a random variable. The distributions can help in selecting the right algorithms, and hence plotting the distribution of the data is an important part of the analytical process; this is performed as a part of exploratory data analysis (EDA). The following are some important probability distributions:"
    },
    "mathematics/probability/README": {
      "id": "mathematics/probability/README",
      "title": "Probability",
      "description": "Concepts"
    },
    "mathematics/probability/rules-of-probability": {
      "id": "mathematics/probability/rules-of-probability",
      "title": "Rules of Probability",
      "description": "Probability of Mutually Exclusive Events"
    },
    "mathematics/README": {
      "id": "mathematics/README",
      "title": "Mathematics",
      "description": "For data science, 4 topics are important:"
    },
    "mathematics/statistics/README": {
      "id": "mathematics/statistics/README",
      "title": "Statistics",
      "description": "Concepts"
    },
    "mathematics/statistics/sampling": {
      "id": "mathematics/statistics/sampling",
      "title": "Sampling",
      "description": "- Random sampling: This is the most common probability sampling technique as every single sample is selected randomly from the population data set. This gives an opportunity for each record in the data set an equal chance (probability) to be chosen to be a part of the sample. For example, the HR department wants to conduct a social event. Therefore, it wants to select 50 people out of 300. To provide an equal opportunity to everyone, HR picks the names randomly from a jar containing all the names of employees."
    },
    "mlops/code-snippets": {
      "id": "mlops/code-snippets",
      "title": "MLOps Code Snippets",
      "description": "1. Dask-ML-Parallelize model training:"
    },
    "mlops/ml-lab-tracking/README": {
      "id": "mlops/ml-lab-tracking/README",
      "title": "Experiments Tracking, Model Management, and Dataset Versioning",
      "description": "Training DL models is an iterative process that consumes a lot of time and resources. Therefore, keeping track of all experiments and consistently organizing them can prevent us from wasting our time on unnecessary operations such as training similar models repeatedly on the same set of data. In other words, having well-documented records of all model architectures and their hyperparameter sets, as well as the version of data used during experiments, can help us derive the right conclusion from the experiments, which naturally leads to the project being successful."
    },
    "mlops/mlflow/README": {
      "id": "mlops/mlflow/README",
      "title": "MLflow",
      "description": "MLflow is a platform that makes it simpler to manage the entire machine learning lifecycle. It enables you to track your experiments and their results, deploy and manage your models, and package your machine learning code in a reusable, reproducible format. It provides a central model registry that supports versioning and annotating, as well as model serving capabilities. It does that by redefining experimentation logs and module structure."
    },
    "mlops/README": {
      "id": "mlops/README",
      "title": "MLOps",
      "description": "The boom in AI has seen a rising demand for better AI infrastructure — both in the compute hardware layer and AI framework optimizations that make optimal use of accelerated compute. Unfortunately, organizations often overlook the critical importance of a middle tier: infrastructure software that standardizes the ML life cycle, adding a common platform for teams of data scientists and researchers to standardize their approach and eliminate distracting DevOps work. This process of building the ML life cycle is increasingly known as MLOps, with end-to-end platforms being built to automate and standardize repeatable manual processes. Although dozens of MLOps platforms exist, adopting one can be confusing and cumbersome. What should be considered when employing MLOps? What are the core pillars to MLOps, and which features are most critical?"
    },
    "orchestration/airflow/github-nft/README": {
      "id": "orchestration/airflow/github-nft/README",
      "title": "Github NFT Pipeline",
      "description": "Objective"
    },
    "orchestration/airflow/lab-airflow-conn-py/README": {
      "id": "orchestration/airflow/lab-airflow-conn-py/README",
      "title": "Airflow connection using Python",
      "description": "Set Airflow connection using Python"
    },
    "orchestration/airflow/lab-airflow-email-notifications/README": {
      "id": "orchestration/airflow/lab-airflow-email-notifications/README",
      "title": "Airflow Email Notifications",
      "description": "1. Create AWS SES Identity with Email here."
    },
    "orchestration/airflow/lab-airflow-getting-started/README": {
      "id": "orchestration/airflow/lab-airflow-getting-started/README",
      "title": "Getting Started with Airflow",
      "description": "In this lab, we will learn the followings:"
    },
    "orchestration/airflow/lab-bike-sharing-service-pipeline/README": {
      "id": "orchestration/airflow/lab-bike-sharing-service-pipeline/README",
      "title": "Bike Sharing Service Data Pipeline",
      "description": "This lab will be divided into five different DAG levels. Each DAG level will have specific learning objectives, as follows:"
    },
    "orchestration/airflow/lab-forex-etl/README": {
      "id": "orchestration/airflow/lab-forex-etl/README",
      "title": "Airflow Forex ETL",
      "description": "The ETL process will extract data from fixer.io API, transform it, and load it to a PostgreSQL database. This project aims to have an automated process that constantly feeds the PostgreSQL database with data."
    },
    "orchestration/airflow/lab-imdb-spark-etl/README": {
      "id": "orchestration/airflow/lab-imdb-spark-etl/README",
      "title": "IMDB data procesing and Airflow pipeline",
      "description": "Objective"
    },
    "orchestration/airflow/lab-tolldata/README": {
      "id": "orchestration/airflow/lab-tolldata/README",
      "title": "Creating ETL Data Pipelines using Apache Airflow",
      "description": "Objective"
    },
    "orchestration/airflow/labdev-spotify/README": {
      "id": "orchestration/airflow/labdev-spotify/README",
      "title": "Spotify-Data-Engineering-and-Analysis",
      "description": "Data Engineering and Analysis using Spotify API to get statistical insights about played music"
    },
    "orchestration/airflow/README": {
      "id": "orchestration/airflow/README",
      "title": "Airflow",
      "description": "Apache Airflow is an open source tool for programmatically authoring, scheduling, and monitoring data pipelines. It has over 9 million downloads per month and an active OSS community. Airflow allows data practitioners to define their data pipelines as Python code in a highly extensible and infinitely scalable way."
    },
    "orchestration/azure-data-factory/lab-adf-incremental-loading/README": {
      "id": "orchestration/azure-data-factory/lab-adf-incremental-loading/README",
      "title": "Incremental Data Loading in Azure Data Factory",
      "description": "There are different ways in which we can design incremental loading using ADF. Based on the type of data source, we can have different techniques to implement incremental loading. Some of them are listed here:"
    },
    "orchestration/azure-data-factory/lab-batch-processing-solution/README": {
      "id": "orchestration/azure-data-factory/lab-batch-processing-solution/README",
      "title": "Developing batch processing solutions by using Data Factory, Data Lake, Spark, Azure Synapse Pipelines, PolyBase, and Azure Databricks",
      "description": "B1752509001"
    },
    "orchestration/azure-data-factory/README": {
      "id": "orchestration/azure-data-factory/README",
      "title": "Azure Data Factory",
      "description": "Azure Data Factory is the data orchestration service in Azure. Using Azure Data Factory, you can build pipelines that are capable of reading data from multiple sources, transforming the data, and loading the data into data stores to be consumed by reporting applications such as Power BI. Azure Data Factory much like SQL Server Integration Services (SSIS) in an on-premises world, provides a code-free UI for developing, managing, and maintaining data engineering pipelines."
    },
    "orchestration/datafusion/lab-datafusion-pipeline/README": {
      "id": "orchestration/datafusion/lab-datafusion-pipeline/README",
      "title": "Building and Executing a Pipeline Graph with Data Fusion",
      "description": "Objective"
    },
    "orchestration/datafusion/README": {
      "id": "orchestration/datafusion/README",
      "title": "Cloud Data Fusion",
      "description": "Watch this video//youtu.be/ySMexrnxfSg"
    },
    "orchestration/modern-data-stack/lab-analyse-stacks/README": {
      "id": "orchestration/modern-data-stack/lab-analyse-stacks/README",
      "title": "Analyze and Compare the Modern Data Stacks",
      "description": "1. Go to https://www.moderndatastack.xyz/stacks and select any 5 MDS (Modern Data Stacks)."
    },
    "orchestration/modern-data-stack/lab-metaflow-snowflake/README": {
      "id": "orchestration/modern-data-stack/lab-metaflow-snowflake/README",
      "title": "Sequential Recommendation with the Modern Data Stack",
      "description": "As a use case, we pick a popular RecSys challenge, session-based recommendation"
    },
    "orchestration/modern-data-stack/lab-terraform-gcp/README": {
      "id": "orchestration/modern-data-stack/lab-terraform-gcp/README",
      "title": "Build a modern data stack",
      "description": "In this lab, we will use Terraform to deploy a modern data stack on Google Cloud (GCP)."
    },
    "orchestration/modern-data-stack/README": {
      "id": "orchestration/modern-data-stack/README",
      "title": "Modern Data Stack (MDS)",
      "description": "Big Data Systems with Modern Data Stack"
    },
    "orchestration/nifi/README": {
      "id": "orchestration/nifi/README",
      "title": "NiFi",
      "description": "Apache NiFi is a framework for building data engineering pipelines, and it utilizes DAGs. Apache NiFi was built by the National Security Agency and is used at several federal agencies. Apache NiFi is easier to set up and is useful for new data engineers. The GUI is excellent and while you can use Jython, Clojure, Scala, or Groovy to write processors, you can accomplish a lot with a simple configuration of existing processors."
    },
    "orchestration/prefect/lab-prefect-streaming/README": {
      "id": "orchestration/prefect/lab-prefect-streaming/README",
      "title": "prefect-streaming",
      "description": "Example project demonstrating deployment patterns for real-time streaming workflows"
    },
    "orchestration/prefect/README": {
      "id": "orchestration/prefect/README",
      "title": "Prefect",
      "description": "Labs"
    },
    "orchestration/README": {
      "id": "orchestration/README",
      "title": "Orchestration",
      "description": "Workflow orchestration tools are software platforms that help organizations manage and automate complex business processes across different systems and teams. These tools allow businesses to define, schedule, monitor, and manage workflows, which can help streamline operations, reduce errors, and increase productivity."
    },
    "orchestration/stepfunctions/lab-stepfunction-athena-sns/README": {
      "id": "orchestration/stepfunctions/lab-stepfunction-athena-sns/README",
      "title": "Step Function Athena SNS",
      "description": "Objective: Execute multiple queries (Amazon Athena, Amazon SNS)"
    },
    "orchestration/stepfunctions/lab-stepfunction-ecomm-sqs/README": {
      "id": "orchestration/stepfunctions/lab-stepfunction-ecomm-sqs/README",
      "title": "Step Function Ecommerce SQL",
      "description": "Objective: Orchestrate Queue-based Microservices"
    },
    "orchestration/stepfunctions/README": {
      "id": "orchestration/stepfunctions/README",
      "title": "Step Functions",
      "description": "Labs"
    },
    "processing/apache-beam": {
      "id": "processing/apache-beam",
      "title": "Apache Beam",
      "description": "What is apache beam?"
    },
    "processing/apache-druid": {
      "id": "processing/apache-druid",
      "title": "Druid",
      "description": "One database that meets all the criteria for real-time analytics application is Apache Druid. It enables subsecond performance at scale, provides high concurrency at the best value, and easily ingests and combines real-time streaming data and historical batch data. It is a high-performance, real-time analytics database that is flexible, efficient, and resilient."
    },
    "processing/apache-flink": {
      "id": "processing/apache-flink",
      "title": "Apache Flink",
      "description": "Apache Flink is a widely used data processing engine for scalable streaming ETL, analytics, and event-driven applications. It provides precise time and state management with fault tolerance. Flink can process bounded stream (batch) and unbounded stream (stream) with a unified API or application. After data is processed with Apache Flink, downstream applications can access the curated data with a unified data catalog. With unified metadata, both data processing and data consuming applications can access the tables using the same metadata."
    },
    "processing/apache-kafka": {
      "id": "processing/apache-kafka",
      "title": "Apache Kafka",
      "description": "What is Apache Kafka?"
    },
    "processing/aws-emr": {
      "id": "processing/aws-emr",
      "title": "Amazon EMR",
      "description": "EMR Serverless"
    },
    "processing/aws-kinesis": {
      "id": "processing/aws-kinesis",
      "title": "Kinesis",
      "description": "Easily collect, process, and analyze video and data streams in real time"
    },
    "processing/aws-lambda": {
      "id": "processing/aws-lambda",
      "title": "AWS Lambda",
      "description": "Lambda Function"
    },
    "processing/aws-lambda-snippets": {
      "id": "processing/aws-lambda-snippets",
      "title": "Snippets related to Lambda function",
      "description": "Create a CloudFormation template for creating a Lambda function that writes to an S3 bucket"
    },
    "processing/azure-synapse-analytics": {
      "id": "processing/azure-synapse-analytics",
      "title": "Azure Synapse Analytics",
      "description": "Azure Synapse Analytics workspaces generation 2, formally released in December 2020, is the industry-leading big data solution for processing and consolidating data of business value. Azure Synapse Analytics has three important components:"
    },
    "processing/databricks/boston-crime-analysis/README": {
      "id": "processing/databricks/boston-crime-analysis/README",
      "title": "Boston Crime Analysis",
      "description": "Perform data cleansing, transformation and load 300,000+ records in a PySpark dataframe using PySpark and process the resultant data to CSV, reducing data redundancy by 15% and improving data accuracy by 90%"
    },
    "processing/databricks/lab-cybersecurity-databricks/README": {
      "id": "processing/databricks/lab-cybersecurity-databricks/README",
      "title": "Cybersecurity Databricks",
      "description": "Objective"
    },
    "processing/databricks/lab-databricks-clickstream/README": {
      "id": "processing/databricks/lab-databricks-clickstream/README",
      "title": "Databricks AWS Integration and Clickstream Analysis",
      "description": "Objective"
    },
    "processing/databricks/lab-databricks-deltalake/README": {
      "id": "processing/databricks/lab-databricks-deltalake/README",
      "title": "Databricks Deltalake",
      "description": "Objective: Creation of an elementary Data Lakehouse using Databricks and the Delta lake technology"
    },
    "processing/databricks/lab-databricks-scala-postgres-s3/README": {
      "id": "processing/databricks/lab-databricks-scala-postgres-s3/README",
      "title": "S3 Postgres Scala",
      "description": "Objective"
    },
    "processing/databricks/lab-deltalake-optimizations/README": {
      "id": "processing/databricks/lab-deltalake-optimizations/README",
      "title": "Delta Lake Optimizations",
      "description": "In this lab, we will learn about various Delta Lake optimizations that help us build a more performant Lakehouse."
    },
    "processing/databricks/lab-dlt-dbt/README": {
      "id": "processing/databricks/lab-dlt-dbt/README",
      "title": "dlt vs dbt",
      "description": "Objective"
    },
    "processing/databricks/lab-healthcare-databricks/README": {
      "id": "processing/databricks/lab-healthcare-databricks/README",
      "title": "Unlocking the Power of Health Data With a Modern Data Lakehouse",
      "description": "A single patient produces approximately 80 megabytes of medical data every year. Multiply that across thousands of patients over their lifetime, and you’re looking at petabytes of patient data that contains valuable insights. Unlocking these insights can help streamline clinical operations, accelerate drug R&D and improve patient health outcomes. But first, the data needs to be prepared for downstream analytics and AI. Unfortunately, most healthcare and life sciences organizations spend an inordinate amount of time simply gathering, cleaning and structuring their data."
    },
    "processing/databricks/lab-iot-health-tracker/README": {
      "id": "processing/databricks/lab-iot-health-tracker/README",
      "title": "Real-time Health Tracking and Monitoring System",
      "description": "What you'll build"
    },
    "processing/databricks/lab-loan-application/README": {
      "id": "processing/databricks/lab-loan-application/README",
      "title": "Simplifying Data Engineering and Analytics with Delta",
      "description": "Objective"
    },
    "processing/databricks/lab-pyspark-itversity/README": {
      "id": "processing/databricks/lab-pyspark-itversity/README",
      "title": "Pyspark Itversity - Hadoop and Spark Hands-on Practical Exercises",
      "description": "References"
    },
    "processing/databricks/lab-retail-pos-databricks/README": {
      "id": "processing/databricks/lab-retail-pos-databricks/README",
      "title": "Real-Time Point-of-Sale Analytics With the Data Lakehouse",
      "description": "Introduction"
    },
    "processing/databricks/project-advancedbricks/README": {
      "id": "processing/databricks/project-advancedbricks/README",
      "title": "AdvancedBricks",
      "description": "Advanced Data Engineering with Databricks"
    },
    "processing/databricks/project-bedbricks/README": {
      "id": "processing/databricks/project-bedbricks/README",
      "title": "BedBricks",
      "description": "Databricks PySpark Ecommerce Data Processing Case Study"
    },
    "processing/databricks/project-databricks-de/README": {
      "id": "processing/databricks/project-databricks-de/README",
      "title": "Data Engineering with Databricks",
      "description": "In this project, we will learn basic to advanced concepts of databricks:"
    },
    "processing/databricks/project-databricks-superset/README": {
      "id": "processing/databricks/project-databricks-superset/README",
      "title": "Data Pipeline with Databricks PySpark and Superset",
      "description": "Put on your data engineer hat! In this project, you’ll build a modern, cloud-based, three-layer data Lakehouse. First, you’ll set up your workspace on the Databricks platform, leveraging important Databricks features, before pushing the data into the first two layers of the data lake. Next, using Apache Spark, you’ll build the third layer, used to serve insights to different end-users. Then, you’ll use Delta Lake to turn your existing data lake into a Lakehouse. Finally, you’ll deliver an infrastructure that allows your end-users to perform specific queries, using Apache Superset, and build dashboards on top of the existing data. When you’re done with the projects in this series, you’ll have a complete big data pipeline for a cloud-based data lake—and you’ll understand why the three-layer architecture is so popular."
    },
    "processing/databricks/project-learnerbricks/README": {
      "id": "processing/databricks/project-learnerbricks/README",
      "title": "Data Engineer Learner Path with Databricks",
      "description": "The Data Engineering with Databricks (DEWD) course is designed to prepare students for the Databricks Certified Associate Data Engineer certification exam. The content for this course consists of the Associate-level modules of the Data Engineer Learning Path, and can be delivered as an instructor-led training (ILT) or self-paced (SP) course."
    },
    "processing/databricks/README": {
      "id": "processing/databricks/README",
      "title": "Databricks",
      "description": "Databricks is a platform that enables enterprises to quickly build their Data Lakehouse infrastructure and enable all data personas – data engineers, data scientists, and business intelligence personnel – in their organization to extract and deliver insights from the data. The platform provides a curated experience for each data persona, enabling them to execute their daily workflows. The foundational technologies that enable these experiences are open source – Apache Spark, Delta lake, MLflow, and more."
    },
    "processing/databricks/setup": {
      "id": "processing/databricks/setup",
      "title": "Setup Databricks",
      "description": "Watch and follow this video."
    },
    "processing/dbt": {
      "id": "processing/dbt",
      "title": "dbt",
      "description": "Transform your data in warehouse"
    },
    "processing/gcp-dataproc": {
      "id": "processing/gcp-dataproc",
      "title": "Dataproc",
      "description": "Running Hadoop on Dataproc"
    },
    "processing/gcp-pubsub": {
      "id": "processing/gcp-pubsub",
      "title": "Pub/Sub",
      "description": "Introduction to Pub/Sub"
    },
    "processing/lab-azure-hdinsight-simple-data-processing/README": {
      "id": "processing/lab-azure-hdinsight-simple-data-processing/README",
      "title": "Simple Data Pipeline with HDInsight",
      "description": "Pre-requisites"
    },
    "processing/lab-azure-synapse-data-processing/README": {
      "id": "processing/lab-azure-synapse-data-processing/README",
      "title": "Processing Data Using Azure Synapse Analytics",
      "description": "This lab covers exploring data using Synapse Serverless SQL pool, processing data using Synapse Spark Pools, Working with Synapse Lake database, and integrating Synapse Analytics with Power BI"
    },
    "processing/lab-azure-synapse-dataflows/README": {
      "id": "processing/lab-azure-synapse-dataflows/README",
      "title": "Transforming Data Using Azure Synapse Dataflows",
      "description": "This lab focuses on performing transformations using Synapse Dataflows, optimizing data flows using partitioning, and managing dynamic source schema changes using schema drifting"
    },
    "processing/lab-azure-synapse-implementing-star-schema/README": {
      "id": "processing/lab-azure-synapse-implementing-star-schema/README",
      "title": "Implementing the Serving Layer Star Schema",
      "description": "In this lab, we will learn about implementing the serving layer, which involves implementing star schemas, techniques to read and write different data formats, sharing data between services such as SQL and Spark, and more. Once you complete this lab, you should be able to understand the differences between a Synapse dedicated SQL pool versus traditional SQL systems for implementing the Star schema, the various ways of accessing Parquet data using technologies such as Spark and SQL, and the details involved in storing metadata across services. All this knowledge should help you build a practical and maintainable serving layer in a data lake."
    },
    "processing/lab-confluent-kafka-faker/README": {
      "id": "processing/lab-confluent-kafka-faker/README",
      "title": "Real-time CDC-enabled Extract and Load Pipeline with Kafka on Cloud",
      "description": "Introduction"
    },
    "processing/lab-csv-to-parquet-conversion/README": {
      "id": "processing/lab-csv-to-parquet-conversion/README",
      "title": "CSV to Parquet Transformation with Glue Studio",
      "description": "Task: Process raw (CSV or JSON) data from the Landing S3 bucket and save it into another S3 bucket in a Columnar format with partitioning"
    },
    "processing/lab-dataflow-bigquery-etl": {
      "id": "processing/lab-dataflow-bigquery-etl",
      "title": "ETL Processing on Google Cloud Using Dataflow and BigQuery",
      "description": "Objective"
    },
    "processing/lab-dbt-jaffle-shop/models/docs": {
      "id": "processing/lab-dbt-jaffle-shop/models/docs",
      "title": "docs",
      "description": "{% docs orders_status %}"
    },
    "processing/lab-dbt-jaffle-shop/models/overview": {
      "id": "processing/lab-dbt-jaffle-shop/models/overview",
      "title": "overview",
      "description": "{% docs overview %}"
    },
    "processing/lab-dbt-jaffle-shop/README": {
      "id": "processing/lab-dbt-jaffle-shop/README",
      "title": "dbt Jaffle Shop",
      "description": "Objective"
    },
    "processing/lab-dbt-knoema/README": {
      "id": "processing/lab-dbt-knoema/README",
      "title": "Snowflake dbt Knoema",
      "description": "In this lab, we are going to analyze historical trading performance of a company that has trading desks spread across different regions. As inputs, we are going to leverage datasets available in Knoema Economy Data Atlas that is available in Snowflake Data Marketplace, plus few manual uploads."
    },
    "processing/lab-dbt-nyctaxi-lookup/README": {
      "id": "processing/lab-dbt-nyctaxi-lookup/README",
      "title": "NYC Taxi Analtics and ELT Pipeline with dbt and Postgres",
      "description": "Lineage Graph"
    },
    "processing/lab-dbt-nyctaxi/README": {
      "id": "processing/lab-dbt-nyctaxi/README",
      "title": "dbt NYC Taxi",
      "description": "Objective"
    },
    "processing/lab-dbt-olist/models/staging/olist/stg_ecommerce": {
      "id": "processing/lab-dbt-olist/models/staging/olist/stg_ecommerce",
      "title": "stg_ecommerce",
      "description": "{% docs segment_info %}"
    },
    "processing/lab-dbt-olist/README": {
      "id": "processing/lab-dbt-olist/README",
      "title": "dbt Olist",
      "description": "Objective"
    },
    "processing/lab-dbt-stackexchnge/README": {
      "id": "processing/lab-dbt-stackexchnge/README",
      "title": "Stackexchange dbt Bigquery",
      "description": "Stack Exchange ELT pipeline with dbt and BigQuery"
    },
    "processing/lab-dbt-tickit/README": {
      "id": "processing/lab-dbt-tickit/README",
      "title": "dbt Redshift TICKIT",
      "description": "Building an ELT Pipeline with dbt and Amazon Redshift"
    },
    "processing/lab-dbt-tpch/README": {
      "id": "processing/lab-dbt-tpch/README",
      "title": "Snowflake dbt cloud TPCH",
      "description": "Accelerating Data Teams with Snowflake and dbt Cloud Hands On Lab"
    },
    "processing/lab-emr-serverless/README": {
      "id": "processing/lab-emr-serverless/README",
      "title": "EMR Serverless",
      "description": "Objective"
    },
    "processing/lab-gcp-beam-mapreduce/README": {
      "id": "processing/lab-gcp-beam-mapreduce/README",
      "title": "MapReduce in Beam using Python",
      "description": "Objective"
    },
    "processing/lab-gcp-dataflow-batch-pipeline": {
      "id": "processing/lab-gcp-dataflow-batch-pipeline",
      "title": "GCP Dataflow Batch Pipeline",
      "description": "Objective: Serverless Data Processing with Dataflow - Batch Analytics Pipelines with Cloud Dataflow (Python)"
    },
    "processing/lab-gcp-dataflow-pipeline": {
      "id": "processing/lab-gcp-dataflow-pipeline",
      "title": "GCP Dataflow Pipeline - A Simple Dataflow Pipeline (Python)",
      "description": "Objective"
    },
    "processing/lab-gcp-dataflow-side-inputs": {
      "id": "processing/lab-gcp-dataflow-side-inputs",
      "title": "GCP Dataflow Size Inputs",
      "description": "Objective"
    },
    "processing/lab-gcp-dataflow-stream-pipeline": {
      "id": "processing/lab-gcp-dataflow-stream-pipeline",
      "title": "GCP Dataflow Streaming Pipeline",
      "description": "Objective"
    },
    "processing/lab-gcp-dataprep": {
      "id": "processing/lab-gcp-dataprep",
      "title": "GCP Dataprep",
      "description": "Creating a Data Transformation Pipeline with Cloud Dataprep"
    },
    "processing/lab-gcp-dataproc/README": {
      "id": "processing/lab-gcp-dataproc/README",
      "title": "Running Apache Spark jobs on Cloud Dataproc",
      "description": "Objective"
    },
    "processing/lab-gcp-pubsub": {
      "id": "processing/lab-gcp-pubsub",
      "title": "Streaming Data Processing - Publish Streaming Data into PubSub",
      "description": "Objective"
    },
    "processing/lab-gcp-pubsub-processing": {
      "id": "processing/lab-gcp-pubsub-processing",
      "title": "Streaming Data Processing - Streaming Data Pipelines",
      "description": "Objective"
    },
    "processing/lab-gcp-serverless-dataflow": {
      "id": "processing/lab-gcp-serverless-dataflow",
      "title": "GCP Serverless Dataflow",
      "description": "Objective"
    },
    "processing/lab-getting-started-with-beam/README": {
      "id": "processing/lab-getting-started-with-beam/README",
      "title": "Apache Beam Getting Started",
      "description": "Pipeline 1 - Simple Ingest Data Pipeline"
    },
    "processing/lab-glue-advanced/README": {
      "id": "processing/lab-glue-advanced/README",
      "title": "Advanced Data Engineering and Data Processing with AWS Glue Jobs",
      "description": "Objective"
    },
    "processing/lab-glue-deltalake-cdc-upsert/README": {
      "id": "processing/lab-glue-deltalake-cdc-upsert/README",
      "title": "Handle UPSERT data operations using open-source Delta Lake and AWS Glue",
      "description": "Many customers need an ACID transaction (atomic, consistent, isolated, durable) data lake that can log change data capture (CDC) from operational data sources. There is also demand for merging real-time data into batch data. Delta Lake framework provides these two capabilities. In this lab, we learn how to handle UPSERTs (updates and inserts) of the operational data using natively integrated Delta Lake with AWS Glue, and query the Delta Lake using Amazon Athena."
    },
    "processing/lab-glue-studio-custom-transforms/README": {
      "id": "processing/lab-glue-studio-custom-transforms/README",
      "title": "Glue Studio Custom Transforms",
      "description": "Objective: Create your own reusable visual transforms for AWS Glue Studio"
    },
    "processing/lab-glue-studio-tickets/README": {
      "id": "processing/lab-glue-studio-tickets/README",
      "title": "Tickets ETL with Glue Studio",
      "description": "You can use AWS Glue Studio to create jobs that extract structured or semi-structured data from a data source, perform a transformation of that data, and save the result set in a data target."
    },
    "processing/lab-kafka-cli/README": {
      "id": "processing/lab-kafka-cli/README",
      "title": "Getting started with Kafka and CLI",
      "description": ""
    },
    "processing/lab-kafka-fraud-detection/README": {
      "id": "processing/lab-kafka-fraud-detection/README",
      "title": "Real-time fraud detection by applying filter in Kafka topic",
      "description": "In this lab, we will generate some transactions out of which some would be fraudulent and then at the consumer end, we will check if the transaction is legit or fraud."
    },
    "processing/lab-kafka-nyctaxi/assets/README": {
      "id": "processing/lab-kafka-nyctaxi/assets/README",
      "title": "Kafka with Docker Compose",
      "description": "- zookeeper: a centralized service for maintaining configuration info. Kafka uses it for maintaining metadata knowledge such as topic partitions, etc. Zookeeper is being phased out as a dependency, but for easier deployment we will use it in the lesson."
    },
    "processing/lab-kafka-nyctaxi/README": {
      "id": "processing/lab-kafka-nyctaxi/README",
      "title": "Kafka Streams for NYC Taxi data",
      "description": "Objective"
    },
    "processing/lab-kafka-python/README": {
      "id": "processing/lab-kafka-python/README",
      "title": "Getting started with Kafka and Python",
      "description": ""
    },
    "processing/lab-kafka-spark-streaming/README": {
      "id": "processing/lab-kafka-spark-streaming/README",
      "title": "Realtime Streaming analytics with Apache Kafka and Spark Streaming",
      "description": "Activity 1"
    },
    "processing/lab-kafka-stock-market/README": {
      "id": "processing/lab-kafka-stock-market/README",
      "title": "Stock Market Kafka Real Time",
      "description": "Problem Statement"
    },
    "processing/lab-kafka-toll-analysis/README": {
      "id": "processing/lab-kafka-toll-analysis/README",
      "title": "Data Streaming Pipeline with Kafka for livetolldata",
      "description": "Objective"
    },
    "processing/lab-kinesis-apache-logs/README": {
      "id": "processing/lab-kinesis-apache-logs/README",
      "title": "Real Time Apache Log Analytics with Kinesis",
      "description": "Objective"
    },
    "processing/lab-kinesis-clickstream-anomaly/README": {
      "id": "processing/lab-kinesis-clickstream-anomaly/README",
      "title": "Real-Time Clickstream Anomaly Detection with Kinesis",
      "description": "Architecture"
    },
    "processing/lab-lambda-csv-parquet/README": {
      "id": "processing/lab-lambda-csv-parquet/README",
      "title": "Lambda CSV to Parquet",
      "description": "Create an S3 bucket and IAM user with user-defined policy. Create Lambda layer and lambda function and add the layer to the function. Add S3 trigger for auto-transformation from csv to parquet and query with Glue."
    },
    "processing/lab-ray-air-basics/README": {
      "id": "processing/lab-ray-air-basics/README",
      "title": "Lab: Ray AIR Basics",
      "description": "In this lab, we are learning all 4 high-level APIs of Ray AIR with some examples."
    },
    "processing/lab-ray-core-basics/README": {
      "id": "processing/lab-ray-core-basics/README",
      "title": "Lab: Ray Core Basics",
      "description": "- Recipe 1: Your first Ray API example"
    },
    "processing/lab-snowpark-churnpark/README": {
      "id": "processing/lab-snowpark-churnpark/README",
      "title": "Churn Analytics Demo with dbt Snowpark Python models",
      "description": "PPT//docs.google.com/presentation/d/1IJSeE96bze7DECuDYqsTVv6FaOcNcJ5tTiCWKEkuQQ/edit#slide=id.g158d486fe9e2_12"
    },
    "processing/lab-snowpark-dbtsnowpy/README": {
      "id": "processing/lab-snowpark-dbtsnowpy/README",
      "title": "Modeling with dbt and Snowpark",
      "description": ""
    },
    "processing/lab-snowpark-fifapark/README": {
      "id": "processing/lab-snowpark-fifapark/README",
      "title": "FIFA modeling with Snowpark",
      "description": ""
    },
    "processing/lab-snowpark-jafflepark/models/docs": {
      "id": "processing/lab-snowpark-jafflepark/models/docs",
      "title": "docs",
      "description": "{% docs orders_status %}"
    },
    "processing/lab-snowpark-jafflepark/README": {
      "id": "processing/lab-snowpark-jafflepark/README",
      "title": "Jaffle Shop Modeling with Snowpark",
      "description": ""
    },
    "processing/lab-snowpark-knoema-regression/README": {
      "id": "processing/lab-snowpark-knoema-regression/README",
      "title": "Knoema Regression Model with Snowpark",
      "description": "We are interested in the US Inflation data, so we will use this query to explore the data for the application: What is the US inflation over time?"
    },
    "processing/project-kafka-ikea/README": {
      "id": "processing/project-kafka-ikea/README",
      "title": "Building an event-driven IKEA app with Kafka",
      "description": "This app is designed to break down the event driven architecture for modern apps"
    },
    "processing/ray": {
      "id": "processing/ray",
      "title": "Ray",
      "description": "What is Ray?"
    },
    "processing/snowpark": {
      "id": "processing/snowpark",
      "title": "Snowpark",
      "description": "What is Snowpark"
    },
    "storage/apache-couchdb": {
      "id": "storage/apache-couchdb",
      "title": "Apache CouchDB",
      "description": "Apache CouchDB is an open source NoSQL document database that collects and stores data in JSON-based document formats. Unlike relational databases, CouchDB uses a schema-free data model, which simplifies record management across various computing devices, mobile phones, and web browsers."
    },
    "storage/apache-hudi": {
      "id": "storage/apache-hudi",
      "title": "Apache Hudi",
      "description": "Bring transactions, record-level updates/deletes and change streams to data lakes"
    },
    "storage/apache-iceberg": {
      "id": "storage/apache-iceberg",
      "title": "Apache Iceberg",
      "description": "The open table format for analytic datasets"
    },
    "storage/arrow": {
      "id": "storage/arrow",
      "title": "Apache Arrow or in-memory serialization",
      "description": "When we introduced serialization as a storage raw ingredient at the beginning of this chapter, we mentioned that software could store data in complex objects scattered in memory and connected by pointers, or more orderly, densely packed structures such as Fortran and C arrays. Generally, densely packed in-memory data structures were limited to simple types (e.g., INT64) or fixed-width data structures (e.g., fixed-width strings). More complex structures (e.g., JSON documents) could not be densely stored in memory and required serialization for storage and transfer between systems."
    },
    "storage/athena": {
      "id": "storage/athena",
      "title": "Amazon Athena",
      "description": "Athena is a Serverless Query Service from Amazon based on Presto engine"
    },
    "storage/avro": {
      "id": "storage/avro",
      "title": "Avro",
      "description": "Avro is a row-oriented data format designed for RPCs and data serialization. Avro encodes data into a binary format, with schema metadata specified in JSON. Avro is popular in the Hadoop ecosystem and is also supported by various cloud data tools."
    },
    "storage/avro-vs-parquet-vs-orc": {
      "id": "storage/avro-vs-parquet-vs-orc",
      "title": "Avro vs Parquet vs ORC",
      "description": "B1752502010"
    },
    "storage/aws-dynamodb": {
      "id": "storage/aws-dynamodb",
      "title": "DynamoDB",
      "description": "Getting Started with DynamoDB"
    },
    "storage/azure-datalake": {
      "id": "storage/azure-datalake",
      "title": "Azure Data Lakes",
      "description": "Azure Data Lake is a highly scalable and durable object-based cloud storage solution from Microsoft. It is optimized to store large amounts of structured and semi-structured data such as logs, application data, and documents."
    },
    "storage/azure-sql": {
      "id": "storage/azure-sql",
      "title": "Azure SQL Databases",
      "description": "Azure SQL Database, a fundamental relational database as a service offered in Azure, acts as a source, destination, or even as an intermediate storage layer in data engineering pipelines. Azure SQL Database can be used to consolidate data coming from several relational data sources and build mini data warehouses or data marts. With the introduction of Hyperscale tier in Azure SQL Database, the capacity of Azure SQL Database has increased leaps and bounds too. Securing Azure SQL Database is also pivotal in protecting access to the database. Having a strong understanding of Azure SQL Database's capabilities and security options is essential for any data engineer."
    },
    "storage/bigquery": {
      "id": "storage/bigquery",
      "title": "BigQuery",
      "description": "BigQuery is server-less, highly scalable, and cost-effective Data warehouse designed for Google cloud Platform (GCP) to store and query petabytes of data. The query engine is capable of running SQL queries on terabytes of data in a matter of seconds, and petabytes in only minutes. You get this performance without having to manage any infrastructure and without having to create or rebuild indexes."
    },
    "storage/casestudy-messflix-hypothetical": {
      "id": "storage/casestudy-messflix-hypothetical",
      "title": "Messflix (hypothetical)",
      "description": "Messflix, a movie- and TV-show streaming platform, just hit a wall. A data wall. The company has all the data in the world but complains about not even being able to build a proper recommendation system for its movies and shows. The competition seems to be able to get it done; in fact, the competition is famous for being the first movers in a lot of technology sectors."
    },
    "storage/cassandra": {
      "id": "storage/cassandra",
      "title": "Cassandra",
      "description": "Apache Cassandra is an open source, distributed, decentralized, elastically scalable, highly available, fault-tolerant, tuneably consistent, row-oriented database. Cassandra bases its distribution design on Amazon’s Dynamo and its data model on Google’s Bigtable, with a query language similar to SQL. Created at Facebook, it now powers cloud-scale applications across many industries."
    },
    "storage/code-snippets": {
      "id": "storage/code-snippets",
      "title": "Code Snippets",
      "description": "Connect to Redshift using Python"
    },
    "storage/compression": {
      "id": "storage/compression",
      "title": "Compression",
      "description": "gzip, bzip2, Snappy"
    },
    "storage/csv": {
      "id": "storage/csv",
      "title": "CSV: The nonstandard standard",
      "description": "CSV is a serialization format that data engineers love to hate. The term CSV is essentially a catchall for delimited text, but there is flexibility in conventions of escaping, quote characters, delimiter, and more."
    },
    "storage/data-lakehouses": {
      "id": "storage/data-lakehouses",
      "title": "Data Lakehouses",
      "description": "We have been collecting data for decades. The flat file storages of the 60s led to the data warehouses of the 80s to Massively Parallel Processing (MPP) and NoSQL databases, and eventually to data lakes and now the lakehouses. New paradigms continue to be coined but it would be fair to say that most enterprise organizations have settled on some variation of a data lake and lakehouse:"
    },
    "storage/data-mesh-basics": {
      "id": "storage/data-mesh-basics",
      "title": "Data Mesh Basics",
      "description": "The data mesh is to data as agile is to software engineering, or as microservices are to architecture patterns."
    },
    "storage/data-warehouses": {
      "id": "storage/data-warehouses",
      "title": "Data Warehouses",
      "description": "Enterprises are becoming increasingly data driven, and a key component of any enterprise’s data strategy is a data warehouse—a central repository of integrated data from all across the company. Traditionally, the data warehouse was used by data analysts to create analytical reports. But now it is also increasingly used to populate real-time dashboards, to make ad hoc queries, and to provide decision-making guidance through predictive analytics. Because of these business requirements for advanced analytics and a trend toward cost control, agility, and self-service data access, many organizations are moving to cloud-based data warehouses such as Snowfkake, Amazon Redshift and Google BigQuery."
    },
    "storage/database-storage-engines": {
      "id": "storage/database-storage-engines",
      "title": "Database Storage Engines",
      "description": "To round out the discussion of serialization, we briefly discuss database storage engines. All databases have an underlying storage engine; many don’t expose their storage engines as a separate abstraction (for example, BigQuery, Snowflake). Some (notably, MySQL) support fully pluggable storage engines. Others (e.g., SQL Server) offer major storage engine configuration options (columnar versus row-based storage) that dramatically affect database behavior."
    },
    "storage/datalake-components": {
      "id": "storage/datalake-components",
      "title": "Components of the cloud data lake architecture",
      "description": "There are four key components that create the foundation and serve as building blocks for the cloud data lake architecture. These components are:"
    },
    "storage/datalakes": {
      "id": "storage/datalakes",
      "title": "Data Lakes",
      "description": "What is a Cloud Data Lake Architecture"
    },
    "storage/deltalake": {
      "id": "storage/deltalake",
      "title": "Deltalake",
      "description": "An open format storage layer for your lakehouses"
    },
    "storage/duckdb": {
      "id": "storage/duckdb",
      "title": "DuckDB",
      "description": "DuckDB is a really interesting project aimed at being a SQLite style database with a focus on OLAP (online analytical processing). OLAP is typically associated with analytics due to its design catering to long running queries over large datasets or aggregations over joins of multiple tables with vast amounts of data. DuckDB is an open source project developed by the non-profit organization, DuckDB Labs based in Amsterdam, Netherlands and takes donations and contracting work around their database."
    },
    "storage/gcp-bigtable": {
      "id": "storage/gcp-bigtable",
      "title": "BigTable",
      "description": "High-throughput streaming with Cloud Bigtable"
    },
    "storage/gcp-cloudsql": {
      "id": "storage/gcp-cloudsql",
      "title": "GCP CloudSQL",
      "description": "CloudSQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server with rich extension collections, configuration flags, and developer ecosystems."
    },
    "storage/gcs": {
      "id": "storage/gcs",
      "title": "Google Cloud Storage (GCS)",
      "description": "Google Cloud Storage (GCS) is object storage. It's a service that is fully managed by GCP, which means we don't need to think about any underlying infrastructure for GCS. For example, we don't need to think about pre-sizing the storage, the network bandwidth, number of nodes, or any other infrastructure-related stuff."
    },
    "storage/hadoop-vs-datalake": {
      "id": "storage/hadoop-vs-datalake",
      "title": "On-premises Hadoop cluster vs Cloud data lakes",
      "description": "On-premises versus cloud architectures"
    },
    "storage/hudi": {
      "id": "storage/hudi",
      "title": "Hudi",
      "description": "Hudi stands for Hadoop Update Delete Incremental. This table management technology combines multiple serialization techniques to allow columnar database performance for analytics queries while also supporting atomic, transactional updates. A typical Hudi application is a table that is updated from a CDC stream from a transactional application database. The stream is captured into a row-oriented serialization format, while the bulk of the table is retained in a columnar format. A query runs over both columnar and row-oriented files to return results for the current state of the table. Periodically, a repacking process runs that combines the row and columnar files into updated columnar files to maximize query efficiency."
    },
    "storage/iceberg": {
      "id": "storage/iceberg",
      "title": "Iceberg",
      "description": "Like Hudi, Iceberg is a table management technology. Iceberg can track all files that make up a table. It can also track files in each table snapshot over time, allowing table time travel in a data lake environment. Iceberg supports schema evolution and can readily manage tables at a petabyte scale."
    },
    "storage/interview-questions": {
      "id": "storage/interview-questions",
      "title": "Interview Questions",
      "description": "What is a relational database?"
    },
    "storage/json": {
      "id": "storage/json",
      "title": "JSON and JSONL",
      "description": "JavaScript Object Notation (JSON) has emerged as the new standard for data exchange over APIs, and it has also become an extremely popular format for data storage. In the context of databases, the popularity of JSON has grown apace with the rise of MongoDB and other document stores. Databases such as Snowflake, BigQuery, and SQL Server also offer extensive native support, facilitating easy data exchange between applications, APIs, and database systems."
    },
    "storage/lab-adl-securing-monitoring-lakes/README": {
      "id": "storage/lab-adl-securing-monitoring-lakes/README",
      "title": "Azure Data Lake - Securing and Monitoring",
      "description": "Data Lake forms the key storage layer for data engineering pipelines. Security and the monitoring of Data Lake accounts are key aspects of Data Lake maintenance. This lab will focus on configuring security controls such as firewalls, encryption, and creating private links to a Data Lake account. By the end of this lab, you will have learned how to configure a firewall, virtual network, and private link to secure the Data Lake, encrypt Data Lake using Azure Key Vault, and monitor key user actions in Data Lake."
    },
    "storage/lab-amazon-keyspaces/README": {
      "id": "storage/lab-amazon-keyspaces/README",
      "title": "Amazon Keyspaces",
      "description": "Setup"
    },
    "storage/lab-aws-rds-service/README": {
      "id": "storage/lab-aws-rds-service/README",
      "title": "Lab: AWS RDS Service",
      "description": "1. Create database in RDS DBMS and generate credentials"
    },
    "storage/lab-azure-sql-securing-databases/README": {
      "id": "storage/lab-azure-sql-securing-databases/README",
      "title": "Configuring and Securing Azure SQL Database",
      "description": "Azure SQL Database, a fundamental relational database as a service offered in Azure, acts as a source, destination, or even as an intermediate storage layer in data engineering pipelines. Azure SQL Database can be used to consolidate data coming from several relational data sources and build mini data warehouses or data marts. With the introduction of Hyperscale tier in Azure SQL Database, the capacity of Azure SQL Database has increased leaps and bounds too. Securing Azure SQL Database is also pivotal in protecting access to the database. Having a strong understanding of Azure SQL Database's capabilities and security options is essential for any data engineer."
    },
    "storage/lab-bigquery-analysis/README": {
      "id": "storage/lab-bigquery-analysis/README",
      "title": "BigQuery Analysis",
      "description": "Objective"
    },
    "storage/lab-bigquery-commandline/README": {
      "id": "storage/lab-bigquery-commandline/README",
      "title": "BigQuery Commandline",
      "description": "Objective"
    },
    "storage/lab-bigquery-composer/README": {
      "id": "storage/lab-bigquery-composer/README",
      "title": "GCP BigQuery Composer",
      "description": "Cloud Composer - Copying BigQuery Tables Across Different Locations"
    },
    "storage/lab-bigquery-data-warehousing/README": {
      "id": "storage/lab-bigquery-data-warehousing/README",
      "title": "BigQuery Data Warehousing",
      "description": "Objective"
    },
    "storage/lab-bigquery-ml/README": {
      "id": "storage/lab-bigquery-ml/README",
      "title": "GCP BigQuery ML",
      "description": "Predict Visitor Purchases with a Classification Model in BigQuery ML"
    },
    "storage/lab-bigquery-nyctaxi/README": {
      "id": "storage/lab-bigquery-nyctaxi/README",
      "title": "GCP BigQuery NYC Taxi",
      "description": "Objective"
    },
    "storage/lab-bigquery-optimization/README": {
      "id": "storage/lab-bigquery-optimization/README",
      "title": "BigQuery Optimization",
      "description": "Objective"
    },
    "storage/lab-bigquery-query-optimization/README": {
      "id": "storage/lab-bigquery-query-optimization/README",
      "title": "GCP Bigquery Query Optimization",
      "description": "Objective"
    },
    "storage/lab-biqeury-building-warehouse/README": {
      "id": "storage/lab-biqeury-building-warehouse/README",
      "title": "Building a BigQuery Data Warehouse",
      "description": "Introduction"
    },
    "storage/lab-couchdb-movies-data-migration/README": {
      "id": "storage/lab-couchdb-movies-data-migration/README",
      "title": "Couchdb Movies Data Migration",
      "description": "Objective"
    },
    "storage/lab-data-loading-python/README": {
      "id": "storage/lab-data-loading-python/README",
      "title": "Loading Data in Python",
      "description": "In this lab, we will learn how to load various file formats in Python."
    },
    "storage/lab-datalake-healthcare-s3-glue-athena/readme": {
      "id": "storage/lab-datalake-healthcare-s3-glue-athena/readme",
      "title": "Data Lake on S3",
      "description": "Process flow"
    },
    "storage/lab-db2-bookshop-petsale-data-ingestion/README": {
      "id": "storage/lab-db2-bookshop-petsale-data-ingestion/README",
      "title": "db2",
      "description": "db2 BookShop and PetSale Data Ingestion and Stored Procedure"
    },
    "storage/lab-gcp-cloudsql-nyctaxi/README": {
      "id": "storage/lab-gcp-cloudsql-nyctaxi/README",
      "title": "Loading Taxi Data into Google Cloud SQL",
      "description": "Objective"
    },
    "storage/lab-gcp-streaming-bigtable/README": {
      "id": "storage/lab-gcp-streaming-bigtable/README",
      "title": "Lab - GCP Streaming Bigtable",
      "description": "Objective"
    },
    "storage/lab-getting-started-with-cassandra/cassandra-cli": {
      "id": "storage/lab-getting-started-with-cassandra/cassandra-cli",
      "title": "Working with Cassandra and CLI",
      "description": "Cassandra and Shell"
    },
    "storage/lab-getting-started-with-cassandra/getting-started": {
      "id": "storage/lab-getting-started-with-cassandra/getting-started",
      "title": "Cassandra Getting Started",
      "description": "NoSQL data modeling and analysis with Apache Cassandra"
    },
    "storage/lab-glue-emr-iceberg-serverless-lakehouse/README": {
      "id": "storage/lab-glue-emr-iceberg-serverless-lakehouse/README",
      "title": "Build a serverless transactional data lake with Apache Iceberg, Amazon EMR Serverless, and Amazon Athena",
      "description": "Since the deluge of big data over a decade ago, many organizations have learned to build applications to process and analyze petabytes of data. Data lakes have served as a central repository to store structured and unstructured data at any scale and in various formats. However, as data processing at scale solutions grow, organizations need to build more and more features on top of their data lakes. One important feature is to run different workloads such as business intelligence (BI), Machine Learning (ML), Data Science and data exploration, and Change Data Capture (CDC) of transactional data, without having to maintain multiple copies of data. Additionally, the task of maintaining and managing files in the data lake can be tedious and sometimes complex."
    },
    "storage/lab-intro-to-dynamodb/README": {
      "id": "storage/lab-intro-to-dynamodb/README",
      "title": "Introduction to DynamoDB",
      "description": "Description"
    },
    "storage/lab-mongodb-basics/README": {
      "id": "storage/lab-mongodb-basics/README",
      "title": "Lab - MongoDB Basics",
      "description": "Environment Setup"
    },
    "storage/lab-mongodb-pandas/README": {
      "id": "storage/lab-mongodb-pandas/README",
      "title": "MongoDB to CSV conversion",
      "description": "Pull a noSQL data from MongoDB and convert into Pandas dataframe"
    },
    "storage/lab-mysql-data-ingestion/README": {
      "id": "storage/lab-mysql-data-ingestion/README",
      "title": "Lab: Data Ingestion to MySQL",
      "description": "Process Flow"
    },
    "storage/lab-mysql-police-api-etl/README": {
      "id": "storage/lab-mysql-police-api-etl/README",
      "title": "Police API Data Engineering Task",
      "description": "Using the Police UK API extracted the data related to street-level crime and outcome data and nearest police stations"
    },
    "storage/lab-postgres-bash-etl/README": {
      "id": "storage/lab-postgres-bash-etl/README",
      "title": "ETL with bash script",
      "description": "Objective"
    },
    "storage/lab-postgres-crime-reports/README": {
      "id": "storage/lab-postgres-crime-reports/README",
      "title": "Postgres Crime Reports",
      "description": "Building a Database for Crime Reports"
    },
    "storage/lab-postgres-getting-started/README": {
      "id": "storage/lab-postgres-getting-started/README",
      "title": "Getting Started with Postgres",
      "description": ""
    },
    "storage/lab-postgres-trips-stored-procedure/README": {
      "id": "storage/lab-postgres-trips-stored-procedure/README",
      "title": "Generate Trips Data using Stored Procedure",
      "description": "- Step 1 - Connect to the database"
    },
    "storage/lab-processing-json-data/README": {
      "id": "storage/lab-processing-json-data/README",
      "title": "JSON Data Processing",
      "description": "In this lab, we will learn how to process JSON data."
    },
    "storage/lab-production-cleaning-deltalake/README": {
      "id": "storage/lab-production-cleaning-deltalake/README",
      "title": "The Easy Ways to Clean Up Production Messes",
      "description": "When it comes to working with production data, messes are bound to happen. Whether it’s data inconsistency, schema errors, or other issues, cleaning up production messes can be a time-consuming and frustrating process. Fortunately, Delta Lake provides a powerful toolset for handling these types of issues, making it easy to fix production messes quickly and efficiently."
    },
    "storage/lab-read-s3-delta-in-python/README": {
      "id": "storage/lab-read-s3-delta-in-python/README",
      "title": "Lab: Read Delta Tables stored in Amazon S3 with Python",
      "description": ""
    },
    "storage/lab-redshift-data-loading": {
      "id": "storage/lab-redshift-data-loading",
      "title": "Data Loading into Redshift",
      "description": "1. Create 8 Tables based on TPCH data model in Redshift (DDL)"
    },
    "storage/lab-redshift-data-loading-analysis": {
      "id": "storage/lab-redshift-data-loading-analysis",
      "title": "Data Loading and Analysis",
      "description": "1. Load csv and json data from S3 into Redshift using COPY command"
    },
    "storage/lab-redshift-data-loading-python": {
      "id": "storage/lab-redshift-data-loading-python",
      "title": "Data Loading using Python",
      "description": "1. Connect to Redshift and Read data using Psycopg2 library"
    },
    "storage/lab-redshift-immersion/README": {
      "id": "storage/lab-redshift-immersion/README",
      "title": "Redshift Advanced",
      "description": "Data Load with Redshift Spectrum"
    },
    "storage/lab-redshift-ml/README": {
      "id": "storage/lab-redshift-ml/README",
      "title": "Amazon Redshift ML",
      "description": "In this Lab, we will Create, Train and Deploy Multi Layer Perceptron (MLP) models using Amazon Redshift ML."
    },
    "storage/lab-redshift-ongoing-load-elt": {
      "id": "storage/lab-redshift-ongoing-load-elt",
      "title": "Redshift Ongoing Load - ELT",
      "description": "This lab demonstrates how you can modernize your ongoing data loads using Stored Procedures, Materialized Views and Pre-defined Functions to transform data within Redshift."
    },
    "storage/lab-redshift-scd": {
      "id": "storage/lab-redshift-scd",
      "title": "Implement a slowly changing dimension in Redshift",
      "description": "1. Learn how to create a type 2 dimension table by adding slowly changing tracking columns"
    },
    "storage/lab-redshift-scd-2": {
      "id": "storage/lab-redshift-scd-2",
      "title": "Redshift Slowly Changing Dimension",
      "description": "Data loading into a SCD table involves a first-time bulk data loading, referred to as the initial data load. This is followed by continuous or regular data loading, referred to as an incremental data load, to keep the records up to date with changes in the source tables."
    },
    "storage/lab-redshift-spectrum-query-datalake": {
      "id": "storage/lab-redshift-spectrum-query-datalake",
      "title": "Redshift Spectrum Query Data Lake",
      "description": "In this lab, we show you how to query data in your Amazon S3 data lake with Amazon Redshift without loading or moving data. We will also demonstrate how you can leverage views which union data in Redshift Managed storage with data in S3. You can query structured and semi-structured data from files in Amazon S3 without having to copy or move data into Amazon Redshift tables."
    },
    "storage/lab-redshift-spectrum-query-tuning": {
      "id": "storage/lab-redshift-spectrum-query-tuning",
      "title": "Redshift Spectrum Query Tuning",
      "description": "In this lab, we show you how to diagnose your Redshift Spectrum query performance and optimize performance by leveraging partitions, optimizing storage, and predicate pushdown."
    },
    "storage/lab-redshift-table-design-query-tuning": {
      "id": "storage/lab-redshift-table-design-query-tuning",
      "title": "Redshift Table Design and Query Tuning",
      "description": "1. Setting distribution and sort keys"
    },
    "storage/lab-scd-glue-delta/README": {
      "id": "storage/lab-scd-glue-delta/README",
      "title": "Lab: Implement slowly changing dimensions in a data lake using AWS Glue and Delta",
      "description": "process-flow drawio"
    },
    "storage/lab-snowflake-getting-started/README": {
      "id": "storage/lab-snowflake-getting-started/README",
      "title": "Snowflake Getting Started",
      "description": "Connect to Snowflake using Python"
    },
    "storage/lab-snowflake-snowsql/README": {
      "id": "storage/lab-snowflake-snowsql/README",
      "title": "Snowflake SnowSQL",
      "description": "Objective"
    },
    "storage/lab-snowpark-churn/README": {
      "id": "storage/lab-snowpark-churn/README",
      "title": "Snowpark Churn",
      "description": "Links"
    },
    "storage/lab-sqlite-basics/README": {
      "id": "storage/lab-sqlite-basics/README",
      "title": "SQLite Basics",
      "description": "The following entity-relationship- (ER) diagram for the books database shows the database’s tables and the relationships among them:"
    },
    "storage/lab-sqlite-hipolabs-api/README": {
      "id": "storage/lab-sqlite-hipolabs-api/README",
      "title": "Edu Hipolabs API",
      "description": "Objective"
    },
    "storage/lab-zero-to-snowflake": {
      "id": "storage/lab-zero-to-snowflake",
      "title": "Getting Started with Snowflake - Zero to Snowflake",
      "description": "Follow this:"
    },
    "storage/mongodb": {
      "id": "storage/mongodb",
      "title": "MongoDB",
      "description": ""
    },
    "storage/mysql": {
      "id": "storage/mysql",
      "title": "MySQL",
      "description": ""
    },
    "storage/orc": {
      "id": "storage/orc",
      "title": "ORC",
      "description": "Optimized Row Columnar (ORC) is a columnar storage format similar to Parquet. ORC was very popular for use with Apache Hive; while still widely used, we generally see it much less than Apache Parquet, and it enjoys somewhat less support in modern cloud ecosystem tools. For example, Snowflake and BigQuery support Parquet file import and export; while they can read from ORC files, neither tool can export to ORC."
    },
    "storage/parquet": {
      "id": "storage/parquet",
      "title": "Parquet",
      "description": "Parquet stores data in a columnar format and is designed to realize excellent read and write performance in a data lake environment. Parquet solves a few problems that frequently bedevil data engineers. Parquet-encoded data builds in schema information and natively supports nested data, unlike CSV. Furthermore, Parquet is portable; while databases such as BigQuery and Snowflake serialize data in proprietary columnar formats and offer excellent query performance on data stored internally, a huge performance hit occurs when interoperating with external tools. Data must be deserialized, reserialized into an exchangeable format, and exported to use data lake tools such as Spark and Presto. Parquet files in a data lake may be a superior option to proprietary cloud data warehouses in a polyglot tool environment."
    },
    "storage/parquet-vs-csv": {
      "id": "storage/parquet-vs-csv",
      "title": "Parquet vs CSV",
      "description": "While CSV is simple and the most widely used data format (Excel, Google Sheets), there are several distinct advantages for Parquet, including:"
    },
    "storage/postgres": {
      "id": "storage/postgres",
      "title": "Postgres",
      "description": "Picking the right database management system is a difficult task due to the vast number of options on the market. Depending on the business model, you can pick a commercial database or an open source database with commercial support. In addition to this, there are several technical and non-technical factors to assess. When it comes to picking a relational database management system, PostgreSQL stands at the top for several reasons. The PostgreSQL slogan, \"The world's most advanced open source database,\" emphasizes the sophistication of its features and the high degree of community confidence."
    },
    "storage/postgres-vs-mysql": {
      "id": "storage/postgres-vs-mysql",
      "title": "Postgres vs MySQL",
      "description": "When it comes to choosing a relational database management system (RDBMS), two popular options are PostgreSQL and MySQL. Both have been around for decades and have proven to be highly reliable, secure, and scalable. However, they have different strengths and weaknesses that make one more suitable for certain use cases than the other."
    },
    "storage/project-athena-federated/README": {
      "id": "storage/project-athena-federated/README",
      "title": "Building Federated Query System using Amazon Athena",
      "description": "Activity 1: Athena Lab Environment Setup"
    },
    "storage/redshift": {
      "id": "storage/redshift",
      "title": "Amazon Redshift",
      "description": "Amazon Redshift is a data warehousing service optimized for online analytical processing (OLAP) applications. You can start with just a few hundred gigabytes (GB) of data and scale to a petabyte (PB) or more. Designing your database for analytical processing lets you take full advantage of Amazon Redshift's columnar architecture."
    },
    "storage/serialization": {
      "id": "storage/serialization",
      "title": "Serialization",
      "description": "Many serialization algorithms and formats are available to data engineers. While the abundance of options is a significant source of pain in data engineering, they are also a massive opportunity for performance improvements. We’ve sometimes seen job performance improve by a factor of 100 simply by switching from CSV to Parquet serialization. As data moves through a pipeline, engineers will also manage reserialization—conversion from one format to another. Sometimes data engineers have no choice but to accept data in an ancient, nasty form; they must design processes to deserialize this format and handle exceptions, and then clean up and convert data for consistent, fast downstream processing and consumption."
    },
    "storage/snowflake": {
      "id": "storage/snowflake",
      "title": "Snowflake",
      "description": "Snowflake is the Data Cloud that enables you to build data-intensive applications without operational burden, so you can focus on data and analytics instead of infrastructure management."
    },
    "storage/sqlite": {
      "id": "storage/sqlite",
      "title": "SQLite",
      "description": "SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. SQLite is the most used database engine in the world."
    },
    "storage/why-datalakes": {
      "id": "storage/why-datalakes",
      "title": "Why we need Data Lakes?",
      "description": "Before the cloud data lake architecture"
    },
    "storage/xml": {
      "id": "storage/xml",
      "title": "XML",
      "description": "Extensible Markup Language (XML) was popular when HTML and the internet were new, but it is now viewed as legacy; it is generally slow to deserialize and serialize for data engineering applications. XML is another standard that data engineers are often forced to interact with as they exchange data with legacy systems and software. JSON has largely replaced XML for plain-text object serialization."
    },
    "visualization/flask/README": {
      "id": "visualization/flask/README",
      "title": "Flask",
      "description": "Shazam API Song Analytics"
    },
    "visualization/looker-studio/lab-gcp-streaming-analytics": {
      "id": "visualization/looker-studio/lab-gcp-streaming-analytics",
      "title": "Streaming Analytics and Dashboards",
      "description": "Objective"
    },
    "visualization/looker-studio/README": {
      "id": "visualization/looker-studio/README",
      "title": "Looker Studio",
      "description": "Labs"
    },
    "visualization/preset/README": {
      "id": "visualization/preset/README",
      "title": "Setup Preset",
      "description": "Go to https://preset.io/ and Click on “Start for free” button at the top-right corner. Follow the instructions to create a free account there. You can skip the database connection part, we will connect to the database later."
    },
    "visualization/README": {
      "id": "visualization/README",
      "title": "Visualization",
      "description": "Steps"
    },
    "visualization/streamlit/README": {
      "id": "visualization/streamlit/README",
      "title": "Streamlit",
      "description": "Amazon Fashion Recommender"
    }
  }
}