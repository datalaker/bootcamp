{
  "unversionedId": "foundations/programming-languages/pyspark/lab-spark-optimizations/README",
  "id": "foundations/programming-languages/pyspark/lab-spark-optimizations/README",
  "title": "Spark Optimizations for Analytics Workloads",
  "description": "Optimizations in Apache Spark play a crucial role while building big data solutions. Knowledge and experience in tuning Spark-based workloads help organizations save costs and time while running these workloads on the cloud. In this lab, we will learn about various optimization techniques concerning Spark DataFrames and big data analytics in general. We will learn about the limitations of the collect() method and inferSchema when reading data. This will be followed by an overview of the best practices for working with CSV files, Parquet files, Pandas projects, and Koalas projects. Also, we will learn about some powerful optimization techniques, such as column predicate pushdown, column pruning, and partitioning strategies.",
  "source": "@site/docs/01-foundations/04-programming-languages/pyspark/lab-spark-optimizations/README.md",
  "sourceDirName": "01-foundations/04-programming-languages/pyspark/lab-spark-optimizations",
  "slug": "/foundations/programming-languages/pyspark/lab-spark-optimizations/",
  "permalink": "/docs/foundations/programming-languages/pyspark/lab-spark-optimizations/",
  "draft": false,
  "tags": [],
  "version": "current",
  "lastUpdatedBy": "sparsh",
  "lastUpdatedAt": 1681047270,
  "formattedLastUpdatedAt": "Apr 9, 2023",
  "frontMatter": {},
  "sidebar": "tutorialSidebar",
  "previous": {
    "title": "Pyspark NYC Taxi",
    "permalink": "/docs/foundations/programming-languages/pyspark/lab-pyspark-nyctaxi/"
  },
  "next": {
    "title": "Spark Optimizations",
    "permalink": "/docs/foundations/programming-languages/pyspark/lab-spark-optimizations-2/"
  }
}