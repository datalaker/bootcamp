{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupBy() and Filtering Aggregated Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupBy()` is a transformation operation in PySpark that is used to group the data in a Spark DataFrame or RDD based on one or more specified columns. It returns a GroupedData object which can then be used to perform aggregation operations such as `count()`, `sum()`, `avg()`, etc. on the grouped data.\n",
    "\n",
    "The `groupBy()` operation is commonly used for tasks such as calculating statistics for different categories, grouping data by time intervals, and creating pivot tables.\n",
    "\n",
    "For example, if you have a DataFrame with columns \"category\" and \"value\", you can use `groupBy(\"category\")` to group the data by category and then apply an aggregation function such as `sum(\"value\")` to calculate the total value for each category. This would give you a new DataFrame with one row for each category and the total value for that category.\n",
    "\n",
    "Here are a couple of examples with syntax for `groupBy()` in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, avg, max, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - Grouping data by a single column and counting the number of occurrences for each value in that column --- `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| fruit|\n",
      "+------+\n",
      "| apple|\n",
      "|banana|\n",
      "| apple|\n",
      "|orange|\n",
      "|banana|\n",
      "|banana|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with a single column \"fruit\" and several rows\n",
    "data = [(\"apple\",), (\"banana\",), (\"apple\",), (\"orange\",), (\"banana\",), (\"banana\",)]\n",
    "df = spark.createDataFrame(data, [\"fruit\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| fruit|count|\n",
      "+------+-----+\n",
      "| apple|    2|\n",
      "|banana|    3|\n",
      "|orange|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group the data by the \"fruit\" column and count the number of occurrences for each fruit\n",
    "grouped_df = df.groupBy(\"fruit\").count()\n",
    "\n",
    "# show the results\n",
    "grouped_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Grouping data by multiple columns and computing the average value for each group - `avg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         city|temperature|\n",
      "+-------------+-----------+\n",
      "|     New York|       10.0|\n",
      "|     New York|       12.0|\n",
      "|  Los Angeles|       20.0|\n",
      "|  Los Angeles|       22.0|\n",
      "|San Francisco|       15.0|\n",
      "|San Francisco|       18.0|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with columns \"city\" and \"temperature\"\n",
    "data = [(\"New York\", 10.0), (\"New York\", 12.0),\n",
    "        (\"Los Angeles\", 20.0), (\"Los Angeles\", 22.0),\n",
    "        (\"San Francisco\", 15.0), (\"San Francisco\", 18.0)]\n",
    "df = spark.createDataFrame(data, [\"city\", \"temperature\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+\n",
      "|         city|avg_temperature|\n",
      "+-------------+---------------+\n",
      "|     New York|           11.0|\n",
      "|  Los Angeles|           21.0|\n",
      "|San Francisco|           16.5|\n",
      "+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group the data by the \"city\" column and compute the average temperature for each city\n",
    "grouped_df = df.groupBy(\"city\").agg(avg(\"temperature\").alias(\"avg_temperature\"))\n",
    "\n",
    "# show the results\n",
    "grouped_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - Grouping data by multiple columns and computing the maximum value of another column - `max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+-----------+\n",
      "|         city|year|temperature|\n",
      "+-------------+----+-----------+\n",
      "|     New York|2021|       10.0|\n",
      "|     New York|2022|       12.0|\n",
      "|  Los Angeles|2021|       20.0|\n",
      "|  Los Angeles|2022|       22.0|\n",
      "|San Francisco|2021|       15.0|\n",
      "|San Francisco|2022|       18.0|\n",
      "+-------------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with columns \"city\", \"year\", and \"temperature\"\n",
    "data = [(\"New York\", 2021, 10.0), (\"New York\", 2022, 12.0),\n",
    "        (\"Los Angeles\", 2021, 20.0), (\"Los Angeles\", 2022, 22.0),\n",
    "        (\"San Francisco\", 2021, 15.0), (\"San Francisco\", 2022, 18.0)]\n",
    "df = spark.createDataFrame(data, [\"city\", \"year\", \"temperature\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---------------+\n",
      "|         city|year|max_temperature|\n",
      "+-------------+----+---------------+\n",
      "|     New York|2021|           10.0|\n",
      "|  Los Angeles|2021|           20.0|\n",
      "|     New York|2022|           12.0|\n",
      "|  Los Angeles|2022|           22.0|\n",
      "|San Francisco|2021|           15.0|\n",
      "|San Francisco|2022|           18.0|\n",
      "+-------------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group the data by the \"city\" and \"year\" columns and compute the maximum temperature for each group\n",
    "grouped_df = df.groupBy([\"city\", \"year\"]).agg(max(\"temperature\").alias(\"max_temperature\"))\n",
    "\n",
    "# show the results\n",
    "grouped_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4 - Grouping data by a column and computing multiple aggregate functions for another column - `avg()`, `sum()`, `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         city|temperature|\n",
      "+-------------+-----------+\n",
      "|     New York|       10.0|\n",
      "|     New York|       12.0|\n",
      "|  Los Angeles|       20.0|\n",
      "|  Los Angeles|       22.0|\n",
      "|San Francisco|       15.0|\n",
      "|San Francisco|       18.0|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with columns \"city\" and \"temperature\"\n",
    "data = [(\"New York\", 10.0), (\"New York\", 12.0),\n",
    "        (\"Los Angeles\", 20.0), (\"Los Angeles\", 22.0),\n",
    "        (\"San Francisco\", 15.0), (\"San Francisco\", 18.0)]\n",
    "df = spark.createDataFrame(data, [\"city\", \"temperature\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----------------+----------------+\n",
      "|         city|avg_temperature|total_temperature|num_measurements|\n",
      "+-------------+---------------+-----------------+----------------+\n",
      "|     New York|           11.0|             22.0|               2|\n",
      "|  Los Angeles|           21.0|             42.0|               2|\n",
      "|San Francisco|           16.5|             33.0|               2|\n",
      "+-------------+---------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group the data by the \"city\" column and compute the average, sum, and count of the temperature for each city\n",
    "grouped_df = df.groupBy(\"city\").agg(avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "                                     sum(\"temperature\").alias(\"total_temperature\"),\n",
    "                                     count(\"temperature\").alias(\"num_measurements\"))\n",
    "\n",
    "# show the results\n",
    "grouped_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5 - Filtering groups based on an aggregate condition - `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|department|salary|\n",
      "+----------+------+\n",
      "|        IT|  5000|\n",
      "|        IT|  6000|\n",
      "|     Sales|  7000|\n",
      "|     Sales|  8000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with columns \"department\" and \"salary\"\n",
    "data = [(\"IT\", 5000), (\"IT\", 6000), (\"Sales\", 7000), (\"Sales\", 8000)]\n",
    "df = spark.createDataFrame(data, [\"department\", \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|department|avg_salary|\n",
      "+----------+----------+\n",
      "|     Sales|    7500.0|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group the data by the \"department\" column and compute the average salary for each department\n",
    "grouped_df = df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg_salary\"))\n",
    "\n",
    "# filter the groups to include only those where the average salary is greater than 6000\n",
    "filtered_df = grouped_df.filter(\"avg_salary > 6000\")\n",
    "\n",
    "# show the results\n",
    "filtered_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6 - Filtering groups based on the count of rows in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|department|salary|\n",
      "+----------+------+\n",
      "|        IT|  5000|\n",
      "|        IT|  6000|\n",
      "|     Sales|  7000|\n",
      "|     Sales|  8000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with columns \"department\" and \"salary\"\n",
    "data = [(\"IT\", 5000), (\"IT\", 6000), (\"Sales\", 7000), (\"Sales\", 8000)]\n",
    "df = spark.createDataFrame(data, [\"department\", \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|department|num_employees|\n",
      "+----------+-------------+\n",
      "|        IT|            2|\n",
      "|     Sales|            2|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group the data by the \"department\" column and compute the count of rows in each group\n",
    "grouped_df = df.groupBy(\"department\").agg(count(\"*\").alias(\"num_employees\"))\n",
    "\n",
    "# filter the groups to include only those with more than one employee\n",
    "filtered_df = grouped_df.filter(\"num_employees > 1\")\n",
    "\n",
    "# show the results\n",
    "filtered_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe has covered the usage of `groupBy()` and aggregate functions on a Spark DataFrame. Additionally, it has demonstrated how to apply these operations to multiple columns and finally how to filter the data based on the aggregated column. By following the examples provided in this recipe, you can gain a better understanding of how to leverage these features in your own PySpark projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
