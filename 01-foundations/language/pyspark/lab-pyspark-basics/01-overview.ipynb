{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from various sources using spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark provides a range of APIs and libraries that can be used to extract data from various sources for ETL (Extract, Transform, Load) processes. Some common sources of data that can be accessed using Spark include the following."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can read data from flat files such as CSV, JSON, and text files using the spark.read.format() method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from a CSV file\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/path/to/file.csv\")\n",
    "\n",
    "#read data from a JSON file\n",
    "df = spark.read.format(\"json\").load(\"/path/to/file.json\")\n",
    "\n",
    "#read data from a text file\n",
    "df = spark.read.text(\"/path/to/file.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can read data from relational databases using JDBC drivers. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from a MySQL database\n",
    "jdbc_url = \"jdbc:mysql://hostname:port/database\"\\\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").load()\n",
    "\n",
    "#read data from a PostgreSQL database\n",
    "jdbc_url = \"jdbc:postgresql://hostname:port/database\"\\\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NoSQL databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can read data from NoSQL databases such as MongoDB and Cassandra using their respective connectors. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from a MongoDB collection\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://hostname:port/database.collection\").load()\n",
    "\n",
    "#read data from a Cassandra table\n",
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\").option(\"keyspace\", \"keyspace_name\").option(\"table\", \"table_name\").load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web APIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can read data from web APIs using the requests library. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#send a GET request to a web API\n",
    "response = requests.get(\"https://api.example.com/endpoint\")\n",
    "\n",
    "#convert the response to a Spark DataFrame\n",
    "df = spark.read.json(response.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can read data from cloud storage services such as Amazon S3, Google Cloud Storage, and Azure Blob Storage. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from an Amazon S3 bucket\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://bucket_name/path/to/file.csv\")\n",
    "\n",
    "#read data from a Google Cloud Storage bucket\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"gs://bucket_name/path/to/file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using XML files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is a powerful open-source data processing framework that can be used for ETL (extract, transform, and load) tasks involving XML files. To process XML files with Spark, you can use the built-in XML processing library, called spark-xml, which provides a DataFrame API for working with XML data.\n",
    "\n",
    "Here is an example of how to use Spark to read an XML file and convert it to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\\\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#create a SparkSession\\\n",
    "spark = SparkSession.builder.appName(\"XML Processing\").getOrCreate()\n",
    "\n",
    "#read an XML file\\\n",
    "df = spark.read.format(\"xml\").option(\"rowTag\", \"book\").load(\"books.xml\")\n",
    "\n",
    "#print the schema of the DataFrame\\\n",
    "df.printSchema()\n",
    "\n",
    "#show the first 20 rows of the DataFrame\\\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example assumes that the XML file is structured like this:\n",
    "\n",
    "```xml\n",
    "<books>\\\n",
    "    <book>\\\n",
    "        <title>The Great Gatsby</title>\\\n",
    "        <author>F. Scott Fitzgerald</author>\\\n",
    "        <year>1925</year>\\\n",
    "        <price>9.99</price>\\\n",
    "    </book>\\\n",
    "    <book>\\\n",
    "        <title>Moby-Dick</title>\\\n",
    "        <author>Herman Melville</author>\\\n",
    "        <year>1851</year>\\\n",
    "        <price>14.99</price>\\\n",
    "    </book>\\\n",
    "    ...\\\n",
    "</books>\n",
    "```\n",
    "\n",
    "Once you have the DataFrame, you can perform various data transformation and data cleaning operations using the DataFrame API and SQL queries. Once you finish cleaning and transform data, you can save the data back to any data store of your choosing (such as parquet,json,csv)\n",
    "\n",
    "You can also use spark-xml library to write the dataframe back to the XML file. The syntax is similar to the above read process, but you would use the write function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"xml\").option(\"rootTag\",\"books\").save(\"newBooks.xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data into suitable format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have extracted data from various sources using Apache Spark, the next step in the ETL (Extract, Transform, Load) process is to transform the data into a suitable format for your needs. Spark provides a range of APIs and libraries that can be used to transform data, including the following."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrames provide a range of methods for transforming data, such as select, filter, groupBy, join, sort, and withColumn. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select specific columns\n",
    "df = df.select(\"col1\", \"col2\")\n",
    "\n",
    "#filter rows based on a condition\n",
    "df = df.filter(df[\"col1\"] > 5)\n",
    "\n",
    "#group rows by a column and compute aggregates\n",
    "df = df.groupBy(\"col1\").agg({\"col2\": \"mean\"})\n",
    "\n",
    "#join two DataFrames on a common column\n",
    "df = df1.join(df2, df1[\"col1\"] == df2[\"col1\"], \"inner\")\n",
    "\n",
    "#sort rows by a column\n",
    "df = df.sort(\"col1\", \"asc\")\n",
    "\n",
    "#add a new column based on existing columns\n",
    "df = df.withColumn(\"new_col\", df[\"col1\"] + df[\"col2\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyspark.sql.functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pyspark.sql.functions module provides a range of functions for transforming data, such as lower, upper, trim, substring, date_format, and when. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper, trim, substring, date_format, when\n",
    "\n",
    "#convert a column to lowercase\n",
    "df = df.withColumn(\"col1\", lower(df[\"col1\"]))\n",
    "\n",
    "#convert a column to uppercase\n",
    "df = df.withColumn(\"col1\", upper(df[\"col1\"]))\n",
    "\n",
    "#trim leading and trailing whitespace from a column\n",
    "df = df.withColumn(\"col1\", trim(df[\"col1\"]))\n",
    "\n",
    "#extract a substring from a column\n",
    "df = df.withColumn(\"col1\", substring(df[\"col1\"], 1, 3))\n",
    "\n",
    "#format a date column\n",
    "df = df.withColumn(\"col1\", date_format(df[\"col1\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "#add a new column based on a conditional expression\n",
    "df = df.withColumn(\"new_col\", when(df[\"col1\"] > 5, 1).otherwise(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined functions (UDFs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define your own functions using Python or Scala and use them to transform data in Spark. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a Python function\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "#register the function as a UDF\n",
    "from pyspark.sql.functions import udf\n",
    "add_one_udf = udf(add_one)\n",
    "\n",
    "#use the UDF to transform a column\n",
    "df = df.withColumn(\"col1\", add_one_udf(df[\"col1\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, you can use a range of APIs and libraries in Apache Spark to transform data into a suitable format for your needs, including DataFrame operations, pyspark.sql.functions, and user-defined functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a target system using Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have transformed the data using Apache Spark, the final step in the ETL (Extract, Transform, Load) process is to load it into a target system for further analysis and processing. Spark provides a range of APIs and libraries that can be used to load data into various target systems, including the following."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can write data to flat files such as CSV, JSON, and text files using the df.write.format() method. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to a CSV file\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"/path/to/file.csv\")\n",
    "\n",
    "#write data to a JSON file\n",
    "df.write.format(\"json\").save(\"/path/to/file.json\")\n",
    "\n",
    "#write data to a text file\n",
    "df.write.text(\"/path/to/file.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can write data to relational databases using JDBC drivers. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to a MySQL database\n",
    "jdbc_url = \"jdbc:mysql://hostname:port/database\"\\\n",
    "df.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").save()\n",
    "\n",
    "#write data to a PostgreSQL database\n",
    "jdbc_url = \"jdbc:postgresql://hostname:port/database\"\\\n",
    "df.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NoSQL databases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can write data to NoSQL databases such as MongoDB and Cassandra using their respective connectors. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to a MongoDB collection\n",
    "df.write.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", \"mongodb://hostname:port/database.collection\").save()\n",
    "\n",
    "#write data to a Cassandra table\n",
    "df.write.format(\"org.apache.spark.sql.cassandra\").option(\"keyspace\", \"keyspace_name\").option(\"table\", \"table_name\").save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud storage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can write data to cloud storage services such as Amazon S3, Google Cloud Storage, and Azure Blob Storage. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write data to an Amazon S3 bucket\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"s3a://bucket_name/path/to/file.csv\")\n",
    "\n",
    "#write data to a Google Cloud Storage bucket\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"gs://bucket_name/path/to/file.csv\")\n",
    "\n",
    "#write data to an Azure Blob Storage container\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"wasb://container_name@storage_account.blob.core.windows.net/path/to/file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, you can use a range of APIs and libraries in Apache Spark to load data into various target systems, including flat files, relational databases, NoSQL databases, and cloud storage services."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example using Pyspark for ETL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a code example in Pyspark that shows how to use Apache Spark for ETL (Extract, Transform, Load) processes using a PostgreSQL database as the data source and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "#read data from a PostgreSQL database\n",
    "jdbc_url = \"jdbc:postgresql://hostname:port/database\"\n",
    "\n",
    "df = spark.read.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").load()\n",
    "\n",
    "#transform the data\n",
    "df = df.filter(df[\"col1\"] > 5)\n",
    "\n",
    "df = df.withColumn(\"col2\", df[\"col2\"].upper())\n",
    "\n",
    "df = df.sort(\"col3\", \"asc\")\n",
    "\n",
    "#write data to a PostgreSQL database\n",
    "jdbc_url = \"jdbc:postgresql://hostname:port/database\"\n",
    "\n",
    "df.write.format(\"jdbc\").option(\"url\", jdbc_url).option(\"dbtable\", \"table_name\").option(\"user\", \"username\").option(\"password\", \"password\").save()\n",
    "\n",
    "#stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first create a SparkSession, which is used to create a connection to the Spark cluster. Then, we read data from a PostgreSQL database using the `spark.read.format(\"jdbc\")` method and the PostgreSQL JDBC driver.\n",
    "\n",
    "Next, we transform the data using a series of `DataFrame` operations, such as `filter`, `withColumn`, and `sort`. Finally, we write the transformed data back to a PostgreSQL database using the `df.write.format(\"jdbc\")` method and the PostgreSQL JDBC driver.\n",
    "\n",
    "This is just one example of how you can use Apache Spark for ETL tasks involving a PostgreSQL database. You can customize the code to fit your specific needs, such as adding more transformation steps or specifying different options for reading and writing data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
