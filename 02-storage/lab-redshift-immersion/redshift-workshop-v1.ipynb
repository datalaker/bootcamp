{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZf2xdtzAYWV"
      },
      "source": [
        "### Create Cloudformation Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDgPfcNWAmHG",
        "outputId": "28f9f32b-2793-431f-fefc-a1d90f89bac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"StackId\": \"arn:aws:cloudformation:us-east-1:684199068947:stack/RedshiftWorkshop/2940aec0-6c03-11ed-bb3f-0e972e325d17\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!aws cloudformation create-stack \\\n",
        "--stack-name RedshiftWorkshop \\\n",
        "--template-body file://redshift_workshop.yml \\\n",
        "--capabilities CAPABILITY_NAMED_IAM \\\n",
        "--parameters \\\n",
        "ParameterKey=EETeamRoleArn,ParameterValue=arn:aws:iam::684199068947:user/sparsh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVfVg1miLgU"
      },
      "source": [
        "### Plan for the Future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEzoNQoGiJKD"
      },
      "source": [
        "In this final part of this lab, we will compare different strategies for maintaining more recent or HOT data within Redshift direct-attached storage, and keeping older COLD data in S3 by performing the following steps:\n",
        "\n",
        "- Allow for trailing 5 quarters reporting by adding the Q4 2015 data to Redshift DAS:\n",
        "    - Anticipating that we’ll want to ”age-off” the oldest quarter on a 3 month basis, architect your DAS table to make this easy to maintain and query.\n",
        "    - Adjust your Redshift Spectrum table to exclude the Q4 2015 data.\n",
        "- Develop and execute a plan to move the Q4 2015 data to S3.\n",
        "    - What are the discrete steps to be performed?\n",
        "    - What extra-Redshift functionality must be leveraged?\n",
        "    - Simulating the extra-Redshift steps with the existing Parquet data, age-off the Q4 2015 data from Redshift DAS and perform any needed steps to maintain a single version of the truth.\n",
        "- There are several options to accomplish this goal. Anticipating that we’ll want to ”age-off” the oldest quarter on a 3 month basis, architect your DAS table to make this easy to maintain and query. How about something like this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tju666zG5jyu"
      },
      "source": [
        "### Query scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmJHhcOO5ig5"
      },
      "source": [
        "Amazon Redshift allows you to schedule your SQL queries for executions in recurring schedules. You can now schedule time sensitive or long running queries, loading or unloading your data, stored procedures or refreshing your materialized views on a regular schedule. You can use the Amazon Redshift Console or Amazon Redshift Data API to schedule your SQL queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcIThqzj5qPg"
      },
      "source": [
        "1. Navigate, back to Redshift query editor, ensure the query to call your stored procedure is in the editor and click on the Schedule button.\n",
        "1. Select IAM Role, select the cluster, and provide a database name and database user.\n",
        "1. Enter a query name as well as the query text.\n",
        "1. Provide values for Repeat By, Repeat every, and Repeat time. When you select “Repeat at time (UTC)” enter a time that is little later than current time so you can observe the execution. Optionally, you can enable monitoring via Amazon SNS notifications. For this example, we can leave this Disabled.\n",
        "1. Navigate to the scheduled queries tab and you can see your query scheduler has been created.\n",
        "1. Click on the schedule and after successful execution on scheduled time, you can see the status is “success”.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        "- https://catalog.us-east-1.prod.workshops.aws/workshops/9f29cdba-66c0-445e-8cbb-28a092cb5ba7/en-US"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \n[Clang 6.0 (clang-600.0.57)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
