{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Deploy solution resources with AWS CloudFormation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the stack by running the below cli command. It will create:\n",
    "\n",
    "- Two S3 buckets: one for scripts and query results, and one for the data lake storage\n",
    "- An Athena workgroup\n",
    "- An EMR Serverless application\n",
    "- An AWS Glue database and tables on external public S3 buckets of TPC-DS data\n",
    "- An AWS Glue database for the data lake\n",
    "- An [AWS Identity and Access Management](http://aws.amazon.com/iam) (IAM) role and polices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"StackId\": \"arn:aws:cloudformation:us-east-1:684199068947:stack/DataLakehouse/a6dd6700-c014-11ed-b4c4-0ae42d5d0cc1\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws cloudformation create-stack \\\n",
    "--stack-name DataLakehouse \\\n",
    "--template-body file://cfn.yml \\\n",
    "--capabilities CAPABILITY_NAMED_IAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the stack creation is complete, check the **Outputs** tab of the stack to verify the resources created.\n",
    "\n",
    "![cfn](https://user-images.githubusercontent.com/62965911/224493013-44176a92-7986-498f-9b59-3186022493f8.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Upload Spark scripts to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'684199068947'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AWS_ACCOUNT_ID = !aws sts get-caller-identity --query \"Account\" --output text\n",
    "AWS_ACCOUNT_ID = AWS_ACCOUNT_ID[0]\n",
    "AWS_ACCOUNT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ingest-iceberg.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ingest-iceberg.py\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.types import DoubleType, FloatType, LongType, StructType, StructField, StringType\n",
    "\n",
    "from pyspark.sql.functions import col, lit\n",
    "#from datetime import datetime\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()    \n",
    "\n",
    "\n",
    "#Variables \n",
    "DB_NAME = \"datalake\"\n",
    "TABLE_NAME_CUSTOMER = \"customer_iceberg\"\n",
    "TABLE_NAME_WEBSALES = \"web_sales_iceberg\"\n",
    "TABLE_NAME_CADRESS = \"customer_address_iceberg\"\n",
    "TABLE_NAME_DATEDIM = \"date_dim_iceberg\"\n",
    "TABLE_NAME_HOUSEHOLD = \"household_demographics_iceberg\"\n",
    "TABLE_NAME_INCOMEBAND = \"income_band_iceberg\"\n",
    "TABLE_NAME_ITEM = \"item_iceberg\"\n",
    "TABLE_NAME_PROMOTION = \"promotion_iceberg\"\n",
    "TABLE_NAME_TIMEDIM = \"time_dim_iceberg\"\n",
    "TABLE_NAME_WEBPAGE = \"web_page_iceberg\"\n",
    "TABLE_NAME_WEBRETURNS =  \"web_returns_iceberg\"\n",
    "TPC_DS_DATABASE = \"tpc-source\"\n",
    "\n",
    "\n",
    "#Create the customer table in Iceberg \n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_CUSTOMER}`(\n",
    "        c_customer_sk             int,\n",
    "        c_customer_id             string,\n",
    "        c_current_cdemo_sk        int,\n",
    "        c_current_hdemo_sk        int,\n",
    "        c_current_addr_sk         int,\n",
    "        c_first_shipto_date_sk    int,\n",
    "        c_first_sales_date_sk     int,\n",
    "        c_salutation              string,\n",
    "        c_first_name              string,\n",
    "        c_last_name               string,\n",
    "        c_preferred_cust_flag     string,\n",
    "        c_birth_day               int,\n",
    "        c_birth_month             int,\n",
    "        c_birth_year              int,\n",
    "        c_birth_country           string,\n",
    "        c_login                   string,\n",
    "        c_email_address           string,\n",
    "        c_last_review_date        string\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (c_birth_country)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#Insert data into customer table\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_CUSTOMER}` \n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.customer\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "#Create the websales table in Iceberg  \n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_WEBSALES}`(\n",
    "        ws_sold_date_sk           int,\n",
    "        ws_sold_time_sk           int,\n",
    "        ws_ship_date_sk           int,\n",
    "        ws_item_sk                int,\n",
    "        ws_bill_customer_sk       int,\n",
    "        ws_bill_cdemo_sk          int,\n",
    "        ws_bill_hdemo_sk          int,\n",
    "        ws_bill_addr_sk           int,\n",
    "        ws_ship_customer_sk       int,\n",
    "        ws_ship_cdemo_sk          int,\n",
    "        ws_ship_hdemo_sk          int,\n",
    "        ws_ship_addr_sk           int,\n",
    "        ws_web_page_sk            int,\n",
    "        ws_web_site_sk            int,\n",
    "        ws_ship_mode_sk           int,\n",
    "        ws_warehouse_sk           int,\n",
    "        ws_promo_sk               int,\n",
    "        ws_order_number           int,\n",
    "        ws_quantity               int,\n",
    "        ws_wholesale_cost         double,\n",
    "        ws_list_price             double,\n",
    "        ws_sales_price            double,\n",
    "        ws_ext_discount_amt       double,\n",
    "        ws_ext_sales_price        double,\n",
    "        ws_ext_wholesale_cost     double,\n",
    "        ws_ext_list_price         double,\n",
    "        ws_ext_tax                double,\n",
    "        ws_coupon_amt             double,\n",
    "        ws_ext_ship_cost          double,\n",
    "        ws_net_paid               double,\n",
    "        ws_net_paid_inc_tax       double,\n",
    "        ws_net_paid_inc_ship      double,\n",
    "        ws_net_paid_inc_ship_tax  double,\n",
    "        ws_net_profit             double\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (ws_warehouse_sk)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#Insert data into websales table\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_WEBSALES}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.web_sales\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "#Create the customer adresse table customer_address\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_CADRESS}`(\n",
    "        ca_address_sk          int,\n",
    "        ca_address_id          string,\n",
    "        ca_street_number       string,\n",
    "        ca_street_name         string,\n",
    "        ca_street_type         string,\n",
    "        ca_suite_number        string,\n",
    "        ca_city                string,\n",
    "        ca_county              string,\n",
    "        ca_state               string,\n",
    "        ca_zip                 string,\n",
    "        ca_country             string,\n",
    "        ca_gmt_offset          float,\n",
    "        ca_location_type       string\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (ca_country, ca_city)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#Insert data into customer address table\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_CADRESS}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.customer_address\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "#Create the customer adresse table date_dim\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_DATEDIM}`(\n",
    "        d_date_sk              int,\n",
    "        d_date_id              string,\n",
    "        d_date                 date,\n",
    "        d_month_seq            int,\n",
    "        d_week_seq             int,\n",
    "        d_quarter_seq          int,\n",
    "        d_year                 int,\n",
    "        d_dow                  int,\n",
    "        d_moy                  int,\n",
    "        d_dom                  int,\n",
    "        d_qoy                  int,\n",
    "        d_fy_year              int,\n",
    "        d_fy_quarter_seq       int,\n",
    "        d_fy_week_seq          int,\n",
    "        d_day_name             string,\n",
    "        d_quarter_name         string,\n",
    "        d_holiday              string,\n",
    "        d_weekend              string,\n",
    "        d_following_holiday    string,\n",
    "        d_first_dom            int,\n",
    "        d_last_dom             int,\n",
    "        d_same_day_ly          int,\n",
    "        d_same_day_lq          int,\n",
    "        d_current_day          string,\n",
    "        d_current_week         string,\n",
    "        d_current_month        string,\n",
    "        d_current_quarter      string,\n",
    "        d_current_year         string\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (d_year)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#insert date_dim data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_DATEDIM}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.date_dim\n",
    "    \"\"\")\n",
    "\n",
    "#Create the table household_demographics\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_HOUSEHOLD}`(\n",
    "        hd_demo_sk             int,\n",
    "        hd_income_band_sk      int,\n",
    "        hd_buy_potential       string,\n",
    "        hd_dep_count           int,\n",
    "        hd_vehicle_count       int\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (hd_buy_potential)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#insert household_demographic data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_HOUSEHOLD}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.household_demographics\n",
    "    \"\"\")    \n",
    "\n",
    "\n",
    "#Create the table income_band\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_INCOMEBAND}`(   \n",
    "        ib_income_band_sk      int,\n",
    "        ib_lower_bound         int,\n",
    "        ib_upper_bound         int\n",
    "    )\n",
    "    USING iceberg\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#insert household_demographic data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_INCOMEBAND}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.income_band\n",
    "    \"\"\")    \n",
    "\n",
    "\n",
    "\n",
    "#Create table item\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_ITEM}`(   \n",
    "        i_item_sk              int,\n",
    "        i_item_id              string,\n",
    "        i_rec_start_date       string,\n",
    "        i_rec_end_date         string,\n",
    "        i_item_desc            string,\n",
    "        i_current_price        double,\n",
    "        i_wholesale_cost       double,\n",
    "        i_brand_id             int,\n",
    "        i_brand                string,\n",
    "        i_class_id             int,\n",
    "        i_class                string,\n",
    "        i_category_id          int,\n",
    "        i_category             string,\n",
    "        i_manufact_id          int,\n",
    "        i_manufact             string,\n",
    "        i_size                 string,\n",
    "        i_formulation          string,\n",
    "        i_color                string,\n",
    "        i_units                string,\n",
    "        i_container            string,\n",
    "        i_manager_id           string,\n",
    "        i_product_name         string\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (i_category)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#insert item data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_ITEM}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.item\n",
    "    \"\"\")   \n",
    "\n",
    "#Create the promotion table promotion\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_PROMOTION}`(   \n",
    "        p_promo_sk             int,\n",
    "        p_promo_id             string,\n",
    "        p_start_date_sk        int,\n",
    "        p_end_date_sk          int,\n",
    "        p_item_sk              int,\n",
    "        p_cost                 double,\n",
    "        p_response_target      int,\n",
    "        p_promo_name           string,\n",
    "        p_channel_dmail        string,\n",
    "        p_channel_email        string,\n",
    "        p_channel_catalog      string,\n",
    "        p_channel_tv           string,\n",
    "        p_channel_radio        string,\n",
    "        p_channel_press        string,\n",
    "        p_channel_event        string,\n",
    "        p_channel_demo         string,\n",
    "        p_channel_details      string,\n",
    "        p_purpose              string,\n",
    "        p_discount_active      string\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (p_purpose)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#insert promotion data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_PROMOTION}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.promotion\n",
    "    \"\"\")   \n",
    "\n",
    "\n",
    "#Create the promotion table time_dim\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_TIMEDIM}`(   \n",
    "        t_time_sk              int,\n",
    "        t_time_id              string,\n",
    "        t_time                 int,\n",
    "        t_hour                 int,\n",
    "        t_minute               int,\n",
    "        t_second               int,\n",
    "        t_am_pm                string,\n",
    "        t_shift                string,\n",
    "        t_sub_shift            string,\n",
    "        t_meal_time            string\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (t_hour)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "#insert time_dim data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_TIMEDIM}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.time_dim\n",
    "    \"\"\")   \n",
    "\n",
    "\n",
    "#Create the promotion table web_page\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_WEBPAGE}`(   \n",
    "        wp_web_page_sk         int,\n",
    "        wp_web_page_id         string,\n",
    "        wp_rec_start_date      string,\n",
    "        wp_rec_end_date        string,\n",
    "        wp_creation_date_sk    int,\n",
    "        wp_access_date_sk      int,\n",
    "        wp_autogen_flag        string,\n",
    "        wp_customer_sk         int,\n",
    "        wp_url                 string,\n",
    "        wp_type                string,\n",
    "        wp_char_count          int,\n",
    "        wp_link_count          int,\n",
    "        wp_image_count         int,\n",
    "        wp_max_ad_count        int\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (wp_rec_start_date )\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "\n",
    "#insert web_page data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_WEBPAGE}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.web_page\n",
    "    \"\"\")   \n",
    "\n",
    "\n",
    "#Create the promotion table web_page\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  dev.`{DB_NAME}`.`{TABLE_NAME_WEBRETURNS}`( \n",
    "        ws_sold_date_sk             int,\n",
    "        ws_sold_time_sk             int,\n",
    "        wr_returned_date_sk         int,   \n",
    "        wr_returned_time_sk         int, \n",
    "        wr_item_sk                  int, \n",
    "        wr_refunded_customer_sk     int,\n",
    "        wr_refunded_cdemo_sk        int,   \n",
    "        wr_refunded_hdemo_sk        int,   \n",
    "        wr_refunded_addr_sk         int,    \n",
    "        wr_returning_customer_sk    int,\n",
    "        wr_returning_cdemo_sk       int,   \n",
    "        wr_returning_hdemo_sk       int,  \n",
    "        wr_returning_addr_sk        int,   \n",
    "        wr_web_page_sk              int,         \n",
    "        wr_reason_sk                int,           \n",
    "        wr_order_number             int,\n",
    "        wr_return_quantity          int,     \n",
    "        wr_return_amt               double,  \n",
    "        wr_return_tax               double,     \n",
    "        wr_return_amt_inc_tax       double,\n",
    "        wr_fee                      double,         \n",
    "        wr_return_ship_cost         double,\n",
    "        wr_refunded_cash            double,   \n",
    "        wr_reversed_charge          double, \n",
    "        wr_account_credit           double,  \n",
    "        wr_net_loss                 double \n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (wr_reason_sk)\n",
    "    OPTIONS ('format-version'='2')\n",
    "    \"\"\")\n",
    "       \n",
    "#insert web_returns data\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO dev.`{DB_NAME}`.`{TABLE_NAME_WEBRETURNS}`\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.web_returns\n",
    "    \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting update-item.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile update-item.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, concat\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()    \n",
    "\n",
    "\n",
    "#Variables \n",
    "DB_NAME = \"datalake\"\n",
    "TPC_DS_DATABASE = \"tpc-source\"\n",
    "TABLE_NAME_ITEM = \"item_iceberg\"\n",
    "\n",
    "input_data =  spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM `{TPC_DS_DATABASE}`.item\n",
    "    WHERE i_current_price > 90.0;\n",
    "    ;\"\"\"\n",
    ")\n",
    "\n",
    "#Create a batch of update and insert records\n",
    "temp_data = input_data.withColumn(\"i_brand\",when(col(\"i_brand\").like(\"corpnameless%\"), \"Unknown\") \\\n",
    "                                            .otherwise(col(\"i_brand\")))\n",
    "\n",
    "update_data = temp_data.withColumn(\"i_item_id\", when(temp_data.i_brand != \"Unknown\",(concat(col(\"i_item_id\"), lit(\"N\"))))\\\n",
    "                                                .otherwise(col(\"i_item_id\")))\n",
    "\n",
    "\n",
    "# update table with the batch of new inserts and updated records\n",
    "update_data.createOrReplaceTempView(\"item_records\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO dev.`{DB_NAME}`.`{TABLE_NAME_ITEM}` item\n",
    "    USING item_records changed\n",
    "        ON item.i_item_id = changed.i_item_id\n",
    "    WHEN MATCHED THEN UPDATE SET item.i_brand = changed.i_brand\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./ingest-iceberg.py to s3://datalake-resources-684199068947-us-east-1/scripts/ingest-iceberg.py\n",
      "upload: ./update-item.py to s3://datalake-resources-684199068947-us-east-1/scripts/update-item.py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ingest-iceberg.py s3://datalake-resources-{AWS_ACCOUNT_ID}-us-east-1/scripts/\n",
    "!aws s3 cp update-item.py s3://datalake-resources-{AWS_ACCOUNT_ID}-us-east-1/scripts/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Iceberg tables and ingest TPC-DS data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your Iceberg tables and ingest the data, complete the following steps:\n",
    "\n",
    "1. On the Amazon EMR console, choose **EMR Serverless** in the navigation pane.\n",
    "2. Choose **Manage applications**.\n",
    "3. Choose the application `datalake-app`_._\n",
    "4. Choose **Start application**.\n",
    "\n",
    "Once started, it will provision the pre-initialized capacity as configured at creation (one Spark driver and two Spark executors). The pre-initialized capacity are resources that will be provisioned when you start your application. They can be used instantly when you submit jobs. However, they incur charges even if they’re not used when the application is in a started state. By default, the application is set to stop when idle for 15 minutes.\n",
    "\n",
    "Now that the EMR application has started, we can submit the Spark ingest job `ingest-iceberg.py`. The job creates the Iceberg tables and then loads data from the previously created AWS Glue Data Catalog tables on TPC-DS data in an external bucket.\n",
    "\n",
    "5. Navigate to the `datalake-app`.\n",
    "6. On the **Job runs** tab, choose **Submit job**.\n",
    "7. For **Name**, enter `ingest-data`.\n",
    "8. For **Runtime role**, choose the IAM role created by the CloudFormation stack.\n",
    "9. For **Script location**, enter the S3 path for your resource bucket (`datalake-resource-`_<####>_`-us-east-1>scripts>ingest-iceberg.py`).\n",
    "10. Under **Spark properties**, choose **Edit in text**.\n",
    "11. Enter the following properties, replacing <BUCKET\\_NAME> with your data lake bucket name `datalake-`<####>`-us-east-1` (not datalake-resources)\n",
    "    ```python\n",
    "    --conf spark.executor.cores=2 --conf spark.executor.memory=4g --conf spark.driver.cores=2 --conf spark.driver.memory=8g --conf spark.executor.instances=2 --conf spark.jars=/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.dev.warehouse=s3://<BUCKET_NAME>/warehouse --conf spark.sql.catalog.dev=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.dev.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager --conf spark.sql.catalog.glue_catalog.lock.table=myIcebergLockTab --conf spark.dynamicAllocation.maxExecutors=8 --conf spark.driver.maxResultSize=1G --conf spark.hadoop.hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\n",
    "    ```\n",
    "12. Submit the job."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ingest-emr](https://user-images.githubusercontent.com/62965911/224493016-4889eb63-e5bb-4b85-881b-99f2a352ea7c.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Iceberg tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we provide examples of data warehouse queries from TPC-DS on the Iceberg tables.\n",
    "\n",
    "1. On the Athena console, open the query editor.\n",
    "2. For **Workgroup**, switch to `DatalakeWorkgroup`.\n",
    "3. Choose **Acknowledge**. The queries in `DatalakeWorkgroup` will run on Athena engine version 3.\n",
    "4. On the **Saved queries** tab, choose a query to run on your Iceberg tables.\n",
    "\n",
    "The following queries are listed:\n",
    "\n",
    "- **Query3 –** Report the total extended sales price per item brand of a specific manufacturer for all sales in a specific month of the year.\n",
    "- **Query45 –** Report the total web sales for customers in specific zip codes, cities, counties, or states, or specific items for a given year and quarter.\n",
    "- **Query52 –** Report the total of extended sales price for all items of a specific brand in a specific year and month.\n",
    "- **Query6 –** List all the states with at least 10 customers who during a given month bought items with the price tag at least 20% higher than the average price of items in the same category.\n",
    "- **Query75 –** For 2 consecutive years, track the sales of items by brand, class, and category.\n",
    "- **Query86a –** Roll up the web sales for a given year by category and class, and rank the sales among peers within the parent. For each group, compute the sum of sales and location with the hierarchy and rank within the group.\n",
    "\n",
    "These queries are examples of queries used in decision-making and reporting in an organization. You can run them in the order you want. For this lab, we start with `Query3`.\n",
    "\n",
    "5. Before you run the query, confirm that **Database** is set to `datalake`.\n",
    "6. Now you can run the query.\n",
    "7. Repeat these steps to run the other queries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![athena-ingest](https://user-images.githubusercontent.com/62965911/224493002-a7805257-ea4b-4b4c-9d86-5c2ed8405d8b.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Update the item table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the queries, we prepare a batch of updates and inserts of records into the `item` table.\n",
    "\n",
    "1. First, run the following query to count the number of records in the `item` Iceberg table:\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM \"datalake\".\"item_iceberg\";\n",
    "```\n",
    "\n",
    "This should return 102,000 records.\n",
    "\n",
    "2. Select item records with a price higher than $90:\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM \"datalake\".\"item_iceberg\" WHERE i_current_price > 90.0;\n",
    "```\n",
    "\n",
    "This will return 1,112 records.\n",
    "\n",
    "The `update-item.py` job takes these 1,112 records, modifies 11 records to change the name of the brand to `Unknown`, and changes the remaining 1,101 records’ `i_item_id` key to flag them as new records. As a result, a batch of 11 updates and 1,101 inserts are merged into the `item_iceberg` table.\n",
    "\n",
    "The 11 records to be updated are those with price higher than $90, and the brand name starts with `corpnameless`.\n",
    "\n",
    "3. Run the following query:\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM \"datalake\".\"item_iceberg\" WHERE i_current_price > 90.0 AND i_brand LIKE 'corpnameless%';\n",
    "```\n",
    "\n",
    "The result is 11 records. The `item_update.py` job replaces the brand name with `Unknown` and merges the batch into the Iceberg table.\n",
    "\n",
    "Now you can return to the EMR Serverless console and run the job on the EMR Serverless application.\n",
    "\n",
    "4. On the application details page, choose **Submit job**.\n",
    "5. For **Name**, enter `update-item-job`.\n",
    "6. For **Runtime role**¸ use the same role that you used previously.\n",
    "7. For **S3 URI**, enter the `update-item.py` script location.\n",
    "8. Under **Spark properties**, choose **Edit in text**.\n",
    "9. Enter the following properties, replacing the `<BUCKET-NAME>` with your own `datalake-`_<####>_`-us-east-1`:\n",
    "\n",
    "```python\n",
    "--conf spark.executor.cores=2 --conf spark.executor.memory=8g --conf spark.driver.cores=4 --conf spark.driver.memory=8g --conf spark.executor.instances=2 --conf spark.jars=/usr/share/aws/iceberg/lib/iceberg-spark3-runtime.jar --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.dev=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.dev.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.lock-impl=org.apache.iceberg.aws.glue.DynamoLockManager --conf spark.sql.catalog.glue_catalog.lock.table=myIcebergLockTab --conf spark.dynamicAllocation.maxExecutors=4 --conf spark.driver.maxResultSize=1G --conf spark.sql.catalog.dev.warehouse=s3://<BUCKET-NAME>/warehouse --conf spark.hadoop.hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\n",
    "```\n",
    "\n",
    "10. Then submit the job.\n",
    "11. After the job finishes successfully, return to the Athena console and run the following query:\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM \"datalake\".\"item_iceberg\";\n",
    "```\n",
    "\n",
    "The returned result is 103,101 = 102,000 + (1,112 – 11). The batch was merged successfully.\n",
    "\n",
    "![athena-merge](https://user-images.githubusercontent.com/62965911/224493005-b03af543-06a4-4570-afe4-138be9d68edc.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Time travel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a time travel query, complete the following steps:\n",
    "\n",
    "1. Get the timestamp of the job run via the application details page on the EMR Serverless console, or the Spark UI on the History Server, as shown in the following screenshot.\n",
    "\n",
    "![emr-1](https://user-images.githubusercontent.com/62965911/224493015-3f475945-a8a8-46d3-a0c3-7e14be6160da.png)\n",
    "\n",
    "This time could be just minutes before you ran the update Spark job.\n",
    "\n",
    "2. Convert the timestamp from the format `YYYY/MM/DD hh:mm:ss to YYYY-MM-DDThh:mm:ss.sTZD` with time zone. For example, from `2023/02/20 14:40:41` to `2023-02-20 14:40:41.000 UTC`.\n",
    "3. On the Athena console, run the following query to count the `item` table records at a time before the update job, replacing <TRAVEL\\_TIME> with your time:\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM \"datalake\".\"item_iceberg\" FOR TIMESTAMP AS OF TIMESTAMP <TRAVEL_TIME>;\n",
    "```\n",
    "\n",
    "![athena-1](https://user-images.githubusercontent.com/62965911/224492992-1261591b-a779-4536-aceb-c3389bff9fe8.png)\n",
    "\n",
    "The query will give 102,000 as a result, the expected table size before running the update job.\n",
    "\n",
    "4. Now you can run a query with a timestamp after the successful run of the update job (for example, `2023-02-20 15:06:00.000 UTC`):\n",
    "\n",
    "```sql\n",
    "SELECT count(*) FROM \"datalake\".\"item_iceberg\" FOR TIMESTAMP AS OF TIMESTAMP <TRAVEL_TIME>;\n",
    "```\n",
    "\n",
    "![athena-2](https://user-images.githubusercontent.com/62965911/224493007-e8b855ab-2bb5-4b02-b3c6-fc5fcae698fd.png)\n",
    "\n",
    "The query will now give 103,101 as the size of the table at that time, after the update job successfully finished.\n",
    "\n",
    "Additionally, you can query in Athena based on the version ID of a snapshot in Iceberg. However, for more advanced use cases, such as to roll back to a given version or to find version IDs, you can use Iceberg’s SDK or Spark on Amazon EMR."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Clean up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following steps to clean up your resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on the Amazon S3 console, empty your buckets\n",
    "!aws s3 rm s3://datalake-684199068947-us-east-1 --recursive --quiet\n",
    "!aws s3 rm s3://datalake-resources-684199068947-us-east-1 --recursive --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on the Athena console, delete the workgroup `DatalakeWorkgroup`\n",
    "!aws athena delete-work-group --work-group DatalakeWorkgroup --recursive-delete-option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on the EMR Studio console, stop the application `datalake-app`\n",
    "!aws emr-serverless stop-application --application-id 00f8gkpvcl7vtf09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on the AWS CloudFormation console, delete the CloudFormation stack\n",
    "!aws cloudformation delete-stack --stack-name DataLakehouse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
