{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0b4219-978c-4917-975b-3ad668e20e84",
   "metadata": {},
   "source": [
    "# Retail Business Analytics\n",
    "## Task 1: customers-tab-delimited\n",
    "    Show the client information for those who live in California\n",
    "    The final output must be in text format\n",
    "    Save the results in the result/scenario1/solution folder\n",
    "    Only records with the state value \"CA\" should be included in the result\n",
    "    Only the customer's entire name should be included in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01912ef3-666f-4f5f-b20e-a12c0c2df65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://51e04621b22c:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f866959ef50>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebdcf193-0f1e-428a-9075-4c4d15dc80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('delimiter','\\t').csv('part-m-00000',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a13ede27-c843-4687-8a83-1565271cc3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+---------+---------+--------------------+-----------+---+-----+\n",
      "|_c0|    _c1|      _c2|      _c3|      _c4|                 _c5|        _c6|_c7|  _c8|\n",
      "+---+-------+---------+---------+---------+--------------------+-----------+---+-----+\n",
      "|  1|Richard|Hernandez|XXXXXXXXX|XXXXXXXXX|  6303 Heather Plaza|Brownsville| TX|78521|\n",
      "|  2|   Mary|  Barrett|XXXXXXXXX|XXXXXXXXX|9526 Noble Embers...|  Littleton| CO|80126|\n",
      "|  3|    Ann|    Smith|XXXXXXXXX|XXXXXXXXX|3422 Blue Pioneer...|     Caguas| PR|  725|\n",
      "|  4|   Mary|    Jones|XXXXXXXXX|XXXXXXXXX|  8324 Little Common| San Marcos| CA|92069|\n",
      "|  5| Robert|   Hudson|XXXXXXXXX|XXXXXXXXX|10 Crystal River ...|     Caguas| PR|  725|\n",
      "+---+-------+---------+---------+---------+--------------------+-----------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e70a5a-a6b6-4a0b-b911-13da3fb03172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF('index','first_name','last_name','x1','x2','street','city','state','zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660ea4c1-557f-4718-8d51-ab274f2783b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+---------+---------+--------------------+-----------+-----+--------+\n",
      "|index|first_name|last_name|       x1|       x2|              street|       city|state|zip_code|\n",
      "+-----+----------+---------+---------+---------+--------------------+-----------+-----+--------+\n",
      "|    1|   Richard|Hernandez|XXXXXXXXX|XXXXXXXXX|  6303 Heather Plaza|Brownsville|   TX|   78521|\n",
      "|    2|      Mary|  Barrett|XXXXXXXXX|XXXXXXXXX|9526 Noble Embers...|  Littleton|   CO|   80126|\n",
      "|    3|       Ann|    Smith|XXXXXXXXX|XXXXXXXXX|3422 Blue Pioneer...|     Caguas|   PR|     725|\n",
      "|    4|      Mary|    Jones|XXXXXXXXX|XXXXXXXXX|  8324 Little Common| San Marcos|   CA|   92069|\n",
      "|    5|    Robert|   Hudson|XXXXXXXXX|XXXXXXXXX|10 Crystal River ...|     Caguas|   PR|     725|\n",
      "+-----+----------+---------+---------+---------+--------------------+-----------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b507c40c-f078-4987-b153-073af4772c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df = df.filter('state == \"CA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89019391-f920-4d51-81d0-ff2471cc3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_df.write.format('csv').option('header',True).option('sep','\\t').save('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7348091-a879-4efb-9645-4772b8037ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code for submitting spark job (scenario 1)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "#input data\n",
    "df = spark.read.option('delimiter','\\t').csv(r'data-files/customers-tab-delimited/part-m-00000',inferSchema=True)\n",
    "\n",
    "#apply schema on data, making it a dataframe \n",
    "df = df.toDF('index','first_name','last_name','x1','x2','street','city','state','zip_code')\n",
    "\n",
    "#using filter function, select only rows with state == CA\n",
    "ca_df = df.filter('state == \"CA\"')\n",
    "\n",
    "#now we combine first and last name into full name\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"\"\" this is if you want to add the new column at the end\n",
    "final_df = ca_df.withColumn('full_name',F.concat(F.col('first_name'),\\\n",
    "                               F.lit(' '),\\\n",
    "                               F.col('last_name')))\n",
    "\"\"\"\n",
    "# this is for select the new column onlh\n",
    "final_df = ca_df.select(F.concat(F.col('first_name'),F.lit(' '),F.col('last_name')).alias('full_name'))\n",
    "\n",
    "#save file to the correct directory\n",
    "final_df.write.format('csv').option('header',True).option('sep','\\t').save(r'results/sc1/task1/')\n",
    "\n",
    "#close out app to save resource\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1599f78-c229-460a-8e0c-db82654a9051",
   "metadata": {},
   "source": [
    "### submit job via\n",
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client ca.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aecbd8-589b-4079-8c17-e517c20ce74c",
   "metadata": {},
   "source": [
    "## Task 2 orders parquet\n",
    "    • Show all orders with the order status value \"COMPLETE“\n",
    "    • Save the data in the \"result/scenario2/solution\" directory on HDFS\n",
    "    • Include order number, order date, and current situation in the output\n",
    "    The \"order date\" column should be in the \"YYYY-MM-DD\" format \n",
    "    Use GZIP compression to compress the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c44ae4b6-b81e-4b93-9c59-4d129b453b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://51e04621b22c:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8669336950>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName('spark')\\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fe87a-04f2-4912-8e84-471b09352bc2",
   "metadata": {},
   "source": [
    "Spark SQL caches Parquet metadata for better performance. When Hive metastore Parquet table conversion is enabled, metadata of those converted tables are also cached. If these tables are updated by Hive or other external tools, you need to refresh them manually to ensure consistent metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "426a9542-1b30-49ac-9f57-d229ae2ece79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        value|\n",
      "+-------------+\n",
      "|1374710400000|\n",
      "+-------------+\n",
      "\n",
      "+-------------+-------------------+\n",
      "|        value|           new_date|\n",
      "+-------------+-------------------+\n",
      "|1374710400000|2013-07-25 04:00:00|\n",
      "+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "d = ['1374710400000']\n",
    "df = spark.createDataFrame(d, StringType())\n",
    "df.show()\n",
    "\n",
    "\n",
    "df.withColumn('new_date',from_utc_timestamp(from_unixtime(df.value/1000,\"yyyy-MM-dd hh:mm:ss\"),'UTC-8')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f3e73a05-19b3-48ef-b38b-c5fc93ed0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+---------------+\n",
      "|order_id|   order_date|order_customer_id|   order_status|\n",
      "+--------+-------------+-----------------+---------------+\n",
      "|       1|1374710400000|            11599|         CLOSED|\n",
      "|       2|1374710400000|              256|PENDING_PAYMENT|\n",
      "|       3|1374710400000|            12111|       COMPLETE|\n",
      "|       4|1374710400000|             8827|         CLOSED|\n",
      "|       5|1374710400000|            11318|       COMPLETE|\n",
      "+--------+-------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load('short.parquet')\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98a4a5b0-5e71-4440-baf7-1bf20c4253c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+---------------+-------------------+\n",
      "|order_id|   order_date|order_customer_id|   order_status|           new_date|\n",
      "+--------+-------------+-----------------+---------------+-------------------+\n",
      "|       1|1374710400000|            11599|         CLOSED|2013-07-24 16:00:00|\n",
      "|       2|1374710400000|              256|PENDING_PAYMENT|2013-07-24 16:00:00|\n",
      "|       3|1374710400000|            12111|       COMPLETE|2013-07-24 16:00:00|\n",
      "|       4|1374710400000|             8827|         CLOSED|2013-07-24 16:00:00|\n",
      "|       5|1374710400000|            11318|       COMPLETE|2013-07-24 16:00:00|\n",
      "+--------+-------------+-----------------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalized_date_df = df.withColumn('new_date',from_utc_timestamp(from_unixtime(df['order_date']/1000),'UTC-8'))\n",
    "normalized_date_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5494bc74-447b-4015-956a-d9caf0566cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "final_df = normalized_date_df.withColumn('date',date_format(col('new_date'),'yyyy-MM-dd').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "13ec9b68-375d-47ba-a25c-ce456740e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------+\n",
      "|order_id|order_customer_id|      date|\n",
      "+--------+-----------------+----------+\n",
      "|       1|            11599|2013-07-24|\n",
      "|       2|              256|2013-07-24|\n",
      "|       3|            12111|2013-07-24|\n",
      "|       4|             8827|2013-07-24|\n",
      "|       5|            11318|2013-07-24|\n",
      "+--------+-----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = final_df['order_id','order_customer_id','date']\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "34ec3545-564e-4a73-886f-4383ce8f335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.format('csv').option('header',True).option('sep','\\t').option('compression','gzip').save(r'results/sc1/task2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "466247d2-6521-4679-8af5-42882c18534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bbdc6aa-953d-4b01-bcc0-14f6c28c7812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: long (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb22d919-84e6-4319-bd91-1b59b2c8ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------------+------------+\n",
      "|order_id|   order_date|order_customer_id|order_status|\n",
      "+--------+-------------+-----------------+------------+\n",
      "|       3|1374710400000|            12111|    COMPLETE|\n",
      "|       5|1374710400000|            11318|    COMPLETE|\n",
      "|       6|1374710400000|             7130|    COMPLETE|\n",
      "|       7|1374710400000|             4530|    COMPLETE|\n",
      "|      15|1374710400000|             2568|    COMPLETE|\n",
      "+--------+-------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('order_status == \"COMPLETE\"').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9cfa7453-2dc2-4228-a4b0-994ff097d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = df.filter('order_status == \"COMPLETE\"')\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e4be1-6a25-4a7a-9110-76fc5a6b628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code for submitting spark job (scenario 2)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "#input data\n",
    "df = spark.read.parquet(r'data-files/orders_parquet/shortName.parquet',inferSchema=True)\n",
    "\n",
    "#apply schema on data is not needed because header is included\n",
    "\n",
    "#using filter function, select only rows with order_status == COMPLETE\n",
    "complete_df = df.filter('order_status == \"COMPLETE\"')\n",
    "\n",
    "# after applying the first filter, convert time stamp into standard date format\n",
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime, date_format, col\n",
    "# convert timestamp\n",
    "normalized_date_df = complete_df.withColumn('new_date',from_utc_timestamp(from_unixtime(df['order_date']/1000),'UTC-8'))\n",
    "# extract date\n",
    "final_df = normalized_date_df.withColumn('date',date_format(col('new_date'),'yyyy-MM-dd').cast('date'))\n",
    "# select columns we need\n",
    "final_df = final_df['order_id','order_customer_id','date','order_status']\n",
    "\n",
    "# save file to the correct directory using gzip compression\n",
    "final_df.write.format('csv').option('header',True).option('sep','\\t').option('compression','gzip').save(r'results/sc1/task2/')\n",
    "\n",
    "#close out app to save resource\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ccfa7-6895-49b3-b10a-5a4e377bb50a",
   "metadata": {},
   "source": [
    "### submit job via\n",
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client complete.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff89d07-4b9a-4761-b949-6f6eee904718",
   "metadata": {},
   "source": [
    "## Task 3 customer-tab-delimited\n",
    "    • Produce a list of all consumers who live in the city of \"Caguas\"\n",
    "    • Save the results in the result/scenario3/solution folder\n",
    "    • The result should only contain records with the value \"Caguas\" for the \n",
    "    customer city\n",
    "    Use snappy compression to compress the output \n",
    "    Save the file in the orc format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f09ec0-586e-4f03-a864-871afd0323eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code for submitting spark job (scenario 3)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "#input data\n",
    "df = spark.read.option('delimiter','\\t').csv(r'data-files/customers-tab-delimited/part-m-00000',inferSchema=True)\n",
    "\n",
    "#apply schema on data, making it a dataframe \n",
    "df = df.toDF('index','first_name','last_name','x1','x2',\\\n",
    "              'street','city','state','zip_code')\n",
    "\n",
    "#using filter function, select only rows with city = Caguas\n",
    "ca_df = df.filter('city == \"Caguas\"')\n",
    "\n",
    "#save file to the correct directory\n",
    "ca_df.write.format('orc').option('compression','snappy').option('header',True).save(r'results/sc1/task3/')\n",
    "\n",
    "#close out app to save resource\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bb9ce-bb72-4348-aa38-91a829a8c428",
   "metadata": {},
   "source": [
    "### submit job via\n",
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client caguas.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d1f00-1cab-40e2-8d63-97d2b814f0eb",
   "metadata": {},
   "source": [
    "## Task 4 categories\n",
    "    . Explore the order records saved in the “categories” directory on HDFS\n",
    "    • Save the result files in CSV format\n",
    "    • Save the data in the result/scenario4/solution directory on HDFS\n",
    "    • Use lz4 compression to compress the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6cc70-454f-41c9-8e89-e74c9a5e00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code for submitting spark job (scenario 4)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "#input data\n",
    "df = spark.read.option('delimiter',',').csv(r'data-files/categories/part-m-00000',inferSchema=True)\n",
    "\n",
    "#apply schema on data, making it a dataframe \n",
    "df = df.toDF('index','type','category')\n",
    "\n",
    "#save file to the correct directory\n",
    "df.write.format('csv').option('header',True).option('compression','lz4')\\\n",
    "        .option('sep',',').save(r'results/sc1/task4/')\n",
    "\n",
    "#close out app to save resource\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99fcb8-8be9-41f5-a070-a2124aed6e3f",
   "metadata": {},
   "source": [
    "### submit job via\n",
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client categories.py\n",
    "\n",
    "for compressing, I could not find a way to compressfile straight in the hdfs system, so I resorted to copying it to the local linux file system, compress it then copy it back. Your kernel did not have lz4 library installed\n",
    "\n",
    "hdfs dfs -copyToLocal /user/antnguyen72gmail/results/sc1/task4/part-00000-58f13e38-67f9-44f3-9ba9-858686dc369c-c000.csv\n",
    "\n",
    "lz4 part-00000-58f13e38-67f9-44f3-9ba9-858686dc369c-c000.csv\n",
    "\n",
    "hdfs dfs -put part-00000-58f13e38-67f9-44f3-9ba9-858686dc369c-c000.csv.lz4 /user/antnguyen72gmail/results/sc1/task4/part-00000-58f13e38-67f9-44f3-9ba9-858686dc369c-c000.csv.lz4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbf0f5-7b3b-4521-a3cc-f71fc9056a7e",
   "metadata": {},
   "source": [
    "## Task 5 products_avro\n",
    "    Explore the customer records saved in the “products_avro\" directory on HDFS\n",
    "    • Include the products with a price of more than 1000.0 in the output \n",
    "    • Remove data from the table if the product price is greater than 1000.0\n",
    "    • Save the results in the result/scenario5/solution folde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483ce8b-fa23-43be-8f13-869d30e94dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "# read all files\n",
    "\n",
    "# get a list of all file names from the directory\n",
    "import os\n",
    "file_list = os.listdir(r'data-files/products_avro/')\n",
    "\n",
    "# using loop, add each dataframe objects onto a list\n",
    "df_list = []\n",
    "for line in file_list:\n",
    "    df = spark.read.format(\"avro\").load(f'data-files/products_avro/{line}')\n",
    "    df_list.append(df)\n",
    "\n",
    "# use reduce functional tool to apply a lambda function to am iterable list\n",
    "# in this case, we unioned all the dataframes togther\n",
    "from functools import reduce\n",
    "combined_df = reduce(lambda x,y: x.union(y),df_list)\n",
    "\n",
    "# output rows with product_price is greater than 1000.0\n",
    "combined_df.filter('product_price > 1000.0').show()\n",
    "\n",
    "# remove data from table if the product price is greater than 1000.0\n",
    "final_df = combined_df.filter('product_price <= 1000.0')\n",
    "\n",
    "# save file\n",
    "# tried to save as avro with compression but avro is not compat with snappy\n",
    "# turns out orc is the only compatiable with snappy?? and also avro\n",
    "final_df.write.format('orc').option('compression','snappy').save(r'results/sc1/task5/')\n",
    "\n",
    "# close out spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0242a5fa-e570-4615-8c19-9575c0faca5d",
   "metadata": {},
   "source": [
    "### Job submit\n",
    "\n",
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client avro_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf905e2-ed59-4697-9be0-457e4dfc76b2",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "    Explore the “products_avro” stored in product records\n",
    "\n",
    "    REQUIREMENT:\n",
    "\n",
    "    Only products with a price of more than 1000.0 should be in the output\n",
    "    The pattern \"Treadmill\" appears in the product name\n",
    "    Save the output files in parquet format\n",
    "    Save the data in the result/scenario6/solution directory on HDFS\n",
    "    Use GZIP compression to compress the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096342db-f9f4-4bde-b7b8-5cf55ffe2925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "# read all files\n",
    "\n",
    "# get a list of all file names from the directory\n",
    "import os\n",
    "file_list = os.listdir(r'data-files/products_avro/')\n",
    "\n",
    "# using loop, add each dataframe objects onto a list\n",
    "df_list = []\n",
    "for line in file_list:\n",
    "    df = spark.read.format(\"avro\").load(f'data-files/products_avro/{line}')\n",
    "    df_list.append(df)\n",
    "\n",
    "# use reduce functional tool to apply a lambda function to am iterable list\n",
    "# in this case, we unioned all the dataframes togther\n",
    "from functools import reduce\n",
    "combined_df = reduce(lambda x,y: x.union(y),df_list)\n",
    "\n",
    "# output rows with product_price is greater than 1000.0\n",
    "greater_df = combined_df.filter('product_price > 1000.0')\n",
    "\n",
    "# create a temp view for easier querying\n",
    "greater_df.createOrReplaceTempView('table')\n",
    "\n",
    "# query rows with the pattern \"Treadmill\"\n",
    "final_df = spark.sql(\"select * from table where lower(product_name) like '%treadmill%'\")\n",
    "\n",
    "# save file as parquet format using gzip compression\n",
    "final_df.write.format('parquet').option('compression','gzip').save(r'results/sc1/task6/')\n",
    "\n",
    "# close out spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091b714-ceed-4f13-b4f7-344de5b274d6",
   "metadata": {},
   "source": [
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client avro_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a86708-6d92-4c41-ac00-aefae4c5cd98",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "    Explore the order records that are saved in the “orders parquet” table on HDFS\n",
    "\n",
    "    REQUIREMENT:\n",
    "\n",
    "    Output all PENDING orders in July 2013\n",
    "    Output files should be in JSON format\n",
    "    Save the data in the result/scenario7/solution directory on HDFS.\n",
    "    Only entries with the order status value of \"PENDING\" should be included in the result\n",
    "    Order date should be in the YYY-MM-DD format\n",
    "    Use snappy compression to compress the output, which should just contain the order date and order status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d35623-1b3f-402e-b7b1-c857e6835f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code for submitting spark job (scenario 7)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "#input data\n",
    "df = spark.read.parquet('data-files/orders_parquet/shortName.parquet',inferSchema=True)\n",
    "\n",
    "#apply schema on data is not needed because header is included\n",
    "\n",
    "#using filter function, select only rows with order_status == PENDING\n",
    "pending_df = df.filter('order_status == \"PENDING\"')\n",
    "\n",
    "# after applying the first filter, convert time stamp into standard date format\n",
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime, date_format, col\n",
    "# convert timestamp\n",
    "normalized_date_df = pending_df.withColumn('new_date',from_utc_timestamp(from_unixtime(df['order_date']/1000),'UTC-8'))\n",
    "# extract date\n",
    "final_df = normalized_date_df.withColumn('date',date_format(col('new_date'),'yyyy-MM-dd').cast('date'))\n",
    "\n",
    "# select columns we need\n",
    "final_df = final_df['date','order_status']\n",
    "\n",
    "# save file\n",
    "final_df.write.format('json').option('compression','snappy').save(r'results/sc1/task7/')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92c95d-ec95-4840-9de0-f8f7b5659daf",
   "metadata": {},
   "source": [
    "spark3-submit --conf spark.ui.port=6065 --deploy-mode client pending.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "15d8eed8-c91d-4774-9e95-68c1384f2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.format('json').option('compression','snappy').save(r'results/sc1/task7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "edeb7e87-09a1-43d8-90c4-831a11883428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code for submitting spark job (scenario 2)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "spark = SparkSession.builder.appName('spark').getOrCreate()\n",
    "\n",
    "#input data\n",
    "df = spark.read.parquet(r'short.parquet',inferSchema=True)\n",
    "\n",
    "#apply schema on data is not needed because header is included\n",
    "\n",
    "#using filter function, select only rows with order_status == PENDING\n",
    "pending_df = df.filter('order_status == \"PENDING\"')\n",
    "\n",
    "# after applying the first filter, convert time stamp into standard date format\n",
    "from pyspark.sql.functions import from_utc_timestamp, from_unixtime, date_format, col\n",
    "# convert timestamp\n",
    "normalized_date_df = pending_df.withColumn('new_date',from_utc_timestamp(from_unixtime(df['order_date']/1000),'UTC-8'))\n",
    "# extract date\n",
    "final_df = normalized_date_df.withColumn('date',date_format(col('new_date'),'yyyy-MM-dd').cast('date'))\n",
    "\n",
    "# select columns we need\n",
    "final_df = final_df['date','order_status']\n",
    "\n",
    "# save file\n",
    "final_df.write.format('json').option('compression','snappy').save(r'results/sc1/task7/')\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdb81f-3ce4-4a37-982c-ea20462fa0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
