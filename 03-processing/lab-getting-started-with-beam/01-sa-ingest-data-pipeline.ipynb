{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Apache Beam, building data pipelines in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam is an open-source SDK which allows you to build multiple data pipelines from batch or stream based integrations and run it in a direct or distributed way. You can add various transformations in each pipeline. But the real power of Beam comes from the fact that it is not based on a specific compute engine and therefore is platform independant. You declare which 'runner' you want to use to compute your transformation. It is using your local computing resource by default, but you can specify a Spark engine for example or Cloud DataflowΓÇª\n",
    "\n",
    "In this tutorial, I will create a pipeline ingesting a csv file, computing the mean of the Open and Close columns fo a historical S&P500 dataset. The goal here is not to give an extensive tutorial on Beam features, but rather to give you an overall idea of what you can do with it and if it is worth for you going deeper in building custom pipelines with Beam. Though I only write about batch processing, streaming pipelines are a powerful feature of Beam!\n",
    "\n",
    "Beam's SDK can be used in various languages, Java, Python... however in this tutorial I will focus on Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "At the date of this article Apache Beam (2.8.1) is only compatible with Python 2.7, however a Python 3 version should be available soon. If you have python-snappy installed, Beam may crash. This issue is known and will be fixed in Beam 2.9.\n",
    "\n",
    "    pip install apache-beam\n",
    "\n",
    "## Creating a basic pipeline ingesting CSV\n",
    "\n",
    "## Data\n",
    "\n",
    "For this example we will use a csv containing historical values of the S&P 500. The data looks like that:\n",
    "\n",
    "    Date,Open,High,Low,Close,Volume\n",
    "    03-01-00,1469.25,1478,1438.359985,1455.219971,931800000\n",
    "    04-01-00,1455.219971,1455.219971,1397.430054,1399.420044,1009000000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic pipeline\n",
    "\n",
    "To create a pipeline, we need to instantiate the pipeline object, eventually pass some options, and declaring the steps/transforms of the pipeline.\n",
    "\n",
    "    import apache_beam as beam\n",
    "    from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "    options = PipelineOptions()\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "From the beam documentation:\n",
    "> Use the pipeline options to configure different aspects of your pipeline, such as the pipeline runner that will execute your pipeline and any runner-specific configuration required by the chosen runner. Your pipeline options will potentially include information such as your project ID or a location for storing files.\n",
    "\n",
    "The PipelineOptions() method above is a command line parser that will read any standard option passed the following way:\n",
    "\n",
    "    --<option>=<value>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipeline1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/pipeline1.py\n",
    "import numpy as np\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms principles\n",
    "\n",
    "In Beam, data is represented as a ***PCollection ***object. So to start ingesting data, we need to read from the csv and store this as a ***PCollection*** to which we can then apply transformations. The Read operation is considered as a transform and follows the syntax of all transformations:\n",
    "\n",
    "    [Output PCollection] **=** [Input PCollection] **|** [Transform]\n",
    "\n",
    "These tranforms can then be chained like this:\n",
    "\n",
    "[Final Output PCollection] **=** ([Initial Input PCollection] **|** [First Transform]\n",
    " **|** [Second Transform]\n",
    " **|** [Third Transform])\n",
    "\n",
    "The pipe is the equivalent of an *apply* method.\n",
    "\n",
    "The input and output PCollections, as well as each intermediate PCollection are to be considered as individual data containers. This allows to apply multiple transformations to the same PCollection as the initial PCollection is immutable. For example:\n",
    "\n",
    "    [Output PCollection 1] **=** [Input PCollection] **|** [Transform 1]\n",
    "    [Output PCollection 2] **=** [Input PCollection] **|** [Transform 2]\n",
    "\n",
    "## Reading input data and writing output data\n",
    "\n",
    "So let's start by using one of the readers provided to read our csv, not forgetting to skip the header row:\n",
    "\n",
    "    csv_lines = (p | ReadFromText(input_filename, skip_header_lines=1) |   ...\n",
    "\n",
    "At the other end of our pipeline we want to output a text file. So let\"s use the standard writer:\n",
    "\n",
    "    ... **|** beam**.**io**.**WriteToText(output_filename)\n",
    "\n",
    "## Transforms\n",
    "\n",
    "Now we want to apply some transformations to our PCollection created with the Reader function. Transforms are applied to each element of the PCollection individually.\n",
    "\n",
    "Depending on the worker that you chose, your transforms can be distributed. Instances of your transformation are then executed on each node.\n",
    "> The user code running on each worker generates the output elements that are ultimately added to the final output *PCollection* that the transform produces.\n",
    "\n",
    "Beam has core methods (ParDo, Combine) that allows to apply a custom transform , but also has pre written transforms called [composite transforms](https://beam.apache.org/documentation/programming-guide/#composite-transforms). In our example we will use the ParDo transform to apply our own functions.\n",
    "\n",
    "We have read our csv into a ***PCollection***, so let's split it so we can access the Date and Close items:\n",
    "\n",
    "    ΓÇª beam.ParDo(Split()) ΓÇª\n",
    "\n",
    "And define our split function so we only retain the Date and Close and return it as a dictionnary:\n",
    "\n",
    "    class Split(beam.DoFn):\n",
    "        def process(self, element):\n",
    "            Date,Open,High,Low,Close,Volume = element.split(ΓÇ£,ΓÇ¥)\n",
    "            return [{\n",
    "                \"Open\": float(Open),\n",
    "                \"Close\": float(Close),\n",
    "            }]\n",
    "\n",
    "Now that we have the data we need, we can use one of the [standard combiners](https://beam.apache.org/releases/pydoc/2.6.0/_modules/apache_beam/transforms/combiners.html#Mean) to calculate the mean over the entire PCollection.\n",
    "\n",
    "The first thing to do is to represent the data as a tuple so we can group by a key and then feed CombineValues with what it expects. To do that we use a custom function \"CollectOpen()\" which returns a list of tuples containing (1, <open_value>).\n",
    "\n",
    "    class CollectOpen(beam.DoFn):\n",
    "        def process(self, element):\n",
    "            # Returns a list of tuples containing Date and Open value\n",
    "            result = [(1, element[\"Open\"])]\n",
    "            return result\n",
    "\n",
    "The first parameter of the tuple is fixed since we want to calculate the mean over the whole dataset, but you can make it dynamic to perform the next transform only on a sub-set defined by that key.\n",
    "\n",
    "The GroupByKey function allows to create a PCollection of all elements for which the key (ie the left side of the tuples) is the same.\n",
    "\n",
    "    mean_open = (\n",
    "        csv_lines | beam.ParDo(CollectOpen()) |\n",
    "        \"Grouping keys Open\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean for Open\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "        )\n",
    "    )\n",
    "\n",
    "When you assign a label to a transform, make sure it is unique, otherwise Beam will throw an error.\n",
    "\n",
    "Our final pipeline could look like that if we want to chain everything:\n",
    "\n",
    "    csv_lines = (\n",
    "        p | beam.io.ReadFromText(input_filename) | \n",
    "        beam.ParDo(Split()) |\n",
    "        beam.ParDo(CollectOpen()) |\n",
    "        \"Grouping keys Open\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "        ) | beam**.**io**.**WriteToText(output_filename)\n",
    "    )\n",
    "\n",
    "But we could also write it in a way that allows to add future transformation on the splitted PCollection (like a mean of the close for example):\n",
    "\n",
    "    csv_lines = (\n",
    "        p | beam.io.ReadFromText(input_filename) |\n",
    "        beam.ParDo(Split())\n",
    "    )\n",
    "\n",
    "    mean_open = (\n",
    "        csv_lines | beam.ParDo(CollectOpen()) |\n",
    "        \"Grouping keys Open\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean for Open\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    output = (\n",
    "        mean_open | beam**.**io**.**WriteToText(output_filename)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline1.py\n",
    "\n",
    "# class to split a csv line by elements and keep only the columns we are interested in \n",
    "class Split(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        Date,Open,High,Low,Close,Volume = element.split(\",\")\n",
    "        return [{\n",
    "            'Date': Date,\n",
    "            'Open': float(Open),\n",
    "            'Close': float(Close)\n",
    "        }]\n",
    "        \n",
    "class CollectOpen(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        # Returns a list of tuples containing the 1 key and Open value\n",
    "        result = [(1, element['Open'])]\n",
    "        return result\n",
    "\n",
    "class CollectClose(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        # Returns a list of tuples containing the 1 key and Close value\n",
    "        result = [(1, element['Close'])]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline1.py\n",
    "\n",
    "# class to calculate the standard deviation over an entire PCollection\n",
    "class Standard_deviation(beam.DoFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0.0, 0) # x, x^2, count\n",
    "\n",
    "    def add_input(self, sum_count, input):\n",
    "        (sum, sumsq, count) = sum_count\n",
    "        return sum + input, sumsq + input*input, count + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, sumsqs, counts = zip(*accumulators)\n",
    "        return sum(sums), sum(sumsqs), sum(counts)\n",
    "\n",
    "    def extract_output(self, sum_count):\n",
    "        (sum, sumsq, count) = sum_count\n",
    "        if count:\n",
    "            mean = sum / count\n",
    "            variance = (sumsq / count) - mean*mean\n",
    "            stddev = np.sqrt(variance) if variance > 0 else 0\n",
    "            return {\n",
    "                'mean': mean,\n",
    "                'variance': variance,\n",
    "                'stddev': stddev,\n",
    "                'count': count\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'mean': float('NaN'),\n",
    "                'variance': float('NaN'),\n",
    "                'stddev': float('NaN'),\n",
    "                'count': 0\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline1.py\n",
    "\n",
    "# setting input and output files\n",
    "input_filename = \"./data/sp500.csv\"\n",
    "output_filename = \"./output/result.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to src/pipeline1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a src/pipeline1.py\n",
    "\n",
    "# instantiate the pipeline\n",
    "options = PipelineOptions()\n",
    "\n",
    "with beam.Pipeline(options=options) as p:\n",
    "    # reading the csv and splitting lines by elements we want to retain\n",
    "    csv_lines = (\n",
    "            p | beam.io.ReadFromText(input_filename, skip_header_lines=1) |\n",
    "            beam.ParDo(Split())\n",
    "        )\n",
    "\n",
    "    # calculate the mean for Open values\n",
    "    mean_open = (\n",
    "        csv_lines | beam.ParDo(CollectOpen()) |\n",
    "        \"Grouping keys Open\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean for Open\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # calculate the mean for Close values\n",
    "    mean_close = (\n",
    "        csv_lines | beam.ParDo(CollectClose()) |\n",
    "        \"Grouping keys Close\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean for Close\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # writing results to file\n",
    "    output= ( \n",
    "        { \n",
    "            'Mean Open': mean_open,\n",
    "            'Mean Close': mean_close \n",
    "        } | \n",
    "        beam.CoGroupByKey() | \n",
    "        beam.io.WriteToText(output_filename)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now our pipeline defined end to end. You can run it by command line using the custom arguments we have defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.9 interpreter.\n"
     ]
    }
   ],
   "source": [
    "!python src/pipeline1.py --input ./data/sp500.csv --output ./output/result.txt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result in the file looks like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, {'Mean Open': [1482.5682959997862], 'Mean Close': [1482.764536822227]})\n"
     ]
    }
   ],
   "source": [
    "!cat ./output/result.txt-00000-of-00001"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple transforms on the same PCollection\n",
    "\n",
    "If I want to add another transform operation on the csv_lines PCollection I will obtain a second ΓÇ£transformed PCollectionΓÇ¥. Beam represents it very well in the form of ΓÇ£branchedΓÇ¥ tranformations:\n",
    "\n",
    "![](./img/pipeline1.png)\n",
    "\n",
    "To apply the different transforms we would have :\n",
    "\n",
    "    csv_lines = (\n",
    "        p | beam.io.ReadFromText(input_filename) |\n",
    "        beam.ParDo(Split())\n",
    "    )\n",
    "\n",
    "    mean_open = (\n",
    "        csv_lines | beam.ParDo(CollectOpen()) |\n",
    "        \"Grouping keys Open\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean for Open\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mean_close = (\n",
    "        csv_lines | beam.ParDo(CollectClose()) |\n",
    "        \"Grouping keys Close\" >> beam.GroupByKey() |\n",
    "        \"Calculating mean for Close\" >> beam.CombineValues(\n",
    "            beam.combiners.MeanCombineFn()\n",
    "        )\n",
    "    )\n",
    "\n",
    "But now we have 2 PCollections: mean_open and mean_close, as a result of the transform. We need to merge/join these results to get a PCollection we could write on a file with our writer. Beam has the CoGroupByKeywhich is doing just that. Our output would then look like that:\n",
    "\n",
    "    output= ( \n",
    "        { \n",
    "            \"Mean Open\": mean_open,\n",
    "            \"Mean Close\": mean_close\n",
    "        } | \n",
    "        apache_beam.CoGroupByKey() | \n",
    "        WriteToText(output_filename))\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Readers and Writers\n",
    "\n",
    "In this example we only used the csv reader and text writer, but Beam has much more connectors (ufortunately most of them are available for the Java platform, but a few Python ones are in progress). You can find the list of available connectors and their documentation at:\n",
    "[**Built-in I/O Transforms**\n",
    "*Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing dataΓÇª*beam.apache.org](https://beam.apache.org/documentation/io/built-in/)\n",
    "\n",
    "You can also find a guide to write your own connectors if you feel brave enough:\n",
    "[**Authoring I/O Transforms - Overview**\n",
    "*Apache Beam is an open source, unified model and set of language-specific SDKs for defining and executing dataΓÇª*beam.apache.org](https://beam.apache.org/documentation/io/authoring-overview/)\n",
    "\n",
    "## General Logic when creating data pipelines\n",
    "\n",
    "Whenever a data pipeline needs to be implemented, we want to be clear on the requirements and the end goal of our pipeline/transformations. In Beam documentation I found this little extract which I think is the core of how you should reason when starting to build a pipeline with Beam:\n",
    "> **Where is your input data stored?** How many sets of input data do you have? This will determine what kinds of Read transforms you\"ll need to apply at the start of your pipeline.\n",
    "> **What does your data look like?** It might be plaintext, formatted log files, or rows in a database table. Some Beam transforms work exclusively on PCollections of key/value pairs; you\"ll need to determine if and how your data is keyed and how to best represent that in your pipeline\"s PCollection(s).\n",
    "> **What do you want to do with your data?** The core transforms in the Beam SDKs are general purpose. Knowing how you need to change or manipulate your data will determine how you build core transforms like [ParDo](https://beam.apache.org/documentation/programming-guide/#pardo), or when you use pre-written transforms included with the Beam SDKs.\n",
    "> **What does your output data look like, and where should it go?** This will determine what kinds of Write transforms you\"ll need to apply at the end of your pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a distributed runner\n",
    "\n",
    "As said earlier, instead of using the local compute power (DirectRunner) you can use a distributed compute engine such as Spark. You can do that by setting the following options to your pipeline options (in command line arguments or in an option list):\n",
    "\n",
    "    --runner SparkRunner  --sparkMaster spark://host:port\n",
    "\n",
    "More options are available [here](https://beam.apache.org/documentation/runners/spark/), but these 2 are the basics.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Beam is quite low level when it comes to write custom transformation, then offering the flexibily one might need. It is fast and handles cloud / distributed environments. If you look at a higher level API/SDK, some libraries like tf.transform are actually built on top of Beam and offer you its power while coding less. The trade-off lays in the flexibility you are looking for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343191058819caea96d5cde1bd3b1a75b4807623ce2cda0e1c8499e39ac847e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
