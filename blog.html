<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">Blog | Recohut Data Bootcamp</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://www.recohut.com/blog"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="data science, data engineering, data analytics"><meta data-rh="true" property="og:title" content="Blog | Recohut Data Bootcamp"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/img/branding/favicon-black.svg"><link data-rh="true" rel="canonical" href="https://www.recohut.com/blog"><link data-rh="true" rel="alternate" href="https://www.recohut.com/blog" hreflang="en"><link data-rh="true" rel="alternate" href="https://www.recohut.com/blog" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Recohut Data Bootcamp RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Recohut Data Bootcamp Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-B4S1B1ZDTT"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-B4S1B1ZDTT",{})</script><link rel="stylesheet" href="/assets/css/styles.47c7b9d5.css">
<link rel="preload" href="/assets/js/runtime~main.251db5a0.js" as="script">
<link rel="preload" href="/assets/js/main.1462881d.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/branding/favicon-color.svg" alt="Recohut Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Bootcamp</b></a><a class="navbar__item navbar__link" href="/docs/introduction">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/recohut" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/12/24/ab-mab-tests">A/B and Multi-Armed Bandit Tests</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/12/13/sql-to-snowflake-schema-conversion">SQL Server to Snowflake Schema Conversion</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/12/12/the-complete-python-course-2022">The Complete Python Course (2022)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2022/11/16/opening-material">Opening our Bootcamp material</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2021/10/01/clinical-decision-making">Clinical Decision Making</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2022/12/24/ab-mab-tests">A/B and Multi-Armed Bandit Tests</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-12-24T00:00:00.000Z" itemprop="datePublished">December 24, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lab-1-simple-t-test">Lab 1: Simple T-test<a class="hash-link" href="#lab-1-simple-t-test" title="Direct link to heading">​</a></h2><ul><li>Simple T-test to compare 2 samples of ad-conversions</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lab-2-ab-test">Lab 2: A/B test<a class="hash-link" href="#lab-2-ab-test" title="Direct link to heading">​</a></h2><ul><li>AB test and plots with explanations to understand the basics</li><li>A/B testing step-by-step guide in Python</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lab-3-multi-armed-bandit-mab-test">Lab 3: Multi-Armed Bandit (MAB) test<a class="hash-link" href="#lab-3-multi-armed-bandit-mab-test" title="Direct link to heading">​</a></h2><ul><li>Testing out different bandit methods for effective online testing methods</li><li>Multi-armed Bandit for Banner Ad and 4 Exploration Strategies</li><li>How to build Multi-Armed Bandit Product Recommender</li><li>Solving Multi-armed Bandit Problems</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="solution">Solution<a class="hash-link" href="#solution" title="Direct link to heading">​</a></h2><ol><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-06-27-ab-test-in-five-simple-steps.ipynb" target="_blank" rel="noopener noreferrer">AB test and plots with explanations to understand the basics</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-06-27-ad-conversion-simple-ab-testing.ipynb" target="_blank" rel="noopener noreferrer">Simple T-test to compare 2 samples of ad-conversions</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-06-19-methods-for-effective-online-testing.ipynb" target="_blank" rel="noopener noreferrer">Testing out different bandit methods for effective online testing methods</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-06-28-step-by-step-ab-testing-guide.ipynb" target="_blank" rel="noopener noreferrer">A/B testing step-by-step guide in Python</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-06-28-udacity-abtest-python.ipynb" target="_blank" rel="noopener noreferrer">AB Testing With Python</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-07-02-ads-selection-using-bandits.ipynb" target="_blank" rel="noopener noreferrer">Multi-armed Bandit for Banner Ad and 4 Exploration Strategies</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2021-07-27-mab-product.ipynb" target="_blank" rel="noopener noreferrer">How to build Multi-Armed Bandit Product Recommender</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2022-01-14-mab.ipynb" target="_blank" rel="noopener noreferrer">Multi-Armed Bandits</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2022-01-18-mab.ipynb" target="_blank" rel="noopener noreferrer">Multi-armed Bandit for Banner Ad</a></li><li><a href="https://nbviewer.org/github/sparsh-ai/notebooks/blob/main/2022-01-20-mab.ipynb" target="_blank" rel="noopener noreferrer">Solving Multi-armed Bandit Problems</a></li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/mab">mab</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/ab">ab</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2022/12/13/sql-to-snowflake-schema-conversion">SQL Server to Snowflake Schema Conversion</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-12-13T00:00:00.000Z" itemprop="datePublished">December 13, 2022</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Last week, I was asked to migrate one view from the On-premises MS SQL Server to Snowflake. My first approach is to directly copy paste the DDL (Data Definition Language) and as I predicted, it didn&#x27;t work. But my goal here was to follow TDD (Test Driven Development) - I ran the first version of DDL and Snowflake told me what the error is, I corrected the error and repeated the cycle 6-8 times until the code ran successfully without error and that&#x27;s how I did the migration. </p><p>Couple of points that I noted down as conversion rules are:</p><ol><li>Use <code>LISTAGG</code> for <code>STUFF</code> - you need to left join the tables with LISTAGG for each STUFF statement</li><li>Remove <code>[]</code> brackets</li><li>Use <code>IFNULL</code> for <code>ISNULL</code></li><li><code>CONCAT</code> works better for appending strings compare to <code>+</code></li><li>Use <code>MONTHNAME</code> for <code>DATENAME(month,&lt;&gt;)</code>, same applies for other date cases</li><li><code>CONVERT</code> doesn&#x27;t work, you would need to write custom logic for it (sorry, no direct keyword here!)</li></ol><p>I am sure there are more rules we can list out here, but in my view, these were the only rules I needed to apply to migrate my view. If I got another opportunity to migrate some more views and find some new rules, I will definitely update this list.</p><p>If you already know some rules which are not here, you can mail me at <a href="mailto:sprsag@gmail.com" target="_blank" rel="noopener noreferrer">sprsag@gmail.com</a> or directly message in the Recohut&#x27;s slack channel.</p><p>Thanks for your attention, see you soon!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/snowflake">snowflake</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2022/12/12/the-complete-python-course-2022">The Complete Python Course (2022)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-12-12T00:00:00.000Z" itemprop="datePublished">December 12, 2022</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="https://learning.oreilly.com/covers/urn:orm:video:9781837636778/400w/" class="img_ev3q"></p><p>Python is a powerful object-oriented programming language used in many development areas and is considered a perfect language for scripting. Python is a cross-platform programming language that allows you to code faster with lesser code writing required.</p><p>The course begins with a complete introduction to the capabilities and features of Python and how to set up the language on your computer along with PyCharm IDE. You will learn about multiple programming-paradigms (object-oriented, functional, and imperative). You will explore the concepts of an interpreted language that is dynamically typed and cross-platform. The course advances to explain the concepts of OOP: variables, user input, statements, functions, classes, and objects. You will learn about functions, tuples, dictionaries, and lists in Python. You will also explore various operator modules including math, statistics, and random modules. You will work on practical examples to understand the concepts of Python programming well.</p><p>Upon completion, you will master advanced-level programming skillsets of Python and execute codes successfully. You will be able to complete your quest for learning to program using Python. Using the various built-in Python modules, you will grasp a must-know skill for data science and interpretation.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-you-will-learn">What You Will Learn<a class="hash-link" href="#what-you-will-learn" title="Direct link to heading">​</a></h2><ul><li>Learn multiple programming paradigms (OOPs and functional programs)</li><li>Learn variables, classes, objects, tuples, strings, and operators</li><li>Use dynamically typed interpreted language for lesser coding lines</li><li>Create lists, loops, functions, tokens, sets, and dictionaries</li><li>Understand cross-platform, dynamic, interpreted, and intuitive coding</li><li>Use random, math, and statistical operators to handle data</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="audience">Audience<a class="hash-link" href="#audience" title="Direct link to heading">​</a></h2><p>This course is designed for beginners in programming and those who want to master Python programming skills. Intermediate-level Python programmers who want to enhance their Python Programming Skills and students and Engineers who wish to learn Python as part of their academics. This course would also benefit professional programmers who want to switch to Python Programming from alternative coding platforms.</p><p>The course only requires the learners to have basic computer knowledge to gain from this course, and no other learning prerequisites are required.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-author">About The Author<a class="hash-link" href="#about-the-author" title="Direct link to heading">​</a></h2><p>Amit Diwan: Studyopedia was founded by Amit Diwan in 2018 after working for Tutorialspoint , IIT, IASRI, Sitepoint, DU, and C# Corner. Studyopedia sells courses on Udemy, Tutorialspoint, Geeksforgeeks, and Skillshare, providing video courses to master various technologies and programming languages, databases, frameworks, Python, data science, machine learning, Java, Android, C/C++, HTML5, Bootstrap, JavaScript, jQuery, PHP, CSS, WordPress, Drupal, Joomla, Magento, osCommerce, OpenCart, PrestaShop, and other disciplines. Studyopedia delivers high-quality video courses to millions of students and professionals enrolled through their website on multiple programming languages and technologies.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="resources">Resources<a class="hash-link" href="#resources" title="Direct link to heading">​</a></h2><ol><li><a href="https://learning.oreilly.com/videos/the-complete-python/9781837636778" target="_blank" rel="noopener noreferrer">Video</a></li><li><a href="https://github.com/PacktPublishing/The-Complete-Python-Course-2022-" target="_blank" rel="noopener noreferrer">Code</a></li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/python">python</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/course">course</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/packtpublishing">packtpublishing</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2022/11/16/opening-material">Opening our Bootcamp material</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-11-16T00:00:00.000Z" itemprop="datePublished">November 16, 2022</time> · <!-- -->One min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>For short period of time, to help laid off job seekers, We have decided to open source our data engineering bootcamp material. It&#x27;s the same website we use for students</p><p>Check it out here - <a href="https://datacamp.recohut.com/" target="_blank" rel="noopener noreferrer">https://datacamp.recohut.com/</a></p><div class="theme-admonition theme-admonition-tip alert alert--success admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_S0QG"><p>Curious how we build this docs website with minimum effort and tech behind it?</p><p>Its based on library @docusaurus which is react based static site generator </p><p>It even has a site wide elastic search built in. </p><p>(Try Searching it on website, I am sure you will be impressed)</p></div></div><p>Tag and send your <code>data engineers</code> / <code>data analyst</code> / <code>data scientist</code> friends.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/docusaurus">docusaurus</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/recohut">recohut</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/dataengineering">dataengineering</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2021/10/01/clinical-decision-making">Clinical Decision Making</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>Health insurance can be complicated—especially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America’s Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.</p><p>The objective was to identify the patterns from data to create clinical decision making in Pre-Auth and improve the accuracy in a clinical decision based on historical data analysis. </p><p>Two use cases were identified. Use Case 1 - <em>Supervised Learning Model - to aid clinicians in UM decision making. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Exploratory Data Analysis and Feature Engineering, Train supervised analytical models, model validation and model selection, Create a web service to be plugged into the case processing flow to call the model, and Display the recommendation from the model on UI on the authorization review screen. Use Case 2 - <em>Unsupervised Learning Model - to generate insights from the pre-authorization data. Tasks -</em> Ingest Pre-authorization data from Mongo DB into the analytical environment, Cluster analysis, univariate and multivariate analysis, and Generate insights and display insights on the dashboard.</p><p>Final Deliverables - Model re-training (batch mode), validation and deployment code (python scripts) with Unix command line support, Documentation - PPT, Recorded video, Technical document, Flask API backend system, HTML/PHP Web App frontend UI integration, and Plotly Dash Supervised/Unsupervised learning and insights generation dashboard.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/classification">classification</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/healthcare">healthcare</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2021/10/01/detectron-2">Detectron 2</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-1b615c35cb13ff3ee0b6a399bde1a3c7.png" width="1347" height="901" class="img_ev3q"></p><h1>Introduction</h1><p>Detectron 2 is a next-generation open-source object detection system from Facebook AI Research. With the repo you can use and train the various state-of-the-art models for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection.</p><p>The following is the directory tree of detectron 2:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">detectron2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─checkpoint  &lt;- checkpointer and model catalog handlers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─config      &lt;- default configs and handlers</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─data        &lt;- dataset handlers and data loaders</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─engine      &lt;- predictor and trainer engines</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─evaluation  &lt;- evaluator for each dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─export      &lt;- converter of detectron2 models to caffe2 (ONNX)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─layers      &lt;- custom layers e.g. deformable conv.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─model_zoo   &lt;- pre-trained model links and handler</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─modeling   </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─meta_arch &lt;- meta architecture e.g. R-CNN, RetinaNet</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─backbone  &lt;- backbone network e.g. ResNet, FPN</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  ├─proposal_generator &lt;- region proposal network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">│  └─roi_heads &lt;- head networks for pooled ROIs e.g. box, mask heads</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─solver       &lt;- optimizer and scheduler builders</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">├─structures   &lt;- structure classes e.g. Boxes, Instances, etc</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">└─utils        &lt;- utility modules e.g. visualizer, logger, etc</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>Installation</h1><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token operator" style="color:#393A34">%</span><span class="token operator" style="color:#393A34">%</span><span class="token plain">time</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">U torch</span><span class="token operator" style="color:#393A34">==</span><span class="token number" style="color:#36acaa">1.4</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">cu100 torchvision</span><span class="token operator" style="color:#393A34">==</span><span class="token number" style="color:#36acaa">0.5</span><span class="token operator" style="color:#393A34">+</span><span class="token plain">cu100 </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">f https</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">//</span><span class="token plain">download</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">pytorch</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">org</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">whl</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">torch_stable</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">html</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install cython pyyaml</span><span class="token operator" style="color:#393A34">==</span><span class="token number" style="color:#36acaa">5.1</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">U </span><span class="token string" style="color:#e3116c">&#x27;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&#x27;</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install detectron2 </span><span class="token operator" style="color:#393A34">-</span><span class="token plain">f https</span><span class="token punctuation" style="color:#393A34">:</span><span class="token operator" style="color:#393A34">//</span><span class="token plain">dl</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">fbaipublicfiles</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">com</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">detectron2</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">wheels</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">cu100</span><span class="token operator" style="color:#393A34">/</span><span class="token plain">index</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">html</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2 </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> model_zoo</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">engine </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> DefaultPredictor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">config </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> get_cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">utils</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">visualizer </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> Visualizer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">from</span><span class="token plain"> detectron2</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">data </span><span class="token keyword" style="color:#00009f">import</span><span class="token plain"> MetadataCatalog</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>Inference on pre-trained models</h1><p><img loading="lazy" alt="Original image" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-1-f67813931f498e5451c89d34fc53c18a.png" width="620" height="479" class="img_ev3q"></p><p>Original image</p><p><img loading="lazy" alt="Object detection with Faster-RCNN-101" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-2-2d53fa263a2f78ba53e25191a5174f6a.png" width="744" height="574" class="img_ev3q"></p><p>Object detection with Faster-RCNN-101</p><p><img loading="lazy" alt="Instance segmentation with Mask-RCNN-50" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-3-843f187808ce30f5f39ee16632bcc07e.png" width="744" height="574" class="img_ev3q"></p><p>Instance segmentation with Mask-RCNN-50</p><p><img loading="lazy" alt="Keypoint estimation with Keypoint-RCNN-50" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-4-dc55d3ff169a8b338c25147d005a7357.png" width="744" height="574" class="img_ev3q"></p><p>Keypoint estimation with Keypoint-RCNN-50</p><p><img loading="lazy" alt="Panoptic segmentation with Panoptic-FPN-101" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-5-400fd835e75e14ba61644847b378b47f.png" width="744" height="574" class="img_ev3q"></p><p>Panoptic segmentation with Panoptic-FPN-101</p><p><img loading="lazy" alt="Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-6-9497a38e9bed29af766a327244331cee.png" width="744" height="1148" class="img_ev3q"></p><p>Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison</p><h1>Fine-tuning Balloons Dataset</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-data">Load the data<a class="hash-link" href="#load-the-data" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># download, decompress the data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!unzip balloon_dataset.zip &gt; /dev/null</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="convert-dataset-into-detectron2s-standard-format">Convert dataset into Detectron2&#x27;s standard format<a class="hash-link" href="#convert-dataset-into-detectron2s-standard-format" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.structures import BoxMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># write a function that loads the dataset into detectron2&#x27;s standard format</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def get_balloon_dicts(img_dir):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    json_file = os.path.join(img_dir, &quot;via_region_data.json&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    with open(json_file) as f:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        imgs_anns = json.load(f)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dataset_dicts = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for _, v in imgs_anns.items():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        filename = os.path.join(img_dir, v[&quot;filename&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        height, width = cv2.imread(filename).shape[:2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;file_name&quot;] = filename</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;height&quot;] = height</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;width&quot;] = width</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        annos = v[&quot;regions&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        objs = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for _, anno in annos.items():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            assert not anno[&quot;region_attributes&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            anno = anno[&quot;shape_attributes&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            px = anno[&quot;all_points_x&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            py = anno[&quot;all_points_y&quot;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            poly = list(itertools.chain.from_iterable(poly))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            obj = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;bbox&quot;: [np.min(px), np.min(py), np.max(px), np.max(py)],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;bbox_mode&quot;: BoxMode.XYXY_ABS,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;segmentation&quot;: [poly],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;category_id&quot;: 0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &quot;iscrowd&quot;: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            objs.append(obj)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;annotations&quot;] = objs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dataset_dicts.append(record)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return dataset_dicts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.data import DatasetCatalog, MetadataCatalog</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in [&quot;train&quot;, &quot;val&quot;]:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    DatasetCatalog.register(&quot;balloon/&quot; + d, lambda d=d: get_balloon_dicts(&quot;balloon/&quot; + d))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    MetadataCatalog.get(&quot;balloon/&quot; + d).set(thing_classes=[&quot;balloon&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">balloon_metadata = MetadataCatalog.get(&quot;balloon/train&quot;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-configuration-and-training">Model configuration and training<a class="hash-link" href="#model-configuration-and-training" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.engine import DefaultTrainer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.config import get_cfg</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg = get_cfg()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TRAIN = (&quot;balloon/train&quot;,)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATALOADER.NUM_WORKERS = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.IMS_PER_BATCH = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.BASE_LR = 0.00025</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer = DefaultTrainer(cfg) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.resume_or_load(resume=False)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.train()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="inference-and-visualization">Inference and Visualization<a class="hash-link" href="#inference-and-visualization" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.utils.visualizer import ColorMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># load weights</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Set training data-set path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = (&quot;balloon/val&quot;, )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create predictor (model for inference)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset_dicts = get_balloon_dicts(&quot;balloon/val&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in random.sample(dataset_dicts, 3):    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    im = cv2.imread(d[&quot;file_name&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputs = predictor(im)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = Visualizer(im[:, :, ::-1],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   metadata=balloon_metadata, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   scale=0.8, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cv2_imshow(v.get_image()[:, :, ::-1])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-7.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-7-2e7f9a8cba8cc49ee8ab648cf18bc33b.png" width="819" height="540" class="img_ev3q"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-8.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-8-9d5d0a41fa5a618be93561523b131930.png" width="819" height="614" class="img_ev3q"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-9.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-9-67a9478fd72c9f64dc0f058a5395edb8.png" width="548" height="819" class="img_ev3q"></p><h1>Fine-tuning Chip Dataset</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="load-the-data-1">Load the data<a class="hash-link" href="#load-the-data-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">#get the dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install -q kaggle</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!pip install -q kaggle-cli</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.environ[&#x27;KAGGLE_USERNAME&#x27;] = &quot;sparshag&quot; </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.environ[&#x27;KAGGLE_KEY&#x27;] = &quot;1b1f894d1fa6febe9676681b44ad807b&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!kaggle datasets download -d tannergi/microcontroller-detection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">!unzip microcontroller-detection.zip</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="convert-dataset-into-detectron2s-standard-format-1">Convert dataset into Detectron2&#x27;s standard format<a class="hash-link" href="#convert-dataset-into-detectron2s-standard-format-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Registering the dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from detectron2.structures import BoxMode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def get_microcontroller_dicts(csv_file, img_dir):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df = pd.read_csv(csv_file)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df[&#x27;filename&#x27;] = df[&#x27;filename&#x27;].map(lambda x: img_dir+x)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    classes = [&#x27;Raspberry_Pi_3&#x27;, &#x27;Arduino_Nano&#x27;, &#x27;ESP8266&#x27;, &#x27;Heltec_ESP32_Lora&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    df[&#x27;class_int&#x27;] = df[&#x27;class&#x27;].map(lambda x: classes.index(x))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    dataset_dicts = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    for filename in df[&#x27;filename&#x27;].unique().tolist():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        height, width = cv2.imread(filename).shape[:2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;file_name&quot;] = filename</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;height&quot;] = height</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;width&quot;] = width</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        objs = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for index, row in df[(df[&#x27;filename&#x27;]==filename)].iterrows():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          obj= {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &#x27;bbox&#x27;: [row[&#x27;xmin&#x27;], row[&#x27;ymin&#x27;], row[&#x27;xmax&#x27;], row[&#x27;ymax&#x27;]],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &#x27;bbox_mode&#x27;: BoxMode.XYXY_ABS,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &#x27;category_id&#x27;: row[&#x27;class_int&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">              &quot;iscrowd&quot;: 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">          objs.append(obj)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        record[&quot;annotations&quot;] = objs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dataset_dicts.append(record)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    return dataset_dicts</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">classes = [&#x27;Raspberry_Pi_3&#x27;, &#x27;Arduino_Nano&#x27;, &#x27;ESP8266&#x27;, &#x27;Heltec_ESP32_Lora&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in [&quot;train&quot;, &quot;test&quot;]:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  DatasetCatalog.register(&#x27;microcontroller/&#x27; + d, lambda d=d: get_microcontroller_dicts(&#x27;Microcontroller Detection/&#x27; + d + &#x27;_labels.csv&#x27;, &#x27;Microcontroller Detection/&#x27; + d+&#x27;/&#x27;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  MetadataCatalog.get(&#x27;microcontroller/&#x27; + d).set(thing_classes=classes)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">microcontroller_metadata = MetadataCatalog.get(&#x27;microcontroller/train&#x27;)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-configuration-and-training-1">Model configuration and training<a class="hash-link" href="#model-configuration-and-training-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Train the model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg = get_cfg()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TRAIN = (&#x27;microcontroller/train&#x27;,)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATALOADER.NUM_WORKERS = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.IMS_PER_BATCH = 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.SOLVER.MAX_ITER = 1000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer = DefaultTrainer(cfg) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.resume_or_load(resume=False)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">trainer.train()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-10.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-10-8d5237190905a0e4fadc0b0ed691aef3.png" width="765" height="578" class="img_ev3q"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-11.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-11-4da902dcc9cb45b630b4918e12ffd40b.png" width="640" height="480" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="inference-and-visualization-1">Inference and Visualization<a class="hash-link" href="#inference-and-visualization-1" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.DATASETS.TEST = (&#x27;microcontroller/test&#x27;, )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">df_test = pd.read_csv(&#x27;Microcontroller Detection/test_labels.csv&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">dataset_dicts = DatasetCatalog.get(&#x27;microcontroller/test&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">for d in random.sample(dataset_dicts, 3):    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    im = cv2.imread(d[&quot;file_name&quot;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    outputs = predictor(im)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = Visualizer(im[:, :, ::-1], </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   metadata=microcontroller_metadata, </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   scale=0.8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    cv2_imshow(v.get_image()[:, :, ::-1])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="real-time-webcam-inference">Real-time Webcam inference<a class="hash-link" href="#real-time-webcam-inference" title="Direct link to heading">​</a></h3><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">from IPython.display import display, Javascript</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from google.colab.output import eval_js</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from base64 import b64decode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def take_photo(filename=&#x27;photo.jpg&#x27;, quality=0.8):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  js = Javascript(&#x27;&#x27;&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    async function takePhoto(quality) {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const div = document.createElement(&#x27;div&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const capture = document.createElement(&#x27;button&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      capture.textContent = &#x27;Capture&#x27;;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      div.appendChild(capture);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const video = document.createElement(&#x27;video&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      video.style.display = &#x27;block&#x27;;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const stream = await navigator.mediaDevices.getUserMedia({video: true});</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      document.body.appendChild(div);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      div.appendChild(video);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      video.srcObject = stream;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      await video.play();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      // Resize the output to fit the video element.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      // Wait for Capture to be clicked.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      await new Promise((resolve) =&gt; capture.onclick = resolve);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      const canvas = document.createElement(&#x27;canvas&#x27;);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      canvas.width = video.videoWidth;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      canvas.height = video.videoHeight;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      canvas.getContext(&#x27;2d&#x27;).drawImage(video, 0, 0);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      stream.getVideoTracks()[0].stop();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      div.remove();</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      return canvas.toDataURL(&#x27;image/jpeg&#x27;, quality);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    &#x27;&#x27;&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  display(js)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  data = eval_js(&#x27;takePhoto({})&#x27;.format(quality))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  binary = b64decode(data.split(&#x27;,&#x27;)[1])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  with open(filename, &#x27;wb&#x27;) as f:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    f.write(binary)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  return filename</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">from IPython.display import Image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">try:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  filename = take_photo()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  print(&#x27;Saved to {}&#x27;.format(filename))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Show the image which was just taken.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  display(Image(filename))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">except Exception as err:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # Errors will be thrown if the user does not have a webcam or if they do not</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  # grant the page permission to access it.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  print(str(err))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">model_path = &#x27;/content/output/model_final.pth&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">config_path= model_zoo.get_config_file(&quot;COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Create config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg = get_cfg()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.merge_from_file(config_path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cfg.MODEL.WEIGHTS = model_path</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">predictor = DefaultPredictor(cfg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">im = cv2.imread(&#x27;photo.jpg&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">outputs = predictor(im)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">v = v.draw_instance_predictions(outputs[&quot;instances&quot;].to(&quot;cpu&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cv2_imshow(v.get_image()[:, :, ::-1])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>Fine-tuning on Face dataset</h1><p>The process is same. Here is the output.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-12.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-12-8624038661cf0ab9a13ffd35bcea2092.png" width="1353" height="550" class="img_ev3q"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-13.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-13-9ef0b07eeb23c36fea216ff6d8776168.png" width="700" height="400" class="img_ev3q"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-14.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-14-4eb13d5f04434cff475a78207b96071c.png" width="700" height="400" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="behind-the-scenes">Behind the scenes<a class="hash-link" href="#behind-the-scenes" title="Direct link to heading">​</a></h3><p><img loading="lazy" alt="/img/content-blog-raw-blog-detectron-2-untitled-15.png" src="/assets/images/content-blog-raw-blog-detectron-2-untitled-15-3bcd41db9aeb9c74ee059c1848fbc157.png" width="1071" height="563" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h3><ul><li><a href="https://medium.com/deepvisionguru/how-to-embed-detectron2-in-your-computer-vision-project-817f29149461" target="_blank" rel="noopener noreferrer">How to embed Detectron2 in your computer vision project - blogpost</a></li><li><a href="https://gilberttanner.com/blog/detectron2-train-a-instance-segmentation-model" target="_blank" rel="noopener noreferrer">Detectron2 Train a Instance Segmentation Model by Gilbert Tanner</a></li><li><a href="https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/" target="_blank" rel="noopener noreferrer">How to train Detectron2 with Custom COCO Datasets - DLology</a></li><li><a href="https://towardsdatascience.com/character-recognition-and-segmentation-for-custom-data-using-detectron2-599de82b393c" target="_blank" rel="noopener noreferrer">Character Recognition and Segmentation For Custom Data Using Detectron2 - blogpost</a></li><li><a href="https://www.celantur.com/blog/panoptic-segmentation-in-detectron2/" target="_blank" rel="noopener noreferrer">Training models with Panoptic Segmentation in Detectron2</a></li><li><a href="https://www.kaggle.com/lewisgmorris/image-segmentation-using-detectron2" target="_blank" rel="noopener noreferrer">Image segmentation using Detectron2 - Kaggle</a></li><li><a href="https://towardsdatascience.com/a-beginners-guide-to-object-detection-and-computer-vision-with-facebook-s-detectron2-700b6273390e" target="_blank" rel="noopener noreferrer">A Beginner’s Guide To Object Detection And Computer Vision With Facebook’s Detectron2</a></li><li><a href="https://www.curiousily.com/posts/face-detection-on-custom-dataset-with-detectron2-in-python/" target="_blank" rel="noopener noreferrer">Face Detection on Custom Dataset with Detectron2 and PyTorch using Python</a></li><li><a href="https://www.notion.so/Detectron-2-d31ac9c14a8d4d9888882df14a4e0eee" target="_blank" rel="noopener noreferrer">My Experiment Notion</a></li><li><a href="https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5" target="_blank" rel="noopener noreferrer">Official Colab</a></li><li><a href="https://research.fb.com/wp-content/uploads/2019/12/4.-detectron2.pdf" target="_blank" rel="noopener noreferrer">Official Slide</a></li><li><a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener noreferrer">Official Git</a></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/tool">tool</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/vision">vision</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2021/10/01/distributed-training-of-recommender-systems">Distributed Training of Recommender Systems</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p>The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.</p><p>Recently, Facebook shared its approach to maintain its 12 trillion parameter recommender. Building these large systems is challenging because it requires huge computation and memory resources. And we will soon enter into 100 trillion range. And SMEs will not be left behind due to open-source environment of software architectures and the decreasing cost of hardware, especially on the cloud infrastructure.</p><p>As per one estimate, a model with 100 trillion parameters would require at least 200TB just to store the model, even at 16-bit floating-point accuracy. So we need architectures that can support efficient and distributed training of recommendation models.</p><p><strong><em>Memory-intensive vs Computation-intensive</em></strong>: The increasing parameter comes mostly from the embedding layer which maps each entrance of an ID type feature (such as an user ID and a session ID) into a fixed length low-dimensional embedding vector. Consider the billion scale of entrances for the ID type features in a production recommender system and the wide utilization of feature crosses, the embedding layer usually domains the parameter space, which makes this component extremely <strong>memory-intensive</strong>. On the other hand, these low-dimensional embedding vectors are concatenated with diversified Non-ID type features (e.g., image, audio, video, social network, etc.) to feed a group of increasingly sophisticated neural networks (e.g., convolution, LSTM, multi-head attention) for prediction(s). Furthermore, in practice, multiple objectives can also be combined and optimized simultaneously for multiple tasks. These mechanisms make the rest neural network increasingly <strong>computation-intensive</strong>.</p><p><img loading="lazy" alt="An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network." src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-76057748d7f785bcb03bc9fae4560fc3.png" width="881" height="562" class="img_ev3q"></p><p>An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network.</p><p><a href="https://github.com/alibaba/x-deeplearning" target="_blank" rel="noopener noreferrer">Alibaba&#x27;s XDL</a>, <a href="https://github.com/PaddlePaddle/PaddleRec" target="_blank" rel="noopener noreferrer">Baidu&#x27;s PaddleRec</a>, and <a href="https://github.com/persiaml/persia" target="_blank" rel="noopener noreferrer">Kwai&#x27;s Persia</a> are some open-source frameworks for this large-scale distributed training of recommender systems.</p><aside>📌 ***Synchronous vs Asynchronous Algorithms***: Synchronous algorithms always use the up-to-date gradient to update the model to ensure the model accuracy. However, the overhead of communications for synchronous algorithms starts to become too expensive to scale out the training procedure, causing inefficiency in running time. While asynchronous algorithm have better hardware efficiency, it often leads to a “significant” loss in model accuracy at this scale—for production recommender systems (e.g., Baidu’s search engine). Recall that even 0.1% drop of accuracy would lead to a noticeable loss in revenue.</aside><h3 class="anchor anchorWithStickyNavbar_LWe7" id="parameter-server-framework">Parameter Server Framework<a class="hash-link" href="#parameter-server-framework" title="Direct link to heading">​</a></h3><p>Existing distributed systems for deep learning based recommender models are usually built on top of the parameter server (PS) framework, where one can add elastic distributed storage to hold the increasingly large amount of parameters of the embedding layer. On the other hand, the computation workload does not scale linearly with the increasing parameter scale of the embedding layer—in fact, with an efficient implementation, a lookup operation over a larger embedding table would introduce almost no additional computations.</p><p><img loading="lazy" alt="Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. [Source](https://arxiv.org/pdf/2111.05897v1.pdf)." src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-1-64afd6c4cb479b89e18f624461bb9641.png" width="1170" height="391" class="img_ev3q"></p><p>Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. <a href="https://arxiv.org/pdf/2111.05897v1.pdf" target="_blank" rel="noopener noreferrer">Source</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="persia">PERSIA<a class="hash-link" href="#persia" title="Direct link to heading">​</a></h3><p><strong>PERSIA</strong> (<strong>P</strong>arallel r<strong>E</strong>commendation t<strong>R</strong>aining <strong>S</strong>ystem with hybr<strong>I</strong>d <strong>A</strong>cceleration) is a PyTorch-based system for training deep learning recommendation models on commodity hardware. It supports models containing more than 100 trillion parameters.</p><p>It uses a hybrid training algorithm to tackle the embedding layer and dense neural network modules differently—the embedding layer is trained in an asynchronous fashion to improve the throughput of training samples, while the rest neural network is trained in a synchronous fashion to preserve the statistical efficiency.</p><p>It also uses a distributed system to manage the hybrid computation resources (CPUs and GPUs) to optimize the co-existence of asynchronicity and synchronicity in the training algorithm.</p><p><img loading="lazy" alt="Untitled" src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-2-f8f92456dbc99598ab43bdb238450ac0.png" width="588" height="509" class="img_ev3q"></p><p><img loading="lazy" alt="Untitled" src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-3-c4a1ad9cdcc4ab78213fff42a2bf2d18.png" width="590" height="428" class="img_ev3q"></p><p>Persia includes a data loader module, a embedding PS (Parameter Server) module, a group of embedding workers over CPU nodes, and a group of NN workers over GPU instances. Each module can be dynamically scaled for different model scales and desired training throughput:</p><ul><li>A data loader that fetches training data from distributed storages such as Hadoop, Kafka, etc;</li><li>A embedding parameter server (embedding PS for short) manages the storage and update of the parameters in the embedding layer $\mathrm{w}^{emb}$;</li><li>A group of embedding workers that runs Algorithm 1 for getting the embedding parameters from the embedding PS; aggregating embedding vectors (potentially) and putting embedding gradients back to embedding PS;</li><li>A group of NN workers that runs the forward-/backward- propagation of the neural network $\mathrm{NN_{w^{nn}}(·)}$.</li></ul><p><img loading="lazy" alt="The architecture of Persia." src="/assets/images/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-4-717546b660d1b2fcaff856bf274d18e2.png" width="874" height="563" class="img_ev3q"></p><p>The architecture of Persia.</p><p>Logically, the training procedure is conducted by Persia in a data dispatching based paradigm as below:</p><ol><li>The data loader will dispatch the ID type feature $\mathrm{x^{ID}}$ to an embedding worker—the embedding worker will generate an unique sample ID 𝜉 for this sample, buffer this sample ID with the ID type feature $\mathrm{x_\xi^{ID}}$ locally, and returns this ID 𝜉 back the data loader; the data loader will associate this sample’s Non-ID type features and labels with this unique ID.</li><li>Next, the data loader will dispatch the Non-ID type feature and label(s) $\mathrm{(x<em>\xi^{NID},y</em>\xi)}$ to a NN worker.</li><li>Once a NN worker receives this incomplete training sample, it will issue a request to pull the ID type features’ $\mathrm{(x<em>\xi^{ID})}$ embedding $\mathrm{w</em>\xi^{emb}}$ from some embedding worker according to the sample ID 𝜉—this would trigger the forward propagation in Algorithm 1, where the embedding worker will use the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ to get the corresponding $\mathrm{w</em>\xi^{emb}}$ from the embedding PS.</li><li>Then the embedding worker performs some potential aggregation of original embedding vectors. When this computation finishes, the aggregated embedding vector $\mathrm{w_\xi^{emb}}$ will be transmitted to the NN worker that issues the pull request.</li><li>Once the NN worker gets a group of complete inputs for the dense module, it will create a mini-batch and conduct the training computation of the NN according to Algorithm 2. Note that the parameter of the NN always locates in the device RAM of the NN worker, where the NN workers synchronize the gradients by the AllReduce Paradigm.</li><li>When the iteration of Algorithm 2 is finished, the NN worker will send the gradients of the embedding ($\mathrm{F_\xi^{emb&#x27;}}$) back to the embedding worker (also along with the sample ID 𝜉).</li><li>The embedding worker will query the buffered ID type feature $\mathrm{x<em>\xi^{ID}}$ according to the sample ID 𝜉; compute gradients $\mathrm{F</em>\xi^{emb&#x27;}}$ of the embedding parameters and send the gradients to the embedding PS, so that the embedding PS can finally compute the updates according the embedding parameter’s gradients by its SGD optimizer and update the embedding parameters.</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/distributed">distributed</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/recsys">recsys</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2021/10/01/document-recommendation">Document Recommendation</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->2 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-document-recommendation-untitled.png" src="/assets/images/content-blog-raw-blog-document-recommendation-untitled-ccbcca01bf06db66ebeebcfe4da46778.png" width="1720" height="900" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction"><strong>Introduction</strong><a class="hash-link" href="#introduction" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="business-objective">Business objective<a class="hash-link" href="#business-objective" title="Direct link to heading">​</a></h3><p>For the given user query, recommend relevant documents (BRM_ifam)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="technical-objective">Technical objective<a class="hash-link" href="#technical-objective" title="Direct link to heading">​</a></h3><p>1-to-N mapping of given input text</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="proposed-framework-1--hybrid-recommender-system"><strong>Proposed Framework 1 — Hybrid Recommender System</strong><a class="hash-link" href="#proposed-framework-1--hybrid-recommender-system" title="Direct link to heading">​</a></h2><ul><li>Text → Vector (Universal Sentence Embedding with TF Hub)</li><li>Vector → Content-based Filtering Recommendation</li><li>Index → Interaction Matrix</li><li>Interaction Matrix → Collaborative Filtering Recommendation</li><li>Collaborative + Content-based → Hybrid Recommendation</li><li>Evaluation: Area-under-curve</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="proposed-framework-2--content-based-recommender-system"><strong>Proposed Framework 2 — Content-based Recommender System</strong><a class="hash-link" href="#proposed-framework-2--content-based-recommender-system" title="Direct link to heading">​</a></h2><ol><li>Find A most similar user → Cosine similarity</li><li>For each user in A, find TopK Most Similar Items → Map Argsort</li><li>For each item Find TopL Most Similar Items → Cosine similarity</li><li>Display</li><li>Implement an evaluation metric</li><li>Evaluate</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="results-and-discussion"><strong>Results and Discussion</strong><a class="hash-link" href="#results-and-discussion" title="Direct link to heading">​</a></h2><ul><li>build.py → this script will take the training data as input and save all the required files in the same working directory</li><li>recommend.py → this script will take the user query as input and predict top-K BRM recommendations</li></ul><p>Variables (during recommendation, you will be asked 2–3 choices, the meaning of those choices are as following)</p><ul><li>top-K — how many top items you want to get in recommendation</li><li>secondary items: this will determine how many similar items you would like to add in consideration, for each primary matching item</li><li>sorted by frequency: since multiple input queries might point to same output, therefore this option allows to take that frequence count of outputs in consideration and will move the more frequent items at the top.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="code"><strong>Code</strong><a class="hash-link" href="#code" title="Direct link to heading">​</a></h3><p><a href="https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb" target="_blank" rel="noopener noreferrer">https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/nlp">nlp</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/similarity">similarity</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2021/10/01/fake-voice-detection">Fake Voice Detection</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->3 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-fake-voice-detection-untitled.png" src="/assets/images/content-blog-raw-blog-fake-voice-detection-untitled-f11b5243c4377e8265e1a521da103c79.png" width="1920" height="1080" class="img_ev3q"></p><h1>Introduction</h1><p>Fake audio can be used for malicious purposes which affect directly or indirectly human life. The objective is to differentiate between fake and real voice. Python and deep learning has been used and implemented to achieve the objective. Audio files or video file are being used as an input of this work then model has been trained for uniquely identify features for voice creation and voice detection. Deep learning technique is used to find accuracy between real and fake.</p><p>Speaker recognition usually refers to both speaker identification and speaker verification. A speaker identification system identifies who the speaker is, while an automatic speaker verification (ASV) system decides if an identity claim is true or false.
A general ASV system is robust to zero-effort impostors, they are vulnerable to more sophisticated attacks. Such vulnerability represents one of the security concerns of ASV systems. Spoofing involves an adversary (attacker) who masquerades as the target speaker to gain the access to a system. Such spoofing attacks can happen to various biometric traits, such as fingerprints, iris, face, and voice patterns. We are focusing only on the voice-based spoofing and anti-spoofing techniques for ASV system. The spoofed speech samples can be obtained through speech synthesis, voice conversion, or replay of recorded speech. <strong>Imagine the following scenario…</strong>
Your phone rings, you pick up. It’s your spouse asking you for details about your savings account — they don’t have the account information on hand, but want to deposit money there this afternoon. Later, you realize a bunch of money has went missing! After investigating, you find out that the person masquerading as them on the other line was a voice 100% generated with AI. You’ve just been scammed, and on top of that, can’t believe the voice you thought belonged to your spouse was actually a fake.</p><p>To discern between real and fake audio, the detector uses visual representations of audio clips called spectrograms, which are also used to train speech synthesis models.
Google’s 2019 <a href="https://www.blog.google/outreach-initiatives/google-news-initiative/advancing-research-fake-audio-detection/" target="_blank" rel="noopener noreferrer">AVSSpoof dataset</a> contains over 25,000 clips of audio, featuring both real and fake clips of a variety of male and female speakers.<strong>Temporal Convolution Model</strong></p><h1>Modeling Approach</h1><p>First, raw audio is preprocessed and converted into a mel-frequency spectrogram — this is the input for the model. The model performs convolutions over the time dimension of the spectrogram, then uses masked pooling to prevent overfitting. Finally, the output is passed into a dense layer and a sigmoid activation function, which ultimately outputs a predicted probability between 0 (fake) and 1 (real).
The baseline model achieved 99%, 95%, and 85% accuracy on the train, validation, and test sets respectively. The differing performance is caused by differences between the three datasets. While all three datasets feature distinct and different speakers, the test set uses a different set of fake audio generating algorithms that were not present in the train or validation set.</p><h1>Proposed Framework</h1><h1>Process Flow</h1><ul><li>Voice detection<ul><li>Temporal Convolution model<ul><li>Install packages</li><li>Download pretrained models</li><li>Initialize the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>GMM-UBG model<ul><li>Install packages</li><li>Train the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>Convolutional VAE model<ul><li>Install packages</li><li>Train the model</li><li>Load data</li><li>Detect DeepFakes</li></ul></li><li>Voice Similarity<ul><li>Install packages</li><li>Load data</li><li>Voice similarity match</li><li>Embedding visualization</li></ul></li></ul></li></ul><h1>Models Algorithms</h1><ol><li>Temporal Convolution</li><li>ResNet</li><li>GMM</li><li>Light CNN</li><li>Fusion</li><li>SincNet</li><li>ASSERT</li><li>HOSA</li><li>CVAE</li></ol></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/audio">audio</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/deepfake">deepfake</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2021/10/01/image-similarity-system">Image Similarity System</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/sparsh-ai.png" alt="Sparsh Agarwal"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Data Scientist &amp; Engineer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-image-similarity-system-untitled.png" src="/assets/images/content-blog-raw-blog-image-similarity-system-untitled-59c540a72e9f8afbece0d5a9bd41f513.png" width="1289" height="545" class="img_ev3q"></p><h1>Choice of variables</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-encoder">Image Encoder<a class="hash-link" href="#image-encoder" title="Direct link to heading">​</a></h3><p>We can select any pre-trained image classification model. These models are commonly known as encoders because their job is to encode an image into a feature vector. I analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) <a href="https://tfhub.dev/google/bit/m-r152x4/1" target="_blank" rel="noopener noreferrer">BiT</a>. After basic research, I decided to select BiT model because of its performance and state-of-the-art nature. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page <a href="https://tfhub.dev/google/bit/m-r50x3/1" target="_blank" rel="noopener noreferrer">here</a>. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="vector-similarity-system">Vector Similarity System<a class="hash-link" href="#vector-similarity-system" title="Direct link to heading">​</a></h3><p>Images are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. I explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. I selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page <a href="https://github.com/spotify/annoy" target="_blank" rel="noopener noreferrer">here</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dataset">Dataset<a class="hash-link" href="#dataset" title="Direct link to heading">​</a></h3><p>I listed down 3 datasets from Kaggle that were best fitting the criteria of this use case: 1) <a href="https://www.kaggle.com/bhaskar2443053/fashion-small?" target="_blank" rel="noopener noreferrer">Fashion Product Images (Small)</a>, 2) <a href="https://www.kaggle.com/trolukovich/food11-image-dataset?" target="_blank" rel="noopener noreferrer">Food-11 image dataset</a> and 3) <a href="https://www.kaggle.com/jessicali9530/caltech256?" target="_blank" rel="noopener noreferrer">Caltech 256 Image Dataset</a>. I selected Fashion dataset and Foods dataset.</p><h1>Literature review</h1><ul><li>Determining Image similarity with Quasi-Euclidean Metric <a href="https://arxiv.org/abs/2006.14644v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>CatSIM: A Categorical Image Similarity Metric <a href="https://arxiv.org/abs/2004.09073v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Central Similarity Quantization for Efficient Image and Video Retrieval <a href="https://arxiv.org/abs/1908.00347v5" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval <a href="https://arxiv.org/abs/1803.02987v3" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Model-based Behavioral Cloning with Future Image Similarity Learning <a href="https://arxiv.org/abs/1910.03157v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Why do These Match? Explaining the Behavior of Image Similarity Models <a href="https://arxiv.org/abs/1905.10797v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Learning Non-Metric Visual Similarity for Image Retrieval <a href="https://arxiv.org/abs/1709.01353v2" target="_blank" rel="noopener noreferrer">arxiv</a></li></ul><h1>Process Flow</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-1-data-acquisition">Step 1: Data Acquisition<a class="hash-link" href="#step-1-data-acquisition" title="Direct link to heading">​</a></h3><p>Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-2-encoder-fine-tuning">Step 2: Encoder Fine-tuning<a class="hash-link" href="#step-2-encoder-fine-tuning" title="Direct link to heading">​</a></h3><p>Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.</p><p><img loading="lazy" alt="Fig: a screenshot of encoder fine-tuning process" src="/assets/images/content-blog-raw-blog-image-similarity-system-untitled-1-66cd649b6c0e03c173f3e0734b2b3312.png" width="1195" height="333" class="img_ev3q"></p><p>Fig: a screenshot of encoder fine-tuning process</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-3-image-vectorization">Step 3: Image Vectorization<a class="hash-link" href="#step-3-image-vectorization" title="Direct link to heading">​</a></h3><p>Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-4-metadata-and-indexing">Step 4: Metadata and Indexing<a class="hash-link" href="#step-4-metadata-and-indexing" title="Direct link to heading">​</a></h3><p>We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index object for later use.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="step-5-api-call">Step 5: API Call<a class="hash-link" href="#step-5-api-call" title="Direct link to heading">​</a></h3><p>We will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.</p><h1>Deployment</h1><p>The API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-image-similarity-system-untitled-2.png" src="/assets/images/content-blog-raw-blog-image-similarity-system-untitled-2-ca3d98690fca750590d55d9899c4d862.png" width="1883" height="593" class="img_ev3q"></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/aws-beanstalk">aws beanstalk</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/flask">flask</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/similarity">similarity</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/vision">vision</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/page/2"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Bootcamp. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.251db5a0.js"></script>
<script src="/assets/js/main.1462881d.js"></script>
</body>
</html>